public class ZScoreUDF extends UDF {
public class ZScoreUDF extends UDF {
float bias = 0.f;
float bias = 0.f;
float biasValue = 0.f;
float bias = 0.f;
float bias = 0.f;
float biasValue = 0.f;
String rawArgs = ((WritableConstantStringObjectInspector) argOIs[2]).getWritableConstantValue().toString();
String rawArgs = ((WritableConstantStringObjectInspector) argOIs[2]).getWritableConstantValue().toString();
String rawArgs = ((WritableConstantStringObjectInspector) argOIs[2]).getWritableConstantValue().toString();
String rawArgs = ((WritableConstantStringObjectInspector) argOIs[2]).getWritableConstantValue().toString();
String rawArgs = ((WritableConstantStringObjectInspector) argOIs[2]).getWritableConstantValue().toString();
String rawArgs = ((WritableConstantStringObjectInspector) argOIs[2]).getWritableConstantValue().toString();
protected ObjectInspector featureInputOI;
this.featureInputOI = processFeaturesOI(argOIs[0]);
ObjectInspector featureOutputOI = featureInputOI;
featureOutputOI = PrimitiveObjectInspectorFactory.javaIntObjectInspector;
this.biasKey = (featureOutputOI.getTypeName() == Constants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT
ObjectInspector featureOI = ObjectInspectorUtils.getStandardObjectInspector(featureOutputOI);
final ObjectInspector featureInspector = this.featureInputOI;
final ObjectInspector featureInspector = this.featureInputOI;
final ObjectInspector featureInspector = this.featureInputOI;
protected ObjectInspector featureInputOI;
this.featureInputOI = processFeaturesOI(argOIs[0]);
ObjectInspector featureOutputOI = featureInputOI;
featureOutputOI = PrimitiveObjectInspectorFactory.javaIntObjectInspector;
this.biasKey = (featureOutputOI.getTypeName() == Constants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT
ObjectInspector featureOI = ObjectInspectorUtils.getStandardObjectInspector(featureOutputOI);
final ObjectInspector featureInspector = this.featureInputOI;
final ObjectInspector featureInspector = this.featureInputOI;
final ObjectInspector featureInspector = this.featureInputOI;
public String evaluate(String s, double min, double max) {
return evaluate(s, (float) min, (float) max);
}
public float evaluate(float value, double min, double max) {
return min_max_normalization(value, (float) min, (float) max);
}
public float evaluate(float value, double mean, double stddev) {
return evaluate(value, (float) mean, (float) stddev);
}
public String evaluate(String s, double min, double max) {
return evaluate(s, (float) min, (float) max);
}
public float evaluate(float value, double min, double max) {
return min_max_normalization(value, (float) min, (float) max);
}
public float evaluate(float value, double mean, double stddev) {
return evaluate(value, (float) mean, (float) stddev);
}
public boolean forBinaryClassification();
public boolean forRegression();
public static abstract class BinaryLoss implements LossFunction {
protected void checkTarget(float y) {
if(y == 1.f || y == -1.f) {
}
}
@Override
public boolean forBinaryClassification() {
return true;
}
@Override
public boolean forRegression() {
return false;
}
}
public static abstract class RegressionLoss implements LossFunction {
@Override
public boolean forBinaryClassification() {
return false;
}
@Override
public boolean forRegression() {
return true;
}
}
public static final class SquaredLoss extends RegressionLoss {
return ((p - y) * (p - y)) / 2.f;
public static final class LogLoss extends BinaryLoss {
checkTarget(y);
float z = y * p;
if(z > 18.f) {
return (float) Math.exp(-z);
}
if(z < -18.f) {
return -z;
}
checkTarget(y);
float z = y * p;
if(z > 18.f) {
return (float) Math.exp(-z) * -y;
if(z < -18.f) {
return -y;
}
public static final class HingeLoss extends BinaryLoss {
checkTarget(y);
checkTarget(y);
return (loss > 0.f) ? -y : 0.f;
assert (y == -1.f || y == 1.f) : y;
public static final class SquaredHingeLoss extends BinaryLoss {
checkTarget(y);
float z = y * p;
float d = 1.f - z;
return (d > 0.f) ? (d * d) : 0.f;
checkTarget(y);
float d = 1 - (y * p);
return (d > 0.f) ? -2.f * d * y : 0.f;
public static final class QuantileLoss extends RegressionLoss {
this.tau = 0.5f;
setTau(tau);
if(tau <= 0 || tau >= 1.0) {
}
float e = y - p;
if(e > 0.f) {
return tau * e;
return -(1.f - tau) * e;
float e = y - p;
if(e == 0.f) {
return 0.f;
}
return (e > 0.f) ? -tau : (1.f - tau);
public static final class EpsilonInsensitiveLoss extends RegressionLoss {
import hivemall.common.LossFunctions.EpsilonInsensitiveLoss;
return EpsilonInsensitiveLoss.loss(predicted, target, epsilon);
float e = epsilon * stddev;
return EpsilonInsensitiveLoss.loss(predicted, target, e);
float e = epsilon * stddev;
return EpsilonInsensitiveLoss.loss(predicted, target, e);
public boolean forBinaryClassification();
public boolean forRegression();
public static abstract class BinaryLoss implements LossFunction {
protected void checkTarget(float y) {
if(y == 1.f || y == -1.f) {
}
}
@Override
public boolean forBinaryClassification() {
return true;
}
@Override
public boolean forRegression() {
return false;
}
}
public static abstract class RegressionLoss implements LossFunction {
@Override
public boolean forBinaryClassification() {
return false;
}
@Override
public boolean forRegression() {
return true;
}
}
public static final class SquaredLoss extends RegressionLoss {
return ((p - y) * (p - y)) / 2.f;
public static final class LogLoss extends BinaryLoss {
checkTarget(y);
float z = y * p;
if(z > 18.f) {
return (float) Math.exp(-z);
}
if(z < -18.f) {
return -z;
}
checkTarget(y);
float z = y * p;
if(z > 18.f) {
return (float) Math.exp(-z) * -y;
if(z < -18.f) {
return -y;
}
public static final class HingeLoss extends BinaryLoss {
checkTarget(y);
checkTarget(y);
return (loss > 0.f) ? -y : 0.f;
assert (y == -1.f || y == 1.f) : y;
public static final class SquaredHingeLoss extends BinaryLoss {
checkTarget(y);
float z = y * p;
float d = 1.f - z;
return (d > 0.f) ? (d * d) : 0.f;
checkTarget(y);
float d = 1 - (y * p);
return (d > 0.f) ? -2.f * d * y : 0.f;
public static final class QuantileLoss extends RegressionLoss {
this.tau = 0.5f;
setTau(tau);
if(tau <= 0 || tau >= 1.0) {
}
float e = y - p;
if(e > 0.f) {
return tau * e;
return -(1.f - tau) * e;
float e = y - p;
if(e == 0.f) {
return 0.f;
}
return (e > 0.f) ? -tau : (1.f - tau);
public static final class EpsilonInsensitiveLoss extends RegressionLoss {
import hivemall.common.LossFunctions.EpsilonInsensitiveLoss;
return EpsilonInsensitiveLoss.loss(predicted, target, epsilon);
float e = epsilon * stddev;
return EpsilonInsensitiveLoss.loss(predicted, target, e);
float e = epsilon * stddev;
return EpsilonInsensitiveLoss.loss(predicted, target, e);
if(keyTypeName != HivemallConstants.STRING_TYPE_NAME
&& keyTypeName != HivemallConstants.INT_TYPE_NAME
&& keyTypeName != HivemallConstants.BIGINT_TYPE_NAME) {
this.parseX = (keyTypeName == HivemallConstants.STRING_TYPE_NAME);
this.biasKey = (featureRawOI.getTypeName() == HivemallConstants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT
if(keyTypeName != HivemallConstants.STRING_TYPE_NAME
&& keyTypeName != HivemallConstants.INT_TYPE_NAME
&& keyTypeName != HivemallConstants.BIGINT_TYPE_NAME) {
this.parseX = (keyTypeName == HivemallConstants.STRING_TYPE_NAME);
if(labelTypeName != HivemallConstants.STRING_TYPE_NAME
&& labelTypeName != HivemallConstants.INT_TYPE_NAME) {
this.biasKey = (featureRawOI.getTypeName() == HivemallConstants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT
public static final String VOID_TYPE_NAME = "void";
public static final String BOOLEAN_TYPE_NAME = "boolean";
public static final String TINYINT_TYPE_NAME = "tinyint";
public static final String SMALLINT_TYPE_NAME = "smallint";
public static final String INT_TYPE_NAME = "int";
public static final String BIGINT_TYPE_NAME = "bigint";
public static final String FLOAT_TYPE_NAME = "float";
public static final String DOUBLE_TYPE_NAME = "double";
public static final String STRING_TYPE_NAME = "string";
public static final String DATE_TYPE_NAME = "date";
public static final String DATETIME_TYPE_NAME = "datetime";
public static final String TIMESTAMP_TYPE_NAME = "timestamp";
public static final String BINARY_TYPE_NAME = "binary";
public static final String LIST_TYPE_NAME = "array";
public static final String MAP_TYPE_NAME = "map";
public static final String STRUCT_TYPE_NAME = "struct";
public static final String UNION_TYPE_NAME = "uniontype";
import hivemall.common.HivemallConstants;
if(argOIs[0].getTypeName() != HivemallConstants.INT_TYPE_NAME) {
import hivemall.common.HivemallConstants;
if(keyTypeName != HivemallConstants.STRING_TYPE_NAME
&& keyTypeName != HivemallConstants.INT_TYPE_NAME
&& keyTypeName != HivemallConstants.BIGINT_TYPE_NAME) {
if(weightOI.getTypeName() != HivemallConstants.FLOAT_TYPE_NAME) {
this.biasKey = (featureOutputOI.getTypeName() == HivemallConstants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT
if(keyTypeName != HivemallConstants.STRING_TYPE_NAME
&& keyTypeName != HivemallConstants.INT_TYPE_NAME
&& keyTypeName != HivemallConstants.BIGINT_TYPE_NAME) {
this.parseX = (keyTypeName == HivemallConstants.STRING_TYPE_NAME);
if(keyTypeName != HivemallConstants.STRING_TYPE_NAME
&& keyTypeName != HivemallConstants.INT_TYPE_NAME
&& keyTypeName != HivemallConstants.BIGINT_TYPE_NAME) {
this.parseX = (keyTypeName == HivemallConstants.STRING_TYPE_NAME);
this.biasKey = (featureRawOI.getTypeName() == HivemallConstants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT
if(keyTypeName != HivemallConstants.STRING_TYPE_NAME
&& keyTypeName != HivemallConstants.INT_TYPE_NAME
&& keyTypeName != HivemallConstants.BIGINT_TYPE_NAME) {
this.parseX = (keyTypeName == HivemallConstants.STRING_TYPE_NAME);
if(labelTypeName != HivemallConstants.STRING_TYPE_NAME
&& labelTypeName != HivemallConstants.INT_TYPE_NAME) {
this.biasKey = (featureRawOI.getTypeName() == HivemallConstants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT
public static final String VOID_TYPE_NAME = "void";
public static final String BOOLEAN_TYPE_NAME = "boolean";
public static final String TINYINT_TYPE_NAME = "tinyint";
public static final String SMALLINT_TYPE_NAME = "smallint";
public static final String INT_TYPE_NAME = "int";
public static final String BIGINT_TYPE_NAME = "bigint";
public static final String FLOAT_TYPE_NAME = "float";
public static final String DOUBLE_TYPE_NAME = "double";
public static final String STRING_TYPE_NAME = "string";
public static final String DATE_TYPE_NAME = "date";
public static final String DATETIME_TYPE_NAME = "datetime";
public static final String TIMESTAMP_TYPE_NAME = "timestamp";
public static final String BINARY_TYPE_NAME = "binary";
public static final String LIST_TYPE_NAME = "array";
public static final String MAP_TYPE_NAME = "map";
public static final String STRUCT_TYPE_NAME = "struct";
public static final String UNION_TYPE_NAME = "uniontype";
import hivemall.common.HivemallConstants;
if(argOIs[0].getTypeName() != HivemallConstants.INT_TYPE_NAME) {
import hivemall.common.HivemallConstants;
if(keyTypeName != HivemallConstants.STRING_TYPE_NAME
&& keyTypeName != HivemallConstants.INT_TYPE_NAME
&& keyTypeName != HivemallConstants.BIGINT_TYPE_NAME) {
if(weightOI.getTypeName() != HivemallConstants.FLOAT_TYPE_NAME) {
this.biasKey = (featureOutputOI.getTypeName() == HivemallConstants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT
if(keyTypeName != HivemallConstants.STRING_TYPE_NAME
&& keyTypeName != HivemallConstants.INT_TYPE_NAME
&& keyTypeName != HivemallConstants.BIGINT_TYPE_NAME) {
this.parseX = (keyTypeName == HivemallConstants.STRING_TYPE_NAME);
public List<Integer> evaluate(List<String> values) {
return evaluate(values, null, MurmurHash3UDF.DEFAULT_NUM_FEATURES);
}
public List<Integer> evaluate(List<String> values, String prefix) {
return evaluate(values, prefix, MurmurHash3UDF.DEFAULT_NUM_FEATURES);
}
return hashValues(values, prefix, numFeatures);
}
private static List<Integer> hashValues(List<String> values, String prefix, int numFeatures) {
final int size = values.size();
if(v == null) {
ary[i] = null;
} else {
ary[i] = MurmurHash3UDF.murmurhash3(data, numFeatures);
}
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
public static final int DEFAULT_NUM_FEATURES = 16777216;
public int evaluate(String word) throws UDFArgumentException {
if(word == null) {
throw new UDFArgumentException("argument must not be null");
}
public int evaluate(String word, int numFeatures) throws UDFArgumentException {
if(word == null) {
throw new UDFArgumentException("argument must not be null");
}
public int evaluate(String... words) throws UDFArgumentException {
if(words == null) {
throw new UDFArgumentException("argument must not be null");
}
public int evaluate(String[] words, int numFeatures) throws UDFArgumentException {
if(words == null) {
throw new UDFArgumentException("argument must not be null");
}
return murmurhash3(data, DEFAULT_NUM_FEATURES);
return evaluate(word, DEFAULT_NUM_FEATURES);
this.count = 1;
float score = 0.f;
if(t > total_steps) {
return eta0 / 2.f;
}
if(t > total_steps) {
return eta0 / 2.f;
}
public List<Integer> evaluate(List<String> values) {
return evaluate(values, null, MurmurHash3UDF.DEFAULT_NUM_FEATURES);
}
public List<Integer> evaluate(List<String> values, String prefix) {
return evaluate(values, prefix, MurmurHash3UDF.DEFAULT_NUM_FEATURES);
}
return hashValues(values, prefix, numFeatures);
}
private static List<Integer> hashValues(List<String> values, String prefix, int numFeatures) {
final int size = values.size();
if(v == null) {
ary[i] = null;
} else {
ary[i] = MurmurHash3UDF.murmurhash3(data, numFeatures);
}
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
public static final int DEFAULT_NUM_FEATURES = 16777216;
public int evaluate(String word) throws UDFArgumentException {
return evaluate(word, DEFAULT_NUM_FEATURES);
public int evaluate(String word, int numFeatures) throws UDFArgumentException {
if(word == null) {
throw new UDFArgumentException("argument must not be null");
}
public int evaluate(String... words) throws UDFArgumentException {
if(words == null) {
throw new UDFArgumentException("argument must not be null");
}
public int evaluate(String[] words, int numFeatures) throws UDFArgumentException {
if(words == null) {
throw new UDFArgumentException("argument must not be null");
}
return murmurhash3(data, DEFAULT_NUM_FEATURES);
this.count = 1;
float score = 0.f;
import org.apache.commons.cli.Options;
protected Options getOptions() {
System.out.println("OverrideOptions");
Options opts = super.getOptions();
opts.addOption("c", "cparam", true, "Aggressiveness parameter C [default 1.0]");
return opts;
}
@Override
opts.addOption("b", "bias", true, "Bias clause [default 0.0 (disable)]");
preTrain(target);
protected void preTrain(float target) {}
private OnlineVariance targetStdDev;
this.targetStdDev = new OnlineVariance();
protected void preTrain(float target) {
targetStdDev.handle(target);
}
@Override
float stddev = (float) targetStdDev.stddev();
private OnlineVariance targetStdDev;
this.targetStdDev = new OnlineVariance();
protected void preTrain(float target) {
targetStdDev.handle(target);
}
@Override
float stddev = (float) targetStdDev.stddev();
opts.addOption("b", "bias", true, "Bias clause [default 0.0 (disable)]");
opts.addOption("b", "bias", true, "Bias clause [default 0.0 (disable)]");
preTrain(target);
protected void preTrain(float target) {}
private OnlineVariance targetStdDev;
this.targetStdDev = new OnlineVariance();
protected void preTrain(float target) {
targetStdDev.handle(target);
}
@Override
float stddev = (float) targetStdDev.stddev();
private OnlineVariance targetStdDev;
this.targetStdDev = new OnlineVariance();
protected void preTrain(float target) {
targetStdDev.handle(target);
}
@Override
float stddev = (float) targetStdDev.stddev();
opts.addOption("b", "bias", true, "Bias clause. [default 0.0 (disable)]");
opts.addOption("b", "bias", true, "Bias clause. [default 0.0 (disable)]");
opts.addOption("c", "aggressiveness", true, "Aggressiveness parameter C [default 1.0]");
import org.apache.commons.cli.Options;
protected Options getOptions() {
Options opts = super.getOptions();
opts.addOption("c", "aggressiveness", true, "Aggressiveness parameter C [default 1.0]");
return opts;
}
@Override
public int evaluate(String word, boolean rawValue) throws UDFArgumentException {
if(rawValue) {
if(word == null) {
throw new UDFArgumentException("argument must not be null");
}
return murmurhash3_x86_32(word, 0, word.length(), 0x9747b28c);
} else {
return evaluate(word, DEFAULT_NUM_FEATURES);
}
}
public static final int DEFAULT_NUM_FEATURES = 16777216;
return evaluate(word, DEFAULT_NUM_FEATURES);
}
public int evaluate(String word, boolean rawValue) {
if(rawValue) {
return sha1(word);
} else {
return evaluate(word, DEFAULT_NUM_FEATURES);
}
opts.addOption("power_t", true, "The exponent for inverse scaling learning rate [default 0.1]");
opts.addOption("eta0", true, "The initial learning rate [default 0.1]");
return new InvscalingEtaEstimator(0.1f, 0.1f);
float power_t = Float.parseFloat(cl.getOptionValue("power_t", "0.1"));
public static double inverseErf(final double x) {
double p;
if(w < 6.25) {
w = w - 3.125;
p = -3.6444120640178196996e-21;
} else if(w < 16.0) {
w = Math.sqrt(w) - 3.25;
p = 2.2137376921775787049e-09;
} else if(!Double.isInfinite(w)) {
w = Math.sqrt(w) - 5.0;
p = -2.7109920616438573243e-11;
} else {
p = Double.POSITIVE_INFINITY;
}
return p * x;
}
import hivemall.common.WeightValue;
protected Map<Object, WeightValue> weights;
this.weights = new HashMap<Object, WeightValue>(8192);
WeightValue old_w = weights.get(k);
WeightValue biasWeight = weights.get(biasKey);
protected PredictionResult calcScoreAndNorm(List<?> features) {
WeightValue old_w = weights.get(k);
WeightValue biasWeight = weights.get(biasKey);
return new PredictionResult(score).squaredNorm(squared_norm);
}
protected PredictionResult calcScoreAndVariance(List<?> features) {
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final boolean parseX = this.parseX;
float score = 0.f;
float variance = 0.f;
final Object k;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
k = fv.getFeature();
v = fv.getValue();
} else {
k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
v = 1.f;
}
WeightValue old_w = weights.get(k);
if(old_w == null) {
} else {
}
}
if(biasKey != null) {
WeightValue biasWeight = weights.get(biasKey);
if(biasWeight == null) {
} else {
}
}
return new PredictionResult(score).variance(variance);
protected void update(final List<?> features, final float coeff) {
WeightValue old_w = weights.get(k);
weights.put(k, new WeightValue(new_w));
WeightValue old_bias = weights.get(biasKey);
float new_bias = (old_bias == null) ? coeff * bias : old_bias.getValue()
(coeff * bias);
weights.put(biasKey, new WeightValue(new_bias));
}
}
protected void update(final List<?> features, final float coeff, final float alpha, final float phi) {
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final Object k;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
k = fv.getFeature();
v = fv.getValue();
} else {
k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
v = 1.f;
}
WeightValue old_w = weights.get(k);
float old_cov = (old_w == null) ? 1.f : old_w.getCovariance();
weights.put(k, new WeightValue(new_w, new_cov));
}
if(biasKey != null) {
WeightValue old_bias = weights.get(biasKey);
float new_bias = (old_bias == null) ? coeff * bias : old_bias.getValue()
(coeff * bias);
float old_cov = (old_bias == null) ? 1.f : old_bias.getCovariance();
weights.put(biasKey, new WeightValue(new_bias, new_cov));
for(Map.Entry<Object, WeightValue> e : weights.entrySet()) {
WeightValue v = e.getValue();
FloatWritable fv = new FloatWritable(v.getValue());
forwardMapObj[1] = fv;
private float squaredNorm;
private float variance;
public PredictionResult(float predictedScore) {
this(null, predictedScore);
}
public PredictionResult squaredNorm(float sqnorm) {
this.squaredNorm = sqnorm;
return this;
}
public PredictionResult variance(float var) {
this.variance = var;
return this;
public float getVariance() {
return variance;
}
import hivemall.common.WeightValue;
protected Map<Object, Map<Object, WeightValue>> label2FeatureWeight;
this.label2FeatureWeight = new HashMap<Object, Map<Object, WeightValue>>(64);
Map<Object, WeightValue> weights = label2map.getValue();
Map<Object, WeightValue> weights = label2map.getValue();
protected Margin getMarginAndVariance(final List<?> features, final Object actual_label) {
float correctScore = 0.f;
float correctVariance = 0.f;
Object maxAnotherLabel = null;
float maxAnotherScore = 0.f;
float maxAnotherVariance = 0.f;
Object label = label2map.getKey();
Map<Object, WeightValue> weights = label2map.getValue();
PredictionResult predicted = calcScoreAndVariance(weights, features);
float score = predicted.getScore();
if(label.equals(actual_label)) {
correctScore = score;
correctVariance = predicted.getVariance();
} else {
if(maxAnotherLabel == null || score > maxAnotherScore) {
maxAnotherLabel = label;
maxAnotherScore = score;
maxAnotherVariance = predicted.getVariance();
}
}
}
return new Margin(correctScore, maxAnotherLabel, maxAnotherScore).variance(var);
}
protected final float calcScore(final Map<Object, WeightValue> weights, final List<?> features) {
WeightValue old_w = weights.get(k);
WeightValue biasWeight = weights.get(biasKey);
protected final PredictionResult calcScoreAndVariance(final Map<Object, WeightValue> weights, final List<?> features) {
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final boolean parseX = this.parseX;
float score = 0.f;
float variance = 0.f;
final Object k;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
k = fv.getFeature();
v = fv.getValue();
} else {
k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
v = 1.f;
}
WeightValue old_w = weights.get(k);
if(old_w == null) {
} else {
}
}
if(bias != 0.f) {
WeightValue biasWeight = weights.get(biasKey);
if(biasWeight == null) {
} else {
}
}
return new PredictionResult(score).variance(variance);
}
Map<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new HashMap<Object, WeightValue>(8192);
Map<Object, WeightValue> weightsToSub = null;
weightsToSub = new HashMap<Object, WeightValue>(8192);
WeightValue old_trueclass_w = weightsToAdd.get(k);
float add_w = (old_trueclass_w == null) ? coeff * v : old_trueclass_w.getValue()
weightsToAdd.put(k, new WeightValue(add_w));
WeightValue old_falseclass_w = weightsToSub.get(k);
float sub_w = (old_falseclass_w == null) ? -(coeff * v)
: old_falseclass_w.getValue() - (coeff * v);
weightsToSub.put(k, new WeightValue(sub_w));
WeightValue old_trueclass_bias = weightsToAdd.get(biasKey);
float add_bias = (old_trueclass_bias == null) ? coeff * bias
weightsToAdd.put(biasKey, new WeightValue(add_bias));
WeightValue old_falseclass_bias = weightsToSub.get(biasKey);
: old_falseclass_bias.getValue() - (coeff * bias);
weightsToSub.put(biasKey, new WeightValue(sub_bias));
protected void update(List<?> features, float coeff, Object actual_label, Object missed_label, final float alpha, final float phi) {
assert (actual_label != null);
if(actual_label.equals(missed_label)) {
throw new IllegalArgumentException("Actual label equals to missed label: "
actual_label);
}
Map<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
if(weightsToAdd == null) {
weightsToAdd = new HashMap<Object, WeightValue>(8192);
label2FeatureWeight.put(actual_label, weightsToAdd);
}
Map<Object, WeightValue> weightsToSub = null;
if(missed_label != null) {
weightsToSub = label2FeatureWeight.get(missed_label);
if(weightsToSub == null) {
weightsToSub = new HashMap<Object, WeightValue>(8192);
label2FeatureWeight.put(missed_label, weightsToSub);
}
}
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final Object k;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
k = fv.getFeature();
v = fv.getValue();
} else {
k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
v = 1.f;
}
WeightValue old_correctclass_w = weightsToAdd.get(k);
float add_w = (old_correctclass_w == null) ? coeff * v : old_correctclass_w.getValue()
(coeff * v);
float new_correctcov = covariance(old_correctclass_w, v, alpha, phi);
weightsToAdd.put(k, new WeightValue(add_w, new_correctcov));
if(weightsToSub != null) {
WeightValue old_wrongclass_w = weightsToSub.get(k);
float sub_w = (old_wrongclass_w == null) ? -(coeff * v)
: old_wrongclass_w.getValue() - (coeff * v);
float new_wrongcov = covariance(old_wrongclass_w, v, alpha, phi);
weightsToSub.put(k, new WeightValue(sub_w, new_wrongcov));
}
}
if(biasKey != null) {
WeightValue old_correctclass_bias = weightsToAdd.get(biasKey);
float add_bias = (old_correctclass_bias == null) ? coeff * bias
float new_correctbias_cov = covariance(old_correctclass_bias, bias, alpha, phi);
weightsToAdd.put(biasKey, new WeightValue(add_bias, new_correctbias_cov));
if(weightsToSub != null) {
WeightValue old_wrongclass_bias = weightsToSub.get(biasKey);
float sub_bias = (old_wrongclass_bias == null) ? -(coeff * bias)
: old_wrongclass_bias.getValue() - (coeff * bias);
float new_wrongbias_cov = covariance(old_wrongclass_bias, bias, alpha, phi);
weightsToSub.put(biasKey, new WeightValue(sub_bias, new_wrongbias_cov));
}
}
}
private static float covariance(final WeightValue old_w, final float v, final float alpha, final float phi) {
float old_cov = (old_w == null) ? 1.f : old_w.getCovariance();
}
for(Map.Entry<Object, Map<Object, WeightValue>> label2map : label2FeatureWeight.entrySet()) {
Map<Object, WeightValue> fvmap = label2map.getValue();
for(Map.Entry<Object, WeightValue> entry : fvmap.entrySet()) {
Object k = entry.getKey();
WeightValue v = entry.getValue();
FloatWritable fv = new FloatWritable(v.getValue());
forwardMapObj[2] = fv;
private float variance;
public float get() {
return correctScore - maxIncorrectScore;
}
public Margin variance(float var) {
this.variance = var;
return this;
}
public float getVariance() {
return variance;
PredictionResult margin = calcScoreAndNorm(features);
return new PredictionResult(score).squaredNorm(squared_norm);
protected static void checkTarget(float y) {
public static float hingeLoss(final float p, final float y, final float threshold) {
checkTarget(y);
return squaredHingeLoss(p, y);
public static float squaredHingeLoss(final float p, final float y) {
checkTarget(y);
float z = y * p;
float d = 1.f - z;
return (d > 0.f) ? (d * d) : 0.f;
}
import hivemall.common.FeatureValue;
import hivemall.common.WeightValue;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
update(features, coeff, gamma);
protected void update(final List<?> features, final float coeff, final float alpha) {
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final Object k;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
k = fv.getFeature();
v = fv.getValue();
} else {
k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
v = 1.f;
}
WeightValue old_w = weights.get(k);
float old_cov = (old_w == null) ? 1.f : old_w.getCovariance();
weights.put(k, new WeightValue(new_w, new_cov));
}
if(biasKey != null) {
WeightValue old_bias = weights.get(biasKey);
float new_bias = (old_bias == null) ? coeff * bias : old_bias.getValue()
(coeff * bias);
float old_cov = (old_bias == null) ? 1.f : old_bias.getCovariance();
weights.put(biasKey, new WeightValue(new_bias, new_cov));
}
}
public class MulticlassConfidenceWeightedUDTF extends MulticlassOnlineClassifierUDTF {
import hivemall.common.FeatureValue;
import hivemall.common.WeightValue;
import java.util.HashMap;
import java.util.Map;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
protected void update(List<?> features, float coeff, Object actual_label, Object missed_label, final float alpha, final float phi) {
assert (actual_label != null);
if(actual_label.equals(missed_label)) {
throw new IllegalArgumentException("Actual label equals to missed label: "
actual_label);
}
Map<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
if(weightsToAdd == null) {
weightsToAdd = new HashMap<Object, WeightValue>(8192);
label2FeatureWeight.put(actual_label, weightsToAdd);
}
Map<Object, WeightValue> weightsToSub = null;
if(missed_label != null) {
weightsToSub = label2FeatureWeight.get(missed_label);
if(weightsToSub == null) {
weightsToSub = new HashMap<Object, WeightValue>(8192);
label2FeatureWeight.put(missed_label, weightsToSub);
}
}
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final Object k;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
k = fv.getFeature();
v = fv.getValue();
} else {
k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
v = 1.f;
}
WeightValue old_correctclass_w = weightsToAdd.get(k);
float add_w = (old_correctclass_w == null) ? coeff * v : old_correctclass_w.getValue()
(coeff * v);
float new_correctcov = covariance(old_correctclass_w, v, alpha, phi);
weightsToAdd.put(k, new WeightValue(add_w, new_correctcov));
if(weightsToSub != null) {
WeightValue old_wrongclass_w = weightsToSub.get(k);
float sub_w = (old_wrongclass_w == null) ? -(coeff * v)
: old_wrongclass_w.getValue() - (coeff * v);
float new_wrongcov = covariance(old_wrongclass_w, v, alpha, phi);
weightsToSub.put(k, new WeightValue(sub_w, new_wrongcov));
}
}
if(biasKey != null) {
WeightValue old_correctclass_bias = weightsToAdd.get(biasKey);
float add_bias = (old_correctclass_bias == null) ? coeff * bias
float new_correctbias_cov = covariance(old_correctclass_bias, bias, alpha, phi);
weightsToAdd.put(biasKey, new WeightValue(add_bias, new_correctbias_cov));
if(weightsToSub != null) {
WeightValue old_wrongclass_bias = weightsToSub.get(biasKey);
float sub_bias = (old_wrongclass_bias == null) ? -(coeff * bias)
: old_wrongclass_bias.getValue() - (coeff * bias);
float new_wrongbias_cov = covariance(old_wrongclass_bias, bias, alpha, phi);
weightsToSub.put(biasKey, new WeightValue(sub_bias, new_wrongbias_cov));
}
}
}
private static float covariance(final WeightValue old_w, final float v, final float alpha, final float phi) {
float old_cov = (old_w == null) ? 1.f : old_w.getCovariance();
}
Object missed_label = margin.getMaxIncorrectLabel();
float m = margin.getScore() * y;
if(m >= 1.f) {
float alpha = (1.f - m) * beta;
float m = margin.get();
if(m >= 1.f) {
float alpha = (1.f - m) * beta;
if(m < 1.f) {
float var = margin.getVariance();
float alpha = (1.f - m) * beta;
update(features, alpha, beta);
}
protected float loss(PredictionResult margin, float y) {
float m = margin.getScore() * y;
for(Object f : features) {
for(Object f : features) {
update(features, y, alpha, beta);
protected void update(final List<?> features, final float y, final float alpha, final float beta) {
WeightValue new_w = getNewWeight(old_w, v, y, alpha, beta);
WeightValue new_bias = getNewWeight(old_bias, bias, y, alpha, beta);
private static WeightValue getNewWeight(final WeightValue old, final float x, final float y, final float alpha, final float beta) {
float cv = old_cov * x;
protected void update(final List<?> features, final int y, final float alpha, final float beta) {
Object missed_label = margin.getMaxIncorrectLabel();
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.7]");
if(eta <= 0.5 || eta > 1) {
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5, 1]: "
phi = (float) StatsUtils.probit(eta, 5d);
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5,1] [default 0.7]");
if(eta <= 0.5 || eta > 1) {
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5,1]: "
phi = (float) StatsUtils.probit(eta, 5d);
if(eta <= 0.5 || eta > 1) {
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5,1]: "
phi = (float) StatsUtils.probit(eta, 5d);
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5,1] [default 0.7]");
if(eta <= 0.5 || eta > 1) {
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5,1]: "
phi = (float) StatsUtils.probit(eta, 5d);
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.7]");
if(eta <= 0.5 || eta > 1) {
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5, 1]: "
phi = (float) StatsUtils.probit(eta, 5d);
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5,1] [default 0.7]");
if(eta <= 0.5 || eta > 1) {
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5,1]: "
phi = (float) StatsUtils.probit(eta, 5d);
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.7]");
if(eta <= 0.5 || eta > 1) {
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5,1]: "
phi = (float) StatsUtils.probit(eta, 5d);
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5,1] [default 0.7]");
if(eta <= 0.5 || eta > 1) {
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5,1]: "
phi = (float) StatsUtils.probit(eta, 5d);
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.7]");
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5, 1]: "
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5, 1]: "
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.7]");
throw new UDFArgumentException("Confidence hyperparameter eta must be in range (0.5, 1]: "
private static WeightValue getNewWeight(final WeightValue old, final float v, final float alpha, final float beta, final boolean positive) {
private static WeightValue getNewWeight(final WeightValue old, final float v, final float alpha, final float beta, final boolean positive) {
if(gamma_numer <= 0.f) {
return 0.f;
}
float gamma_denom = 4.f * phi * var;
return gamma_numer / gamma_denom;
WeightValue new_w = getNewWeight(old_w, v, coeff, alpha, phi);
weights.put(k, new_w);
WeightValue new_bias = getNewWeight(old_bias, bias, coeff, alpha, phi);
weights.put(biasKey, new_bias);
private static WeightValue getNewWeight(final WeightValue old, final float v, final float coeff, final float alpha, final float phi) {
final float old_w, old_cov;
if(old == null) {
old_w = 0.f;
old_cov = 1.f;
} else {
old_w = old.get();
old_cov = old.getCovariance();
}
return new WeightValue(new_w, new_cov);
}
update(features, gamma, actual_label, missed_label, phi);
protected void update(List<?> features, float coeff, Object actual_label, Object missed_label, final float phi) {
WeightValue new_correctclass_w = getNewWeight(old_correctclass_w, v, coeff, phi, true);
weightsToAdd.put(k, new_correctclass_w);
WeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, coeff, phi, false);
weightsToSub.put(k, new_wrongclass_w);
WeightValue new_correctclass_bias = getNewWeight(old_correctclass_bias, bias, coeff, phi, true);
weightsToAdd.put(biasKey, new_correctclass_bias);
WeightValue new_wrongclass_bias = getNewWeight(old_wrongclass_bias, bias, coeff, phi, false);
weightsToSub.put(biasKey, new_wrongclass_bias);
private static WeightValue getNewWeight(final WeightValue old, final float v, final float coeff, final float phi, final boolean positive) {
final float old_w, old_cov;
if(old == null) {
old_w = 0.f;
old_cov = 1.f;
} else {
old_w = old.get();
old_cov = old.getCovariance();
}
float delta_w = coeff * old_cov * v;
return new WeightValue(new_w, new_cov);
float m = margin.get();
float gamma_denom = 4.f * phi * var;
return 0.f;
}
return gamma_numer / gamma_denom;
float gamma = getGamma(margin, y);
protected final float getGamma(PredictionResult margin, int y) {
float score = margin.getScore() * y;
float gamma_denom = 4.f * phi * var;
private static WeightValue getNewWeight(final WeightValue old, final float x, final float coeff, final float alpha, final float phi) {
if(!(y == 1.f || y == -1.f)) {
protected void update(List<?> features, float alpha, Object actual_label, Object missed_label, final float phi) {
WeightValue new_correctclass_w = getNewWeight(old_correctclass_w, v, alpha, phi, true);
WeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, phi, false);
WeightValue new_correctclass_bias = getNewWeight(old_correctclass_bias, bias, alpha, phi, true);
WeightValue new_wrongclass_bias = getNewWeight(old_wrongclass_bias, bias, alpha, phi, false);
private static WeightValue getNewWeight(final WeightValue old, final float x, final float alpha, final float phi, final boolean positive) {
float delta_w = alpha * old_cov * x;
update(features, gamma, actual_label, missed_label);
protected void update(List<?> features, float alpha, Object actual_label, Object missed_label) {
Margin margin = getMarginAndVariance(features, actual_label, true);
assert (var != 0);
return getMarginAndVariance(features, actual_label, false);
}
protected Margin getMarginAndVariance(final List<?> features, final Object actual_label, boolean nonZeroVariance) {
float var = 2.f * calcVariance(features);
return new Margin(correctScore, maxAnotherLabel, maxAnotherScore).variance(var);
}
protected final float calcVariance(final List<?> features) {
final boolean parseX = this.parseX;
float variance = 0.f;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
v = fv.getValue();
} else {
v = 1.f;
}
}
if(bias != 0.f) {
}
return variance;
}
Margin margin = getMarginAndVariance(features, actual_label, true);
assert (var != 0);
import hivemall.common.WeightValue;
protected Map<Object, WeightValue> weights;
this.weights = new HashMap<Object, WeightValue>(8192);
WeightValue old_w = weights.get(k);
WeightValue biasWeight = weights.get(biasKey);
protected PredictionResult calcScoreAndNorm(List<?> features) {
WeightValue old_w = weights.get(k);
WeightValue biasWeight = weights.get(biasKey);
return new PredictionResult(score).squaredNorm(squared_norm);
}
protected PredictionResult calcScoreAndVariance(List<?> features) {
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final boolean parseX = this.parseX;
float score = 0.f;
float variance = 0.f;
final Object k;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
k = fv.getFeature();
v = fv.getValue();
} else {
k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
v = 1.f;
}
WeightValue old_w = weights.get(k);
if(old_w == null) {
} else {
}
}
if(biasKey != null) {
WeightValue biasWeight = weights.get(biasKey);
if(biasWeight == null) {
} else {
}
}
return new PredictionResult(score).variance(variance);
protected void update(final List<?> features, final float coeff) {
WeightValue old_w = weights.get(k);
weights.put(k, new WeightValue(new_w));
WeightValue old_bias = weights.get(biasKey);
float new_bias = (old_bias == null) ? coeff * bias : old_bias.getValue()
(coeff * bias);
weights.put(biasKey, new WeightValue(new_bias));
for(Map.Entry<Object, WeightValue> e : weights.entrySet()) {
WeightValue v = e.getValue();
FloatWritable fv = new FloatWritable(v.getValue());
forwardMapObj[1] = fv;
PredictionResult margin = calcScoreAndNorm(features);
import hivemall.common.WeightValue;
protected Map<Object, Map<Object, WeightValue>> label2FeatureWeight;
this.label2FeatureWeight = new HashMap<Object, Map<Object, WeightValue>>(64);
Map<Object, WeightValue> weights = label2map.getValue();
Map<Object, WeightValue> weights = label2map.getValue();
protected Margin getMarginAndVariance(final List<?> features, final Object actual_label) {
return getMarginAndVariance(features, actual_label, false);
}
protected Margin getMarginAndVariance(final List<?> features, final Object actual_label, boolean nonZeroVariance) {
float correctScore = 0.f;
float correctVariance = 0.f;
Object maxAnotherLabel = null;
float maxAnotherScore = 0.f;
float maxAnotherVariance = 0.f;
float var = 2.f * calcVariance(features);
return new Margin(correctScore, maxAnotherLabel, maxAnotherScore).variance(var);
}
Object label = label2map.getKey();
Map<Object, WeightValue> weights = label2map.getValue();
PredictionResult predicted = calcScoreAndVariance(weights, features);
float score = predicted.getScore();
if(label.equals(actual_label)) {
correctScore = score;
correctVariance = predicted.getVariance();
} else {
if(maxAnotherLabel == null || score > maxAnotherScore) {
maxAnotherLabel = label;
maxAnotherScore = score;
maxAnotherVariance = predicted.getVariance();
}
}
}
return new Margin(correctScore, maxAnotherLabel, maxAnotherScore).variance(var);
}
protected final float calcScore(final Map<Object, WeightValue> weights, final List<?> features) {
WeightValue old_w = weights.get(k);
WeightValue biasWeight = weights.get(biasKey);
protected final float calcVariance(final List<?> features) {
final boolean parseX = this.parseX;
float variance = 0.f;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
v = fv.getValue();
} else {
v = 1.f;
}
}
if(bias != 0.f) {
}
return variance;
}
protected final PredictionResult calcScoreAndVariance(final Map<Object, WeightValue> weights, final List<?> features) {
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final boolean parseX = this.parseX;
float score = 0.f;
float variance = 0.f;
final Object k;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
k = fv.getFeature();
v = fv.getValue();
} else {
k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
v = 1.f;
}
WeightValue old_w = weights.get(k);
if(old_w == null) {
} else {
}
}
if(bias != 0.f) {
WeightValue biasWeight = weights.get(biasKey);
if(biasWeight == null) {
} else {
}
}
return new PredictionResult(score).variance(variance);
}
Map<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new HashMap<Object, WeightValue>(8192);
Map<Object, WeightValue> weightsToSub = null;
weightsToSub = new HashMap<Object, WeightValue>(8192);
WeightValue old_trueclass_w = weightsToAdd.get(k);
float add_w = (old_trueclass_w == null) ? coeff * v : old_trueclass_w.getValue()
weightsToAdd.put(k, new WeightValue(add_w));
WeightValue old_falseclass_w = weightsToSub.get(k);
float sub_w = (old_falseclass_w == null) ? -(coeff * v)
: old_falseclass_w.getValue() - (coeff * v);
weightsToSub.put(k, new WeightValue(sub_w));
WeightValue old_trueclass_bias = weightsToAdd.get(biasKey);
float add_bias = (old_trueclass_bias == null) ? coeff * bias
weightsToAdd.put(biasKey, new WeightValue(add_bias));
WeightValue old_falseclass_bias = weightsToSub.get(biasKey);
: old_falseclass_bias.getValue() - (coeff * bias);
weightsToSub.put(biasKey, new WeightValue(sub_bias));
for(Map.Entry<Object, Map<Object, WeightValue>> label2map : label2FeatureWeight.entrySet()) {
Map<Object, WeightValue> fvmap = label2map.getValue();
for(Map.Entry<Object, WeightValue> entry : fvmap.entrySet()) {
Object k = entry.getKey();
WeightValue v = entry.getValue();
FloatWritable fv = new FloatWritable(v.getValue());
forwardMapObj[2] = fv;
Object missed_label = margin.getMaxIncorrectLabel();
protected static void checkTarget(float y) {
if(!(y == 1.f || y == -1.f)) {
public static float hingeLoss(final float p, final float y, final float threshold) {
checkTarget(y);
return squaredHingeLoss(p, y);
public static float squaredHingeLoss(final float p, final float y) {
checkTarget(y);
float z = y * p;
float d = 1.f - z;
return (d > 0.f) ? (d * d) : 0.f;
}
private float variance;
public float get() {
return correctScore - maxIncorrectScore;
}
public Margin variance(float var) {
this.variance = var;
return this;
}
public float getVariance() {
return variance;
private float squaredNorm;
private float variance;
public PredictionResult(float predictedScore) {
this(null, predictedScore);
}
public PredictionResult squaredNorm(float sqnorm) {
this.squaredNorm = sqnorm;
return this;
}
public PredictionResult variance(float var) {
this.variance = var;
return this;
public float getVariance() {
return variance;
}
opts.addOption("power_t", true, "The exponent for inverse scaling learning rate [default 0.1]");
opts.addOption("eta0", true, "The initial learning rate [default 0.1]");
return new InvscalingEtaEstimator(0.1f, 0.1f);
float power_t = Float.parseFloat(cl.getOptionValue("power_t", "0.1"));
return new PredictionResult(score).squaredNorm(squared_norm);
public static double inverseErf(final double x) {
double p;
if(w < 6.25) {
w = w - 3.125;
p = -3.6444120640178196996e-21;
} else if(w < 16.0) {
w = Math.sqrt(w) - 3.25;
p = 2.2137376921775787049e-09;
} else if(!Double.isInfinite(w)) {
w = Math.sqrt(w) - 5.0;
p = -2.7109920616438573243e-11;
} else {
p = Double.POSITIVE_INFINITY;
}
return p * x;
}
return mean;
return mean;
return mean;
opts.addOption("e", "epsilon", true, "Sensitivity to prediction mistakes [default 0.1].");
opts.addOption("e", "epsilon", true, "Sensitivity to prediction mistakes [default 0.1].");
public List<String> evaluate(List<String> values, String prefix, boolean returnStringArray) {
List<Integer> hashValues = hashValues(values, null, MurmurHash3UDF.DEFAULT_NUM_FEATURES);
final int len = hashValues.size();
final String[] stringValues = new String[len];
Integer v = hashValues.get(i);
}
return Arrays.asList(stringValues);
}
}
static List<Integer> hashValues(List<String> values, String prefix, int numFeatures) {
if(values == null) {
return null;
}
if(prefix == null) {
prefix = "";
}
final float old_w;
old_w = 0.f;
old_w = old.getValue();
import hivemall.common.WeightValue;
float v = weight.get();
weights.put(feature, new WeightValue(v));
train(features, target);
import hivemall.common.WeightValue;
protected Map<Object, WeightValue> weights;
this.weights = new HashMap<Object, WeightValue>(8192);
train(features, target);
protected void train(final Collection<?> features, final float target) {
WeightValue old_w = weights.get(k);
WeightValue biasWeight = weights.get(biasKey);
protected PredictionResult calcScoreAndNorm(Collection<?> features) {
WeightValue old_w = weights.get(k);
WeightValue biasWeight = weights.get(biasKey);
protected PredictionResult calcScoreAndVariance(Collection<?> features) {
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final boolean parseX = this.parseX;
float score = 0.f;
float variance = 0.f;
final Object k;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
k = fv.getFeature();
v = fv.getValue();
} else {
k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
v = 1.f;
}
WeightValue old_w = weights.get(k);
if(old_w == null) {
} else {
}
}
if(biasKey != null) {
WeightValue biasWeight = weights.get(biasKey);
if(biasWeight == null) {
} else {
}
}
return new PredictionResult(score).variance(variance);
}
WeightValue old_w = weights.get(x);
weights.put(x, new WeightValue(new_w));
WeightValue old_bias = weights.get(biasKey);
weights.put(biasKey, new WeightValue(new_bias));
for(Map.Entry<Object, WeightValue> e : weights.entrySet()) {
WeightValue v = e.getValue();
FloatWritable fv = new FloatWritable(v.get());
forwardMapObj[1] = fv;
protected void train(Collection<?> features, float target) {
PredictionResult margin = calcScoreAndNorm(features);
if(loss != 0.f) {
float var = margin.getVariance();
update(features, loss, beta);
}
float var = margin.getVariance();
update(features, loss, beta);
@Override
protected void update(final Collection<?> features, final float loss, final float beta) {
float cov_x = old_cov * x;
float new_cov = old_cov - (beta * cov_x * cov_x);
final float old_w;
old_w = 0.f;
old_w = old.getValue();
import hivemall.common.WeightValue;
float v = weight.get();
weights.put(feature, new WeightValue(v));
train(features, target);
import hivemall.common.WeightValue;
protected Map<Object, WeightValue> weights;
this.weights = new HashMap<Object, WeightValue>(8192);
train(features, target);
protected void train(final Collection<?> features, final float target) {
WeightValue old_w = weights.get(k);
WeightValue biasWeight = weights.get(biasKey);
protected PredictionResult calcScoreAndNorm(Collection<?> features) {
WeightValue old_w = weights.get(k);
WeightValue biasWeight = weights.get(biasKey);
protected PredictionResult calcScoreAndVariance(Collection<?> features) {
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final boolean parseX = this.parseX;
float score = 0.f;
float variance = 0.f;
final Object k;
final float v;
if(parseX) {
FeatureValue fv = FeatureValue.parse(f, feature_hashing);
k = fv.getFeature();
v = fv.getValue();
} else {
k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
v = 1.f;
}
WeightValue old_w = weights.get(k);
if(old_w == null) {
} else {
}
}
if(biasKey != null) {
WeightValue biasWeight = weights.get(biasKey);
if(biasWeight == null) {
} else {
}
}
return new PredictionResult(score).variance(variance);
}
WeightValue old_w = weights.get(x);
weights.put(x, new WeightValue(new_w));
WeightValue old_bias = weights.get(biasKey);
weights.put(biasKey, new WeightValue(new_bias));
for(Map.Entry<Object, WeightValue> e : weights.entrySet()) {
WeightValue v = e.getValue();
FloatWritable fv = new FloatWritable(v.get());
forwardMapObj[1] = fv;
protected void train(Collection<?> features, float target) {
PredictionResult margin = calcScoreAndNorm(features);
opts.addOption("e", "epsilon", true, "Sensitivity to prediction mistakes [default 0.1]");
import hivemall.common.LossFunctions.EpsilonInsensitiveLoss;
protected void update(final Collection<?> features, final float coeff, final float beta) {
WeightValue new_w = getNewWeight(old_w, v, coeff, beta);
WeightValue new_bias = getNewWeight(old_bias, bias, coeff, beta);
private static WeightValue getNewWeight(final WeightValue old, final float x, final float coeff, final float beta) {
public static class AROWh extends AROWRegressionUDTF {
protected float epsilon;
@Override
protected Options getOptions() {
Options opts = super.getOptions();
opts.addOption("e", "epsilon", true, "Sensitivity to prediction mistakes [default 0.1]");
return opts;
}
@Override
protected CommandLine processOptions(ObjectInspector[] argOIs) throws UDFArgumentException {
CommandLine cl = super.processOptions(argOIs);
float epsilon = 0.1f;
if(cl != null) {
String opt_epsilon = cl.getOptionValue("epsilon");
if(opt_epsilon != null) {
epsilon = Float.parseFloat(opt_epsilon);
}
}
this.epsilon = epsilon;
return cl;
}
@Override
protected void train(Collection<?> features, float target) {
PredictionResult margin = calcScoreAndVariance(features);
float predicted = margin.getScore();
float loss = loss(target, predicted);
if(loss > 0.f) {
float coeff = (target - predicted) > 0.f ? loss : -loss;
float var = margin.getVariance();
update(features, coeff, beta);
}
}
protected float loss(float target, float predicted) {
return EpsilonInsensitiveLoss.loss(predicted, target, epsilon);
}
}
import hivemall.common.LossFunctions;
}
public static float hingeLoss(final float p, final float y, final float threshold) {
BinaryLoss.checkTarget(y);
float z = y * p;
return threshold - z;
}
public static float hingeLoss(float p, float y) {
return hingeLoss(p, y, 1.f);
}
public static float squaredHingeLoss(final float p, final float y) {
BinaryLoss.checkTarget(y);
float z = y * p;
float d = 1.f - z;
return (d > 0.f) ? (d * d) : 0.f;
}
public static float epsilonInsensitiveLoss(float predicted, float target, float epsilon) {
return Math.abs(target - predicted) - epsilon;
import hivemall.common.LossFunctions;
return LossFunctions.epsilonInsensitiveLoss(predicted, target, epsilon);
import hivemall.common.LossFunctions;
return LossFunctions.epsilonInsensitiveLoss(predicted, target, epsilon);
return LossFunctions.epsilonInsensitiveLoss(predicted, target, e);
return LossFunctions.epsilonInsensitiveLoss(predicted, target, e);
@SuppressWarnings("unchecked")
public <T> T getFeature() {
return (T) feature;
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
if(f == null) {
continue;
}
public static class AROWe extends AROWRegressionUDTF {
import hivemall.common.LossFunctions;
final float y = label > 0 ? 1.f : -1.f;
protected void update(final List<?> features, final float y, final float alpha, final float beta) {
public static class AROWh extends AROWClassifierUDTF {
protected float c;
@Override
protected Options getOptions() {
Options opts = super.getOptions();
opts.addOption("c", "aggressiveness", true, "Aggressiveness parameter C [default 1.0]");
return opts;
}
@Override
protected CommandLine processOptions(ObjectInspector[] argOIs) throws UDFArgumentException {
final CommandLine cl = super.processOptions(argOIs);
float c = 1.f;
if(cl != null) {
String c_str = cl.getOptionValue("c");
if(c_str != null) {
c = Float.parseFloat(c_str);
if(!(c > 0.f)) {
throw new UDFArgumentException("Aggressiveness parameter C must be C > 0: "
c);
}
}
}
this.c = c;
return cl;
}
@Override
protected void train(List<?> features, int label) {
final float y = label > 0 ? 1.f : -1.f;
PredictionResult margin = calcScoreAndVariance(features);
float p = margin.getScore();
float var = margin.getVariance();
update(features, y, alpha, beta);
}
}
protected float loss(float y, float p) {
return LossFunctions.hingeLoss(p, y, c);
}
}
public static class AROWh extends MulticlassAROWClassifierUDTF {
protected float c;
@Override
protected Options getOptions() {
Options opts = super.getOptions();
opts.addOption("c", "aggressiveness", true, "Aggressiveness parameter C [default 1.0]");
return opts;
}
@Override
protected CommandLine processOptions(ObjectInspector[] argOIs) throws UDFArgumentException {
final CommandLine cl = super.processOptions(argOIs);
float c = 1.f;
if(cl != null) {
String c_str = cl.getOptionValue("c");
if(c_str != null) {
c = Float.parseFloat(c_str);
if(!(c > 0.f)) {
throw new UDFArgumentException("Aggressiveness parameter C must be C > 0: "
c);
}
}
}
this.c = c;
return cl;
}
@Override
protected void train(List<?> features, Object actual_label) {
Margin margin = getMarginAndVariance(features, actual_label);
float loss = loss(margin);
if(loss > 0.f) {
float var = margin.getVariance();
float alpha = loss * beta;
Object missed_label = margin.getMaxIncorrectLabel();
update(features, actual_label, missed_label, alpha, beta);
}
}
protected float loss(Margin margin) {
return c - margin.get();
}
}
protected float loss(final float p, final float y) {
import hivemall.common.LossFunctions;
final float y = label > 0 ? 1.f : -1.f;
protected void update(final List<?> features, final float y, final float alpha, final float beta) {
public static class AROWh extends AROWClassifierUDTF {
protected float c;
@Override
protected Options getOptions() {
Options opts = super.getOptions();
opts.addOption("c", "aggressiveness", true, "Aggressiveness parameter C [default 1.0]");
return opts;
}
@Override
protected CommandLine processOptions(ObjectInspector[] argOIs) throws UDFArgumentException {
final CommandLine cl = super.processOptions(argOIs);
float c = 1.f;
if(cl != null) {
String c_str = cl.getOptionValue("c");
if(c_str != null) {
c = Float.parseFloat(c_str);
if(!(c > 0.f)) {
throw new UDFArgumentException("Aggressiveness parameter C must be C > 0: "
c);
}
}
}
this.c = c;
return cl;
}
@Override
protected void train(List<?> features, int label) {
final float y = label > 0 ? 1.f : -1.f;
PredictionResult margin = calcScoreAndVariance(features);
float p = margin.getScore();
float var = margin.getVariance();
update(features, y, alpha, beta);
}
}
protected float loss(final float p, final float y) {
return LossFunctions.hingeLoss(p, y, c);
}
}
public static class AROWh extends MulticlassAROWClassifierUDTF {
protected float c;
@Override
protected Options getOptions() {
Options opts = super.getOptions();
opts.addOption("c", "aggressiveness", true, "Aggressiveness parameter C [default 1.0]");
return opts;
}
@Override
protected CommandLine processOptions(ObjectInspector[] argOIs) throws UDFArgumentException {
final CommandLine cl = super.processOptions(argOIs);
float c = 1.f;
if(cl != null) {
String c_str = cl.getOptionValue("c");
if(c_str != null) {
c = Float.parseFloat(c_str);
if(!(c > 0.f)) {
throw new UDFArgumentException("Aggressiveness parameter C must be C > 0: "
c);
}
}
}
this.c = c;
return cl;
}
@Override
protected void train(List<?> features, Object actual_label) {
Margin margin = getMarginAndVariance(features, actual_label);
float loss = loss(margin);
if(loss > 0.f) {
float var = margin.getVariance();
float alpha = loss * beta;
Object missed_label = margin.getMaxIncorrectLabel();
update(features, actual_label, missed_label, alpha, beta);
}
}
protected float loss(Margin margin) {
return c - margin.get();
}
}
public static class AROWe extends AROWRegressionUDTF {
import hivemall.common.OnlineVariance;
preTrain(target);
protected void preTrain(float target) {}
public static class AROWe2 extends AROWe {
private OnlineVariance targetStdDev;
@Override
public StructObjectInspector initialize(ObjectInspector[] argOIs)
throws UDFArgumentException {
this.targetStdDev = new OnlineVariance();
return super.initialize(argOIs);
}
@Override
protected void preTrain(float target) {
targetStdDev.handle(target);
}
@Override
protected float loss(float target, float predicted) {
float stddev = (float) targetStdDev.stddev();
float e = epsilon * stddev;
return LossFunctions.epsilonInsensitiveLoss(predicted, target, e);
}
}
import hivemall.common.OnlineVariance;
preTrain(target);
protected void preTrain(float target) {}
public static class AROWe2 extends AROWe {
private OnlineVariance targetStdDev;
@Override
public StructObjectInspector initialize(ObjectInspector[] argOIs)
throws UDFArgumentException {
this.targetStdDev = new OnlineVariance();
return super.initialize(argOIs);
}
@Override
protected void preTrain(float target) {
targetStdDev.handle(target);
}
@Override
protected float loss(float target, float predicted) {
float stddev = (float) targetStdDev.stddev();
float e = epsilon * stddev;
return LossFunctions.epsilonInsensitiveLoss(predicted, target, e);
}
}
import java.util.Random;
public static double[] set(double[] src, final int index, final double value) {
public static <T> T[] set(T[] src, final int index, final T value) {
public static float[] toArray(final List<Float> lst) {
public static Float[] toObject(final float[] array) {
public static List<Float> toList(final float[] array) {
public static <T> void shuffle(final T[] array) {
shuffle(array, array.length);
}
public static <T> void shuffle(final T[] array, final int size) {
final Random rnd = new Random();
int randomPosition = rnd.nextInt(size);
T temp = array[i];
array[i] = array[randomPosition];
array[randomPosition] = temp;
}
}
public static <T> void shuffle(final T[] array, final Random rnd) {
shuffle(array, array.length, rnd);
}
public static <T> void shuffle(final T[] array, final int size) {
Random rnd = new Random();
shuffle(array, size, rnd);
}
public static <T> void shuffle(final T[] array, final int size, final Random rnd) {
for(int i = size; i > 1; i--) {
int randomPosition = rnd.nextInt(i);
swap(array, i - 1, randomPosition);
public static void swap(final Object[] arr, final int i, final int j) {
Object tmp = arr[i];
arr[i] = arr[j];
arr[j] = tmp;
}
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
private transient ObjectInspector[] retrunOIs;
final int numArgs = argOIs.length;
if(numArgs < 3) {
int numForwardObjs = numArgs - 2;
this._forwardBuffers = new Object[numBuffers][numForwardObjs];
final ArrayList<String> fieldNames = new ArrayList<String>();
final ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
this.retrunOIs = new ObjectInspector[numArgs];
ObjectInspector rawOI = argOIs[i];
ObjectInspector retOI = ObjectInspectorUtils.getStandardObjectInspector(rawOI, ObjectInspectorCopyOption.WRITABLE);
fieldOIs.add(retOI);
retrunOIs[i] = retOI;
Object arg = args[i];
ObjectInspector returnOI = retrunOIs[i];
forwardObjs[i - 2] = ObjectInspectorUtils.copyToStandardObject(arg, returnOI);
public class MapTailNUDF extends GenericUDF {
import java.util.Random;
public static double[] set(double[] src, final int index, final double value) {
public static <T> T[] set(T[] src, final int index, final T value) {
public static float[] toArray(final List<Float> lst) {
public static Float[] toObject(final float[] array) {
public static List<Float> toList(final float[] array) {
public static <T> void shuffle(final T[] array) {
shuffle(array, array.length);
}
public static <T> void shuffle(final T[] array, final Random rnd) {
shuffle(array, array.length, rnd);
}
public static <T> void shuffle(final T[] array, final int size) {
Random rnd = new Random();
shuffle(array, size, rnd);
}
public static <T> void shuffle(final T[] array, final int size, final Random rnd) {
for(int i = size; i > 1; i--) {
int randomPosition = rnd.nextInt(i);
swap(array, i - 1, randomPosition);
}
}
public static void swap(final Object[] arr, final int i, final int j) {
Object tmp = arr[i];
arr[i] = arr[j];
arr[j] = tmp;
}
amplifier.setDropoutListener(this);
import hivemall.common.RandomDropoutAmplifier;
import hivemall.common.RandomDropoutAmplifier.DropoutListener;
public class RandomAmplifierUDTF extends GenericUDTF implements DropoutListener<Object[]> {
private transient RandomDropoutAmplifier<Object[]> amplifier;
int xtimes = ((WritableConstantIntObjectInspector) argOIs[0]).getWritableConstantValue().get();
int numBuffers = ((WritableConstantIntObjectInspector) argOIs[1]).getWritableConstantValue().get();
this.amplifier = new RandomDropoutAmplifier<Object[]>(numBuffers, xtimes);
amplifier.setDropoutListener(this);
final Object[] row = new Object[args.length - 2];
Object arg = args[i];
ObjectInspector returnOI = retrunOIs[i];
row[i - 2] = ObjectInspectorUtils.copyToStandardObject(arg, returnOI);
amplifier.add(row);
amplifier.sweepAll();
this.amplifier = null;
@Override
public void onDrop(Object[] row) throws HiveException {
forward(row);
int rindex1 = rnd.nextInt(numBuffers);
int rindex2 = rnd.nextInt(numBuffers);
private transient ObjectInspector[] argOIs;
this.argOIs = argOIs;
ObjectInspector argOI = argOIs[i];
row[i - 2] = ObjectInspectorUtils.copyToStandardObject(arg, argOI);
private transient ObjectInspector[] argOIs;
this.argOIs = argOIs;
ObjectInspector argOI = argOIs[i];
row[i - 2] = ObjectInspectorUtils.copyToStandardObject(arg, argOI);
import hivemall.common.WeightValue.WeightValueWithCovar;
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.common.WeightValue.WeightValueWithCovar;
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.common.WeightValue.WeightValueWithCovar;
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.common.WeightValue.WeightValueWithCovar;
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.common.WeightValue.WeightValueWithCovar;
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.common.WeightValue.WeightValueWithCovar;
return new WeightValueWithCovar(new_w, new_cov);
protected final float value;
throw new UnsupportedOperationException();
}
public static final class WeightValueWithCovar extends WeightValue {
final float covariance;
public WeightValueWithCovar(float weight, float covariance) {
super(weight);
this.covariance = covariance;
}
public float getCovariance() {
return covariance;
}
import hivemall.common.WeightValue.WeightValueWithCovar;
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.utils.math.MathUtils;
package hivemall.utils.collections;
package hivemall.utils.math;
import hivemall.utils.math.StatsUtils;
import hivemall.utils.math.StatsUtils;
import hivemall.utils.math.StatsUtils;
import hivemall.utils.math.StatsUtils;
import hivemall.utils.math.MathUtils;
public K getAndFreeKey();
public V getAndFreeValue();
@Override
public K getAndFreeKey() {
K k = _keys[lastEntry];
_keys[lastEntry] = null;
return k;
}
@Override
public V getAndFreeValue() {
V v = _values[lastEntry];
_values[lastEntry] = null;
return v;
}
public boolean isEmpty() {
return _used > 0;
}
import hivemall.utils.collections.OpenHashTable;
import hivemall.utils.collections.OpenHashTable.IMapIterator;
protected OpenHashTable<Object, WeightValue> weights;
this.weights = new OpenHashTable<Object, WeightValue>(8192);
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.getAndFreeKey();
WeightValue v = itor.getAndFreeValue();
import hivemall.utils.collections.OpenHashTable;
OpenHashTable<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashTable<Object, WeightValue>(8192);
OpenHashTable<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashTable<Object, WeightValue>(8192);
import hivemall.utils.collections.OpenHashTable;
OpenHashTable<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashTable<Object, WeightValue>(8192);
OpenHashTable<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashTable<Object, WeightValue>(8192);
import hivemall.utils.collections.OpenHashTable;
import hivemall.utils.collections.OpenHashTable.IMapIterator;
protected Map<Object, OpenHashTable<Object, WeightValue>> label2FeatureWeight;
this.label2FeatureWeight = new HashMap<Object, OpenHashTable<Object, WeightValue>>(64);
OpenHashTable<Object, WeightValue> weights = label2map.getValue();
OpenHashTable<Object, WeightValue> weights = label2map.getValue();
OpenHashTable<Object, WeightValue> weights = label2map.getValue();
protected final float calcScore(final OpenHashTable<Object, WeightValue> weights, final List<?> features) {
protected final PredictionResult calcScoreAndVariance(final OpenHashTable<Object, WeightValue> weights, final List<?> features) {
OpenHashTable<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashTable<Object, WeightValue>(8192);
OpenHashTable<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashTable<Object, WeightValue>(8192);
for(Map.Entry<Object, OpenHashTable<Object, WeightValue>> label2map : label2FeatureWeight.entrySet()) {
OpenHashTable<Object, WeightValue> fvmap = label2map.getValue();
IMapIterator<Object, WeightValue> fvmapItor = fvmap.entries();
while(fvmapItor.next() != -1) {
Object k = fvmapItor.getAndFreeKey();
WeightValue v = fvmapItor.getAndFreeValue();
import hivemall.utils.collections.OpenHashTable;
OpenHashTable<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashTable<Object, WeightValue>(8192);
OpenHashTable<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashTable<Object, WeightValue>(8192);
import hivemall.utils.collections.OpenHashTable;
import hivemall.utils.collections.OpenHashTable.IMapIterator;
protected OpenHashTable<Object, WeightValue> weights;
this.weights = new OpenHashTable<Object, WeightValue>(8192);
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.getAndFreeKey();
WeightValue v = itor.getAndFreeValue();
}
import hivemall.common.WeightValue.WeightValueWithCovar;
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.utils.collections.OpenHashTable;
import hivemall.utils.collections.OpenHashTable.IMapIterator;
protected OpenHashTable<Object, WeightValue> weights;
this.weights = new OpenHashTable<Object, WeightValue>(8192);
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.getAndFreeKey();
WeightValue v = itor.getAndFreeValue();
import hivemall.common.WeightValue.WeightValueWithCovar;
import hivemall.utils.math.StatsUtils;
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.common.WeightValue.WeightValueWithCovar;
import hivemall.utils.math.StatsUtils;
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.common.WeightValue.WeightValueWithCovar;
import hivemall.utils.collections.OpenHashTable;
OpenHashTable<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashTable<Object, WeightValue>(8192);
OpenHashTable<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashTable<Object, WeightValue>(8192);
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.common.WeightValue.WeightValueWithCovar;
import hivemall.utils.collections.OpenHashTable;
import hivemall.utils.math.StatsUtils;
OpenHashTable<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashTable<Object, WeightValue>(8192);
OpenHashTable<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashTable<Object, WeightValue>(8192);
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.utils.collections.OpenHashTable;
import hivemall.utils.collections.OpenHashTable.IMapIterator;
protected Map<Object, OpenHashTable<Object, WeightValue>> label2FeatureWeight;
this.label2FeatureWeight = new HashMap<Object, OpenHashTable<Object, WeightValue>>(64);
OpenHashTable<Object, WeightValue> weights = label2map.getValue();
OpenHashTable<Object, WeightValue> weights = label2map.getValue();
OpenHashTable<Object, WeightValue> weights = label2map.getValue();
protected final float calcScore(final OpenHashTable<Object, WeightValue> weights, final List<?> features) {
protected final PredictionResult calcScoreAndVariance(final OpenHashTable<Object, WeightValue> weights, final List<?> features) {
OpenHashTable<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashTable<Object, WeightValue>(8192);
OpenHashTable<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashTable<Object, WeightValue>(8192);
for(Map.Entry<Object, OpenHashTable<Object, WeightValue>> label2map : label2FeatureWeight.entrySet()) {
OpenHashTable<Object, WeightValue> fvmap = label2map.getValue();
IMapIterator<Object, WeightValue> fvmapItor = fvmap.entries();
while(fvmapItor.next() != -1) {
Object k = fvmapItor.getAndFreeKey();
WeightValue v = fvmapItor.getAndFreeValue();
import hivemall.common.WeightValue.WeightValueWithCovar;
import hivemall.utils.collections.OpenHashTable;
import hivemall.utils.math.StatsUtils;
OpenHashTable<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashTable<Object, WeightValue>(8192);
OpenHashTable<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashTable<Object, WeightValue>(8192);
return new WeightValueWithCovar(new_w, new_cov);
protected final float value;
throw new UnsupportedOperationException();
}
public static final class WeightValueWithCovar extends WeightValue {
final float covariance;
public WeightValueWithCovar(float weight, float covariance) {
super(weight);
this.covariance = covariance;
}
public float getCovariance() {
return covariance;
}
import hivemall.common.WeightValue.WeightValueWithCovar;
return new WeightValueWithCovar(new_w, new_cov);
import hivemall.utils.math.MathUtils;
import hivemall.utils.collections.OpenHashTable;
import hivemall.utils.collections.OpenHashTable.IMapIterator;
protected OpenHashTable<Object, WeightValue> weights;
this.weights = new OpenHashTable<Object, WeightValue>(8192);
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.getAndFreeKey();
WeightValue v = itor.getAndFreeValue();
package hivemall.utils.collections;
package hivemall.utils.math;
package hivemall.utils.math;
import hivemall.utils.collections.OpenHashMap;
import hivemall.utils.collections.OpenHashMap.IMapIterator;
protected OpenHashMap<Object, WeightValue> weights;
this.weights = new OpenHashMap<Object, WeightValue>(8192);
Object k = itor.unsafeGetAndFreeKey();
WeightValue v = itor.unsafeGetAndFreeValue();
import hivemall.utils.collections.OpenHashMap;
OpenHashMap<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashMap<Object, WeightValue>(8192);
OpenHashMap<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashMap<Object, WeightValue>(8192);
import hivemall.utils.collections.OpenHashMap;
OpenHashMap<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashMap<Object, WeightValue>(8192);
OpenHashMap<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashMap<Object, WeightValue>(8192);
import hivemall.utils.collections.OpenHashMap;
import hivemall.utils.collections.OpenHashMap.IMapIterator;
protected Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight;
this.label2FeatureWeight = new HashMap<Object, OpenHashMap<Object, WeightValue>>(64);
OpenHashMap<Object, WeightValue> weights = label2map.getValue();
OpenHashMap<Object, WeightValue> weights = label2map.getValue();
OpenHashMap<Object, WeightValue> weights = label2map.getValue();
protected final float calcScore(final OpenHashMap<Object, WeightValue> weights, final List<?> features) {
protected final PredictionResult calcScoreAndVariance(final OpenHashMap<Object, WeightValue> weights, final List<?> features) {
OpenHashMap<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashMap<Object, WeightValue>(8192);
OpenHashMap<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashMap<Object, WeightValue>(8192);
for(Map.Entry<Object, OpenHashMap<Object, WeightValue>> label2map : label2FeatureWeight.entrySet()) {
OpenHashMap<Object, WeightValue> fvmap = label2map.getValue();
Object k = fvmapItor.unsafeGetAndFreeKey();
WeightValue v = fvmapItor.unsafeGetAndFreeValue();
import hivemall.utils.collections.OpenHashMap;
OpenHashMap<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashMap<Object, WeightValue>(8192);
OpenHashMap<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashMap<Object, WeightValue>(8192);
import hivemall.utils.collections.OpenHashMap;
import hivemall.utils.collections.OpenHashMap.IMapIterator;
protected OpenHashMap<Object, WeightValue> weights;
this.weights = new OpenHashMap<Object, WeightValue>(8192);
Object k = itor.unsafeGetAndFreeKey();
WeightValue v = itor.unsafeGetAndFreeValue();
public static int bitMask(final int numberOfBits) {
if(numberOfBits >= 32) {
return -1;
}
return (numberOfBits == 0 ? 0 : powerOf(2, numberOfBits) - 1);
}
public static int powerOf(final int value, final int powerOf) {
if(powerOf == 0) {
return 0;
}
int r = value;
r = r * value;
}
return r;
}
public static int bitsRequired(int value) {
int bits = 0;
while(value != 0) {
bits;
value >>= 1;
}
return bits;
}
public static double sigmoid(final double x) {
import hivemall.utils.collections.OpenHashMap;
import hivemall.utils.collections.OpenHashMap.IMapIterator;
protected OpenHashMap<Object, WeightValue> weights;
this.weights = new OpenHashMap<Object, WeightValue>(8192);
Object k = itor.unsafeGetAndFreeKey();
WeightValue v = itor.unsafeGetAndFreeValue();
import hivemall.utils.collections.OpenHashMap;
OpenHashMap<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashMap<Object, WeightValue>(8192);
OpenHashMap<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashMap<Object, WeightValue>(8192);
import hivemall.utils.collections.OpenHashMap;
OpenHashMap<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashMap<Object, WeightValue>(8192);
OpenHashMap<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashMap<Object, WeightValue>(8192);
import hivemall.utils.collections.OpenHashMap;
import hivemall.utils.collections.OpenHashMap.IMapIterator;
protected Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight;
this.label2FeatureWeight = new HashMap<Object, OpenHashMap<Object, WeightValue>>(64);
OpenHashMap<Object, WeightValue> weights = label2map.getValue();
OpenHashMap<Object, WeightValue> weights = label2map.getValue();
OpenHashMap<Object, WeightValue> weights = label2map.getValue();
protected final float calcScore(final OpenHashMap<Object, WeightValue> weights, final List<?> features) {
protected final PredictionResult calcScoreAndVariance(final OpenHashMap<Object, WeightValue> weights, final List<?> features) {
OpenHashMap<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashMap<Object, WeightValue>(8192);
OpenHashMap<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashMap<Object, WeightValue>(8192);
for(Map.Entry<Object, OpenHashMap<Object, WeightValue>> label2map : label2FeatureWeight.entrySet()) {
OpenHashMap<Object, WeightValue> fvmap = label2map.getValue();
Object k = fvmapItor.unsafeGetAndFreeKey();
WeightValue v = fvmapItor.unsafeGetAndFreeValue();
import hivemall.utils.collections.OpenHashMap;
OpenHashMap<Object, WeightValue> weightsToAdd = label2FeatureWeight.get(actual_label);
weightsToAdd = new OpenHashMap<Object, WeightValue>(8192);
OpenHashMap<Object, WeightValue> weightsToSub = null;
weightsToSub = new OpenHashMap<Object, WeightValue>(8192);
import hivemall.utils.collections.OpenHashMap;
import hivemall.utils.collections.OpenHashMap.IMapIterator;
protected OpenHashMap<Object, WeightValue> weights;
this.weights = new OpenHashMap<Object, WeightValue>(8192);
Object k = itor.unsafeGetAndFreeKey();
WeightValue v = itor.unsafeGetAndFreeValue();
public static int bitMask(final int numberOfBits) {
if(numberOfBits >= 32) {
return -1;
}
return (numberOfBits == 0 ? 0 : powerOf(2, numberOfBits) - 1);
}
public static int powerOf(final int value, final int powerOf) {
if(powerOf == 0) {
return 0;
}
int r = value;
r = r * value;
}
return r;
}
public static int bitsRequired(int value) {
int bits = 0;
while(value != 0) {
bits;
value >>= 1;
}
return bits;
}
public static double sigmoid(final double x) {
this.weights = new OpenHashMap<Object, WeightValue>(16384);
this.weights = new OpenHashMap<Object, WeightValue>(16384);
this.weights = new OpenHashMap<Object, WeightValue>(16384);
this.weights = new OpenHashMap<Object, WeightValue>(16384);
public final class CosineSimilarityUDF extends UDF {
FeatureValue fv = FeatureValue.parseFeatureAsString(ft);
String f = fv.getFeature();
map1.put(f, v);
FeatureValue fv = FeatureValue.parseFeatureAsString(ft);
double denom = (l1norm1 * l1norm2);
return (float) (dotp / denom);
public static FeatureValue parseFeatureAsString(String s) {
if(s == null) {
return null;
}
if(s.indexOf(':') == -1) {
return new FeatureValue(s, 1.f);
}
String[] fv = s.split(":");
if(fv.length != 1 && fv.length != 2) {
}
float v = (fv.length == 1) ? 1.f : Float.parseFloat(fv[1]);
return new FeatureValue(fv[0], v);
}
import hivemall.HivemallConstants;
import hivemall.UDTFWithOptions;
public abstract class BinaryOnlineClassifierUDTF extends UDTFWithOptions {
@Override
@Override
import hivemall.HivemallConstants;
import hivemall.UDTFWithOptions;
public abstract class MulticlassOnlineClassifierUDTF extends UDTFWithOptions {
@Override
@Override
import hivemall.HivemallConstants;
import hivemall.HivemallConstants;
import hivemall.UDTFWithOptions;
public abstract class OnlineRegressionUDTF extends UDTFWithOptions {
@Override
@Override
package hivemall;
import hivemall.HivemallConstants;
import hivemall.HivemallConstants;
import hivemall.HivemallConstants;
package hivemall.neighborhood.distance;
import hivemall.utils.hashing.MurmurHash3;
return evaluate(word, MurmurHash3.DEFAULT_NUM_FEATURES);
return MurmurHash3.murmurhash3_x86_32(word, 0, word.length(), 0x9747b28c);
return evaluate(word, MurmurHash3.DEFAULT_NUM_FEATURES);
int r = MurmurHash3.murmurhash3_x86_32(word, 0, word.length(), 0x9747b28c) % numFeatures;
import hivemall.utils.math.MathUtils;
public final class MurmurHash3 {
int h = murmurhash3_x86_32(data, 0, data.length(), 0x9747b28c);
int r = MathUtils.moduloPowerOfTwo(h, 16777216);
if(r < 0) {
}
return r;
public static int moduloPowerOfTwo(final int x, final int powerOfTwoY) {
return x & (powerOfTwoY - 1);
}
import hivemall.utils.hashing.MurmurHash3;
return evaluate(values, null, MurmurHash3.DEFAULT_NUM_FEATURES);
return evaluate(values, prefix, MurmurHash3.DEFAULT_NUM_FEATURES);
ary[i] = MurmurHash3.murmurhash3(data, numFeatures);
import hivemall.utils.hashing.MurmurHash3;
List<Integer> hashValues = ArrayHashValuesUDF.hashValues(values, null, MurmurHash3.DEFAULT_NUM_FEATURES);
public static final int DEFAULT_NUM_FEATURES = 16777216;
int r = MathUtils.moduloPowerOfTwo(h, DEFAULT_NUM_FEATURES);
public class RandomizedAmplifier<T> {
public RandomizedAmplifier(int numBuffers, int xtimes) {
import hivemall.common.RandomizedAmplifier;
import hivemall.common.RandomizedAmplifier.DropoutListener;
private transient RandomizedAmplifier<Object[]> amplifier;
this.amplifier = new RandomizedAmplifier<Object[]>(numBuffers, xtimes);
import hivemall.utils.hashing.MurmurHash3;
int hashval = MurmurHash3.murmurhash3(s);
package hivemall;
import hivemall.HivemallConstants;
import hivemall.UDTFWithOptions;
public abstract class BinaryOnlineClassifierUDTF extends UDTFWithOptions {
@Override
@Override
import hivemall.HivemallConstants;
import hivemall.UDTFWithOptions;
public abstract class MulticlassOnlineClassifierUDTF extends UDTFWithOptions {
@Override
@Override
import hivemall.utils.hashing.MurmurHash3;
int hashval = MurmurHash3.murmurhash3(s);
public static FeatureValue parseFeatureAsString(String s) {
if(s == null) {
return null;
}
if(s.indexOf(':') == -1) {
return new FeatureValue(s, 1.f);
}
String[] fv = s.split(":");
if(fv.length != 1 && fv.length != 2) {
}
float v = (fv.length == 1) ? 1.f : Float.parseFloat(fv[1]);
return new FeatureValue(fv[0], v);
}
public class RandomizedAmplifier<T> {
public RandomizedAmplifier(int numBuffers, int xtimes) {
import hivemall.HivemallConstants;
package hivemall.ftvec.amplify;
import hivemall.HivemallConstants;
package hivemall.ftvec.amplify;
import hivemall.HivemallConstants;
import hivemall.common.RandomizedAmplifier;
import hivemall.common.RandomizedAmplifier.DropoutListener;
private transient RandomizedAmplifier<Object[]> amplifier;
this.amplifier = new RandomizedAmplifier<Object[]>(numBuffers, xtimes);
import hivemall.utils.hashing.MurmurHash3;
return evaluate(values, null, MurmurHash3.DEFAULT_NUM_FEATURES);
return evaluate(values, prefix, MurmurHash3.DEFAULT_NUM_FEATURES);
ary[i] = MurmurHash3.murmurhash3(data, numFeatures);
import hivemall.utils.hashing.MurmurHash3;
List<Integer> hashValues = ArrayHashValuesUDF.hashValues(values, null, MurmurHash3.DEFAULT_NUM_FEATURES);
import hivemall.utils.hashing.MurmurHash3;
return evaluate(word, MurmurHash3.DEFAULT_NUM_FEATURES);
return MurmurHash3.murmurhash3_x86_32(word, 0, word.length(), 0x9747b28c);
return evaluate(word, MurmurHash3.DEFAULT_NUM_FEATURES);
int r = MurmurHash3.murmurhash3_x86_32(word, 0, word.length(), 0x9747b28c) % numFeatures;
import hivemall.HivemallConstants;
import hivemall.HivemallConstants;
import hivemall.UDTFWithOptions;
public abstract class OnlineRegressionUDTF extends UDTFWithOptions {
@Override
@Override
public static int moduloPowerOfTwo(final int x, final int powerOfTwoY) {
return x & (powerOfTwoY - 1);
}
return result & 0x7FFFFFFF;
public static FeatureValue parseFeatureAsString(String s) {
if(s.indexOf(':') == -1) {
return new FeatureValue(s, 1.f);
}
String[] fv = s.split(":");
if(fv.length != 1 && fv.length != 2) {
}
float v = (fv.length == 1) ? 1.f : Float.parseFloat(fv[1]);
return new FeatureValue(fv[0], v);
}
import java.util.Collections;
public float evaluate(List<String> ftvec1, List<String> ftvec2, boolean noWeight) {
if(noWeight) {
map1.put(ft, 1.f);
} else {
FeatureValue fv = FeatureValue.parseFeatureAsString(ft);
float v = fv.getValue();
String f = fv.getFeature();
map1.put(f, v);
}
if(noWeight) {
if(map1.containsKey(ft)) {
}
} else {
FeatureValue fv = FeatureValue.parseFeatureAsString(ft);
float v2 = fv.getValue();
String f2 = fv.getFeature();
Float v1 = map1.get(f2);
if(v1 != null) {
}
public float evaluate(List<Integer> ftvec1, List<Integer> ftvec2) {
if(ftvec1 == null || ftvec2 == null) {
return 0.f;
}
Collections.sort(ftvec1);
double dotp = 0.f;
for(Integer f : ftvec2) {
if(Collections.binarySearch(ftvec1, f) >= 0) {
}
}
double l1norm1 = Math.sqrt(ftvec1.size());
double l1norm2 = Math.sqrt(ftvec2.size());
double denom = (l1norm1 * l1norm2);
if(denom <= 0.f) {
return 0.f;
} else {
return (float) (dotp / denom);
}
}
public final class MinHashUDTF extends UDTFWithOptions {
public static FeatureValue parseFeatureAsString(String s) {
if(s.indexOf(':') == -1) {
return new FeatureValue(s, 1.f);
}
String[] fv = s.split(":");
if(fv.length != 1 && fv.length != 2) {
}
float v = (fv.length == 1) ? 1.f : Float.parseFloat(fv[1]);
return new FeatureValue(fv[0], v);
}
import java.util.Collections;
public float evaluate(List<String> ftvec1, List<String> ftvec2, boolean noWeight) {
if(noWeight) {
map1.put(ft, 1.f);
} else {
FeatureValue fv = FeatureValue.parseFeatureAsString(ft);
float v = fv.getValue();
String f = fv.getFeature();
map1.put(f, v);
}
if(noWeight) {
if(map1.containsKey(ft)) {
}
} else {
FeatureValue fv = FeatureValue.parseFeatureAsString(ft);
float v2 = fv.getValue();
String f2 = fv.getFeature();
Float v1 = map1.get(f2);
if(v1 != null) {
}
public float evaluate(List<Integer> ftvec1, List<Integer> ftvec2) {
if(ftvec1 == null || ftvec2 == null) {
return 0.f;
}
Collections.sort(ftvec1);
double dotp = 0.f;
for(Integer f : ftvec2) {
if(Collections.binarySearch(ftvec1, f) >= 0) {
}
}
double l1norm1 = Math.sqrt(ftvec1.size());
double l1norm2 = Math.sqrt(ftvec2.size());
double denom = (l1norm1 * l1norm2);
if(denom <= 0.f) {
return 0.f;
} else {
return (float) (dotp / denom);
}
}
public final class MinHashUDTF extends UDTFWithOptions {
return result & 0x7FFFFFFF;
public List<Integer> evaluate(List<Integer> features, int numHashes, int keyGroups)
public List<Integer> evaluate(List<String> features, int numHashes, int keyGroups, boolean noWeight)
public List<Integer> evaluate(List<Integer> features, int numHashes, int keyGroups)
public List<Integer> evaluate(List<String> features, int numHashes, int keyGroups, boolean noWeight)
return hammingDistance(a, b);
private static int hammingDistance(final Long a, final Long b) {
long xor = a.longValue() ^ b.longValue();
return Long.bitCount(xor);
}
public int evaluate(long a) {
return Long.bitCount(a);
public int evaluate(long a, long b) {
long innerProduct = a & b;
return Long.bitCount(innerProduct);
long innerProduct = a.get(i).longValue() & b.get(i).longValue();
public int evaluate(long a, long b) {
public static int hammingDistance(final long a, final long b) {
return Long.bitCount(a ^ b);
import java.math.BigInteger;
public String evaluate(List<Integer> features) throws HiveException {
public String evaluate(List<Integer> features, int numHashes) throws HiveException {
public String evaluate(List<String> features, boolean noWeight) throws HiveException {
public String evaluate(List<String> features, int numHashes, boolean noWeight)
private static String computeSignatures(final List<FeatureValue> features, final int numHashes, final int[] seeds)
BigInteger value = BigInteger.valueOf(0L);
value = value.setBit(i);
return value.toString();
import java.math.BigInteger;
public int evaluate(String a, String b) {
BigInteger ai = new BigInteger(a);
BigInteger bi = new BigInteger(b);
return hammingDistance(ai, bi);
}
public static int hammingDistance(final BigInteger a, final BigInteger b) {
BigInteger xor = a.xor(b);
return xor.bitCount();
}
import java.math.BigInteger;
public float evaluate(String a, String b) {
return evaluate(a, b, 128);
}
public float evaluate(String a, String b, int k) {
BigInteger ai = new BigInteger(a);
BigInteger bi = new BigInteger(b);
int countMatches = k - HammingDistanceUDF.hammingDistance(ai, bi);
float jaccard = countMatches / (float) k;
return 2.f * (jaccard - 0.5f);
}
import java.math.BigInteger;
public int evaluate(String a) {
BigInteger ai = new BigInteger(a);
return ai.bitCount();
}
public int evaluate(String a, String b) {
BigInteger ai = new BigInteger(a);
BigInteger bi = new BigInteger(b);
BigInteger innerProduct = ai.and(bi);
return innerProduct.bitCount();
}
import java.math.BigInteger;
public int evaluate(long a, long b) {
return hammingDistance(a, b);
}
public int evaluate(String a, String b) {
BigInteger ai = new BigInteger(a);
BigInteger bi = new BigInteger(b);
return hammingDistance(ai, bi);
public static int hammingDistance(final long a, final long b) {
return Long.bitCount(a ^ b);
}
public static int hammingDistance(final BigInteger a, final BigInteger b) {
BigInteger xor = a.xor(b);
return xor.bitCount();
}
import java.math.BigInteger;
public int evaluate(long a) {
return Long.bitCount(a);
}
public int evaluate(String a) {
BigInteger ai = new BigInteger(a);
return ai.bitCount();
public int evaluate(long a, long b) {
long innerProduct = a & b;
return Long.bitCount(innerProduct);
public int evaluate(String a, String b) {
BigInteger ai = new BigInteger(a);
BigInteger bi = new BigInteger(b);
BigInteger innerProduct = ai.and(bi);
return innerProduct.bitCount();
}
long innerProduct = a.get(i).longValue() & b.get(i).longValue();
import java.math.BigInteger;
public String evaluate(List<Integer> features) throws HiveException {
public String evaluate(List<Integer> features, int numHashes) throws HiveException {
public String evaluate(List<String> features, boolean noWeight) throws HiveException {
public String evaluate(List<String> features, int numHashes, boolean noWeight)
private static String computeSignatures(final List<FeatureValue> features, final int numHashes, final int[] seeds)
BigInteger value = BigInteger.valueOf(0L);
value = value.setBit(i);
return value.toString();
package hivemall.knn.distance;
package hivemall.knn.distance;
package hivemall.knn.distance;
package hivemall.knn.distance;
package hivemall.knn.lsh;
import static hivemall.HivemallConstants.BIAS_CLAUSE;
import static hivemall.HivemallConstants.BIAS_CLAUSE_INT;
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
if(!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
&& !BIGINT_TYPE_NAME.equals(keyTypeName)) {
this.parseX = STRING_TYPE_NAME.equals(keyTypeName);
this.biasKey = INT_TYPE_NAME.equals(featureRawOI.getTypeName()) ? BIAS_CLAUSE_INT
: new Text(BIAS_CLAUSE);
import static hivemall.HivemallConstants.BIAS_CLAUSE;
import static hivemall.HivemallConstants.BIAS_CLAUSE_INT;
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
if(!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
&& !BIGINT_TYPE_NAME.equals(keyTypeName)) {
this.parseX = STRING_TYPE_NAME.equals(keyTypeName);
if(!STRING_TYPE_NAME.equals(labelTypeName) && !INT_TYPE_NAME.equals(labelTypeName)) {
this.biasKey = INT_TYPE_NAME.equals(featureRawOI.getTypeName()) ? BIAS_CLAUSE_INT
: new Text(BIAS_CLAUSE);
import static hivemall.HivemallConstants.INT_TYPE_NAME;
if(!INT_TYPE_NAME.equals(argOIs[0].getTypeName())) {
import static hivemall.HivemallConstants.INT_TYPE_NAME;
if(!INT_TYPE_NAME.equals(argOIs[0].getTypeName())) {
if(!INT_TYPE_NAME.equals(argOIs[1].getTypeName())) {
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
if(!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
&& !BIGINT_TYPE_NAME.equals(keyTypeName)) {
this.parseX = STRING_TYPE_NAME.equals(keyTypeName);
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.FLOAT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
if(!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
&& !BIGINT_TYPE_NAME.equals(keyTypeName)) {
if(!FLOAT_TYPE_NAME.equals(weightOI.getTypeName())) {
import static hivemall.HivemallConstants.BIAS_CLAUSE;
import static hivemall.HivemallConstants.BIAS_CLAUSE_INT;
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
this.biasKey = (featureOutputOI.getTypeName() == INT_TYPE_NAME) ? BIAS_CLAUSE_INT
: new Text(BIAS_CLAUSE);
if(keyTypeName != STRING_TYPE_NAME && keyTypeName != INT_TYPE_NAME
&& keyTypeName != BIGINT_TYPE_NAME) {
throw new UDFArgumentTypeException(0, "1st argument must be List of key type [Int|BitInt|Text]: "
this.parseX = (keyTypeName == STRING_TYPE_NAME);
this.biasKey = INT_TYPE_NAME.equals(featureOutputOI.getTypeName()) ? BIAS_CLAUSE_INT
if(!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
&& !BIGINT_TYPE_NAME.equals(keyTypeName)) {
this.parseX = STRING_TYPE_NAME.equals(keyTypeName);
import static hivemall.HivemallConstants.BIAS_CLAUSE;
import static hivemall.HivemallConstants.BIAS_CLAUSE_INT;
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
if(!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
&& !BIGINT_TYPE_NAME.equals(keyTypeName)) {
this.parseX = STRING_TYPE_NAME.equals(keyTypeName);
this.biasKey = INT_TYPE_NAME.equals(featureRawOI.getTypeName()) ? BIAS_CLAUSE_INT
: new Text(BIAS_CLAUSE);
import static hivemall.HivemallConstants.BIAS_CLAUSE;
import static hivemall.HivemallConstants.BIAS_CLAUSE_INT;
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
if(!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
&& !BIGINT_TYPE_NAME.equals(keyTypeName)) {
this.parseX = STRING_TYPE_NAME.equals(keyTypeName);
if(!STRING_TYPE_NAME.equals(labelTypeName) && !INT_TYPE_NAME.equals(labelTypeName)) {
this.biasKey = INT_TYPE_NAME.equals(featureRawOI.getTypeName()) ? BIAS_CLAUSE_INT
: new Text(BIAS_CLAUSE);
import static hivemall.HivemallConstants.INT_TYPE_NAME;
if(!INT_TYPE_NAME.equals(argOIs[0].getTypeName())) {
import static hivemall.HivemallConstants.INT_TYPE_NAME;
if(!INT_TYPE_NAME.equals(argOIs[0].getTypeName())) {
if(!INT_TYPE_NAME.equals(argOIs[1].getTypeName())) {
package hivemall.knn.distance;
package hivemall.knn.distance;
package hivemall.knn.distance;
package hivemall.knn.distance;
package hivemall.knn.lsh;
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
if(!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
&& !BIGINT_TYPE_NAME.equals(keyTypeName)) {
this.parseX = STRING_TYPE_NAME.equals(keyTypeName);
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.FLOAT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
if(!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
&& !BIGINT_TYPE_NAME.equals(keyTypeName)) {
if(!FLOAT_TYPE_NAME.equals(weightOI.getTypeName())) {
import static hivemall.HivemallConstants.BIAS_CLAUSE;
import static hivemall.HivemallConstants.BIAS_CLAUSE_INT;
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
this.biasKey = INT_TYPE_NAME.equals(featureOutputOI.getTypeName()) ? BIAS_CLAUSE_INT
: new Text(BIAS_CLAUSE);
if(!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
&& !BIGINT_TYPE_NAME.equals(keyTypeName)) {
throw new UDFArgumentTypeException(0, "1st argument must be List of key type [Int|BitInt|Text]: "
this.parseX = STRING_TYPE_NAME.equals(keyTypeName);
return loss / (2.f * sqnorm);
float eta = loss / (2.f * sqnorm);
import java.lang.reflect.Array;
public static Object[] subarray(Object[] array, int startIndexInclusive, int endIndexExclusive) {
if(array == null) {
return null;
}
if(startIndexInclusive < 0) {
startIndexInclusive = 0;
}
if(endIndexExclusive > array.length) {
endIndexExclusive = array.length;
}
int newSize = endIndexExclusive - startIndexInclusive;
Class<?> type = array.getClass().getComponentType();
if(newSize <= 0) {
return (Object[]) Array.newInstance(type, 0);
}
Object[] subarray = (Object[]) Array.newInstance(type, newSize);
System.arraycopy(array, startIndexInclusive, subarray, 0, newSize);
return subarray;
}
return loss / (2.f * sqnorm);
float eta = loss / (2.f * sqnorm);
import java.lang.reflect.Array;
public static Object[] subarray(Object[] array, int startIndexInclusive, int endIndexExclusive) {
if(array == null) {
return null;
}
if(startIndexInclusive < 0) {
startIndexInclusive = 0;
}
if(endIndexExclusive > array.length) {
endIndexExclusive = array.length;
}
int newSize = endIndexExclusive - startIndexInclusive;
Class<?> type = array.getClass().getComponentType();
if(newSize <= 0) {
return (Object[]) Array.newInstance(type, 0);
}
Object[] subarray = (Object[]) Array.newInstance(type, newSize);
System.arraycopy(array, startIndexInclusive, subarray, 0, newSize);
return subarray;
}
import java.util.List;
public int evaluate(List<String> words) throws UDFArgumentException {
return evaluate(words, MurmurHash3.DEFAULT_NUM_FEATURES);
public int evaluate(List<String> words, int numFeatures) throws UDFArgumentException {
final int size = words.size();
if(size == 0) {
b.append(words.get(0));
String v = words.get(i);
b.append(v);
return evaluate(values, prefix, numFeatures, false);
public List<Integer> evaluate(List<String> values, String prefix, int numFeatures, boolean useIndexAsPrefix) {
return hashValues(values, prefix, numFeatures, useIndexAsPrefix);
}
static List<Integer> hashValues(List<String> values, String prefix, int numFeatures, boolean useIndexAsPrefix) {
if(useIndexAsPrefix) {
}
return evaluate(values, prefix, false);
}
public List<String> evaluate(List<String> values, String prefix, boolean useIndexAsPrefix) {
List<Integer> hashValues = ArrayHashValuesUDF.hashValues(values, null, MurmurHash3.DEFAULT_NUM_FEATURES, useIndexAsPrefix);
public List<Integer> evaluate(List<String> values, String prefix, boolean useIndexAsPrefix) {
return evaluate(values, prefix, MurmurHash3.DEFAULT_NUM_FEATURES, useIndexAsPrefix);
}
public List<Integer> evaluate(List<String> values, String prefix, boolean useIndexAsPrefix) {
return evaluate(values, prefix, MurmurHash3.DEFAULT_NUM_FEATURES, useIndexAsPrefix);
public List<Integer> evaluate(List<String> values, String prefix, int numFeatures) {
return evaluate(values, prefix, numFeatures, false);
}
public List<Integer> evaluate(List<String> values, String prefix, int numFeatures, boolean useIndexAsPrefix) {
return hashValues(values, prefix, numFeatures, useIndexAsPrefix);
}
static List<Integer> hashValues(List<String> values, String prefix, int numFeatures, boolean useIndexAsPrefix) {
if(useIndexAsPrefix) {
}
return evaluate(values, prefix, false);
}
public List<String> evaluate(List<String> values, String prefix, boolean useIndexAsPrefix) {
List<Integer> hashValues = ArrayHashValuesUDF.hashValues(values, null, MurmurHash3.DEFAULT_NUM_FEATURES, useIndexAsPrefix);
import java.util.List;
public int evaluate(List<String> words) throws UDFArgumentException {
return evaluate(words, MurmurHash3.DEFAULT_NUM_FEATURES);
public int evaluate(List<String> words, int numFeatures) throws UDFArgumentException {
final int size = words.size();
if(size == 0) {
b.append(words.get(0));
String v = words.get(i);
b.append(v);
public class AddBiasUDF extends UDF {
import hivemall.utils.WritableUtils;
import org.apache.hadoop.io.Text;
public Text terminate() {
return WritableUtils.val(partial.label);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public DoubleWritable terminate() {
return val(partial.positiveSum / partial.positiveCnt);
return val(0.d);
return val(partial.negativeSum / partial.negativeCnt);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public DoubleWritable terminate() {
return val(partial.positiveSum / partial.positiveCnt);
return val(0.d);
return val(partial.negativeSum / partial.negativeCnt);
import hivemall.utils.WritableUtils;
import org.apache.hadoop.io.Text;
public List<Text> evaluate(List<String> ftvec) {
public List<Text> evaluate(List<String> ftvec, String biasClause) {
public List<Text> evaluate(List<String> ftvec, String biasClause, float biasValue) {
return WritableUtils.val(newvec);
import org.apache.hadoop.io.FloatWritable;
private List<FloatWritable> partial;
FloatWritable[] array = new FloatWritable[nDims];
partial.set(feature, new FloatWritable(weight));
public List<FloatWritable> terminatePartial() {
public boolean merge(List<FloatWritable> other) {
this.partial = new ArrayList<FloatWritable>(other);
FloatWritable x = other.set(i, null);
public List<FloatWritable> terminate() {
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.IntWritable;
public Map<IntWritable, FloatWritable> evaluate(Map<IntWritable, FloatWritable> arg) {
Map<IntWritable, FloatWritable> ret = new TreeMap<IntWritable, FloatWritable>();
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
public List<IntWritable> evaluate(List<String> values) {
public List<IntWritable> evaluate(List<String> values, String prefix) {
public List<IntWritable> evaluate(List<String> values, String prefix, boolean useIndexAsPrefix) {
public List<IntWritable> evaluate(List<String> values, String prefix, int numFeatures) {
public List<IntWritable> evaluate(List<String> values, String prefix, int numFeatures, boolean useIndexAsPrefix) {
static List<IntWritable> hashValues(List<String> values, String prefix, int numFeatures, boolean useIndexAsPrefix) {
final IntWritable[] ary = new IntWritable[size];
ary[i] = val(MurmurHash3.murmurhash3(data, numFeatures));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
public List<Text> evaluate(List<String> values, String prefix) {
public List<Text> evaluate(List<String> values, String prefix, boolean useIndexAsPrefix) {
List<IntWritable> hashValues = ArrayHashValuesUDF.hashValues(values, null, MurmurHash3.DEFAULT_NUM_FEATURES, useIndexAsPrefix);
final Text[] stringValues = new Text[len];
IntWritable v = hashValues.get(i);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
public IntWritable evaluate(String word) throws UDFArgumentException {
public IntWritable evaluate(String word, boolean rawValue) throws UDFArgumentException {
return val(MurmurHash3.murmurhash3_x86_32(word, 0, word.length(), 0x9747b28c));
public IntWritable evaluate(String word, int numFeatures) throws UDFArgumentException {
return val(r);
public IntWritable evaluate(List<String> words) throws UDFArgumentException {
public IntWritable evaluate(List<String> words, int numFeatures) throws UDFArgumentException {
return val(0);
import static hivemall.utils.WritableUtils.val;
import java.util.List;
import org.apache.hadoop.io.IntWritable;
public IntWritable evaluate(String word) {
public IntWritable evaluate(String word, boolean rawValue) {
return val(sha1(word));
public IntWritable evaluate(String word, int numFeatures) {
return val(r);
public IntWritable evaluate(List<String> words) {
return evaluate(words, DEFAULT_NUM_FEATURES);
public IntWritable evaluate(List<String> words, int numFeatures) {
int wlength = words.size();
if(wlength == 0) {
return val(0);
b.append(words.get(0));
b.append(words.get(i));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.Text;
public Text evaluate(String s, double min, double max) {
public Text evaluate(String s, float min, float max) {
return val(ret);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.FloatWritable;
public FloatWritable evaluate(float value, float min, float max) {
return val(min_max_normalization(value, min, max));
public FloatWritable evaluate(float value, double min, double max) {
return val(min_max_normalization(value, (float) min, (float) max));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.FloatWritable;
public FloatWritable evaluate(float value, double mean, double stddev) {
public FloatWritable evaluate(float value, float mean, float stddev) {
return val((value - mean) / stddev);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.FloatWritable;
public FloatWritable evaluate(List<String> ftvec1, List<String> ftvec2, boolean noWeight) {
return val(0.f);
final double denom = (l1norm1 * l1norm2);
return val(0.f);
return val((float) (dotp / denom));
public FloatWritable evaluate(List<Integer> ftvec1, List<Integer> ftvec2) {
return val(0.f);
final double denom = (l1norm1 * l1norm2);
return val(0.f);
return val((float) (dotp / denom));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
public IntWritable evaluate(long a, long b) {
return val(hammingDistance(a, b));
public IntWritable evaluate(String a, String b) {
return val(hammingDistance(ai, bi));
public IntWritable evaluate(List<LongWritable> a, List<LongWritable> b) {
final List<LongWritable> r;
return val(result);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.FloatWritable;
public FloatWritable evaluate(long a, long b) {
public FloatWritable evaluate(long a, long b, int k) {
return val(2.f * (jaccard - 0.5f));
public FloatWritable evaluate(String a, String b) {
public FloatWritable evaluate(String a, String b, int k) {
return val(2.f * (jaccard - 0.5f));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
public IntWritable evaluate(long a) {
return val(Long.bitCount(a));
public IntWritable evaluate(String a) {
return val(ai.bitCount());
public IntWritable evaluate(List<Long> a) {
return val(result);
public IntWritable evaluate(long a, long b) {
return val(Long.bitCount(innerProduct));
public IntWritable evaluate(String a, String b) {
return val(innerProduct.bitCount());
public IntWritable evaluate(List<Long> a, List<Long> b) {
return val(result);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
public List<IntWritable> evaluate(List<Integer> features) throws HiveException {
public List<IntWritable> evaluate(List<Integer> features, int numHashes, int keyGroups)
public List<IntWritable> evaluate(List<String> features, boolean noWeight) throws HiveException {
public List<IntWritable> evaluate(List<String> features, int numHashes, int keyGroups, boolean noWeight)
private static List<IntWritable> computeSignatures(final List<FeatureValue> features, final int numHashes, final int keyGroups, final int[] seeds)
final IntWritable[] hashes = new IntWritable[numHashes];
hashes[i] = val(getSignature(minhashes, keyGroups));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.Text;
public Text evaluate(List<Integer> features) throws HiveException {
public Text evaluate(List<Integer> features, int numHashes) throws HiveException {
return val(computeSignatures(featureList, numHashes, seeds));
public Text evaluate(List<String> features, boolean noWeight) throws HiveException {
public Text evaluate(List<String> features, int numHashes, boolean noWeight)
return val(computeSignatures(featureList, numHashes, seeds));
import org.apache.hadoop.io.FloatWritable;
public List<FloatWritable> evaluate(int nDims) {
return new ArrayList<FloatWritable>(nDims);
import org.apache.hadoop.io.IntWritable;
public List<IntWritable> evaluate(List<IntWritable> original, IntWritable target) {
public List<IntWritable> evaluate(List<IntWritable> original, List<IntWritable> targets) {
import org.apache.hadoop.io.IntWritable;
public List<IntWritable> evaluate(List<IntWritable> ary) {
Set<IntWritable> s = new TreeSet<IntWritable>(ary);
return new ArrayList<IntWritable>(s);
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
public List<IntWritable> evaluate(List<IntWritable> original, IntWritable key) {
int toIndex = original.lastIndexOf(key);
public List<Text> evaluate(List<Text> original, Text key) {
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
public List<IntWritable> evaluate(List<IntWritable> original, IntWritable key) {
int fromIndex = original.indexOf(key);
public List<Text> evaluate(List<Text> original, Text key) {
import org.apache.hadoop.io.IntWritable;
public List<IntWritable> evaluate(List<IntWritable> array, int fromIndex, int toIndex) {
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.FloatWritable;
public FloatWritable evaluate(float x) {
public FloatWritable evaluate(double x) {
return val(v);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.IntWritable;
public DoubleWritable evaluate(Map<IntWritable, FloatWritable> map, List<IntWritable> keys) {
for(IntWritable k : keys) {
FloatWritable v = map.get(k);
return val(sum);
import hivemall.utils.WritableUtils;
import org.apache.hadoop.io.Text;
public List<Text> evaluate(String query) {
public List<Text> evaluate(String query, String regex) {
return WritableUtils.val(words);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.BooleanWritable;
public BooleanWritable evaluate(String word) {
return val(Arrays.binarySearch(stopwords, word) != -1);
import org.apache.hadoop.io.Text;
while(original.remove(target))
;
public List<Text> evaluate(List<Text> original, Text target) {
while(original.remove(target))
;
return original;
}
import org.apache.hadoop.io.LongWritable;
public static LongWritable val(final long v) {
return new LongWritable(v);
}
package hivemall.tools.math;
import hivemall.utils.WritableUtils;
import org.apache.hadoop.io.Text;
public Text terminate() {
return WritableUtils.val(partial.label);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public DoubleWritable terminate() {
return val(partial.positiveSum / partial.positiveCnt);
return val(0.d);
return val(partial.negativeSum / partial.negativeCnt);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public DoubleWritable terminate() {
return val(partial.positiveSum / partial.positiveCnt);
return val(0.d);
return val(partial.negativeSum / partial.negativeCnt);
import hivemall.utils.WritableUtils;
import org.apache.hadoop.io.Text;
public class AddBiasUDF extends UDF {
public List<Text> evaluate(List<String> ftvec) {
public List<Text> evaluate(List<String> ftvec, String biasClause) {
public List<Text> evaluate(List<String> ftvec, String biasClause, float biasValue) {
return WritableUtils.val(newvec);
import org.apache.hadoop.io.FloatWritable;
private List<FloatWritable> partial;
FloatWritable[] array = new FloatWritable[nDims];
partial.set(feature, new FloatWritable(weight));
public List<FloatWritable> terminatePartial() {
public boolean merge(List<FloatWritable> other) {
this.partial = new ArrayList<FloatWritable>(other);
FloatWritable x = other.set(i, null);
public List<FloatWritable> terminate() {
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.IntWritable;
public Map<IntWritable, FloatWritable> evaluate(Map<IntWritable, FloatWritable> arg) {
Map<IntWritable, FloatWritable> ret = new TreeMap<IntWritable, FloatWritable>();
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
public List<IntWritable> evaluate(List<String> values) {
public List<IntWritable> evaluate(List<String> values, String prefix) {
public List<IntWritable> evaluate(List<String> values, String prefix, boolean useIndexAsPrefix) {
public List<IntWritable> evaluate(List<String> values, String prefix, int numFeatures) {
public List<IntWritable> evaluate(List<String> values, String prefix, int numFeatures, boolean useIndexAsPrefix) {
static List<IntWritable> hashValues(List<String> values, String prefix, int numFeatures, boolean useIndexAsPrefix) {
final IntWritable[] ary = new IntWritable[size];
ary[i] = val(MurmurHash3.murmurhash3(data, numFeatures));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
public List<Text> evaluate(List<String> values, String prefix) {
public List<Text> evaluate(List<String> values, String prefix, boolean useIndexAsPrefix) {
List<IntWritable> hashValues = ArrayHashValuesUDF.hashValues(values, null, MurmurHash3.DEFAULT_NUM_FEATURES, useIndexAsPrefix);
final Text[] stringValues = new Text[len];
IntWritable v = hashValues.get(i);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
public IntWritable evaluate(String word) throws UDFArgumentException {
public IntWritable evaluate(String word, boolean rawValue) throws UDFArgumentException {
return val(MurmurHash3.murmurhash3_x86_32(word, 0, word.length(), 0x9747b28c));
public IntWritable evaluate(String word, int numFeatures) throws UDFArgumentException {
return val(r);
public IntWritable evaluate(List<String> words) throws UDFArgumentException {
public IntWritable evaluate(List<String> words, int numFeatures) throws UDFArgumentException {
return val(0);
import static hivemall.utils.WritableUtils.val;
import java.util.List;
import org.apache.hadoop.io.IntWritable;
public IntWritable evaluate(String word) {
public IntWritable evaluate(String word, boolean rawValue) {
return val(sha1(word));
public IntWritable evaluate(String word, int numFeatures) {
return val(r);
public IntWritable evaluate(List<String> words) {
return evaluate(words, DEFAULT_NUM_FEATURES);
public IntWritable evaluate(List<String> words, int numFeatures) {
int wlength = words.size();
if(wlength == 0) {
return val(0);
b.append(words.get(0));
b.append(words.get(i));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.Text;
public Text evaluate(String s, double min, double max) {
public Text evaluate(String s, float min, float max) {
return val(ret);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.FloatWritable;
public FloatWritable evaluate(float value, float min, float max) {
return val(min_max_normalization(value, min, max));
public FloatWritable evaluate(float value, double min, double max) {
return val(min_max_normalization(value, (float) min, (float) max));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.FloatWritable;
public FloatWritable evaluate(float value, double mean, double stddev) {
public FloatWritable evaluate(float value, float mean, float stddev) {
return val((value - mean) / stddev);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.FloatWritable;
public FloatWritable evaluate(List<String> ftvec1, List<String> ftvec2, boolean noWeight) {
return val(0.f);
final double denom = (l1norm1 * l1norm2);
return val(0.f);
return val((float) (dotp / denom));
public FloatWritable evaluate(List<Integer> ftvec1, List<Integer> ftvec2) {
return val(0.f);
final double denom = (l1norm1 * l1norm2);
return val(0.f);
return val((float) (dotp / denom));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
public IntWritable evaluate(long a, long b) {
return val(hammingDistance(a, b));
public IntWritable evaluate(String a, String b) {
return val(hammingDistance(ai, bi));
public IntWritable evaluate(List<LongWritable> a, List<LongWritable> b) {
final List<LongWritable> r;
return val(result);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.FloatWritable;
public FloatWritable evaluate(long a, long b) {
public FloatWritable evaluate(long a, long b, int k) {
return val(2.f * (jaccard - 0.5f));
public FloatWritable evaluate(String a, String b) {
public FloatWritable evaluate(String a, String b, int k) {
return val(2.f * (jaccard - 0.5f));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
public IntWritable evaluate(long a) {
return val(Long.bitCount(a));
public IntWritable evaluate(String a) {
return val(ai.bitCount());
public IntWritable evaluate(List<Long> a) {
return val(result);
public IntWritable evaluate(long a, long b) {
return val(Long.bitCount(innerProduct));
public IntWritable evaluate(String a, String b) {
return val(innerProduct.bitCount());
public IntWritable evaluate(List<Long> a, List<Long> b) {
return val(result);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.IntWritable;
public List<IntWritable> evaluate(List<Integer> features) throws HiveException {
public List<IntWritable> evaluate(List<Integer> features, int numHashes, int keyGroups)
public List<IntWritable> evaluate(List<String> features, boolean noWeight) throws HiveException {
public List<IntWritable> evaluate(List<String> features, int numHashes, int keyGroups, boolean noWeight)
private static List<IntWritable> computeSignatures(final List<FeatureValue> features, final int numHashes, final int keyGroups, final int[] seeds)
final IntWritable[] hashes = new IntWritable[numHashes];
hashes[i] = val(getSignature(minhashes, keyGroups));
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.Text;
public Text evaluate(List<Integer> features) throws HiveException {
public Text evaluate(List<Integer> features, int numHashes) throws HiveException {
return val(computeSignatures(featureList, numHashes, seeds));
public Text evaluate(List<String> features, boolean noWeight) throws HiveException {
public Text evaluate(List<String> features, int numHashes, boolean noWeight)
return val(computeSignatures(featureList, numHashes, seeds));
import org.apache.hadoop.io.FloatWritable;
public List<FloatWritable> evaluate(int nDims) {
return new ArrayList<FloatWritable>(nDims);
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
public List<IntWritable> evaluate(List<IntWritable> original, IntWritable target) {
while(original.remove(target))
;
public List<IntWritable> evaluate(List<IntWritable> original, List<IntWritable> targets) {
public List<Text> evaluate(List<Text> original, Text target) {
while(original.remove(target))
;
return original;
}
import org.apache.hadoop.io.IntWritable;
public List<IntWritable> evaluate(List<IntWritable> ary) {
Set<IntWritable> s = new TreeSet<IntWritable>(ary);
return new ArrayList<IntWritable>(s);
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
public List<IntWritable> evaluate(List<IntWritable> original, IntWritable key) {
int toIndex = original.lastIndexOf(key);
public List<Text> evaluate(List<Text> original, Text key) {
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
public List<IntWritable> evaluate(List<IntWritable> original, IntWritable key) {
int fromIndex = original.indexOf(key);
public List<Text> evaluate(List<Text> original, Text key) {
import org.apache.hadoop.io.IntWritable;
public List<IntWritable> evaluate(List<IntWritable> array, int fromIndex, int toIndex) {
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.IntWritable;
public DoubleWritable evaluate(Map<IntWritable, FloatWritable> map, List<IntWritable> keys) {
for(IntWritable k : keys) {
FloatWritable v = map.get(k);
return val(sum);
package hivemall.tools.math;
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.FloatWritable;
public FloatWritable evaluate(float x) {
public FloatWritable evaluate(double x) {
return val(v);
import hivemall.utils.WritableUtils;
import org.apache.hadoop.io.Text;
public List<Text> evaluate(String query) {
public List<Text> evaluate(String query, String regex) {
return WritableUtils.val(words);
import static hivemall.utils.WritableUtils.val;
import org.apache.hadoop.io.BooleanWritable;
public BooleanWritable evaluate(String word) {
return val(Arrays.binarySearch(stopwords, word) != -1);
import org.apache.hadoop.hive.ql.exec.MapredContextAccessor;
MapredContext ctx = MapredContextAccessor.get();
}
import org.apache.hadoop.hive.ql.exec.MapredContextAccessor;
MapredContext ctx = MapredContextAccessor.get();
import hivemall.utils.HiveUtils;
String rawArgs = HiveUtils.getConstString(argOIs[2]);
import hivemall.utils.HiveUtils;
String rawArgs = HiveUtils.getConstString(argOIs[2]);
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantStringObjectInspector;
public static String getConstString(ObjectInspector oi) {
WritableConstantStringObjectInspector stringOI = (WritableConstantStringObjectInspector) oi;
return stringOI.getWritableConstantValue().toString();
}
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
public static String getConstString(ObjectInspector oi) throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
public static Object getConstValue(ObjectInspector oi) throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
return ((ConstantObjectInspector) oi).getWritableConstantValue();
}
public static PrimitiveObjectInspector asPrimitiveObjectInspector(ObjectInspector oi)
throws UDFArgumentException {
if(oi.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentException("Is not PrimitiveObjectInspector: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
return (PrimitiveObjectInspector) oi;
}
import hivemall.utils.hadoop.HiveUtils;
import hivemall.utils.lang.ArrayUtils;
import hivemall.utils.hadoop.WritableUtils;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import hivemall.utils.hadoop.WritableUtils;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import hivemall.utils.hadoop.HiveUtils;
import static hivemall.utils.hadoop.WritableUtils.val;
import hivemall.utils.hadoop.HiveUtils;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import hivemall.utils.hadoop.WritableUtils;
import static hivemall.utils.hadoop.WritableUtils.val;
package hivemall.utils.hadoop;
package hivemall.utils.hadoop;
package hivemall.utils.lang;
package hivemall.utils.lang;
Object key = keyOI.getPrimitiveJavaObject(arg);
import hivemall.utils.hadoop.HiveUtils;
String rawArgs = HiveUtils.getConstString(argOIs[2]);
import hivemall.utils.lang.ArrayUtils;
import hivemall.utils.hadoop.WritableUtils;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import hivemall.utils.hadoop.WritableUtils;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import hivemall.utils.hadoop.HiveUtils;
String rawArgs = HiveUtils.getConstString(argOIs[2]);
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import static hivemall.utils.hadoop.WritableUtils.val;
import hivemall.utils.hadoop.WritableUtils;
import static hivemall.utils.hadoop.WritableUtils.val;
package hivemall.utils.hadoop;
package hivemall.utils.lang;
package hivemall.utils.lang;
import java.io.FileInputStream;
import java.io.InputStreamReader;
import java.net.URI;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hive.ql.exec.MapredContext;
import org.apache.hadoop.hive.ql.exec.MapredContextAccessor;
import org.apache.hadoop.io.compress.CodecPool;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.io.compress.CompressionInputStream;
import org.apache.hadoop.io.compress.Decompressor;
private static void loadValues(OpenHashMap<Object, Object> map, File file, PrimitiveObjectInspector keyOI, PrimitiveObjectInspector valueOI)
final BufferedReader reader = getBufferedReader(file);
private static BufferedReader getBufferedReader(File file) throws IOException {
URI fileuri = file.toURI();
Path path = new Path(fileuri);
MapredContext context = MapredContextAccessor.get();
Configuration conf = context.getJobConf();
CompressionCodecFactory ccf = new CompressionCodecFactory(conf);
CompressionCodec codec = ccf.getCodec(path);
if(codec == null) {
return new BufferedReader(new FileReader(file));
} else {
Decompressor decompressor = CodecPool.getDecompressor(codec);
FileInputStream fis = new FileInputStream(file);
CompressionInputStream cis = codec.createInputStream(fis, decompressor);
BufferedReader br = new BufferedReader(new InputStreamReader(cis));
return br;
}
}
import java.io.FileInputStream;
import java.io.InputStreamReader;
import java.net.URI;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hive.ql.exec.MapredContext;
import org.apache.hadoop.hive.ql.exec.MapredContextAccessor;
import org.apache.hadoop.io.compress.CodecPool;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.io.compress.CompressionInputStream;
import org.apache.hadoop.io.compress.Decompressor;
private static void loadValues(OpenHashMap<Object, Object> map, File file, PrimitiveObjectInspector keyOI, PrimitiveObjectInspector valueOI)
final BufferedReader reader = getBufferedReader(file);
private static BufferedReader getBufferedReader(File file) throws IOException {
URI fileuri = file.toURI();
Path path = new Path(fileuri);
MapredContext context = MapredContextAccessor.get();
Configuration conf = context.getJobConf();
CompressionCodecFactory ccf = new CompressionCodecFactory(conf);
CompressionCodec codec = ccf.getCodec(path);
if(codec == null) {
return new BufferedReader(new FileReader(file));
} else {
Decompressor decompressor = CodecPool.getDecompressor(codec);
FileInputStream fis = new FileInputStream(file);
CompressionInputStream cis = codec.createInputStream(fis, decompressor);
BufferedReader br = new BufferedReader(new InputStreamReader(cis));
return br;
}
}
import hivemall.LearnerBaseUDTF;
public abstract class BinaryOnlineClassifierUDTF extends LearnerBaseUDTF {
import hivemall.LearnerBaseUDTF;
public abstract class MulticlassOnlineClassifierUDTF extends LearnerBaseUDTF {
import hivemall.LearnerBaseUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
public abstract class OnlineRegressionUDTF extends LearnerBaseUDTF {
protected PrimitiveObjectInspector featureInputOI;
protected String preloadedModelFile;
if(preloadedModelFile != null) {
loadPredictionModel(weights, preloadedModelFile, featureInputOI);
}
this.count = 1;
protected PrimitiveObjectInspector processFeaturesOI(ObjectInspector arg)
throws UDFArgumentException {
return HiveUtils.asPrimitiveObjectInspector(featureRawOI);
opts.addOption("loadmodel", true, "Model file name in the distributed cache");
String modelfile = null;
modelfile = cl.getOptionValue("loadmodel");
this.preloadedModelFile = modelfile;
import hivemall.utils.hadoop.HadoopUtils;
LazySimpleSerDe serde = HiveUtils.getKeyValueLineSerde(keyOI, valueOI);
final BufferedReader reader = HadoopUtils.getBufferedReader(file);
import java.util.Properties;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.serde2.SerDeException;
import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
public static LazySimpleSerDe getKeyValueLineSerde(PrimitiveObjectInspector keyOI, PrimitiveObjectInspector valueOI)
throws SerDeException {
LazySimpleSerDe serde = new LazySimpleSerDe();
Configuration conf = new Configuration();
Properties tbl = new Properties();
tbl.setProperty("columns", "key,value");
serde.initialize(conf, tbl);
return serde;
}
public static LazySimpleSerDe getLineSerde(PrimitiveObjectInspector... OIs)
throws SerDeException {
if(OIs.length == 0) {
throw new IllegalArgumentException("OIs must be specified");
}
LazySimpleSerDe serde = new LazySimpleSerDe();
Configuration conf = new Configuration();
Properties tbl = new Properties();
StringBuilder columnNames = new StringBuilder();
StringBuilder columnTypes = new StringBuilder();
columnTypes.append(OIs[i].getTypeName()).append(',');
}
columnNames.deleteCharAt(columnNames.length() - 1);
columnTypes.deleteCharAt(columnTypes.length() - 1);
tbl.setProperty("columns", columnNames.toString());
tbl.setProperty("columns.types", columnTypes.toString());
serde.initialize(conf, tbl);
return serde;
}
import hivemall.common.WeightValue.WeightValueWithCovar;
import org.apache.hadoop.hive.serde2.objectinspector.StructField;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
return false;
StructField keyRef = lineOI.getStructFieldRef("key");
StructField valueRef = lineOI.getStructFieldRef("value");
Object k = ((PrimitiveObjectInspector) keyRef.getFieldObjectInspector()).getPrimitiveWritableObject(f0);
float v = ((FloatObjectInspector) valueRef.getFieldObjectInspector()).get(f1);
loadPredictionModel(map, f, keyOI, valueOI, covarOI);
LazySimpleSerDe serde = HiveUtils.getLineSerde(keyOI, valueOI, covarOI);
StructField c1ref = lineOI.getStructFieldRef("c1");
StructField c2ref = lineOI.getStructFieldRef("c2");
StructField c3ref = lineOI.getStructFieldRef("c3");
Object f2 = fields.get(2);
if(f0 == null || f1 == null || f2 == null) {
Object k = ((PrimitiveObjectInspector) c1ref.getFieldObjectInspector()).getPrimitiveWritableObject(f0);
float v = ((FloatObjectInspector) c2ref.getFieldObjectInspector()).get(f1);
float cov = ((FloatObjectInspector) c3ref.getFieldObjectInspector()).get(f1);
map.put(k, new WeightValueWithCovar(v, cov));
protected boolean returnCovariance() {
return true;
}
@Override
import hivemall.common.WeightValue.WeightValueWithCovar;
return getReturnOI(featureRawOI);
protected StructObjectInspector getReturnOI(ObjectInspector featureRawOI) {
ArrayList<String> fieldNames = new ArrayList<String>();
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
fieldNames.add("feature");
ObjectInspector featureOI = ObjectInspectorUtils.getStandardObjectInspector(featureRawOI);
fieldOIs.add(featureOI);
fieldNames.add("weight");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
if(returnCovariance()) {
fieldNames.add("covar");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
}
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
}
if(returnCovariance()) {
final Object[] forwardMapObj = new Object[3];
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.unsafeGetAndFreeKey();
WeightValueWithCovar v = (WeightValueWithCovar) itor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.get());
FloatWritable cov = new FloatWritable(v.getCovariance());
forwardMapObj[0] = k;
forwardMapObj[1] = fv;
forwardMapObj[2] = cov;
forward(forwardMapObj);
}
} else {
final Object[] forwardMapObj = new Object[2];
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.unsafeGetAndFreeKey();
WeightValue v = itor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.get());
forwardMapObj[0] = k;
forwardMapObj[1] = fv;
forward(forwardMapObj);
}
protected boolean returnCovariance() {
return true;
}
@Override
protected boolean returnCovariance() {
return true;
}
@Override
protected boolean returnCovariance() {
return true;
}
@Override
protected boolean returnCovariance() {
return true;
}
@Override
import hivemall.common.WeightValue.WeightValueWithCovar;
return getReturnOI(labelRawOI, featureRawOI);
protected StructObjectInspector getReturnOI(ObjectInspector labelRawOI, ObjectInspector featureRawOI) {
ArrayList<String> fieldNames = new ArrayList<String>();
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
fieldNames.add("label");
ObjectInspector labelOI = ObjectInspectorUtils.getStandardObjectInspector(labelRawOI);
fieldOIs.add(labelOI);
fieldNames.add("feature");
ObjectInspector featureOI = ObjectInspectorUtils.getStandardObjectInspector(featureRawOI);
fieldOIs.add(featureOI);
fieldNames.add("weight");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
if(returnCovariance()) {
fieldNames.add("covar");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
}
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
}
if(returnCovariance()) {
final Object[] forwardMapObj = new Object[4];
for(Map.Entry<Object, OpenHashMap<Object, WeightValue>> label2map : label2FeatureWeight.entrySet()) {
Object label = label2map.getKey();
forwardMapObj[0] = label;
OpenHashMap<Object, WeightValue> fvmap = label2map.getValue();
IMapIterator<Object, WeightValue> fvmapItor = fvmap.entries();
while(fvmapItor.next() != -1) {
Object k = fvmapItor.unsafeGetAndFreeKey();
WeightValueWithCovar v = (WeightValueWithCovar) fvmapItor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.getValue());
FloatWritable cov = new FloatWritable(v.getCovariance());
forwardMapObj[1] = k;
forwardMapObj[2] = fv;
forwardMapObj[3] = cov;
forward(forwardMapObj);
}
}
} else {
final Object[] forwardMapObj = new Object[3];
for(Map.Entry<Object, OpenHashMap<Object, WeightValue>> label2map : label2FeatureWeight.entrySet()) {
Object label = label2map.getKey();
forwardMapObj[0] = label;
OpenHashMap<Object, WeightValue> fvmap = label2map.getValue();
IMapIterator<Object, WeightValue> fvmapItor = fvmap.entries();
while(fvmapItor.next() != -1) {
Object k = fvmapItor.unsafeGetAndFreeKey();
WeightValue v = fvmapItor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.getValue());
forwardMapObj[1] = k;
forwardMapObj[2] = fv;
forward(forwardMapObj);
}
protected boolean returnCovariance() {
return true;
}
@Override
protected boolean returnCovariance() {
return true;
}
@Override
import hivemall.utils.hadoop.HiveUtils;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@Deprecated
protected PrimitiveObjectInspector processFeaturesOI(ObjectInspector arg)
throws UDFArgumentException {
return HiveUtils.asPrimitiveObjectInspector(featureOI);
import hivemall.common.WeightValue.WeightValueWithCovar;
return getReturnOI(featureOutputOI);
protected StructObjectInspector getReturnOI(ObjectInspector featureOutputOI) {
ArrayList<String> fieldNames = new ArrayList<String>();
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
fieldNames.add("feature");
ObjectInspector featureOI = ObjectInspectorUtils.getStandardObjectInspector(featureOutputOI);
fieldOIs.add(featureOI);
fieldNames.add("weight");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
if(returnCovariance()) {
fieldNames.add("covar");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
}
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
}
if(returnCovariance()) {
final Object[] forwardMapObj = new Object[3];
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.unsafeGetAndFreeKey();
WeightValueWithCovar v = (WeightValueWithCovar) itor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.get());
FloatWritable cov = new FloatWritable(v.getCovariance());
forwardMapObj[0] = k;
forwardMapObj[1] = fv;
forwardMapObj[2] = cov;
forward(forwardMapObj);
}
} else {
final Object[] forwardMapObj = new Object[2];
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.unsafeGetAndFreeKey();
WeightValue v = itor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.get());
forwardMapObj[0] = k;
forwardMapObj[1] = fv;
forward(forwardMapObj);
}
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.Options;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
protected boolean feature_hashing;
protected float bias;
protected String preloadedModelFile;
public LearnerBaseUDTF() {}
@Override
protected Options getOptions() {
Options opts = new Options();
opts.addOption("fh", "fhash", false, "Enable feature hashing (only used when feature is TEXT type) [default: off]");
opts.addOption("b", "bias", true, "Bias clause [default 0.0 (disable)]");
opts.addOption("loadmodel", true, "Model file name in the distributed cache");
return opts;
}
@Override
protected CommandLine processOptions(ObjectInspector[] argOIs) throws UDFArgumentException {
boolean fhashFlag = false;
float biasValue = 0.f;
String modelfile = null;
CommandLine cl = null;
if(argOIs.length >= 3) {
String rawArgs = HiveUtils.getConstString(argOIs[2]);
cl = parseOptions(rawArgs);
if(cl.hasOption("fh")) {
fhashFlag = true;
}
String biasStr = cl.getOptionValue("b");
if(biasStr != null) {
biasValue = Float.parseFloat(biasStr);
}
modelfile = cl.getOptionValue("loadmodel");
}
this.feature_hashing = fhashFlag;
this.bias = biasValue;
this.preloadedModelFile = modelfile;
return cl;
}
float cov = ((FloatObjectInspector) c3ref.getFieldObjectInspector()).get(f2);
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
PrimitiveObjectInspector featureInputOI = processFeaturesOI(argOIs[0]);
this.labelOI = (IntObjectInspector) argOIs[1];
processOptions(argOIs);
ObjectInspector featureOutputOI = featureInputOI;
if(parseX && feature_hashing) {
featureOutputOI = PrimitiveObjectInspectorFactory.javaIntObjectInspector;
}
if(bias != 0.f) {
this.biasKey = INT_TYPE_NAME.equals(featureOutputOI.getTypeName()) ? BIAS_CLAUSE_INT
: new Text(BIAS_CLAUSE);
} else {
this.biasKey = null;
}
this.weights = new OpenHashMap<Object, WeightValue>(16384);
if(preloadedModelFile != null) {
loadPredictionModel(weights, preloadedModelFile, featureInputOI);
}
return getReturnOI(featureOutputOI);
}
protected PrimitiveObjectInspector processFeaturesOI(ObjectInspector arg)
throws UDFArgumentException {
this.featureListOI = (ListObjectInspector) arg;
return HiveUtils.asPrimitiveObjectInspector(featureRawOI);
import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
import hivemall.utils.hadoop.HadoopUtils;
import hivemall.utils.hadoop.HiveUtils;
import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import org.apache.hadoop.hive.serde2.SerDeException;
import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.StructField;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector;
protected PrimitiveObjectInspector labelInputOI;
PrimitiveObjectInspector featureInputOI = processFeaturesOI(argOIs[0]);
this.labelInputOI = HiveUtils.asPrimitiveObjectInspector(argOIs[1]);
String labelTypeName = labelInputOI.getTypeName();
if(!STRING_TYPE_NAME.equals(labelTypeName) && !INT_TYPE_NAME.equals(labelTypeName)) {
throw new UDFArgumentTypeException(0, "label must be a type [Int|Text]: "
labelTypeName);
}
processOptions(argOIs);
ObjectInspector featureOutputOI = featureInputOI;
if(parseX && feature_hashing) {
featureOutputOI = PrimitiveObjectInspectorFactory.javaIntObjectInspector;
}
if(bias != 0.f) {
this.biasKey = INT_TYPE_NAME.equals(featureOutputOI.getTypeName()) ? BIAS_CLAUSE_INT
: new Text(BIAS_CLAUSE);
} else {
this.biasKey = null;
}
this.label2FeatureWeight = new HashMap<Object, OpenHashMap<Object, WeightValue>>(64);
if(preloadedModelFile != null) {
loadPredictionModel(label2FeatureWeight, preloadedModelFile, labelInputOI, featureInputOI);
}
return getReturnOI(labelInputOI, featureOutputOI);
}
protected PrimitiveObjectInspector processFeaturesOI(ObjectInspector arg)
throws UDFArgumentException {
this.featureListOI = (ListObjectInspector) arg;
return HiveUtils.asPrimitiveObjectInspector(featureRawOI);
Object label = ObjectInspectorUtils.copyToStandardObject(args[1], labelInputOI);
protected void loadPredictionModel(Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight, String filename, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI) {
try {
if(returnCovariance()) {
loadPredictionModel(label2FeatureWeight, new File(filename), labelOI, featureOI, writableFloatObjectInspector, writableFloatObjectInspector);
} else {
loadPredictionModel(label2FeatureWeight, new File(filename), labelOI, featureOI, writableFloatObjectInspector);
}
} catch (IOException e) {
} catch (SerDeException e) {
}
}
private static void loadPredictionModel(Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI)
throws IOException, SerDeException {
if(!file.exists()) {
return;
}
if(!file.getName().endsWith(".crc")) {
if(file.isDirectory()) {
for(File f : file.listFiles()) {
loadPredictionModel(label2FeatureWeight, f, labelOI, featureOI, weightOI);
}
} else {
LazySimpleSerDe serde = HiveUtils.getLineSerde(labelOI, featureOI, weightOI);
StructObjectInspector lineOI = (StructObjectInspector) serde.getObjectInspector();
StructField c1ref = lineOI.getStructFieldRef("c1");
StructField c2ref = lineOI.getStructFieldRef("c2");
StructField c3ref = lineOI.getStructFieldRef("c3");
final BufferedReader reader = HadoopUtils.getBufferedReader(file);
try {
String line;
while((line = reader.readLine()) != null) {
Text lineText = new Text(line);
Object lineObj = serde.deserialize(lineText);
List<Object> fields = lineOI.getStructFieldsDataAsList(lineObj);
Object f0 = fields.get(0);
Object f1 = fields.get(1);
Object f2 = fields.get(2);
if(f0 == null || f1 == null || f2 == null) {
}
Object label = ((PrimitiveObjectInspector) c1ref.getFieldObjectInspector()).getPrimitiveWritableObject(f0);
OpenHashMap<Object, WeightValue> map = label2FeatureWeight.get(label);
if(map == null) {
map = new OpenHashMap<Object, WeightValue>(8192);
label2FeatureWeight.put(label, map);
}
Object k = ((PrimitiveObjectInspector) c2ref.getFieldObjectInspector()).getPrimitiveWritableObject(f1);
float v = ((FloatObjectInspector) c3ref.getFieldObjectInspector()).get(f2);
map.put(k, new WeightValue(v));
}
} finally {
reader.close();
}
}
}
}
private static void loadPredictionModel(Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
throws IOException, SerDeException {
if(!file.exists()) {
return;
}
if(!file.getName().endsWith(".crc")) {
if(file.isDirectory()) {
for(File f : file.listFiles()) {
loadPredictionModel(label2FeatureWeight, f, labelOI, featureOI, weightOI, covarOI);
}
} else {
LazySimpleSerDe serde = HiveUtils.getLineSerde(labelOI, featureOI, weightOI, covarOI);
StructObjectInspector lineOI = (StructObjectInspector) serde.getObjectInspector();
StructField c1ref = lineOI.getStructFieldRef("c1");
StructField c2ref = lineOI.getStructFieldRef("c2");
StructField c3ref = lineOI.getStructFieldRef("c3");
StructField c4ref = lineOI.getStructFieldRef("c4");
final BufferedReader reader = HadoopUtils.getBufferedReader(file);
try {
String line;
while((line = reader.readLine()) != null) {
Text lineText = new Text(line);
Object lineObj = serde.deserialize(lineText);
List<Object> fields = lineOI.getStructFieldsDataAsList(lineObj);
Object f0 = fields.get(0);
Object f1 = fields.get(1);
Object f2 = fields.get(2);
Object f3 = fields.get(3);
if(f0 == null || f1 == null || f2 == null || f3 == null) {
}
Object label = ((PrimitiveObjectInspector) c1ref.getFieldObjectInspector()).getPrimitiveWritableObject(f0);
OpenHashMap<Object, WeightValue> map = label2FeatureWeight.get(label);
if(map == null) {
map = new OpenHashMap<Object, WeightValue>(8192);
label2FeatureWeight.put(label, map);
}
Object k = ((PrimitiveObjectInspector) c2ref.getFieldObjectInspector()).getPrimitiveWritableObject(f1);
float v = ((FloatObjectInspector) c3ref.getFieldObjectInspector()).get(f2);
float cov = ((FloatObjectInspector) c4ref.getFieldObjectInspector()).get(f3);
map.put(k, new WeightValueWithCovar(v, cov));
}
} finally {
reader.close();
}
}
}
}
public boolean isFeatureHashingEnabled() {
return feature_hashing;
}
public float getBias() {
return bias;
}
protected boolean returnCovariance() {
return true;
}
@Override
import hivemall.LearnerBaseUDTF;
import hivemall.common.WeightValue.WeightValueWithCovar;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
public abstract class BinaryOnlineClassifierUDTF extends LearnerBaseUDTF {
PrimitiveObjectInspector featureInputOI = processFeaturesOI(argOIs[0]);
this.labelOI = (IntObjectInspector) argOIs[1];
processOptions(argOIs);
ObjectInspector featureOutputOI = featureInputOI;
if(parseX && feature_hashing) {
featureOutputOI = PrimitiveObjectInspectorFactory.javaIntObjectInspector;
}
if(bias != 0.f) {
this.biasKey = INT_TYPE_NAME.equals(featureOutputOI.getTypeName()) ? BIAS_CLAUSE_INT
: new Text(BIAS_CLAUSE);
} else {
this.biasKey = null;
}
this.weights = new OpenHashMap<Object, WeightValue>(16384);
if(preloadedModelFile != null) {
loadPredictionModel(weights, preloadedModelFile, featureInputOI);
}
return getReturnOI(featureOutputOI);
}
protected PrimitiveObjectInspector processFeaturesOI(ObjectInspector arg)
throws UDFArgumentException {
this.featureListOI = (ListObjectInspector) arg;
return HiveUtils.asPrimitiveObjectInspector(featureRawOI);
}
protected StructObjectInspector getReturnOI(ObjectInspector featureRawOI) {
if(returnCovariance()) {
fieldNames.add("covar");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
if(returnCovariance()) {
final Object[] forwardMapObj = new Object[3];
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.unsafeGetAndFreeKey();
WeightValueWithCovar v = (WeightValueWithCovar) itor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.get());
FloatWritable cov = new FloatWritable(v.getCovariance());
forwardMapObj[0] = k;
forwardMapObj[1] = fv;
forwardMapObj[2] = cov;
forward(forwardMapObj);
}
} else {
final Object[] forwardMapObj = new Object[2];
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.unsafeGetAndFreeKey();
WeightValue v = itor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.get());
forwardMapObj[0] = k;
forwardMapObj[1] = fv;
forward(forwardMapObj);
}
protected boolean returnCovariance() {
return true;
}
@Override
protected boolean returnCovariance() {
return true;
}
@Override
protected boolean returnCovariance() {
return true;
}
@Override
protected boolean returnCovariance() {
return true;
}
@Override
import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
import hivemall.LearnerBaseUDTF;
import hivemall.common.WeightValue.WeightValueWithCovar;
import hivemall.utils.hadoop.HadoopUtils;
import hivemall.utils.hadoop.HiveUtils;
import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import org.apache.hadoop.hive.serde2.SerDeException;
import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.StructField;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector;
public abstract class MulticlassOnlineClassifierUDTF extends LearnerBaseUDTF {
protected PrimitiveObjectInspector labelInputOI;
PrimitiveObjectInspector featureInputOI = processFeaturesOI(argOIs[0]);
this.labelInputOI = HiveUtils.asPrimitiveObjectInspector(argOIs[1]);
String labelTypeName = labelInputOI.getTypeName();
if(!STRING_TYPE_NAME.equals(labelTypeName) && !INT_TYPE_NAME.equals(labelTypeName)) {
throw new UDFArgumentTypeException(0, "label must be a type [Int|Text]: "
labelTypeName);
}
processOptions(argOIs);
ObjectInspector featureOutputOI = featureInputOI;
if(parseX && feature_hashing) {
featureOutputOI = PrimitiveObjectInspectorFactory.javaIntObjectInspector;
}
if(bias != 0.f) {
this.biasKey = INT_TYPE_NAME.equals(featureOutputOI.getTypeName()) ? BIAS_CLAUSE_INT
: new Text(BIAS_CLAUSE);
} else {
this.biasKey = null;
}
this.label2FeatureWeight = new HashMap<Object, OpenHashMap<Object, WeightValue>>(64);
if(preloadedModelFile != null) {
loadPredictionModel(label2FeatureWeight, preloadedModelFile, labelInputOI, featureInputOI);
}
return getReturnOI(labelInputOI, featureOutputOI);
}
protected PrimitiveObjectInspector processFeaturesOI(ObjectInspector arg)
throws UDFArgumentException {
this.featureListOI = (ListObjectInspector) arg;
return HiveUtils.asPrimitiveObjectInspector(featureRawOI);
}
protected StructObjectInspector getReturnOI(ObjectInspector labelRawOI, ObjectInspector featureRawOI) {
if(returnCovariance()) {
fieldNames.add("covar");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
Object label = ObjectInspectorUtils.copyToStandardObject(args[1], labelInputOI);
if(returnCovariance()) {
final Object[] forwardMapObj = new Object[4];
for(Map.Entry<Object, OpenHashMap<Object, WeightValue>> label2map : label2FeatureWeight.entrySet()) {
Object label = label2map.getKey();
forwardMapObj[0] = label;
OpenHashMap<Object, WeightValue> fvmap = label2map.getValue();
IMapIterator<Object, WeightValue> fvmapItor = fvmap.entries();
while(fvmapItor.next() != -1) {
Object k = fvmapItor.unsafeGetAndFreeKey();
WeightValueWithCovar v = (WeightValueWithCovar) fvmapItor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.getValue());
FloatWritable cov = new FloatWritable(v.getCovariance());
forwardMapObj[1] = k;
forwardMapObj[2] = fv;
forwardMapObj[3] = cov;
forward(forwardMapObj);
}
}
} else {
final Object[] forwardMapObj = new Object[3];
for(Map.Entry<Object, OpenHashMap<Object, WeightValue>> label2map : label2FeatureWeight.entrySet()) {
Object label = label2map.getKey();
forwardMapObj[0] = label;
OpenHashMap<Object, WeightValue> fvmap = label2map.getValue();
IMapIterator<Object, WeightValue> fvmapItor = fvmap.entries();
while(fvmapItor.next() != -1) {
Object k = fvmapItor.unsafeGetAndFreeKey();
WeightValue v = fvmapItor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.getValue());
forwardMapObj[1] = k;
forwardMapObj[2] = fv;
forward(forwardMapObj);
}
protected void loadPredictionModel(Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight, String filename, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI) {
try {
if(returnCovariance()) {
loadPredictionModel(label2FeatureWeight, new File(filename), labelOI, featureOI, writableFloatObjectInspector, writableFloatObjectInspector);
} else {
loadPredictionModel(label2FeatureWeight, new File(filename), labelOI, featureOI, writableFloatObjectInspector);
}
} catch (IOException e) {
} catch (SerDeException e) {
}
}
private static void loadPredictionModel(Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI)
throws IOException, SerDeException {
if(!file.exists()) {
return;
}
if(!file.getName().endsWith(".crc")) {
if(file.isDirectory()) {
for(File f : file.listFiles()) {
loadPredictionModel(label2FeatureWeight, f, labelOI, featureOI, weightOI);
}
} else {
LazySimpleSerDe serde = HiveUtils.getLineSerde(labelOI, featureOI, weightOI);
StructObjectInspector lineOI = (StructObjectInspector) serde.getObjectInspector();
StructField c1ref = lineOI.getStructFieldRef("c1");
StructField c2ref = lineOI.getStructFieldRef("c2");
StructField c3ref = lineOI.getStructFieldRef("c3");
final BufferedReader reader = HadoopUtils.getBufferedReader(file);
try {
String line;
while((line = reader.readLine()) != null) {
Text lineText = new Text(line);
Object lineObj = serde.deserialize(lineText);
List<Object> fields = lineOI.getStructFieldsDataAsList(lineObj);
Object f0 = fields.get(0);
Object f1 = fields.get(1);
Object f2 = fields.get(2);
if(f0 == null || f1 == null || f2 == null) {
}
Object label = ((PrimitiveObjectInspector) c1ref.getFieldObjectInspector()).getPrimitiveWritableObject(f0);
OpenHashMap<Object, WeightValue> map = label2FeatureWeight.get(label);
if(map == null) {
map = new OpenHashMap<Object, WeightValue>(8192);
label2FeatureWeight.put(label, map);
}
Object k = ((PrimitiveObjectInspector) c2ref.getFieldObjectInspector()).getPrimitiveWritableObject(f1);
float v = ((FloatObjectInspector) c3ref.getFieldObjectInspector()).get(f2);
map.put(k, new WeightValue(v));
}
} finally {
reader.close();
}
}
}
}
private static void loadPredictionModel(Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
throws IOException, SerDeException {
if(!file.exists()) {
return;
}
if(!file.getName().endsWith(".crc")) {
if(file.isDirectory()) {
for(File f : file.listFiles()) {
loadPredictionModel(label2FeatureWeight, f, labelOI, featureOI, weightOI, covarOI);
}
} else {
LazySimpleSerDe serde = HiveUtils.getLineSerde(labelOI, featureOI, weightOI, covarOI);
StructObjectInspector lineOI = (StructObjectInspector) serde.getObjectInspector();
StructField c1ref = lineOI.getStructFieldRef("c1");
StructField c2ref = lineOI.getStructFieldRef("c2");
StructField c3ref = lineOI.getStructFieldRef("c3");
StructField c4ref = lineOI.getStructFieldRef("c4");
final BufferedReader reader = HadoopUtils.getBufferedReader(file);
try {
String line;
while((line = reader.readLine()) != null) {
Text lineText = new Text(line);
Object lineObj = serde.deserialize(lineText);
List<Object> fields = lineOI.getStructFieldsDataAsList(lineObj);
Object f0 = fields.get(0);
Object f1 = fields.get(1);
Object f2 = fields.get(2);
Object f3 = fields.get(3);
if(f0 == null || f1 == null || f2 == null || f3 == null) {
}
Object label = ((PrimitiveObjectInspector) c1ref.getFieldObjectInspector()).getPrimitiveWritableObject(f0);
OpenHashMap<Object, WeightValue> map = label2FeatureWeight.get(label);
if(map == null) {
map = new OpenHashMap<Object, WeightValue>(8192);
label2FeatureWeight.put(label, map);
}
Object k = ((PrimitiveObjectInspector) c2ref.getFieldObjectInspector()).getPrimitiveWritableObject(f1);
float v = ((FloatObjectInspector) c3ref.getFieldObjectInspector()).get(f2);
float cov = ((FloatObjectInspector) c4ref.getFieldObjectInspector()).get(f3);
map.put(k, new WeightValueWithCovar(v, cov));
}
} finally {
reader.close();
}
}
}
}
protected boolean returnCovariance() {
return true;
}
@Override
protected boolean returnCovariance() {
return true;
}
@Override
import hivemall.utils.hadoop.HiveUtils;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@Deprecated
protected PrimitiveObjectInspector processFeaturesOI(ObjectInspector arg)
throws UDFArgumentException {
return HiveUtils.asPrimitiveObjectInspector(featureOI);
import hivemall.LearnerBaseUDTF;
import hivemall.common.WeightValue.WeightValueWithCovar;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
public abstract class OnlineRegressionUDTF extends LearnerBaseUDTF {
protected PrimitiveObjectInspector featureInputOI;
if(preloadedModelFile != null) {
loadPredictionModel(weights, preloadedModelFile, featureInputOI);
}
this.count = 1;
return getReturnOI(featureOutputOI);
protected PrimitiveObjectInspector processFeaturesOI(ObjectInspector arg)
throws UDFArgumentException {
return HiveUtils.asPrimitiveObjectInspector(featureRawOI);
protected StructObjectInspector getReturnOI(ObjectInspector featureOutputOI) {
ArrayList<String> fieldNames = new ArrayList<String>();
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
fieldNames.add("feature");
ObjectInspector featureOI = ObjectInspectorUtils.getStandardObjectInspector(featureOutputOI);
fieldOIs.add(featureOI);
fieldNames.add("weight");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
if(returnCovariance()) {
fieldNames.add("covar");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
if(returnCovariance()) {
final Object[] forwardMapObj = new Object[3];
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.unsafeGetAndFreeKey();
WeightValueWithCovar v = (WeightValueWithCovar) itor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.get());
FloatWritable cov = new FloatWritable(v.getCovariance());
forwardMapObj[0] = k;
forwardMapObj[1] = fv;
forwardMapObj[2] = cov;
forward(forwardMapObj);
}
} else {
final Object[] forwardMapObj = new Object[2];
IMapIterator<Object, WeightValue> itor = weights.entries();
while(itor.next() != -1) {
Object k = itor.unsafeGetAndFreeKey();
WeightValue v = itor.unsafeGetAndFreeValue();
FloatWritable fv = new FloatWritable(v.get());
forwardMapObj[0] = k;
forwardMapObj[1] = fv;
forward(forwardMapObj);
}
import hivemall.utils.hadoop.HadoopUtils;
LazySimpleSerDe serde = HiveUtils.getKeyValueLineSerde(keyOI, valueOI);
final BufferedReader reader = HadoopUtils.getBufferedReader(file);
import java.util.Properties;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.serde2.SerDeException;
import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
public static LazySimpleSerDe getKeyValueLineSerde(PrimitiveObjectInspector keyOI, PrimitiveObjectInspector valueOI)
throws SerDeException {
LazySimpleSerDe serde = new LazySimpleSerDe();
Configuration conf = new Configuration();
Properties tbl = new Properties();
tbl.setProperty("columns", "key,value");
serde.initialize(conf, tbl);
return serde;
}
public static LazySimpleSerDe getLineSerde(PrimitiveObjectInspector... OIs)
throws SerDeException {
if(OIs.length == 0) {
throw new IllegalArgumentException("OIs must be specified");
}
LazySimpleSerDe serde = new LazySimpleSerDe();
Configuration conf = new Configuration();
Properties tbl = new Properties();
StringBuilder columnNames = new StringBuilder();
StringBuilder columnTypes = new StringBuilder();
columnTypes.append(OIs[i].getTypeName()).append(',');
}
columnNames.deleteCharAt(columnNames.length() - 1);
columnTypes.deleteCharAt(columnTypes.length() - 1);
tbl.setProperty("columns", columnNames.toString());
tbl.setProperty("columns.types", columnTypes.toString());
serde.initialize(conf, tbl);
return serde;
}
@Override
public String toString() {
}
@Override
public String toString() {
}
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog("Hivemall");
if(!map.isEmpty()) {
}
private static void loadPredictionModel(OpenHashMap<Object, WeightValue> map, File file, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
loadPredictionModel(map, f, featureOI, weightOI, covarOI);
LazySimpleSerDe serde = HiveUtils.getLineSerde(featureOI, weightOI, covarOI);
private static final Log logger = LogFactory.getLog(LearnerBaseUDTF.class);
PrimitiveObjectInspector keyRefOI = (PrimitiveObjectInspector) keyRef.getFieldObjectInspector();
FloatObjectInspector varRefOI = (FloatObjectInspector) valueRef.getFieldObjectInspector();
Object k = keyRefOI.getPrimitiveWritableObject(keyRefOI.copyObject(f0));
float v = varRefOI.get(f1);
PrimitiveObjectInspector c1oi = (PrimitiveObjectInspector) c1ref.getFieldObjectInspector();
FloatObjectInspector c2oi = (FloatObjectInspector) c2ref.getFieldObjectInspector();
FloatObjectInspector c3oi = (FloatObjectInspector) c3ref.getFieldObjectInspector();
Object k = c1oi.getPrimitiveWritableObject(c1oi.copyObject(f0));
float v = c2oi.get(f1);
float cov = c3oi.get(f2);
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(LearnerBaseUDTF.class);
if(!map.isEmpty()) {
}
PrimitiveObjectInspector keyRefOI = (PrimitiveObjectInspector) keyRef.getFieldObjectInspector();
FloatObjectInspector varRefOI = (FloatObjectInspector) valueRef.getFieldObjectInspector();
Object k = keyRefOI.getPrimitiveWritableObject(keyRefOI.copyObject(f0));
float v = varRefOI.get(f1);
private static void loadPredictionModel(OpenHashMap<Object, WeightValue> map, File file, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
loadPredictionModel(map, f, featureOI, weightOI, covarOI);
LazySimpleSerDe serde = HiveUtils.getLineSerde(featureOI, weightOI, covarOI);
PrimitiveObjectInspector c1oi = (PrimitiveObjectInspector) c1ref.getFieldObjectInspector();
FloatObjectInspector c2oi = (FloatObjectInspector) c2ref.getFieldObjectInspector();
FloatObjectInspector c3oi = (FloatObjectInspector) c3ref.getFieldObjectInspector();
Object k = c1oi.getPrimitiveWritableObject(c1oi.copyObject(f0));
float v = c2oi.get(f1);
float cov = c3oi.get(f2);
@Override
public String toString() {
}
@Override
public String toString() {
}
final long lines;
lines = loadPredictionModel(map, new File(filename), keyOI, writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(map, new File(filename), keyOI, writableFloatObjectInspector);
private static long loadPredictionModel(OpenHashMap<Object, WeightValue> map, File file, PrimitiveObjectInspector keyOI, WritableFloatObjectInspector valueOI)
long count = 0L;
return count;
count;
return count;
private static long loadPredictionModel(OpenHashMap<Object, WeightValue> map, File file, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
long count = 0L;
return count;
count;
return count;
if(f0 == null || f1 == null) {
float cov = (f2 == null) ? WeightValueWithCovar.DEFAULT_COVAR
: c3oi.get(f2);
public static float DEFAULT_COVAR = 1.f;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(MulticlassOnlineClassifierUDTF.class);
final long lines;
lines = loadPredictionModel(label2FeatureWeight, new File(filename), labelOI, featureOI, writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(label2FeatureWeight, new File(filename), labelOI, featureOI, writableFloatObjectInspector);
if(!label2FeatureWeight.isEmpty()) {
long totalFeatures = 0L;
StringBuilder statsBuf = new StringBuilder(256);
for(Map.Entry<Object, OpenHashMap<Object, WeightValue>> e : label2FeatureWeight.entrySet()) {
Object label = e.getKey();
int numFeatures = e.getValue().size();
statsBuf.append('\n').append("Label: ").append(label).append(", Number of Features: ").append(numFeatures);
}
}
private static long loadPredictionModel(Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI)
long count = 0L;
return count;
PrimitiveObjectInspector c1refOI = (PrimitiveObjectInspector) c1ref.getFieldObjectInspector();
PrimitiveObjectInspector c2refOI = (PrimitiveObjectInspector) c2ref.getFieldObjectInspector();
FloatObjectInspector c3refOI = (FloatObjectInspector) c3ref.getFieldObjectInspector();
count;
Object label = c1refOI.getPrimitiveWritableObject(c1refOI.copyObject(f0));
Object k = c2refOI.getPrimitiveWritableObject(c2refOI.copyObject(f1));
float v = c3refOI.get(f2);
return count;
private static long loadPredictionModel(Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
long count = 0L;
return count;
PrimitiveObjectInspector c1refOI = (PrimitiveObjectInspector) c1ref.getFieldObjectInspector();
PrimitiveObjectInspector c2refOI = (PrimitiveObjectInspector) c2ref.getFieldObjectInspector();
FloatObjectInspector c3refOI = (FloatObjectInspector) c3ref.getFieldObjectInspector();
FloatObjectInspector c4refOI = (FloatObjectInspector) c4ref.getFieldObjectInspector();
count;
if(f0 == null || f1 == null || f2 == null) {
Object label = c1refOI.getPrimitiveWritableObject(c1refOI.copyObject(f0));
Object k = c2refOI.getPrimitiveWritableObject(c2refOI.copyObject(f1));
float v = c3refOI.get(f2);
float cov = (f3 == null) ? WeightValueWithCovar.DEFAULT_COVAR
: c4refOI.get(f3);
return count;
final long lines;
lines = loadPredictionModel(map, new File(filename), keyOI, writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(map, new File(filename), keyOI, writableFloatObjectInspector);
private static long loadPredictionModel(OpenHashMap<Object, WeightValue> map, File file, PrimitiveObjectInspector keyOI, WritableFloatObjectInspector valueOI)
long count = 0L;
return count;
count;
return count;
private static long loadPredictionModel(OpenHashMap<Object, WeightValue> map, File file, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
long count = 0L;
return count;
count;
if(f0 == null || f1 == null) {
float cov = (f2 == null) ? WeightValueWithCovar.DEFAULT_COVAR
: c3oi.get(f2);
return count;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(MulticlassOnlineClassifierUDTF.class);
final long lines;
lines = loadPredictionModel(label2FeatureWeight, new File(filename), labelOI, featureOI, writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(label2FeatureWeight, new File(filename), labelOI, featureOI, writableFloatObjectInspector);
if(!label2FeatureWeight.isEmpty()) {
long totalFeatures = 0L;
StringBuilder statsBuf = new StringBuilder(256);
for(Map.Entry<Object, OpenHashMap<Object, WeightValue>> e : label2FeatureWeight.entrySet()) {
Object label = e.getKey();
int numFeatures = e.getValue().size();
statsBuf.append('\n').append("Label: ").append(label).append(", Number of Features: ").append(numFeatures);
}
}
private static long loadPredictionModel(Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI)
long count = 0L;
return count;
PrimitiveObjectInspector c1refOI = (PrimitiveObjectInspector) c1ref.getFieldObjectInspector();
PrimitiveObjectInspector c2refOI = (PrimitiveObjectInspector) c2ref.getFieldObjectInspector();
FloatObjectInspector c3refOI = (FloatObjectInspector) c3ref.getFieldObjectInspector();
count;
Object label = c1refOI.getPrimitiveWritableObject(c1refOI.copyObject(f0));
Object k = c2refOI.getPrimitiveWritableObject(c2refOI.copyObject(f1));
float v = c3refOI.get(f2);
return count;
private static long loadPredictionModel(Map<Object, OpenHashMap<Object, WeightValue>> label2FeatureWeight, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
long count = 0L;
return count;
PrimitiveObjectInspector c1refOI = (PrimitiveObjectInspector) c1ref.getFieldObjectInspector();
PrimitiveObjectInspector c2refOI = (PrimitiveObjectInspector) c2ref.getFieldObjectInspector();
FloatObjectInspector c3refOI = (FloatObjectInspector) c3ref.getFieldObjectInspector();
FloatObjectInspector c4refOI = (FloatObjectInspector) c4ref.getFieldObjectInspector();
count;
if(f0 == null || f1 == null || f2 == null) {
Object label = c1refOI.getPrimitiveWritableObject(c1refOI.copyObject(f0));
Object k = c2refOI.getPrimitiveWritableObject(c2refOI.copyObject(f1));
float v = c3refOI.get(f2);
float cov = (f3 == null) ? WeightValueWithCovar.DEFAULT_COVAR
: c4refOI.get(f3);
return count;
public static float DEFAULT_COVAR = 1.f;
import hivemall.utils.datetime.StopWatch;
final StopWatch elapsed = new StopWatch();
import hivemall.utils.datetime.StopWatch;
final StopWatch elapsed = new StopWatch();
import hivemall.utils.datetime.StopWatch;
final StopWatch elapsed = new StopWatch();
import hivemall.utils.datetime.StopWatch;
final StopWatch elapsed = new StopWatch();
import java.io.Reader;
BufferedReader br = new BufferedReaderExt(new InputStreamReader(cis), decompressor);
private static class BufferedReaderExt extends BufferedReader {
private Decompressor decompressor;
BufferedReaderExt(Reader in, Decompressor decompressor) {
super(in);
this.decompressor = decompressor;
}
@Override
public void close() throws IOException {
super.close();
if(decompressor != null) {
CodecPool.returnDecompressor(decompressor);
this.decompressor = null;
}
}
}
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
private boolean multipleKeyLookup;
private boolean multipleDefaultValues;
private PrimitiveObjectInspector keyInputOI;
private PrimitiveObjectInspector valueInputOI;
private ListObjectInspector keysInputOI;
private ListObjectInspector valuesInputOI;
ObjectInspector argOI2 = argOIs[2];
this.multipleDefaultValues = (argOI2.getCategory() == Category.LIST);
if(multipleDefaultValues) {
this.valuesInputOI = (ListObjectInspector) argOI2;
ObjectInspector valuesElemOI = valuesInputOI.getListElementObjectInspector();
valueInputOI = HiveUtils.asPrimitiveObjectInspector(valuesElemOI);
} else {
this.defaultValue = HiveUtils.getConstValue(argOI2);
valueInputOI = HiveUtils.asPrimitiveObjectInspector(argOI2);
}
ObjectInspector valueOutputOI = ObjectInspectorUtils.getStandardObjectInspector(valueInputOI, ObjectInspectorCopyOption.WRITABLE);
final ObjectInspector outputOI;
this.multipleKeyLookup = false;
this.keyInputOI = (PrimitiveObjectInspector) argOIs[1];
outputOI = valueOutputOI;
this.multipleKeyLookup = true;
this.keysInputOI = (ListObjectInspector) argOIs[1];
ObjectInspector keysElemOI = keysInputOI.getListElementObjectInspector();
this.keyInputOI = HiveUtils.asPrimitiveObjectInspector(keysElemOI);
outputOI = ObjectInspectorFactory.getStandardMapObjectInspector(keyInputOI, valueOutputOI);
loadValues(map, new File(filepath), keyInputOI, valueInputOI);
return outputOI;
PrimitiveObjectInspector keyRefOI = (PrimitiveObjectInspector) keyRef.getFieldObjectInspector();
PrimitiveObjectInspector valueRefOI = (PrimitiveObjectInspector) valueRef.getFieldObjectInspector();
Object f0 = fields.get(0);
Object f1 = fields.get(1);
Object k = keyRefOI.getPrimitiveJavaObject(f0);
Object v = valueRefOI.getPrimitiveWritableObject(valueRefOI.copyObject(f1));
if(multipleKeyLookup) {
if(multipleDefaultValues) {
Object arg2 = args[2].get();
return gets(arg1, arg2);
} else {
return gets(arg1);
}
private Object get(Object arg) {
Object key = keyInputOI.getPrimitiveJavaObject(arg);
List<?> keys = keysInputOI.getList(arg);
if(k == null) {
continue;
}
Object kj = keyInputOI.getPrimitiveJavaObject(k);
private Map<Object, Object> gets(Object argKeys, Object argValues) throws HiveException {
final List<?> keys = keysInputOI.getList(argKeys);
final List<?> defaultValues = valuesInputOI.getList(argValues);
final int numKeys = keys.size();
if(numKeys != defaultValues.size()) {
}
final Map<Object, Object> map = new HashMap<Object, Object>();
Object k = keys.get(i);
if(k == null) {
continue;
}
Object kj = keyInputOI.getPrimitiveJavaObject(k);
Object v = cache.get(kj);
if(v == null) {
v = defaultValues.get(i);
if(v != null) {
v = valueInputOI.getPrimitiveWritableObject(valueInputOI.copyObject(v));
}
}
map.put(k, v);
}
return map;
}
"\n\tdistcache_gets(const string FILEPATH, object key, const object defaultValue)::value_type"
"\n\tdistcache_gets(const string FILEPATH, object[] key, object[] defaultValues)::map<key_type, value_type>";
protected boolean skipUntouched;
opts.addOption("outputs_touched", false, "Outputs weights touched in training");
boolean outputs_touched = false;
outputs_touched = cl.hasOption("outputs_touched");
this.skipUntouched = outputs_touched;
map.put(k, new WeightValue(v, false));
map.put(k, new WeightValueWithCovar(v, cov, false));
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(BinaryOnlineClassifierUDTF.class);
protected int count;
this.count = 0;
if(features.isEmpty()) {
return;
}
count;
int numForwarded = 0;
if(skipUntouched && !v.isTouched()) {
}
if(skipUntouched && !v.isTouched()) {
}
" rows");
protected int count;
this.count = 0;
count;
long numForwarded = 0L;
if(skipUntouched && !v.isTouched()) {
}
if(skipUntouched && !v.isTouched()) {
}
" rows");
map.put(k, new WeightValue(v, false));
map.put(k, new WeightValueWithCovar(v, cov, false));
protected final boolean touched;
this(weight, true);
}
public WeightValue(float weight, boolean touched) {
this.touched = touched;
public boolean isTouched() {
return touched;
}
this(weight, covariance, true);
}
public WeightValueWithCovar(float weight, float covariance, boolean touched) {
super(weight, touched);
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(OnlineRegressionUDTF.class);
this.count = 0;
if(features.isEmpty()) {
return;
}
train(features, target);
int numForwarded = 0;
if(skipUntouched && !v.isTouched()) {
}
if(skipUntouched && !v.isTouched()) {
}
" rows");
protected boolean skipUntouched;
opts.addOption("outputs_touched", false, "Outputs weights touched in training");
boolean outputs_touched = false;
outputs_touched = cl.hasOption("outputs_touched");
this.skipUntouched = outputs_touched;
map.put(k, new WeightValue(v, false));
map.put(k, new WeightValueWithCovar(v, cov, false));
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(BinaryOnlineClassifierUDTF.class);
protected int count;
this.count = 0;
if(features.isEmpty()) {
return;
}
count;
int numForwarded = 0;
if(skipUntouched && !v.isTouched()) {
}
if(skipUntouched && !v.isTouched()) {
}
" rows");
protected int count;
this.count = 0;
count;
long numForwarded = 0L;
if(skipUntouched && !v.isTouched()) {
}
if(skipUntouched && !v.isTouched()) {
}
" rows");
map.put(k, new WeightValue(v, false));
map.put(k, new WeightValueWithCovar(v, cov, false));
protected final boolean touched;
this(weight, true);
}
public WeightValue(float weight, boolean touched) {
this.touched = touched;
public boolean isTouched() {
return touched;
}
this(weight, covariance, true);
}
public WeightValueWithCovar(float weight, float covariance, boolean touched) {
super(weight, touched);
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(OnlineRegressionUDTF.class);
this.count = 0;
if(features.isEmpty()) {
return;
}
train(features, target);
int numForwarded = 0;
if(skipUntouched && !v.isTouched()) {
}
if(skipUntouched && !v.isTouched()) {
}
" rows");
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
private boolean multipleKeyLookup;
private boolean multipleDefaultValues;
private PrimitiveObjectInspector keyInputOI;
private PrimitiveObjectInspector valueInputOI;
private ListObjectInspector keysInputOI;
private ListObjectInspector valuesInputOI;
ObjectInspector argOI2 = argOIs[2];
this.multipleDefaultValues = (argOI2.getCategory() == Category.LIST);
if(multipleDefaultValues) {
this.valuesInputOI = (ListObjectInspector) argOI2;
ObjectInspector valuesElemOI = valuesInputOI.getListElementObjectInspector();
valueInputOI = HiveUtils.asPrimitiveObjectInspector(valuesElemOI);
} else {
this.defaultValue = HiveUtils.getConstValue(argOI2);
valueInputOI = HiveUtils.asPrimitiveObjectInspector(argOI2);
}
ObjectInspector valueOutputOI = ObjectInspectorUtils.getStandardObjectInspector(valueInputOI, ObjectInspectorCopyOption.WRITABLE);
final ObjectInspector outputOI;
this.multipleKeyLookup = false;
this.keyInputOI = (PrimitiveObjectInspector) argOIs[1];
outputOI = valueOutputOI;
this.multipleKeyLookup = true;
this.keysInputOI = (ListObjectInspector) argOIs[1];
ObjectInspector keysElemOI = keysInputOI.getListElementObjectInspector();
this.keyInputOI = HiveUtils.asPrimitiveObjectInspector(keysElemOI);
outputOI = ObjectInspectorFactory.getStandardMapObjectInspector(keyInputOI, valueOutputOI);
loadValues(map, new File(filepath), keyInputOI, valueInputOI);
return outputOI;
PrimitiveObjectInspector keyRefOI = (PrimitiveObjectInspector) keyRef.getFieldObjectInspector();
PrimitiveObjectInspector valueRefOI = (PrimitiveObjectInspector) valueRef.getFieldObjectInspector();
Object f0 = fields.get(0);
Object f1 = fields.get(1);
Object k = keyRefOI.getPrimitiveJavaObject(f0);
Object v = valueRefOI.getPrimitiveWritableObject(valueRefOI.copyObject(f1));
if(multipleKeyLookup) {
if(multipleDefaultValues) {
Object arg2 = args[2].get();
return gets(arg1, arg2);
} else {
return gets(arg1);
}
private Object get(Object arg) {
Object key = keyInputOI.getPrimitiveJavaObject(arg);
List<?> keys = keysInputOI.getList(arg);
if(k == null) {
continue;
}
Object kj = keyInputOI.getPrimitiveJavaObject(k);
private Map<Object, Object> gets(Object argKeys, Object argValues) throws HiveException {
final List<?> keys = keysInputOI.getList(argKeys);
final List<?> defaultValues = valuesInputOI.getList(argValues);
final int numKeys = keys.size();
if(numKeys != defaultValues.size()) {
}
final Map<Object, Object> map = new HashMap<Object, Object>();
Object k = keys.get(i);
if(k == null) {
continue;
}
Object kj = keyInputOI.getPrimitiveJavaObject(k);
Object v = cache.get(kj);
if(v == null) {
v = defaultValues.get(i);
if(v != null) {
v = valueInputOI.getPrimitiveWritableObject(valueInputOI.copyObject(v));
}
}
map.put(k, v);
}
return map;
}
"\n\tdistcache_gets(const string FILEPATH, object key, const object defaultValue)::value_type"
"\n\tdistcache_gets(const string FILEPATH, object[] key, object[] defaultValues)::map<key_type, value_type>";
import java.io.Reader;
BufferedReader br = new BufferedReaderExt(new InputStreamReader(cis), decompressor);
private static class BufferedReaderExt extends BufferedReader {
private Decompressor decompressor;
BufferedReaderExt(Reader in, Decompressor decompressor) {
super(in);
this.decompressor = decompressor;
}
@Override
public void close() throws IOException {
super.close();
if(decompressor != null) {
CodecPool.returnDecompressor(decompressor);
this.decompressor = null;
}
}
}
return val(extractFeature(featureVector));
if(fv != null) {
output[i] = new Text(extractFeature(fv));
public static String extractFeature(final String ftvec) {
int pos = ftvec.indexOf(":");
if(pos > 0) {
return ftvec.substring(0, pos);
} else {
return ftvec;
}
}
this.parseX = STRING_TYPE_NAME.equals(keyTypeName);
float v = weight.get();
if(v == 0.f) {
}
import hivemall.ftvec.ExtractFeatureUDF;
@Description(name = "distcache_gets", value = "_FUNC_(filepath, key, default_value [, parseKey]) - Returns map<key_type, value_type>|value_type")
private boolean parseKey;
if(argOIs.length != 3 && argOIs.length != 4) {
throw new UDFArgumentException("Invalid number of arguments for distcache_gets(FILEPATH, KEYS, DEFAULT_VAL, PARSE_KEY): "
if(argOIs.length == 4) {
this.parseKey = HiveUtils.getConstBoolean(argOIs[3]);
} else {
this.parseKey = false;
}
if(parseKey && !HiveUtils.isStringOI(keyInputOI)) {
throw new UDFArgumentException("parseKey=true is only available for string typed key(s)");
}
Object value = cache.get(lookupKey(key));
final Object v = cache.get(lookupKey(kj));
Object v = cache.get(lookupKey(kj));
private Object lookupKey(final Object key) {
if(parseKey) {
String keyStr = key.toString();
return ExtractFeatureUDF.extractFeature(keyStr);
} else {
return key;
}
}
"\n\tdistcache_gets(const string FILEPATH, object[] keys, const object defaultValue [, const boolean parseKey])::map<key_type, value_type>"
"\n\tdistcache_gets(const string FILEPATH, object key, const object defaultValue [, const boolean parseKey])::value_type"
"\n\tdistcache_gets(const string FILEPATH, object[] key, object[] defaultValues [, const boolean parseKey])::map<key_type, value_type>";
import static hivemall.HivemallConstants.BOOLEAN_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBooleanObjectInspector;
public static boolean getConstBoolean(ObjectInspector oi) throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
String typeName = oi.getTypeName();
if(!BOOLEAN_TYPE_NAME.equals(typeName)) {
throw new UDFArgumentException("argument must be a boolean value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
WritableConstantBooleanObjectInspector booleanOI = (WritableConstantBooleanObjectInspector) oi;
return booleanOI.getWritableConstantValue().get();
}
public static boolean isStringOI(ObjectInspector oi) {
String typeName = oi.getTypeName();
return STRING_TYPE_NAME.equals(typeName);
}
import hivemall.common.FeatureValue;
final Object feature;
if(parseX) {
FeatureValue fv = FeatureValue.parse(k, feature_hashing);
feature = fv.getFeature();
} else {
feature = ObjectInspectorUtils.copyToStandardObject(k, featureInspector);
}
return val(extractFeature(featureVector));
if(fv != null) {
output[i] = new Text(extractFeature(fv));
public static String extractFeature(final String ftvec) {
int pos = ftvec.indexOf(":");
if(pos > 0) {
return ftvec.substring(0, pos);
} else {
return ftvec;
}
}
import hivemall.common.FeatureValue;
this.parseX = STRING_TYPE_NAME.equals(keyTypeName);
float v = weight.get();
if(v == 0.f) {
}
final Object feature;
if(parseX) {
FeatureValue fv = FeatureValue.parse(k, feature_hashing);
feature = fv.getFeature();
} else {
feature = ObjectInspectorUtils.copyToStandardObject(k, featureInspector);
}
import hivemall.ftvec.ExtractFeatureUDF;
@Description(name = "distcache_gets", value = "_FUNC_(filepath, key, default_value [, parseKey]) - Returns map<key_type, value_type>|value_type")
private boolean parseKey;
if(argOIs.length != 3 && argOIs.length != 4) {
throw new UDFArgumentException("Invalid number of arguments for distcache_gets(FILEPATH, KEYS, DEFAULT_VAL, PARSE_KEY): "
if(argOIs.length == 4) {
this.parseKey = HiveUtils.getConstBoolean(argOIs[3]);
} else {
this.parseKey = false;
}
if(parseKey && !HiveUtils.isStringOI(keyInputOI)) {
throw new UDFArgumentException("parseKey=true is only available for string typed key(s)");
}
Object value = cache.get(lookupKey(key));
final Object v = cache.get(lookupKey(kj));
Object v = cache.get(lookupKey(kj));
private Object lookupKey(final Object key) {
if(parseKey) {
String keyStr = key.toString();
return ExtractFeatureUDF.extractFeature(keyStr);
} else {
return key;
}
}
"\n\tdistcache_gets(const string FILEPATH, object[] keys, const object defaultValue [, const boolean parseKey])::map<key_type, value_type>"
"\n\tdistcache_gets(const string FILEPATH, object key, const object defaultValue [, const boolean parseKey])::value_type"
"\n\tdistcache_gets(const string FILEPATH, object[] key, object[] defaultValues [, const boolean parseKey])::map<key_type, value_type>";
import static hivemall.HivemallConstants.BOOLEAN_TYPE_NAME;
import static hivemall.HivemallConstants.STRING_TYPE_NAME;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBooleanObjectInspector;
public static boolean getConstBoolean(ObjectInspector oi) throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
String typeName = oi.getTypeName();
if(!BOOLEAN_TYPE_NAME.equals(typeName)) {
throw new UDFArgumentException("argument must be a boolean value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
WritableConstantBooleanObjectInspector booleanOI = (WritableConstantBooleanObjectInspector) oi;
return booleanOI.getWritableConstantValue().get();
}
public static boolean isStringOI(ObjectInspector oi) {
String typeName = oi.getTypeName();
return STRING_TYPE_NAME.equals(typeName);
}
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
@UDFType(deterministic = false, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "addBias", value = "_FUNC_(feature_vector in array<string>) - Returns features with a bias in array<string>")
@UDFType(deterministic = true, stateful = false)
public final class AddBiasUDF extends UDF {
import java.util.Arrays;
import org.apache.hadoop.io.IntWritable;
public List<IntWritable> evaluate(List<IntWritable> ftvec, IntWritable biasClause) {
int size = ftvec.size();
ftvec.toArray(newvec);
newvec[size] = biasClause;
return Arrays.asList(newvec);
}
import hivemall.utils.hadoop.HadoopUtils;
import hivemall.utils.hadoop.HadoopUtils;
return val(HadoopUtils.getTaskId());
import org.apache.hadoop.mapred.JobConf;
public static int getTaskId() {
MapredContext ctx = MapredContextAccessor.get();
JobConf conf = ctx.getJobConf();
int taskid = conf.getInt("mapred.task.partition", -1);
if(taskid == -1) {
throw new IllegalStateException("mapred.task.partition is not set");
}
return taskid;
}
private boolean dense;
private boolean ordered;
opts.addOption("nd", "n_dims", true, "The size of feature dimensions [DEFAULT: 200]");
opts.addOption("dense", false, "Make a dense dataset or not. If not specified, a sparse dataset is generated.\n"
opts.addOption("ordered", false, "Sort features if specified (used only for sparse dataset)");
this.n_dimensions = Primitives.parseInt(cl.getOptionValue("n_dims"), 200);
this.dense = cl.hasOption("dense");
this.ordered = cl.hasOption("ordered");
if(dense) {
if(dense) {
} else {
fieldOIs.add(ObjectInspectorFactory.getStandardListObjectInspector(PrimitiveObjectInspectorFactory.javaStringObjectInspector));
if(dense) {
} else {
this.featuresArray = new String[N_BUFFERS][n_features];
if(dense) {
} else {
generateSparseData();
if(retry >= 3) {
if(ordered) {
Arrays.sort(features);
}
if(dense) {
forwardObjs[1] = Arrays.asList(featuresFloatArray[i]);
forwardObjs[1] = Arrays.asList(featuresArray[i]);
public static int compare(int x, int y) {
return (x < y) ? -1 : ((x == y) ? 0 : 1);
}
import java.util.Comparator;
Arrays.sort(features, new Comparator<String>() {
@Override
public int compare(String o1, String o2) {
int i1 = Integer.parseInt(o1.split(":")[0]);
int i2 = Integer.parseInt(o2.split(":")[0]);
return Primitives.compare(i1, i2);
}
});
import hivemall.utils.hadoop.HadoopUtils;
private Random rnd1 = null, rnd2 = null;
if(rnd1 == null) {
assert (rnd2 == null);
int taskid = HadoopUtils.getTaskId();
this.rnd1 = new Random(seed);
}
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
import org.apache.hadoop.io.IntWritable;
@Description(name = "addBias", value = "_FUNC_(feature_vector in array<string>) - Returns features with a bias in array<string>")
@UDFType(deterministic = true, stateful = false)
public final class AddBiasUDF extends UDF {
public List<IntWritable> evaluate(List<IntWritable> ftvec, IntWritable biasClause) {
int size = ftvec.size();
ftvec.toArray(newvec);
newvec[size] = biasClause;
return Arrays.asList(newvec);
}
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
@UDFType(deterministic = false, stateful = false)
import hivemall.utils.hadoop.HadoopUtils;
import hivemall.utils.hadoop.HadoopUtils;
return val(HadoopUtils.getTaskId());
import org.apache.hadoop.mapred.JobConf;
public static int getTaskId() {
MapredContext ctx = MapredContextAccessor.get();
JobConf conf = ctx.getJobConf();
int taskid = conf.getInt("mapred.task.partition", -1);
if(taskid == -1) {
throw new IllegalStateException("mapred.task.partition is not set");
}
return taskid;
}
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "regression_datagen", value = "_FUNC_(options string) - Generates a regression dataset")
opts.addOption("nd", "n_dims", true, "The size of feature dimensions [DEFAULT: 500]");
this.n_dimensions = Primitives.parseInt(cl.getOptionValue("n_dims"), 500);
opts.addOption("nd", "n_dims", true, "The size of feature dimensions [DEFAULT: 200]");
this.n_dimensions = Primitives.parseInt(cl.getOptionValue("n_dims"), 200);
int searchClearBitsFrom = 0;
if(retry < 3) {
--i;
retry;
continue;
searchClearBitsFrom = used.nextClearBit(searchClearBitsFrom);
f = searchClearBitsFrom;
retry = 0;
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "regression_datagen", value = "_FUNC_(options string) - Generates a regression dataset")
int searchClearBitsFrom = 0;
if(retry < 3) {
--i;
retry;
continue;
searchClearBitsFrom = used.nextClearBit(searchClearBitsFrom);
f = searchClearBitsFrom;
retry = 0;
@Description(name = "lr_datagen", value = "_FUNC_(options string) - Generates a logistic regression dataset")
public final class LogisticRegressionDataGeneratorUDTF extends UDTFWithOptions {
private boolean sort;
private boolean classification;
opts.addOption("sort", false, "Sort features if specified (used only for sparse dataset)");
opts.addOption("cl", "classification", false, "Toggle this option on to generate a classification dataset");
this.sort = cl.hasOption("ordered");
this.classification = cl.hasOption("classification");
float sign = (label >= prob_one) ? 1.f : 0.f;
labels[position] = classification ? sign : label;
if(sort) {
float sign = (label >= prob_one) ? 1.f : 0.f;
labels[position] = classification ? sign : label;
@Description(name = "lr_datagen", value = "_FUNC_(options string) - Generates a logistic regression dataset")
public final class LogisticRegressionDataGeneratorUDTF extends UDTFWithOptions {
private boolean sort;
private boolean classification;
opts.addOption("sort", false, "Sort features if specified (used only for sparse dataset)");
opts.addOption("cl", "classification", false, "Toggle this option on to generate a classification dataset");
this.sort = cl.hasOption("ordered");
this.classification = cl.hasOption("classification");
float sign = (label >= prob_one) ? 1.f : 0.f;
labels[position] = classification ? sign : label;
if(sort) {
float sign = (label >= prob_one) ? 1.f : 0.f;
labels[position] = classification ? sign : label;
import hivemall.utils.lang.NumberUtils;
this.n_examples = NumberUtils.parseInt(cl.getOptionValue("n_examples"), 1000);
this.n_features = NumberUtils.parseInt(cl.getOptionValue("n_features"), 10);
this.n_dimensions = NumberUtils.parseInt(cl.getOptionValue("n_dims"), 200);
public static int parseInt(String s, int defaultValue) {
if(s == null) {
return defaultValue;
}
return parseInt(s);
}
import hivemall.utils.lang.NumberUtils;
this.n_examples = NumberUtils.parseInt(cl.getOptionValue("n_examples"), 1000);
this.n_features = NumberUtils.parseInt(cl.getOptionValue("n_features"), 10);
this.n_dimensions = NumberUtils.parseInt(cl.getOptionValue("n_dims"), 200);
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.SMALLINT_TYPE_NAME;
import static hivemall.HivemallConstants.TINYINT_TYPE_NAME;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantByteObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantIntObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantLongObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantShortObjectInspector;
public static boolean isBigInt(ObjectInspector oi) {
String typeName = oi.getTypeName();
return BIGINT_TYPE_NAME.equals(typeName);
}
public static int getConstInt(ObjectInspector oi) throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
String typeName = oi.getTypeName();
if(!INT_TYPE_NAME.equals(typeName)) {
throw new UDFArgumentException("argument must be a int value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
WritableConstantIntObjectInspector intOI = (WritableConstantIntObjectInspector) oi;
return intOI.getWritableConstantValue().get();
}
public static long getConstLong(ObjectInspector oi) throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
String typeName = oi.getTypeName();
if(!BIGINT_TYPE_NAME.equals(typeName)) {
throw new UDFArgumentException("argument must be a bigint value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
WritableConstantLongObjectInspector longOI = (WritableConstantLongObjectInspector) oi;
return longOI.getWritableConstantValue().get();
}
public static long getAsConstLong(ObjectInspector numberOI) throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(numberOI)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(numberOI));
}
String typeName = numberOI.getTypeName();
if(BIGINT_TYPE_NAME.equals(typeName)) {
WritableConstantLongObjectInspector longOI = (WritableConstantLongObjectInspector) numberOI;
return longOI.getWritableConstantValue().get();
} else if(INT_TYPE_NAME.equals(typeName)) {
WritableConstantIntObjectInspector intOI = (WritableConstantIntObjectInspector) numberOI;
return (long) intOI.getWritableConstantValue().get();
} else if(SMALLINT_TYPE_NAME.equals(typeName)) {
WritableConstantShortObjectInspector shortOI = (WritableConstantShortObjectInspector) numberOI;
return (long) shortOI.getWritableConstantValue().get();
} else if(TINYINT_TYPE_NAME.equals(typeName)) {
WritableConstantByteObjectInspector byteOI = (WritableConstantByteObjectInspector) numberOI;
return (long) byteOI.getWritableConstantValue().get();
}
throw new UDFArgumentException("Unexpected argument type to cast as long: "
TypeInfoUtils.getTypeInfoFromObjectInspector(numberOI));
}
import static hivemall.HivemallConstants.BIGINT_TYPE_NAME;
import static hivemall.HivemallConstants.INT_TYPE_NAME;
import static hivemall.HivemallConstants.SMALLINT_TYPE_NAME;
import static hivemall.HivemallConstants.TINYINT_TYPE_NAME;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantByteObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantIntObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantLongObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantShortObjectInspector;
public static boolean isBigInt(ObjectInspector oi) {
String typeName = oi.getTypeName();
return BIGINT_TYPE_NAME.equals(typeName);
}
public static int getConstInt(ObjectInspector oi) throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
String typeName = oi.getTypeName();
if(!INT_TYPE_NAME.equals(typeName)) {
throw new UDFArgumentException("argument must be a int value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
WritableConstantIntObjectInspector intOI = (WritableConstantIntObjectInspector) oi;
return intOI.getWritableConstantValue().get();
}
public static long getConstLong(ObjectInspector oi) throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
String typeName = oi.getTypeName();
if(!BIGINT_TYPE_NAME.equals(typeName)) {
throw new UDFArgumentException("argument must be a bigint value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
WritableConstantLongObjectInspector longOI = (WritableConstantLongObjectInspector) oi;
return longOI.getWritableConstantValue().get();
}
public static long getAsConstLong(ObjectInspector numberOI) throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(numberOI)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(numberOI));
}
String typeName = numberOI.getTypeName();
if(BIGINT_TYPE_NAME.equals(typeName)) {
WritableConstantLongObjectInspector longOI = (WritableConstantLongObjectInspector) numberOI;
return longOI.getWritableConstantValue().get();
} else if(INT_TYPE_NAME.equals(typeName)) {
WritableConstantIntObjectInspector intOI = (WritableConstantIntObjectInspector) numberOI;
return (long) intOI.getWritableConstantValue().get();
} else if(SMALLINT_TYPE_NAME.equals(typeName)) {
WritableConstantShortObjectInspector shortOI = (WritableConstantShortObjectInspector) numberOI;
return (long) shortOI.getWritableConstantValue().get();
} else if(TINYINT_TYPE_NAME.equals(typeName)) {
WritableConstantByteObjectInspector byteOI = (WritableConstantByteObjectInspector) numberOI;
return (long) byteOI.getWritableConstantValue().get();
}
throw new UDFArgumentException("Unexpected argument type to cast as long: "
TypeInfoUtils.getTypeInfoFromObjectInspector(numberOI));
}
this.sort = cl.hasOption("sort");
this.sort = cl.hasOption("sort");
opts.addOption("output_untouched", false, "Output feature weights not touched in the training");
if(modelfile != null) {
output_untouched = cl.hasOption("output_untouched");
}
this.skipUntouched = output_untouched ? false : true;
opts.addOption("output_untouched", false, "Output feature weights not touched in the training");
if(modelfile != null) {
output_untouched = cl.hasOption("output_untouched");
}
this.skipUntouched = output_untouched ? false : true;
public static class ArgminCovarUDAFEvaluator implements UDAFEvaluator {
public static class ArgminCovarUDAFEvaluator implements UDAFEvaluator {
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
import org.apache.hadoop.io.Text;
@Description(name = "rescale", value = "_FUNC_(value, min, max) - Returns rescaled value by min-max normalization")
@UDFType(deterministic = true, stateful = false)
public final class RescaleUDF extends UDF {
public FloatWritable evaluate(final float value, final float min, final float max) {
public FloatWritable evaluate(final float value, final double min, final double max) {
public Text evaluate(final String s, final double min, final double max) {
return evaluate(s, (float) min, (float) max);
}
public Text evaluate(final String s, final float min, final float max) {
String[] fv = s.split(":");
if(fv.length != 2) {
}
float v = Float.parseFloat(fv[1]);
float scaled_v = min_max_normalization(v, min, max);
return val(ret);
}
private static float min_max_normalization(final float value, final float min, final float max) {
return (value - min) / (max - min);
}
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "zscore", value = "_FUNC_(value, mean, stddev) - Returns a standard score (zscore)")
@UDFType(deterministic = true, stateful = false)
public final class ZScoreUDF extends UDF {
private HadoopUtils() {}
private HiveUtils() {}
private WritableUtils() {}
private ArrayUtils() {}
private BitUtils() {}
private MathUtils() {}
private Primes() {}
private StatsUtils() {}
public static float l2norm(final float[] elements) {
double sqsum = 0.d;
for(float e : elements) {
}
return (float) Math.sqrt(sqsum);
}
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
import org.apache.hadoop.io.Text;
@Description(name = "rescale", value = "_FUNC_(value, min, max) - Returns rescaled value by min-max normalization")
@UDFType(deterministic = true, stateful = false)
public final class RescaleUDF extends UDF {
public FloatWritable evaluate(final float value, final float min, final float max) {
public FloatWritable evaluate(final float value, final double min, final double max) {
public Text evaluate(final String s, final double min, final double max) {
return evaluate(s, (float) min, (float) max);
}
public Text evaluate(final String s, final float min, final float max) {
String[] fv = s.split(":");
if(fv.length != 2) {
}
float v = Float.parseFloat(fv[1]);
float scaled_v = min_max_normalization(v, min, max);
return val(ret);
}
private static float min_max_normalization(final float value, final float min, final float max) {
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "zscore", value = "_FUNC_(value, mean, stddev) - Returns a standard score (zscore)")
@UDFType(deterministic = true, stateful = false)
public final class ZScoreUDF extends UDF {
private HadoopUtils() {}
private HiveUtils() {}
private WritableUtils() {}
private ArrayUtils() {}
private BitUtils() {}
private MathUtils() {}
public static float l2norm(final float[] elements) {
double sqsum = 0.d;
for(float e : elements) {
}
return (float) Math.sqrt(sqsum);
}
private Primes() {}
private StatsUtils() {}
@Description(name = "norm", value = "_FUNC_(ftvec string) - Returned a L2 normalized value")
@Description(name = "norm", value = "_FUNC_(ftvec string) - Returned a L2 normalized value")
@Description(name = "normalize", value = "_FUNC_(ftvec string) - Returned a L2 normalized value")
@Description(name = "normalize", value = "_FUNC_(ftvec string) - Returned a L2 normalized value")
replaced1.set(storedObj);
replaced2.set(storedObj);
private T object;
private long timestamp;
void set(T object) {
this.object = object;
this.timestamp = System.nanoTime();
}
replaced1.set(storedObj);
replaced2.set(storedObj);
private T object;
private long timestamp;
void set(T object) {
this.object = object;
this.timestamp = System.nanoTime();
}
@Description(name = "argmin_kld", value = "_FUNC_([mean,] covar) - Returns mean or covar that minimize a KL-distance among distributions", extended = "The returned value is (1.0 / (sum(1.0 / covar))) * (sum(mean / covar)")
return empty ? null : new FloatWritable(1.f / sum_inv_covar);
opts.addOption("phi", "confidence", true, "Confidence parameter [default 1.0]");
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
float phi = 1.f;
opts.addOption("phi", "confidence", true, "Confidence parameter [default 1.0]");
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
float phi = 1.f;
opts.addOption("phi", "confidence", true, "Confidence parameter [default 1.0]");
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
float phi = 1.f;
opts.addOption("phi", "confidence", true, "Confidence parameter [default 1.0]");
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
float phi = 1.f;
public static final float DEFAULT_COVAR = 1.f;
opts.addOption("phi", "confidence", true, "Confidence parameter [default 1.0]");
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
float phi = 1.f;
opts.addOption("phi", "confidence", true, "Confidence parameter [default 1.0]");
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
float phi = 1.f;
opts.addOption("phi", "confidence", true, "Confidence parameter [default 1.0]");
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
float phi = 1.f;
opts.addOption("phi", "confidence", true, "Confidence parameter [default 1.0]");
opts.addOption("eta", "hyper_c", true, "Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
float phi = 1.f;
public static final float DEFAULT_COVAR = 1.f;
@Description(name = "argmin_kld", value = "_FUNC_([mean,] covar) - Returns mean or covar that minimize a KL-distance among distributions", extended = "The returned value is (1.0 / (sum(1.0 / covar))) * (sum(mean / covar)")
return empty ? null : new FloatWritable(1.f / sum_inv_covar);
import hivemall.utils.lang.Copyable;
@Override
@Override
free(lastEntry);
@Override
@Override
public <T extends Copyable<V>> void getValue(T probe) {
probe.copyTo(getValue());
private void free(int index) {
if(index >= 0) {
keys[index] = null;
values[index] = null;
}
import org.apache.hadoop.io.LongWritable;
public static int parseInt(final Object o) {
if(o instanceof Integer) {
return ((Integer) o).intValue();
}
if(o instanceof IntWritable) {
return ((IntWritable) o).get();
}
if(o instanceof LongWritable) {
long l = ((LongWritable) o).get();
if(l > 0x7fffffffL) {
throw new IllegalArgumentException("feature index must be less than "
}
return (int) l;
}
String s = o.toString();
return Integer.parseInt(s);
}
public static final String BIAS_CLAUSE = "0";
@Description(name = "add_bias", value = "_FUNC_(feature_vector in array<string>) - Returns features with a bias in array<string>")
return evaluate(ftvec, HivemallConstants.BIAS_CLAUSE);
public static FeatureValue parse(Object o) {
private boolean parseFeature;
this.parseFeature = STRING_TYPE_NAME.equals(keyTypeName);
List<FeatureValue> ftvec = parseFeatures(features, featureInspector, parseFeature);
fv = FeatureValue.parse(f);
fv = FeatureValue.parse(f);
fv = FeatureValue.parse(f);
void copyFrom(T another);
public void copyFrom(MutableFloat another) {
this.value = another.value;
}
@Override
public void copyFrom(MutableInt another) {
this.value = another.value;
}
@Override
probe.copyFrom(getValue());
import hivemall.utils.lang.Copyable;
public class WeightValue implements Copyable<WeightValue> {
protected float value;
protected boolean touched;
public WeightValue() {}
public void set(float weight) {
this.value = weight;
}
public boolean hasCovariance() {
return false;
public void setCovariance(float cov) {
throw new UnsupportedOperationException();
}
public void copyTo(WeightValue probe) {
probe.value = this.value;
probe.touched = this.touched;
}
@Override
public void copyFrom(WeightValue another) {
this.value = another.value;
this.touched = another.touched;
}
@Override
float covariance;
public WeightValueWithCovar() {
super();
}
@Override
public boolean hasCovariance() {
return true;
}
@Override
public void setCovariance(float cov) {
this.covariance = cov;
}
@Override
public void copyTo(WeightValue probe) {
super.copyTo(probe);
((WeightValueWithCovar) probe).covariance = this.covariance;
}
@Override
public void copyFrom(WeightValue another) {
super.copyFrom(another);
this.covariance = ((WeightValueWithCovar) another).covariance;
}
@Override
import hivemall.common.PredictionModel;
protected boolean dense_model;
protected int model_dims;
opts.addOption("dense", "densemodel", true, "The dimension of model");
boolean denseModel = false;
int modelDims = -1;
String dimStr = cl.getOptionValue("densemodel");
if(dimStr != null) {
modelDims = Integer.parseInt(dimStr);
denseModel = true;
}
this.dense_model = denseModel;
this.model_dims = modelDims;
protected void loadPredictionModel(PredictionModel model, String filename, PrimitiveObjectInspector keyOI) {
lines = loadPredictionModel(model, new File(filename), keyOI, writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(model, new File(filename), keyOI, writableFloatObjectInspector);
if(model.size() > 0) {
private static long loadPredictionModel(PredictionModel model, File file, PrimitiveObjectInspector keyOI, WritableFloatObjectInspector valueOI)
model.set(k, new WeightValue(v, false));
private static long loadPredictionModel(PredictionModel model, File file, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
model.set(k, new WeightValueWithCovar(v, cov, false));
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
model.set(k, new_w);
old_w = old.get();
import hivemall.common.DenseModel;
import hivemall.common.PredictionModel;
import hivemall.common.SparseModel;
import hivemall.utils.collections.IMapIterator;
protected boolean parseFeature;
protected PredictionModel model;
this.model = dense_model ? new DenseModel(model_dims) : new SparseModel();
loadPredictionModel(model, preloadedModelFile, featureInputOI);
return getReturnOI(featureInputOI);
this.parseFeature = STRING_TYPE_NAME.equals(keyTypeName);
final boolean parseFeature = this.parseFeature;
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
if(old_w != 0f) {
final boolean parseX = this.parseFeature;
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
if(old_w != 0f) {
final boolean parseFeature = this.parseFeature;
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
model.set(k, new WeightValue(new_w));
if(model != null) {
final WeightValueWithCovar probe = new WeightValueWithCovar();
final FloatWritable fv = new FloatWritable();
final FloatWritable cov = new FloatWritable();
final IMapIterator<Object, WeightValue> itor = model.entries();
itor.getValue(probe);
if(skipUntouched && !probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
cov.set(probe.getCovariance());
final WeightValue probe = new WeightValue();
final FloatWritable fv = new FloatWritable();
final IMapIterator<Object, WeightValue> itor = model.entries();
itor.getValue(probe);
if(skipUntouched && !probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
this.model = null;
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
model.set(k, new_w);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
model.set(k, new_w);
old_v = old.get();
import hivemall.common.PredictionModel;
PredictionModel model2add = label2model.get(actual_label);
if(model2add == null) {
model2add = createModel();
label2model.put(actual_label, model2add);
PredictionModel model2sub = null;
model2sub = label2model.get(missed_label);
if(model2sub == null) {
model2sub = createModel();
label2model.put(missed_label, model2sub);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_correctclass_w = model2add.get(k);
model2add.set(k, new_correctclass_w);
if(model2sub != null) {
WeightValue old_wrongclass_w = model2sub.get(k);
model2sub.set(k, new_wrongclass_w);
old_v = old.get();
import hivemall.common.PredictionModel;
PredictionModel model2add = label2model.get(actual_label);
if(model2add == null) {
model2add = createModel();
label2model.put(actual_label, model2add);
PredictionModel model2sub = null;
model2sub = label2model.get(missed_label);
if(model2sub == null) {
model2sub = createModel();
label2model.put(missed_label, model2sub);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_correctclass_w = model2add.get(k);
model2add.set(k, new_correctclass_w);
if(model2sub != null) {
WeightValue old_wrongclass_w = model2sub.get(k);
model2sub.set(k, new_wrongclass_w);
import hivemall.common.DenseModel;
import hivemall.common.PredictionModel;
import hivemall.common.SparseModel;
import hivemall.utils.collections.IMapIterator;
protected boolean parseFeature;
protected Map<Object, PredictionModel> label2model;
this.label2model = new HashMap<Object, PredictionModel>(64);
loadPredictionModel(label2model, preloadedModelFile, labelInputOI, featureInputOI);
return getReturnOI(labelInputOI, featureInputOI);
}
protected final PredictionModel createModel() {
return dense_model ? new DenseModel(model_dims) : new SparseModel(8192);
this.parseFeature = STRING_TYPE_NAME.equals(keyTypeName);
PredictionModel model = label2map.getValue();
float score = calcScore(model, features);
PredictionModel model = label2map.getValue();
float score = calcScore(model, features);
PredictionModel model = label2map.getValue();
PredictionResult predicted = calcScoreAndVariance(model, features);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
protected final float calcScore(final PredictionModel model, final List<?> features) {
final boolean parseX = this.parseFeature;
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
if(old_w != 0f) {
final boolean parseX = this.parseFeature;
FeatureValue fv = FeatureValue.parse(f);
protected final PredictionResult calcScoreAndVariance(final PredictionModel model, final List<?> features) {
final boolean parseX = this.parseFeature;
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
PredictionModel model2add = label2model.get(actual_label);
if(model2add == null) {
model2add = createModel();
label2model.put(actual_label, model2add);
PredictionModel model2sub = null;
model2sub = label2model.get(missed_label);
if(model2sub == null) {
model2sub = createModel();
label2model.put(missed_label, model2sub);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
float old_trueclass_w = model2add.getWeight(k);
model2add.set(k, new WeightValue(add_w));
if(model2sub != null) {
float old_falseclass_w = model2sub.getWeight(k);
float sub_w = old_falseclass_w - (coeff * v);
model2sub.set(k, new WeightValue(sub_w));
if(label2model != null) {
final WeightValueWithCovar probe = new WeightValueWithCovar();
final FloatWritable fv = new FloatWritable();
final FloatWritable cov = new FloatWritable();
for(Map.Entry<Object, PredictionModel> entry : label2model.entrySet()) {
Object label = entry.getKey();
PredictionModel model = entry.getValue();
IMapIterator<Object, WeightValue> itor = model.entries();
while(itor.next() != -1) {
itor.getValue(probe);
if(skipUntouched && !probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
cov.set(probe.getCovariance());
final WeightValue probe = new WeightValue();
final FloatWritable fv = new FloatWritable();
for(Map.Entry<Object, PredictionModel> entry : label2model.entrySet()) {
Object label = entry.getKey();
PredictionModel model = entry.getValue();
IMapIterator<Object, WeightValue> itor = model.entries();
while(itor.next() != -1) {
itor.getValue(probe);
if(skipUntouched && !probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
this.label2model = null;
protected void loadPredictionModel(Map<Object, PredictionModel> label2model, String filename, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI) {
lines = loadPredictionModel(label2model, new File(filename), labelOI, featureOI, writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(label2model, new File(filename), labelOI, featureOI, writableFloatObjectInspector);
if(!label2model.isEmpty()) {
for(Map.Entry<Object, PredictionModel> e : label2model.entrySet()) {
private long loadPredictionModel(Map<Object, PredictionModel> label2model, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI)
PredictionModel model = label2model.get(label);
if(model == null) {
model = createModel();
label2model.put(label, model);
model.set(k, new WeightValue(v, false));
private long loadPredictionModel(Map<Object, PredictionModel> label2model, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
PredictionModel model = label2model.get(label);
if(model == null) {
model = createModel();
label2model.put(label, model);
model.set(k, new WeightValueWithCovar(v, cov, false));
import hivemall.common.PredictionModel;
PredictionModel model2add = label2model.get(actual_label);
if(model2add == null) {
model2add = createModel();
label2model.put(actual_label, model2add);
PredictionModel model2sub = null;
model2sub = label2model.get(missed_label);
if(model2sub == null) {
model2sub = createModel();
label2model.put(missed_label, model2sub);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_correctclass_w = model2add.get(k);
model2add.set(k, new_correctclass_w);
if(model2sub != null) {
WeightValue old_wrongclass_w = model2sub.get(k);
model2sub.set(k, new_wrongclass_w);
old_v = old.get();
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
model.set(k, new_w);
old_w = old.get();
this.parseFeature = STRING_TYPE_NAME.equals(keyTypeName);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(k);
if(!model.contains(feature)) {
model.set(feature, new WeightValue(v));
import hivemall.common.DenseModel;
import hivemall.common.PredictionModel;
import hivemall.common.SparseModel;
import hivemall.utils.collections.IMapIterator;
protected boolean parseFeature;
protected PredictionModel model;
this.model = dense_model ? new DenseModel(model_dims) : new SparseModel();
loadPredictionModel(model, preloadedModelFile, featureInputOI);
this.parseFeature = STRING_TYPE_NAME.equals(keyTypeName);
final boolean parseFeature = this.parseFeature;
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
if(old_w != 0f) {
final boolean parseX = this.parseFeature;
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
if(old_w != 0f) {
final boolean parseFeature = this.parseFeature;
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(x);
model.set(x, new WeightValue(new_w));
if(model != null) {
final WeightValueWithCovar probe = new WeightValueWithCovar();
final FloatWritable fv = new FloatWritable();
final FloatWritable cov = new FloatWritable();
final IMapIterator<Object, WeightValue> itor = model.entries();
itor.getValue(probe);
if(skipUntouched && !probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
cov.set(probe.getCovariance());
final WeightValue probe = new WeightValue();
final FloatWritable fv = new FloatWritable();
final IMapIterator<Object, WeightValue> itor = model.entries();
itor.getValue(probe);
if(skipUntouched && !probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
this.model = null;
@Description(name = "argmin_kld", value = "_FUNC_(mean, covar) - Returns mean or covar that minimize a KL-distance among distributions", extended = "The returned value is (1.0 / (sum(1.0 / covar))) * (sum(mean / covar)")
import hivemall.utils.lang.Primitives;
opts.addOption("dense", "densemodel", false, "Use dense model or not");
opts.addOption("dims", "feature_dimensions", true, "The dimension of model");
denseModel = cl.hasOption("densemodel");
if(denseModel) {
modelDims = Primitives.parseInt(cl.getOptionValue("dims"), 2 ^ 24);
public void copyTo(WeightValue another) {
another.value = this.value;
another.touched = this.touched;
public void copyTo(WeightValue another) {
super.copyTo(another);
((WeightValueWithCovar) another).covariance = this.covariance;
void copyTo(T another);
public void copyTo(MutableFloat another) {
another.setValue(value);
public void copyTo(MutableInt another) {
another.setValue(value);
int h = MurmurHash3.murmurhash3(data, numFeatures);
final int wlength = words.size();
return val(1);
final int wlength = words.size();
if(wlength == 0) {
return val(1);
}
opts.addOption("dims", "feature_dimensions", true, "The dimension of model [default: 16777216 (2^24)]");
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "is_stopword", value = "_FUNC_(word) - Returns whether stopword or not")
@UDFType(deterministic = true, stateful = false)
public final class StopwordUDF extends UDF {
this.lastEntry = curEntry;
final float y = label > 0 ? 1f : -1f;
protected void update(final List<?> features, final float y, final float alpha, final float beta) {
return v == null ? 1.f : v.covariance;
WeightValue v = new WeightValue(w);
v.setTouched(w != 0f);
return v;
WeightValueWithCovar v = new WeightValueWithCovar(w, cov);
v.setTouched(w != 0.f || cov != 1.f);
return v;
float w = weights[cursor];
tmpWeight.value = w;
float cov = 1.f;
cov = covars[cursor];
((WeightValueWithCovar) tmpWeight).covariance = cov;
tmpWeight.setTouched(w != 0.f || cov != 1.f);
public void setTouched(boolean touched) {
this.touched = touched;
}
if(!probe.isTouched()) {
if(!probe.isTouched()) {
if(!probe.isTouched()) {
if(!probe.isTouched()) {
if(!probe.isTouched()) {
if(!probe.isTouched()) {
return getReturnOI(featureInputOI);
denseModel = cl.hasOption("dense");
modelDims = Primitives.parseInt(cl.getOptionValue("dims"), 16777216);
import org.apache.commons.cli.Options;
final float y = label > 0 ? 1f : -1f;
private void checkBounds(final int index) {
if(index >= size) {
size);
}
}
checkBounds(i);
checkBounds(i);
checkBounds(i);
checkBounds(i);
checkBounds(i);
checkBounds(i);
checkBounds(i);
import hivemall.common.DenseModel;
import hivemall.common.SparseModel;
protected PredictionModel createModel() {
return dense_model ? new DenseModel(model_dims) : new SparseModel();
}
PrimitiveObjectInspector featureOutputOI = dense_model ? PrimitiveObjectInspectorFactory.javaIntObjectInspector
: featureInputOI;
this.model = createModel();
loadPredictionModel(model, preloadedModelFile, featureOutputOI);
return getReturnOI(featureOutputOI);
PrimitiveObjectInspector featureOutputOI = dense_model ? PrimitiveObjectInspectorFactory.javaIntObjectInspector
: featureInputOI;
loadPredictionModel(label2model, preloadedModelFile, labelInputOI, featureOutputOI);
return getReturnOI(labelInputOI, featureOutputOI);
@Override
protected PredictionModel createModel() {
PrimitiveObjectInspector featureOutputOI = dense_model ? PrimitiveObjectInspectorFactory.javaIntObjectInspector
: featureInputOI;
this.model = createModel();
loadPredictionModel(model, preloadedModelFile, featureOutputOI);
return getReturnOI(featureOutputOI);
import hivemall.utils.math.MathUtils;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
public final class DenseModel implements PredictionModel {
private static final Log logger = LogFactory.getLog(DenseModel.class);
private int size;
private float[] weights;
private float[] covars;
Arrays.fill(covars, 1f);
private void ensureCapacity(final int index) {
int bits = MathUtils.bitsRequired(index);
int oldSize = size;
this.size = newSize;
this.weights = Arrays.copyOf(weights, newSize);
if(covars != null) {
this.covars = Arrays.copyOf(covars, newSize);
Arrays.fill(covars, oldSize, newSize, 1f);
}
if(i >= size) {
return null;
}
ensureCapacity(i);
if(i >= size) {
return 0f;
}
if(i >= size) {
return 1f;
}
ensureCapacity(i);
ensureCapacity(i);
if(i >= size) {
return false;
}
return (IMapIterator<K, V>) new Itr();
private final WeightValueWithCovar tmpWeight;
private Itr() {
this.tmpWeight = new WeightValueWithCovar();
return cursor < size;
cursor;
tmpWeight.covariance = cov;
protected boolean useCovariance() {
return dense_model ? new DenseModel(model_dims, useCovariance()) : new SparseModel();
if(useCovariance()) {
protected boolean useCovariance() {
if(useCovariance()) {
if(useCovariance()) {
protected boolean useCovariance() {
protected boolean useCovariance() {
protected boolean useCovariance() {
protected boolean useCovariance() {
return dense_model ? new DenseModel(model_dims, useCovariance()) : new SparseModel(8192);
if(useCovariance()) {
if(useCovariance()) {
if(useCovariance()) {
protected boolean useCovariance() {
protected boolean useCovariance() {
if(useCovariance()) {
if(useCovariance()) {
final float y = label > 0 ? 1f : -1f;
protected void update(List<?> features, float y, float p) {
protected void update(List<?> features, float y, float p) {
public static final String CONFKEY_RAND_AMPLIFY_SEED = "hivemall.amplify.seed";
public final class RandomizedAmplifier<T> {
@SuppressWarnings("unchecked")
public RandomizedAmplifier(int numBuffers, int xtimes, long seed) {
if(numBuffers < 1) {
}
if(xtimes < 1) {
}
this.numBuffers = numBuffers;
this.xtimes = xtimes;
this.slots = new AgedObject[xtimes][numBuffers];
this.position = 0;
this.randoms = new Random[xtimes];
}
}
import hivemall.HivemallConstants;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hive.ql.exec.MapredContext;
import org.apache.hadoop.mapred.JobConf;
private boolean useSeed;
private long seed;
private transient ObjectInspector[] argOIs;
public void configure(MapredContext mapredContext) {
super.configure(mapredContext);
JobConf jobconf = mapredContext.getJobConf();
String seed = jobconf.get(HivemallConstants.CONFKEY_RAND_AMPLIFY_SEED);
this.useSeed = (seed != null);
if(useSeed) {
this.seed = Long.parseLong(seed);
}
}
@Override
this.amplifier = useSeed ? new RandomizedAmplifier<Object[]>(numBuffers, xtimes, seed)
: new RandomizedAmplifier<Object[]>(numBuffers, xtimes);
if(useSeed) {
}
public static final short ZERO = 0;
public static float halfFloatToFloat(final short f16) {
int i = ((f16 & 0xFFFF) >> 10) & 0xFF;
public static short floatToHalfFloat(final float f32) {
int bits = Float.floatToRawIntBits(f32);
public static int halfFloatToFloatBits(final short f16) {
int i = f16 >> 10;
}
public static short floatBitsToHalfFloat(final int f32b) {
int i = (f32b >> 23) & 0x1FF;
}
public static final short ONE;
ONE = floatToHalfFloat(1f);
protected int getInitialModelSize() {
return 8192;
public static final float MAX_FLOAT = 65520;
public static final float EPSILON = 0.00097656f;
import hivemall.common.SpaceEfficientDenseModel;
protected boolean disable_halffloat;
opts.addOption("disable_halffloat", false, "Toggle this option to disable the use of SpaceEfficientDenseModel");
boolean disableHalfFloat = false;
disableHalfFloat = cl.hasOption("disable_halffloat");
this.disable_halffloat = disableHalfFloat;
if(dense_model) {
if(model_dims > 16777216) {
return new SpaceEfficientDenseModel(model_dims, useCovariance());
} else {
return new DenseModel(model_dims, useCovariance());
}
} else {
return new SparseModel(getInitialModelSize());
}
}
protected int getInitialModelSize() {
return 16384;
boolean useCovar = useCovariance();
return new SpaceEfficientDenseModel(model_dims, useCovar);
return new DenseModel(model_dims, useCovar);
int initModelSize = getInitialModelSize();
" initial dimensions");
return new SparseModel(initModelSize);
public static final String BIAS_CLAUSE = "0";
public static final String CONFKEY_RAND_AMPLIFY_SEED = "hivemall.amplify.seed";
import hivemall.common.DenseModel;
import hivemall.common.PredictionModel;
import hivemall.common.SpaceEfficientDenseModel;
import hivemall.common.SparseModel;
import hivemall.utils.lang.Primitives;
protected boolean dense_model;
protected int model_dims;
protected boolean disable_halffloat;
protected boolean useCovariance() {
opts.addOption("dense", "densemodel", false, "Use dense model or not");
opts.addOption("dims", "feature_dimensions", true, "The dimension of model [default: 16777216 (2^24)]");
opts.addOption("disable_halffloat", false, "Toggle this option to disable the use of SpaceEfficientDenseModel");
boolean denseModel = false;
int modelDims = -1;
boolean disableHalfFloat = false;
denseModel = cl.hasOption("dense");
if(denseModel) {
modelDims = Primitives.parseInt(cl.getOptionValue("dims"), 16777216);
disableHalfFloat = cl.hasOption("disable_halffloat");
this.dense_model = denseModel;
this.model_dims = modelDims;
this.disable_halffloat = disableHalfFloat;
protected PredictionModel createModel() {
if(dense_model) {
boolean useCovar = useCovariance();
if(model_dims > 16777216) {
return new SpaceEfficientDenseModel(model_dims, useCovar);
} else {
return new DenseModel(model_dims, useCovar);
}
} else {
int initModelSize = getInitialModelSize();
" initial dimensions");
return new SparseModel(initModelSize);
}
}
protected int getInitialModelSize() {
return 16384;
}
protected void loadPredictionModel(PredictionModel model, String filename, PrimitiveObjectInspector keyOI) {
if(useCovariance()) {
lines = loadPredictionModel(model, new File(filename), keyOI, writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(model, new File(filename), keyOI, writableFloatObjectInspector);
if(model.size() > 0) {
private static long loadPredictionModel(PredictionModel model, File file, PrimitiveObjectInspector keyOI, WritableFloatObjectInspector valueOI)
model.set(k, new WeightValue(v, false));
private static long loadPredictionModel(PredictionModel model, File file, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
model.set(k, new WeightValueWithCovar(v, cov, false));
protected boolean useCovariance() {
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
model.set(k, new_w);
old_w = old.get();
import hivemall.common.PredictionModel;
import hivemall.utils.collections.IMapIterator;
protected boolean parseFeature;
protected PredictionModel model;
PrimitiveObjectInspector featureOutputOI = dense_model ? PrimitiveObjectInspectorFactory.javaIntObjectInspector
: featureInputOI;
this.model = createModel();
loadPredictionModel(model, preloadedModelFile, featureOutputOI);
this.parseFeature = STRING_TYPE_NAME.equals(keyTypeName);
if(useCovariance()) {
final float y = label > 0 ? 1f : -1f;
final boolean parseFeature = this.parseFeature;
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
if(old_w != 0f) {
final boolean parseX = this.parseFeature;
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
if(old_w != 0f) {
final boolean parseFeature = this.parseFeature;
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
protected void update(List<?> features, float y, float p) {
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
model.set(k, new WeightValue(new_w));
if(model != null) {
if(useCovariance()) {
final WeightValueWithCovar probe = new WeightValueWithCovar();
final FloatWritable fv = new FloatWritable();
final FloatWritable cov = new FloatWritable();
final IMapIterator<Object, WeightValue> itor = model.entries();
itor.getValue(probe);
if(!probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
cov.set(probe.getCovariance());
final WeightValue probe = new WeightValue();
final FloatWritable fv = new FloatWritable();
final IMapIterator<Object, WeightValue> itor = model.entries();
itor.getValue(probe);
if(!probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
this.model = null;
protected boolean useCovariance() {
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
model.set(k, new_w);
import org.apache.commons.cli.Options;
final float y = label > 0 ? 1f : -1f;
protected void update(List<?> features, float y, float p) {
protected boolean useCovariance() {
final float y = label > 0 ? 1f : -1f;
protected void update(final List<?> features, final float y, final float alpha, final float beta) {
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
model.set(k, new_w);
old_v = old.get();
import hivemall.common.PredictionModel;
protected boolean useCovariance() {
PredictionModel model2add = label2model.get(actual_label);
if(model2add == null) {
model2add = createModel();
label2model.put(actual_label, model2add);
PredictionModel model2sub = null;
model2sub = label2model.get(missed_label);
if(model2sub == null) {
model2sub = createModel();
label2model.put(missed_label, model2sub);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_correctclass_w = model2add.get(k);
model2add.set(k, new_correctclass_w);
if(model2sub != null) {
WeightValue old_wrongclass_w = model2sub.get(k);
model2sub.set(k, new_wrongclass_w);
old_v = old.get();
import hivemall.common.PredictionModel;
protected boolean useCovariance() {
PredictionModel model2add = label2model.get(actual_label);
if(model2add == null) {
model2add = createModel();
label2model.put(actual_label, model2add);
PredictionModel model2sub = null;
model2sub = label2model.get(missed_label);
if(model2sub == null) {
model2sub = createModel();
label2model.put(missed_label, model2sub);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_correctclass_w = model2add.get(k);
model2add.set(k, new_correctclass_w);
if(model2sub != null) {
WeightValue old_wrongclass_w = model2sub.get(k);
model2sub.set(k, new_wrongclass_w);
import hivemall.common.PredictionModel;
import hivemall.utils.collections.IMapIterator;
protected boolean parseFeature;
protected Map<Object, PredictionModel> label2model;
PrimitiveObjectInspector featureOutputOI = dense_model ? PrimitiveObjectInspectorFactory.javaIntObjectInspector
: featureInputOI;
this.label2model = new HashMap<Object, PredictionModel>(64);
loadPredictionModel(label2model, preloadedModelFile, labelInputOI, featureOutputOI);
@Override
protected int getInitialModelSize() {
return 8192;
}
this.parseFeature = STRING_TYPE_NAME.equals(keyTypeName);
if(useCovariance()) {
PredictionModel model = label2map.getValue();
float score = calcScore(model, features);
PredictionModel model = label2map.getValue();
float score = calcScore(model, features);
PredictionModel model = label2map.getValue();
PredictionResult predicted = calcScoreAndVariance(model, features);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
protected final float calcScore(final PredictionModel model, final List<?> features) {
final boolean parseX = this.parseFeature;
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
if(old_w != 0f) {
final boolean parseX = this.parseFeature;
FeatureValue fv = FeatureValue.parse(f);
protected final PredictionResult calcScoreAndVariance(final PredictionModel model, final List<?> features) {
final boolean parseX = this.parseFeature;
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
PredictionModel model2add = label2model.get(actual_label);
if(model2add == null) {
model2add = createModel();
label2model.put(actual_label, model2add);
PredictionModel model2sub = null;
model2sub = label2model.get(missed_label);
if(model2sub == null) {
model2sub = createModel();
label2model.put(missed_label, model2sub);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
float old_trueclass_w = model2add.getWeight(k);
model2add.set(k, new WeightValue(add_w));
if(model2sub != null) {
float old_falseclass_w = model2sub.getWeight(k);
float sub_w = old_falseclass_w - (coeff * v);
model2sub.set(k, new WeightValue(sub_w));
if(label2model != null) {
if(useCovariance()) {
final WeightValueWithCovar probe = new WeightValueWithCovar();
final FloatWritable fv = new FloatWritable();
final FloatWritable cov = new FloatWritable();
for(Map.Entry<Object, PredictionModel> entry : label2model.entrySet()) {
Object label = entry.getKey();
PredictionModel model = entry.getValue();
IMapIterator<Object, WeightValue> itor = model.entries();
while(itor.next() != -1) {
itor.getValue(probe);
if(!probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
cov.set(probe.getCovariance());
final WeightValue probe = new WeightValue();
final FloatWritable fv = new FloatWritable();
for(Map.Entry<Object, PredictionModel> entry : label2model.entrySet()) {
Object label = entry.getKey();
PredictionModel model = entry.getValue();
IMapIterator<Object, WeightValue> itor = model.entries();
while(itor.next() != -1) {
itor.getValue(probe);
if(!probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
this.label2model = null;
protected void loadPredictionModel(Map<Object, PredictionModel> label2model, String filename, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI) {
if(useCovariance()) {
lines = loadPredictionModel(label2model, new File(filename), labelOI, featureOI, writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(label2model, new File(filename), labelOI, featureOI, writableFloatObjectInspector);
if(!label2model.isEmpty()) {
for(Map.Entry<Object, PredictionModel> e : label2model.entrySet()) {
private long loadPredictionModel(Map<Object, PredictionModel> label2model, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI)
PredictionModel model = label2model.get(label);
if(model == null) {
model = createModel();
label2model.put(label, model);
model.set(k, new WeightValue(v, false));
private long loadPredictionModel(Map<Object, PredictionModel> label2model, File file, PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
PredictionModel model = label2model.get(label);
if(model == null) {
model = createModel();
label2model.put(label, model);
model.set(k, new WeightValueWithCovar(v, cov, false));
import hivemall.common.PredictionModel;
protected boolean useCovariance() {
PredictionModel model2add = label2model.get(actual_label);
if(model2add == null) {
model2add = createModel();
label2model.put(actual_label, model2add);
PredictionModel model2sub = null;
model2sub = label2model.get(missed_label);
if(model2sub == null) {
model2sub = createModel();
label2model.put(missed_label, model2sub);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_correctclass_w = model2add.get(k);
model2add.set(k, new_correctclass_w);
if(model2sub != null) {
WeightValue old_wrongclass_w = model2sub.get(k);
model2sub.set(k, new_wrongclass_w);
old_v = old.get();
public static FeatureValue parse(Object o) {
public final class RandomizedAmplifier<T> {
@SuppressWarnings("unchecked")
public RandomizedAmplifier(int numBuffers, int xtimes, long seed) {
if(numBuffers < 1) {
}
if(xtimes < 1) {
}
this.numBuffers = numBuffers;
this.xtimes = xtimes;
this.slots = new AgedObject[xtimes][numBuffers];
this.position = 0;
this.randoms = new Random[xtimes];
}
}
import hivemall.utils.lang.Copyable;
public class WeightValue implements Copyable<WeightValue> {
protected float value;
protected boolean touched;
public WeightValue() {}
public void set(float weight) {
this.value = weight;
}
public boolean hasCovariance() {
return false;
public void setCovariance(float cov) {
throw new UnsupportedOperationException();
}
public void setTouched(boolean touched) {
this.touched = touched;
}
@Override
public void copyTo(WeightValue another) {
another.value = this.value;
another.touched = this.touched;
}
@Override
public void copyFrom(WeightValue another) {
this.value = another.value;
this.touched = another.touched;
}
float covariance;
public WeightValueWithCovar() {
super();
}
@Override
public boolean hasCovariance() {
return true;
}
@Override
public void setCovariance(float cov) {
this.covariance = cov;
}
@Override
public void copyTo(WeightValue another) {
super.copyTo(another);
((WeightValueWithCovar) another).covariance = this.covariance;
}
@Override
public void copyFrom(WeightValue another) {
super.copyFrom(another);
this.covariance = ((WeightValueWithCovar) another).covariance;
}
@Override
@Description(name = "argmin_kld", value = "_FUNC_(mean, covar) - Returns mean or covar that minimize a KL-distance among distributions", extended = "The returned value is (1.0 / (sum(1.0 / covar))) * (sum(mean / covar)")
@Description(name = "add_bias", value = "_FUNC_(feature_vector in array<string>) - Returns features with a bias in array<string>")
return evaluate(ftvec, HivemallConstants.BIAS_CLAUSE);
import hivemall.HivemallConstants;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hive.ql.exec.MapredContext;
import org.apache.hadoop.mapred.JobConf;
private boolean useSeed;
private long seed;
private transient ObjectInspector[] argOIs;
public void configure(MapredContext mapredContext) {
super.configure(mapredContext);
JobConf jobconf = mapredContext.getJobConf();
String seed = jobconf.get(HivemallConstants.CONFKEY_RAND_AMPLIFY_SEED);
this.useSeed = (seed != null);
if(useSeed) {
this.seed = Long.parseLong(seed);
}
}
@Override
this.amplifier = useSeed ? new RandomizedAmplifier<Object[]>(numBuffers, xtimes, seed)
: new RandomizedAmplifier<Object[]>(numBuffers, xtimes);
if(useSeed) {
}
int h = MurmurHash3.murmurhash3(data, numFeatures);
return val(1);
final int wlength = words.size();
return val(1);
private boolean parseFeature;
this.parseFeature = STRING_TYPE_NAME.equals(keyTypeName);
List<FeatureValue> ftvec = parseFeatures(features, featureInspector, parseFeature);
fv = FeatureValue.parse(f);
fv = FeatureValue.parse(f);
fv = FeatureValue.parse(f);
protected boolean useCovariance() {
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
model.set(k, new_w);
old_w = old.get();
this.parseFeature = STRING_TYPE_NAME.equals(keyTypeName);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(k);
if(!model.contains(feature)) {
model.set(feature, new WeightValue(v));
import hivemall.common.PredictionModel;
import hivemall.utils.collections.IMapIterator;
protected boolean parseFeature;
protected PredictionModel model;
PrimitiveObjectInspector featureOutputOI = dense_model ? PrimitiveObjectInspectorFactory.javaIntObjectInspector
: featureInputOI;
this.model = createModel();
loadPredictionModel(model, preloadedModelFile, featureOutputOI);
this.parseFeature = STRING_TYPE_NAME.equals(keyTypeName);
if(useCovariance()) {
final boolean parseFeature = this.parseFeature;
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
if(old_w != 0f) {
final boolean parseX = this.parseFeature;
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(k);
if(old_w != 0f) {
final boolean parseFeature = this.parseFeature;
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
WeightValue old_w = model.get(k);
if(parseFeature) {
FeatureValue fv = FeatureValue.parse(f);
float old_w = model.getWeight(x);
model.set(x, new WeightValue(new_w));
if(model != null) {
if(useCovariance()) {
final WeightValueWithCovar probe = new WeightValueWithCovar();
final FloatWritable fv = new FloatWritable();
final FloatWritable cov = new FloatWritable();
final IMapIterator<Object, WeightValue> itor = model.entries();
itor.getValue(probe);
if(!probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
cov.set(probe.getCovariance());
final WeightValue probe = new WeightValue();
final FloatWritable fv = new FloatWritable();
final IMapIterator<Object, WeightValue> itor = model.entries();
itor.getValue(probe);
if(!probe.isTouched()) {
Object k = itor.getKey();
fv.set(probe.get());
this.model = null;
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "is_stopword", value = "_FUNC_(word) - Returns whether stopword or not")
@UDFType(deterministic = true, stateful = false)
public final class StopwordUDF extends UDF {
import hivemall.utils.lang.Copyable;
@Override
@Override
free(lastEntry);
this.lastEntry = curEntry;
@Override
@Override
public <T extends Copyable<V>> void getValue(T probe) {
probe.copyFrom(getValue());
private void free(int index) {
if(index >= 0) {
keys[index] = null;
values[index] = null;
}
import org.apache.hadoop.io.LongWritable;
public static int parseInt(final Object o) {
if(o instanceof Integer) {
return ((Integer) o).intValue();
}
if(o instanceof IntWritable) {
return ((IntWritable) o).get();
}
if(o instanceof LongWritable) {
long l = ((LongWritable) o).get();
if(l > 0x7fffffffL) {
throw new IllegalArgumentException("feature index must be less than "
}
return (int) l;
}
String s = o.toString();
return Integer.parseInt(s);
}
if(disable_halffloat == false && model_dims > 16777216) {
if(disable_halffloat == false && model_dims > 16777216) {
import hivemall.io.DenseModel;
import hivemall.io.PredictionModel;
import hivemall.io.SpaceEfficientDenseModel;
import hivemall.io.SparseModel;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionModel;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.PredictionResult;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.Margin;
import hivemall.io.PredictionModel;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.Margin;
import hivemall.io.PredictionModel;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.Margin;
import hivemall.io.PredictionModel;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.Margin;
import hivemall.io.PredictionResult;
import hivemall.io.FeatureValue;
import hivemall.io.Margin;
import hivemall.io.PredictionModel;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
package hivemall.io;
import hivemall.io.WeightValue.WeightValueWithCovar;
package hivemall.io;
package hivemall.io;
package hivemall.io;
package hivemall.io;
package hivemall.io;
import hivemall.io.WeightValue.WeightValueWithCovar;
package hivemall.io;
import hivemall.io.WeightValue.WeightValueWithCovar;
package hivemall.io;
import hivemall.io.FeatureValue;
import hivemall.io.FeatureValue;
import hivemall.io.FeatureValue;
import hivemall.io.FeatureValue;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.WeightValue;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionModel;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.PredictionResult;
import hivemall.utils.lang.CommandLineUtils;
return CommandLineUtils.parseOptions(args, opts);
import hivemall.io.FeatureValue;
import hivemall.utils.hadoop.WritableUtils;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
import org.apache.hadoop.io.Writable;
protected List<FeatureValue> parseFeatures(final List<?> features, final ObjectInspector featureInspector, final boolean parseFeature) {
final int numFeatures = features.size();
if(numFeatures == 0) {
return Collections.emptyList();
}
final List<FeatureValue> list = new ArrayList<FeatureValue>(numFeatures);
for(Object f : features) {
if(f == null) {
continue;
}
final FeatureValue fv;
if(parseFeature) {
fv = FeatureValue.parse(f);
} else {
Object o = ObjectInspectorUtils.copyToStandardObject(f, featureInspector, ObjectInspectorCopyOption.WRITABLE);
Writable k = WritableUtils.toWritable(o);
fv = new FeatureValue(k, 1.f);
}
list.add(fv);
}
return list;
}
import org.apache.hadoop.io.ByteWritable;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.VIntWritable;
import org.apache.hadoop.io.VLongWritable;
import org.apache.hadoop.io.Writable;
public static Writable toWritable(Object object) {
if(object == null) {
}
if(object instanceof Writable) {
return (Writable) object;
}
if(object instanceof String) {
return new Text((String) object);
}
if(object instanceof Long) {
return new VLongWritable((Long) object);
}
if(object instanceof Integer) {
return new VIntWritable((Integer) object);
}
if(object instanceof Byte) {
return new ByteWritable((Byte) object);
}
if(object instanceof Double) {
return new DoubleWritable((Double) object);
}
if(object instanceof Float) {
return new FloatWritable((Float) object);
}
if(object instanceof Boolean) {
return new BooleanWritable((Boolean) object);
}
if(object instanceof byte[]) {
return new BytesWritable((byte[]) object);
}
return new BytesWritable(object.toString().getBytes());
}
import hivemall.io.DenseModel;
import hivemall.io.PredictionModel;
import hivemall.io.SpaceEfficientDenseModel;
import hivemall.io.SparseModel;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.utils.hadoop.WritableUtils;
import hivemall.utils.lang.CommandLineUtils;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
import org.apache.hadoop.io.Writable;
return CommandLineUtils.parseOptions(args, opts);
protected List<FeatureValue> parseFeatures(final List<?> features, final ObjectInspector featureInspector, final boolean parseFeature) {
final int numFeatures = features.size();
if(numFeatures == 0) {
return Collections.emptyList();
}
final List<FeatureValue> list = new ArrayList<FeatureValue>(numFeatures);
for(Object f : features) {
if(f == null) {
continue;
}
final FeatureValue fv;
if(parseFeature) {
fv = FeatureValue.parse(f);
} else {
Object o = ObjectInspectorUtils.copyToStandardObject(f, featureInspector, ObjectInspectorCopyOption.WRITABLE);
Writable k = WritableUtils.toWritable(o);
fv = new FeatureValue(k, 1.f);
}
list.add(fv);
}
return list;
}
import hivemall.io.FeatureValue;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionModel;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.PredictionResult;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.Margin;
import hivemall.io.PredictionModel;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.Margin;
import hivemall.io.PredictionModel;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.Margin;
import hivemall.io.PredictionModel;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.Margin;
import hivemall.io.PredictionResult;
import hivemall.io.FeatureValue;
import hivemall.io.Margin;
import hivemall.io.PredictionModel;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
package hivemall.io;
import hivemall.io.WeightValue.WeightValueWithCovar;
package hivemall.io;
package hivemall.io;
package hivemall.io;
package hivemall.io;
package hivemall.io;
import hivemall.io.WeightValue.WeightValueWithCovar;
package hivemall.io;
import hivemall.io.WeightValue.WeightValueWithCovar;
package hivemall.io;
import hivemall.io.FeatureValue;
import hivemall.io.FeatureValue;
import hivemall.io.FeatureValue;
import hivemall.io.FeatureValue;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.FeatureValue;
import hivemall.io.WeightValue;
import hivemall.io.FeatureValue;
import hivemall.io.PredictionModel;
import hivemall.io.PredictionResult;
import hivemall.io.WeightValue;
import hivemall.io.WeightValue.WeightValueWithCovar;
import hivemall.io.PredictionResult;
import org.apache.hadoop.io.ByteWritable;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.VIntWritable;
import org.apache.hadoop.io.VLongWritable;
import org.apache.hadoop.io.Writable;
public static Writable toWritable(Object object) {
if(object == null) {
}
if(object instanceof Writable) {
return (Writable) object;
}
if(object instanceof String) {
return new Text((String) object);
}
if(object instanceof Long) {
return new VLongWritable((Long) object);
}
if(object instanceof Integer) {
return new VIntWritable((Integer) object);
}
if(object instanceof Byte) {
return new ByteWritable((Byte) object);
}
if(object instanceof Double) {
return new DoubleWritable((Double) object);
}
if(object instanceof Float) {
return new FloatWritable((Float) object);
}
if(object instanceof Boolean) {
return new BooleanWritable((Boolean) object);
}
if(object instanceof byte[]) {
return new BytesWritable((byte[]) object);
}
return new BytesWritable(object.toString().getBytes());
}
public final class DenseModel extends PredictionModel {
private short[] clocks;
super();
this.clocks = null;
}
@Override
public void configureClock() {
if(clocks == null) {
this.clocks = new short[size];
}
}
@Override
public boolean hasClock() {
return clocks != null;
if(clocks != null) {
this.clocks = Arrays.copyOf(clocks, newSize);
}
float covar = 1.f;
covar = value.getCovariance();
short clock = 0;
if(clocks != null && value.isTouched()) {
clocks[i] = clock;
}
onUpdate(i, weight, covar, clock);
short clock = 0;
if(clocks != null) {
clocks[i] = clock;
}
onUpdate(i, weight, clock);
short clock = 0;
if(clocks != null) {
clocks[i] = clock;
}
onUpdate(i, weight, covar, clock);
configureClock(v, cursor, w);
configureClock(v, cursor, w, cov);
configureClock(tmpWeight, cursor, w, cov);
void configureClock(final WeightValue weight, final int index, final float w) {
if(clocks == null) {
if(w != 0.f) {
weight.setClock((short) 1);
}
} else {
weight.setClock(clocks[index]);
}
}
void configureClock(final WeightValue weight, final int index, final float w, final float cov) {
if(clocks == null) {
if(w != 0.f || cov != 1.f) {
weight.setClock((short) 1);
}
} else {
weight.setClock(clocks[index]);
}
}
public abstract class PredictionModel {
protected ModelUpdateHandler handler;
public PredictionModel() {}
public ModelUpdateHandler getUpdateHandler() {
return handler;
}
public void setUpdateHandler(ModelUpdateHandler handler) {
this.handler = handler;
}
protected final void onUpdate(final int feature, final float weight, final short clock) {
if(handler != null) {
handler.onUpdate(feature, weight, 1.f, clock);
}
}
protected final void onUpdate(final int feature, final float weight, final float covar, final short clock) {
if(handler != null) {
handler.onUpdate(feature, weight, covar, clock);
}
}
protected final void onUpdate(final Object feature, final WeightValue value) {
if(handler != null) {
float weight = value.get();
short clock = value.getClock();
if(value.hasCovariance()) {
float covar = value.getCovariance();
handler.onUpdate(feature, weight, covar, clock);
} else {
handler.onUpdate(feature, weight, 1.f, clock);
}
}
}
public abstract void configureClock();
public abstract boolean hasClock();
public abstract int size();
public abstract boolean contains(Object feature);
public abstract <T extends WeightValue> T get(Object feature);
public abstract <T extends WeightValue> void set(Object feature, T value);
public abstract float getWeight(Object feature);
public abstract float getCovariance(Object feature);
@Deprecated
public abstract void setValue(Object feature, float weight);
@Deprecated
public abstract void setValue(Object feature, float weight, float covar);
public abstract <K, V extends WeightValue> IMapIterator<K, V> entries();
public final class SpaceEfficientDenseModel extends PredictionModel {
private short[] clocks;
super();
this.clocks = null;
}
@Override
public void configureClock() {
if(clocks == null) {
this.clocks = new short[size];
}
}
@Override
public boolean hasClock() {
return clocks != null;
if(clocks != null) {
this.clocks = Arrays.copyOf(clocks, newSize);
}
float covar = 1.f;
covar = value.getCovariance();
short clock = 0;
if(clocks != null && value.isTouched()) {
clocks[i] = clock;
}
onUpdate(i, weight, covar, clock);
short clock = 0;
if(clocks != null) {
clocks[i] = clock;
}
onUpdate(i, weight, clock);
short clock = 0;
if(clocks != null) {
clocks[i] = clock;
}
onUpdate(i, weight, covar, clock);
configureClock(v, cursor, w);
configureClock(v, cursor, w, cov);
configureClock(tmpWeight, cursor, w, cov);
void configureClock(final WeightValue weight, final int index, final float w) {
if(clocks == null) {
if(w != 0.f) {
weight.setClock((short) 1);
}
} else {
weight.setClock(clocks[index]);
}
}
void configureClock(final WeightValue weight, final int index, final float w, final float cov) {
if(clocks == null) {
if(w != 0.f || cov != 1.f) {
weight.setClock((short) 1);
}
} else {
weight.setClock(clocks[index]);
}
}
public final class SparseModel extends PredictionModel {
private boolean clockEnabled;
super();
this.clockEnabled = false;
}
@Override
public void configureClock() {
this.clockEnabled = true;
}
@Override
public boolean hasClock() {
return clockEnabled;
assert (feature != null);
assert (value != null);
if(clockEnabled && value.isTouched()) {
WeightValue old = weights.get(feature);
if(old != null) {
value.setClock(newclock);
}
}
set(feature, new WeightValue(weight));
set(feature, new WeightValueWithCovar(weight, covar));
protected short clock;
this.clock = touched ? (short) 1 : (short) 0;
return clock > 0;
public short getClock() {
return clock;
}
public void setClock(short clock) {
this.clock = clock;
another.clock = this.clock;
this.clock = another.clock;
public static String getJobId() {
MapredContext ctx = MapredContextAccessor.get();
JobConf conf = ctx.getJobConf();
return conf.get("mapred.job.id");
}
public static short parseShort(String s, short defaultValue) {
if(s == null) {
return defaultValue;
}
return Short.parseShort(s);
}
public boolean hasCovariance() {
return covars != null;
}
@Override
public void _set(Object feature, float weight, short clock) {
clocks[i] = clock;
public void _set(Object feature, float weight, float covar, short clock) {
clocks[i] = clock;
void onUpdate(Object feature, float weight, float covar, short clock) throws Exception;
try {
handler.onUpdate(feature, weight, covar, clock);
} catch (Exception e) {
throw new RuntimeException(e);
}
final float weight = value.get();
final short clock = value.getClock();
final float covar = value.getCovariance();
try {
handler.onUpdate(feature, weight, covar, clock);
} catch (Exception e) {
throw new RuntimeException(e);
}
try {
handler.onUpdate(feature, weight, 1.f, clock);
} catch (Exception e) {
throw new RuntimeException(e);
}
public abstract boolean hasCovariance();
public abstract void _set(Object feature, float weight, short clock);
public abstract void _set(Object feature, float weight, float covar, short clock);
public boolean hasCovariance() {
return covars != null;
}
@Override
public void _set(Object feature, float weight, short clock) {
clocks[i] = clock;
public void _set(Object feature, float weight, float covar, short clock) {
clocks[i] = clock;
private final boolean hasCovar;
public SparseModel(int size, boolean hasCovar) {
this.hasCovar = hasCovar;
public boolean hasCovariance() {
return hasCovar;
}
@Override
onUpdate(feature, value);
public void _set(Object feature, float weight, short clock) {
WeightValue w = new WeightValue(weight);
w.setClock(clock);
weights.put(feature, w);
public void _set(Object feature, float weight, float covar, short clock) {
WeightValue w = new WeightValueWithCovar(weight, covar);
w.setClock(clock);
weights.put(feature, w);
import hivemall.mix.MixMessage.MixEventName;
import hivemall.mix.client.MixClient;
import hivemall.utils.io.IOUtils;
import org.apache.hadoop.hive.ql.metadata.HiveException;
protected String mixConnectInfo;
protected boolean ssl;
protected MixClient client;
opts.addOption("mix", "mix_servers", true, "Comma separated list of MIX servers");
opts.addOption("ssl", false, "Use SSL for the communication with mix servers");
String mixConnectInfo = null;
boolean ssl = false;
mixConnectInfo = cl.getOptionValue("mix");
ssl = cl.hasOption("ssl");
this.mixConnectInfo = mixConnectInfo;
this.ssl = ssl;
final PredictionModel model;
final boolean useCovar = useCovariance();
model = new SpaceEfficientDenseModel(model_dims, useCovar);
model = new DenseModel(model_dims, useCovar);
model = new SparseModel(initModelSize, useCovar);
if(mixConnectInfo != null) {
MixClient client = configureMixClient(mixConnectInfo, useCovar, model);
model.setUpdateHandler(client);
this.client = client;
}
return model;
}
protected MixClient configureMixClient(String connectURIs, boolean ssl, PredictionModel model) {
assert (connectURIs != null);
assert (model != null);
String jobId = HadoopUtils.getJobId();
MixEventName event = useCovariance() ? MixEventName.argminKLD : MixEventName.average;
MixClient client = new MixClient(event, jobId, connectURIs, ssl, model);
return client;
@Override
public void close() throws HiveException {
if(client != null) {
IOUtils.closeQuietly(client);
this.client = null;
}
}
public final void close() throws HiveException {
super.close();
public final void close() throws HiveException {
super.close();
public final void close() throws HiveException {
super.close();
}
assert (clock >= 0) : clock;
assert (clock >= 0) : clock;
assert (newclock >= 0) : newclock;
private float sum_mean_div_covar;
private float sum_inv_covar;
public void add(float localWeight, float covar, short clock) {
private float scaledSumWeights;
public void add(float localWeight, float covar, short clock) {
protected float minCovariance;
protected short totalClock;
assert (totalClock >= 0) : totalClock;
private static final short CLOCK_ZERO = 0;
responseMsg = new MixMessage(event, feature, averagedWeight, minCovar, CLOCK_ZERO);
this.syncThreshold = Primitives.parseShort(cl.getOptionValue("sync"), (short) 30);
opts.addOption("sync", "sync_threshold", true, "Synchronization threshold using clock difference [default: 30]");
protected int mixThreshold;
opts.addOption("mix_threshold", true, "Threshold to mix local updates [default: 2]");
int mixThreshold = 2;
mixThreshold = Primitives.parseInt(cl.getOptionValue("mix_threshold"), 2);
this.mixThreshold = mixThreshold;
model.configureClock();
MixClient client = configureMixClient(mixConnectInfo, model);
protected MixClient configureMixClient(String connectURIs, PredictionModel model) {
MixClient client = new MixClient(event, jobId, connectURIs, ssl, mixThreshold, model);
private final int mixThreshold;
public MixClient(MixEventName event, String groupID, String connectURIs, boolean ssl, int mixThreshold, PredictionModel model) {
if(mixThreshold < 1) {
}
this.mixThreshold = mixThreshold;
assert (clock >= 0) : clock;
if(clock < mixThreshold) {
}
@Override
public void setClock(int feature, short clock) {
clocks[feature] = clock;
}
boolean onUpdate(Object feature, float weight, float covar, short clock) throws Exception;
public static final short CLOCK_ZERO = 0;
final boolean resetClock;
resetClock = handler.onUpdate(feature, weight, covar, clock);
if(resetClock) {
setClock(feature, CLOCK_ZERO);
}
final boolean resetClock;
resetClock = handler.onUpdate(feature, weight, covar, clock);
resetClock = handler.onUpdate(feature, weight, 1.f, clock);
if(resetClock) {
value.setClock(CLOCK_ZERO);
}
public void setClock(int feature, short clock) {
throw new UnsupportedOperationException();
}
@Override
public void setClock(int feature, short clock) {
clocks[feature] = clock;
}
public boolean onUpdate(Object feature, float weight, float covar, short clock)
throws Exception {
return true;
protected int numMixed;
public PredictionModel() {
this.numMixed = 0;
}
public final int getNumMixed() {
return numMixed;
}
float covar = msg.getCovariance();
} else {
model._set(feature, weight, clock);
Options opts = getOptions();
static Options getOptions() {
Options opts = new Options();
opts.addOption("p", "port", true, "port number of the mix server [default: 11212]");
opts.addOption("ssl", false, "Use SSL for the mix communication [default: false]");
opts.addOption("scale", "scalemodel", true, "Scale values of prediction models to avoid overflow [default: 1.0 (no-scale)]");
opts.addOption("sync", "sync_threshold", true, "Synchronization threshold using clock difference [default: 30]");
return opts;
}
map = new ConcurrentHashMap<Object, PartialResult>(EXPECTED_MODEL_SIZE);
ConcurrentMap<Object, PartialResult> existing = groupMap.putIfAbsent(groupID, map);
if(existing != null) {
map = existing;
}
PartialResult existing = map.putIfAbsent(feature, partial);
if(existing != null) {
partial = existing;
}
if(responseMsg != null) {
import io.netty.channel.ChannelHandler.Sharable;
@Sharable
this.syncThreshold = Primitives.parseShort(cl.getOptionValue("sync"), (short) 10);
opts.addOption("sync", "sync_threshold", true, "Synchronization threshold using clock difference [default: 10]");
int numMixed = model.getNumMixed();
long numMixed = 0L;
int numMixed = model.getNumMixed();
protected String mixSessionName;
protected MixClient mixClient;
opts.addOption("mix_session", "mix_session_name", true, "Mix session name [default: ${mapred.job.id}]");
opts.addOption("mix_threshold", true, "Threshold to mix local updates [default: 3]");
String mixSessionName = null;
int mixThreshold = 3;
mixSessionName = cl.getOptionValue("mix_session");
this.mixSessionName = mixSessionName;
this.mixClient = client;
String jobId = (mixSessionName == null) ? HadoopUtils.getJobId() : mixSessionName;
if(mixClient != null) {
IOUtils.closeQuietly(mixClient);
this.mixClient = null;
this.syncThreshold = Primitives.parseShort(cl.getOptionValue("sync"), (short) 30);
opts.addOption("sync", "sync_threshold", true, "Synchronization threshold using clock difference [default: 30]");
addWeight(localWeight, clock);
protected void addWeight(float localWeight, int clock) {
ObjectDecoder decoder = new ObjectDecoder(ClassResolvers.weakCachingConcurrentResolver(null));
ObjectDecoder decoder = new ObjectDecoder(ClassResolvers.weakCachingConcurrentResolver(null));
public static int toUnsignedShort(final short v) {
}
public static short parseShort(final String s, final short defaultValue) {
public static int parseInt(final String s, final int defaultValue) {
public static float parseFloat(final String s, final float defaultValue) {
public static boolean parseBoolean(final String s, final boolean defaultValue) {
public static int compare(final int x, final int y) {
private byte[] deltaUpdates;
this.deltaUpdates = null;
this.deltaUpdates = new byte[size];
public void resetDeltaUpdates(int feature) {
deltaUpdates[feature] = 0;
this.deltaUpdates = Arrays.copyOf(deltaUpdates, newSize);
int delta = 0;
assert (delta > 0) : delta;
deltaUpdates[i] = (byte) delta;
onUpdate(i, weight, covar, clock, delta);
deltaUpdates[i] = 0;
deltaUpdates[i] = 0;
private void configureClock(final WeightValue weight, final int index, final float w) {
private void configureClock(final WeightValue weight, final int index, final float w, final float cov) {
boolean onUpdate(Object feature, float weight, float covar, short clock, int deltaUpdates)
throws Exception;
public static final byte BYTE0 = 0;
protected final void onUpdate(final int feature, final float weight, final float covar, final short clock, final int deltaUpdates) {
assert (deltaUpdates > 0) : deltaUpdates;
final boolean resetDeltaUpdates;
resetDeltaUpdates = handler.onUpdate(feature, weight, covar, clock, deltaUpdates);
if(resetDeltaUpdates) {
resetDeltaUpdates(feature);
final int deltaUpdates = value.getDeltaUpdates();
final boolean resetDeltaUpdates;
resetDeltaUpdates = handler.onUpdate(feature, weight, covar, clock, deltaUpdates);
resetDeltaUpdates = handler.onUpdate(feature, weight, 1.f, clock, deltaUpdates);
if(resetDeltaUpdates) {
value.setDeltaUpdates(BYTE0);
public void resetDeltaUpdates(int feature) {
private byte[] deltaUpdates;
this.deltaUpdates = null;
this.deltaUpdates = new byte[size];
public void resetDeltaUpdates(int feature) {
deltaUpdates[feature] = 0;
this.deltaUpdates = Arrays.copyOf(deltaUpdates, newSize);
int delta = 0;
assert (delta > 0) : delta;
deltaUpdates[i] = (byte) delta;
onUpdate(i, weight, covar, clock, delta);
deltaUpdates[i] = 0;
deltaUpdates[i] = 0;
assert (newDelta > 0) : newclock;
value.setDeltaUpdates(newDelta);
protected byte deltaUpdates;
this.deltaUpdates = 0;
public final float get() {
public final void set(float weight) {
public final boolean isTouched() {
public final short getClock() {
public final void setClock(short clock) {
public final byte getDeltaUpdates() {
return deltaUpdates;
}
public final void setDeltaUpdates(byte deltaUpdates) {
this.deltaUpdates = deltaUpdates;
}
another.deltaUpdates = this.deltaUpdates;
this.deltaUpdates = another.deltaUpdates;
opts.addOption("mix_threshold", true, "Threshold to mix local updates in range (0,127] [default: 3]");
int mixThreshold = -1;
mixThreshold = Primitives.parseInt(cl.getOptionValue("mix_threshold"), 3);
if(mixThreshold > Byte.MAX_VALUE) {
throw new UDFArgumentException("mix_threshold must be in range (0,127]: "
mixThreshold);
}
private int deltaUpdates;
public MixMessage(MixEventName event, Object feature, float weight, short clock, int deltaUpdates) {
this(event, feature, weight, 0.f, clock, deltaUpdates);
public MixMessage(MixEventName event, Object feature, float weight, float covariance, short clock, int deltaUpdates) {
throw new IllegalArgumentException("feature is null");
}
if(deltaUpdates < 1 || deltaUpdates > Byte.MAX_VALUE) {
this.deltaUpdates = deltaUpdates;
public int getDeltaUpdates() {
return deltaUpdates;
}
out.writeInt(deltaUpdates);
this.deltaUpdates = in.readInt();
if(mixThreshold < 1 || mixThreshold > Byte.MAX_VALUE) {
public boolean onUpdate(Object feature, float weight, float covar, short clock, int deltaUpdates)
assert (deltaUpdates > 0) : deltaUpdates;
if(deltaUpdates < mixThreshold) {
MixMessage msg = new MixMessage(event, feature, weight, covar, clock, deltaUpdates);
int deltaUpdates = requestMsg.getDeltaUpdates();
partial.add(weight, covar, clock, deltaUpdates);
short totalClock = partial.getClock();
public void add(float localWeight, float covar, short clock, int deltaUpdates) {
private short totalUpdates;
this.totalUpdates = 0;
public void add(float localWeight, float covar, short clock, int deltaUpdates) {
addWeight(localWeight, deltaUpdates);
protected void addWeight(float localWeight, int deltaUpdates) {
assert (totalUpdates > 0) : totalUpdates;
return (scaledSumWeights / totalUpdates) * scale;
public abstract void add(float localWeight, float covar, short clock, int deltaUpdates);
public short getClock() {
if(deltaUpdates < 0 || deltaUpdates > Byte.MAX_VALUE) {
import java.net.InetAddress;
import java.net.UnknownHostException;
InetAddress addr = getInetAddress(endpointURI);
return new InetSocketAddress(addr, defaultPort);
InetAddress addr = getInetAddress(host);
return new InetSocketAddress(addr, port);
public static InetAddress getInetAddress(final String addressOrName) {
try {
return InetAddress.getByName(addressOrName);
} catch (UnknownHostException e) {
}
}
public static boolean isIPAddress(final String ip) {
return ip.matches("^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$");
}
import io.netty.channel.ChannelHandler.Sharable;
@Sharable
int index = Math.abs(hashcode) % numNodes;
public static void putChar(final byte[] b, final int off, final char val) {
b[off] = (byte) (val >>> 8);
}
ObjectDecoder decoder = new ObjectDecoder(ClassResolvers.cacheDisabled(null));
ObjectDecoder decoder = new ObjectDecoder(ClassResolvers.cacheDisabled(null));
import hivemall.io.SynchronizedModelWrapper;
PredictionModel model;
model = new SynchronizedModelWrapper(model);
assert (model != null);
public final class DenseModel extends AbstractPredictionModel {
public interface PredictionModel {
ModelUpdateHandler getUpdateHandler();
void setUpdateHandler(ModelUpdateHandler handler);
int getNumMixed();
boolean hasCovariance();
void configureClock();
boolean hasClock();
void resetDeltaUpdates(int feature);
int size();
boolean contains(Object feature);
<T extends WeightValue> T get(Object feature);
<T extends WeightValue> void set(Object feature, T value);
float getWeight(Object feature);
float getCovariance(Object feature);
void _set(Object feature, float weight, short clock);
void _set(Object feature, float weight, float covar, short clock);
<K, V extends WeightValue> IMapIterator<K, V> entries();
}
public final class SpaceEfficientDenseModel extends AbstractPredictionModel {
public final class SparseModel extends AbstractPredictionModel {
WeightValue w = weights.get(feature);
if(w == null) {
w = new WeightValue(weight);
w.setClock(clock);
weights.put(feature, w);
} else {
w.set(weight);
w.setClock(clock);
w.setDeltaUpdates(BYTE0);
}
WeightValue w = weights.get(feature);
if(w == null) {
w = new WeightValueWithCovar(weight, covar);
w.setClock(clock);
weights.put(feature, w);
} else {
w.set(weight);
w.setCovariance(covar);
w.setClock(clock);
w.setDeltaUpdates(BYTE0);
}
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
IWeightValue new_w = getNewWeight(old_w, v, y, alpha, beta);
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float y, final float alpha, final float beta) {
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
final IMapIterator<Object, IWeightValue> itor = model.entries();
final IMapIterator<Object, IWeightValue> itor = model.entries();
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
IWeightValue new_w = getNewWeight(old_w, v, coeff, alpha, phi);
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float coeff, final float alpha, final float phi) {
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
IWeightValue new_w = getNewWeight(old_w, v, y, alpha, beta);
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float y, final float alpha, final float beta) {
import hivemall.io.IWeightValue;
IWeightValue old_correctclass_w = model2add.get(k);
IWeightValue new_correctclass_w = getNewWeight(old_correctclass_w, v, alpha, beta, true);
IWeightValue old_wrongclass_w = model2sub.get(k);
IWeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, beta, false);
private static IWeightValue getNewWeight(final IWeightValue old, final float v, final float alpha, final float beta, final boolean positive) {
import hivemall.io.IWeightValue;
IWeightValue old_correctclass_w = model2add.get(k);
IWeightValue new_correctclass_w = getNewWeight(old_correctclass_w, v, alpha, phi, true);
IWeightValue old_wrongclass_w = model2sub.get(k);
IWeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, phi, false);
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float alpha, final float phi, final boolean positive) {
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
IMapIterator<Object, IWeightValue> itor = model.entries();
IMapIterator<Object, IWeightValue> itor = model.entries();
import hivemall.io.IWeightValue;
IWeightValue old_correctclass_w = model2add.get(k);
IWeightValue new_correctclass_w = getNewWeight(old_correctclass_w, v, alpha, beta, true);
IWeightValue old_wrongclass_w = model2sub.get(k);
IWeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, beta, false);
private static IWeightValue getNewWeight(final IWeightValue old, final float v, final float alpha, final float beta, final boolean positive) {
protected final void onUpdate(final Object feature, final IWeightValue value) {
public <T extends IWeightValue> T get(Object feature) {
final int i = HiveUtils.parseInt(feature);
public <T extends IWeightValue> void set(Object feature, T value) {
public <K, V extends IWeightValue> IMapIterator<K, V> entries() {
private final class Itr implements IMapIterator<Number, IWeightValue> {
public IWeightValue getValue() {
public <T extends Copyable<IWeightValue>> void getValue(T probe) {
tmpWeight.setCovariance(cov);
private void configureClock(final IWeightValue weight, final int index, final float w) {
private void configureClock(final IWeightValue weight, final int index, final float w, final float cov) {
<T extends IWeightValue> T get(Object feature);
<T extends IWeightValue> void set(Object feature, T value);
<K, V extends IWeightValue> IMapIterator<K, V> entries();
public <T extends IWeightValue> T get(Object feature) {
final int i = HiveUtils.parseInt(feature);
public <T extends IWeightValue> void set(Object feature, T value) {
public <K, V extends IWeightValue> IMapIterator<K, V> entries() {
private final class Itr implements IMapIterator<Number, IWeightValue> {
public IWeightValue getValue() {
public <T extends Copyable<IWeightValue>> void getValue(T probe) {
tmpWeight.setCovariance(cov);
void configureClock(final IWeightValue weight, final int index, final float w) {
void configureClock(final IWeightValue weight, final int index, final float w, final float cov) {
import hivemall.io.WeightValueWithClock.WeightValueWithCovarClock;
private final OpenHashMap<Object, IWeightValue> weights;
this.weights = new OpenHashMap<Object, IWeightValue>(size);
public <T extends IWeightValue> T get(Object feature) {
public <T extends IWeightValue> void set(Object feature, T value) {
final IWeightValue wrapperValue = wrapIfRequired(value);
if(clockEnabled && wrapperValue.isTouched()) {
IWeightValue old = weights.get(feature);
wrapperValue.setClock(newclock);
wrapperValue.setDeltaUpdates(newDelta);
weights.put(feature, wrapperValue);
onUpdate(feature, wrapperValue);
}
private IWeightValue wrapIfRequired(final IWeightValue value) {
if(clockEnabled) {
if(value.hasCovariance()) {
return new WeightValueWithCovarClock(value);
} else {
return new WeightValueWithClock(value);
}
} else {
return value;
}
IWeightValue v = weights.get(feature);
return v == null ? 0.f : v.get();
IWeightValue v = weights.get(feature);
return v == null ? 1.f : v.getCovariance();
IWeightValue w = weights.get(feature);
throw new IllegalStateException("Previous weight not found");
w.set(weight);
w.setClock(clock);
w.setDeltaUpdates(BYTE0);
IWeightValue w = weights.get(feature);
throw new IllegalStateException("Previous weight not found");
w.set(weight);
w.setCovariance(covar);
w.setClock(clock);
w.setDeltaUpdates(BYTE0);
public <K, V extends IWeightValue> IMapIterator<K, V> entries() {
public <K, V extends IWeightValue> IMapIterator<K, V> entries() {
public <T extends IWeightValue> T get(Object feature) {
public <T extends IWeightValue> void set(Object feature, T value) {
public class WeightValue implements IWeightValue {
protected boolean touched;
this.touched = touched;
@Override
@Override
@Override
@Override
@Override
@Override
return touched;
@Override
public final void setTouched(boolean touched) {
this.touched = touched;
}
@Override
throw new UnsupportedOperationException();
@Override
throw new UnsupportedOperationException();
@Override
throw new UnsupportedOperationException();
@Override
throw new UnsupportedOperationException();
public void copyTo(IWeightValue another) {
another.set(value);
another.setTouched(touched);
public void copyFrom(IWeightValue another) {
this.value = another.get();
this.touched = another.isTouched();
private float covariance;
public void copyTo(IWeightValue another) {
another.setCovariance(covariance);
public void copyFrom(IWeightValue another) {
this.covariance = another.getCovariance();
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
IWeightValue new_w = getNewWeight(old_w, v, coeff, beta);
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float coeff, final float beta) {
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
final IMapIterator<Object, IWeightValue> itor = model.entries();
final IMapIterator<Object, IWeightValue> itor = model.entries();
return new WeightValue(w);
return new WeightValueWithCovar(w, cov);
return new WeightValue(w);
return new WeightValueWithCovar(w, cov);
assert (deltaUpdates >= 0) : deltaUpdates;
if(!value.isTouched()) {
return;
}
public <T extends IWeightValue> T get(final Object feature) {
public <T extends IWeightValue> void set(final Object feature, final T value) {
if(clockEnabled && value.isTouched()) {
public float getWeight(final Object feature) {
public float getCovariance(final Object feature) {
public void _set(final Object feature, final float weight, final short clock) {
final IWeightValue w = weights.get(feature);
public void _set(final Object feature, final float weight, final float covar, final short clock) {
final IWeightValue w = weights.get(feature);
public boolean contains(final Object feature) {
if(src.isTouched()) {
this.clock = 1;
this.deltaUpdates = 1;
} else {
this.clock = 0;
this.deltaUpdates = 0;
}
return deltaUpdates > 0;
if(deltaUpdates < 0) {
}
@Override
public String toString() {
}
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(SparseModel.class);
wrapperValue.setDeltaUpdates((byte) newDelta);
logger.info(wrapperValue);
if(deltaUpdates < 1) {
return;
}
private final MixServerHandler requestHandler;
this.requestHandler = msgHandler;
pipeline.addLast(decoder, requestHandler, encoder);
private final MixClientHandler responseHandler;
this.responseHandler = msgHandler;
pipeline.addLast(encoder, decoder, responseHandler);
import hivemall.mix.MixMessageDecoder;
import hivemall.mix.MixMessageEncoder;
MixMessageEncoder encoder = new MixMessageEncoder();
MixMessageDecoder decoder = new MixMessageDecoder();
import hivemall.mix.MixMessageDecoder;
import hivemall.mix.MixMessageEncoder;
MixMessageEncoder encoder = new MixMessageEncoder();
MixMessageDecoder decoder = new MixMessageDecoder();
return createModel(null);
}
protected PredictionModel createModel(String label) {
MixClient client = configureMixClient(mixConnectInfo, label, model);
protected MixClient configureMixClient(String connectURIs, String label, PredictionModel model) {
String jobId = (mixSessionName == null) ? "DEFAULT" : mixSessionName;
if(label != null) {
}
import io.netty.channel.ChannelOption;
b.option(ChannelOption.SO_KEEPALIVE, true);
pipeline.addLast(decoder, encoder, requestHandler);
ctx.writeAndFlush(responseMsg).syncUninterruptibly();
ctx.writeAndFlush(responseMsg);
WeightValue v = new WeightValue(w);
v.setTouched(w != 0f);
return v;
WeightValueWithCovar v = new WeightValueWithCovar(w, cov);
v.setTouched(w != 0.f || cov != 1.f);
return v;
tmpWeight.setTouched(w != 0.f || cov != 1.f);
WeightValue v = new WeightValue(w);
v.setTouched(w != 0f);
return v;
WeightValueWithCovar v = new WeightValueWithCovar(w, cov);
v.setTouched(w != 0.f || cov != 1.f);
return v;
tmpWeight.setTouched(w != 0.f || cov != 1.f);
protected final List<FeatureValue> parseFeatures(final List<?> features, final ObjectInspector featureInspector, final boolean parseFeature) {
public static long parseLong(final String s, final long defaultValue) {
if(s == null) {
return defaultValue;
}
return Long.parseLong(s);
}
import hivemall.mix.store.SessionStore;
import hivemall.mix.store.SessionStore.IdleSessionSweeper;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import javax.annotation.Nonnull;
private final long sessionTTLinSec;
private final long sweepIntervalInSec;
this.sessionTTLinSec = Primitives.parseLong(cl.getOptionValue("ttl"), 120L);
this.sweepIntervalInSec = Primitives.parseLong(cl.getOptionValue("sweep"), 90L);
opts.addOption("ttl", "session_ttl", true, "The TTL in sec that an idle session lives [default: 120 sec]");
opts.addOption("sweep", "session_sweep_interval", true, "The interval in sec that the session expiry thread runs [default: 90 sec]");
SessionStore sessionStore = new SessionStore();
MixServerHandler msgHandler = new MixServerHandler(sessionStore, syncThreshold, scale);
Runnable cleanSessionTask = new IdleSessionSweeper(sessionStore, sessionTTLinSec * 1000L);
final ScheduledExecutorService idleSessionChecker = Executors.newScheduledThreadPool(1);
try {
acceptConnections(initializer, port);
} finally {
idleSessionChecker.shutdownNow();
}
}
private static void acceptConnections(@Nonnull MixServerInitializer initializer, int port)
throws InterruptedException {
import hivemall.mix.store.PartialArgminKLD;
import hivemall.mix.store.PartialAverage;
import hivemall.mix.store.PartialResult;
import hivemall.mix.store.SessionStore;
import javax.annotation.Nonnegative;
import javax.annotation.Nonnull;
@Nonnull
private final SessionStore sessionStore;
public MixServerHandler(@Nonnull SessionStore sessionStore, @Nonnegative int syncThreshold, @Nonnegative float scale) {
this.sessionStore = sessionStore;
@Nonnull
private PartialResult getPartialResult(@Nonnull MixMessage msg) {
ConcurrentMap<Object, PartialResult> map = sessionStore.get(groupID);
package hivemall.mix.store;
import javax.annotation.Nonnegative;
import javax.annotation.concurrent.GuardedBy;
@GuardedBy("lock()")
@GuardedBy("lock()")
public PartialArgminKLD(@Nonnegative float scale) {
package hivemall.mix.store;
import javax.annotation.Nonnegative;
import javax.annotation.concurrent.GuardedBy;
@GuardedBy("lock()")
@GuardedBy("lock()")
public void add(float localWeight, float covar, short clock, @Nonnegative int deltaUpdates) {
package hivemall.mix.store;
import javax.annotation.Nonnegative;
import javax.annotation.concurrent.GuardedBy;
@GuardedBy("lock()")
@GuardedBy("lock()")
public final void lock() {
public final void unlock() {
public abstract void add(float localWeight, float covar, short clock, @Nonnegative int deltaUpdates);
public final float getMinCovariance() {
protected final void setMinCovariance(float covar) {
public final short getClock() {
protected final void incrClock(short clock) {
public final int diffClock(short clock) {
acceptConnections(initializer, port);
String jobId = (mixSessionName == null) ? HadoopUtils.getJobId() : mixSessionName;
import javax.annotation.Nonnull;
@Nonnull
if(ctx == null) {
throw new IllegalStateException("MapredContext is not set");
}
if(conf == null) {
throw new IllegalStateException("JobConf is not set");
}
String jobId = conf.get("mapred.job.id");
if(jobId == null) {
jobId = conf.get("mapreduce.job.id");
if(jobId == null) {
throw new IllegalStateException("Both mapred.job.id and mapreduce.job.id are not set");
}
}
return jobId;
if(ctx == null) {
throw new IllegalStateException("MapredContext is not set");
}
if(conf == null) {
throw new IllegalStateException("JobConf is not set");
}
taskid = conf.getInt("mapreduce.task.partition", -1);
if(taskid == -1) {
throw new IllegalStateException("Both mapred.task.partition and mapreduce.task.partition are not set");
}
import java.util.Iterator;
import java.util.Map.Entry;
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.TaskID;
private static final class BufferedReaderExt extends BufferedReader {
String appId = conf.get("mapreduce.tez.session.tokill-application-id");
if(appId != null) {
return appId;
}
String taskidStr = conf.get("mapred.task.id");
if(taskidStr == null) {
}
jobId = getJobIdFromTaskId(taskidStr);
@Nonnull
public static String getJobIdFromTaskId(@Nonnull String taskidStr) {
taskidStr = taskidStr.replace("task", "task_");
taskidStr = taskidStr.substring(0, taskidStr.lastIndexOf('_'));
}
TaskID taskId = TaskID.forName(taskidStr);
JobID jobId = taskId.getJobID();
return jobId.toString();
}
JobConf jobconf = ctx.getJobConf();
if(jobconf == null) {
int taskid = jobconf.getInt("mapred.task.partition", -1);
taskid = jobconf.getInt("mapreduce.task.partition", -1);
throw new IllegalStateException("Both mapred.task.partition and mapreduce.task.partition are not set: "
toString(jobconf));
@Nonnull
public static String toString(@Nonnull JobConf jobconf) {
return toString(jobconf, null);
}
@Nonnull
public static String toString(@Nonnull JobConf jobconf, @Nullable String regexKey) {
final Iterator<Entry<String, String>> itor = jobconf.iterator();
boolean hasNext = itor.hasNext();
if(!hasNext) {
return "";
}
final StringBuilder buf = new StringBuilder(1024);
do {
Entry<String, String> e = itor.next();
hasNext = itor.hasNext();
String k = e.getKey();
if(k == null) {
continue;
}
if(regexKey == null || k.matches(regexKey)) {
String v = e.getValue();
buf.append(k).append('=').append(v);
if(hasNext) {
buf.append(',');
}
}
} while(hasNext);
return buf.toString();
}
this.sweepIntervalInSec = Primitives.parseLong(cl.getOptionValue("sweep"), 60L);
opts.addOption("sweep", "session_sweep_interval", true, "The interval in sec that the session expiry thread runs [default: 60 sec]");
String jobId = (mixSessionName == null) ? MixClient.DUMMY_JOB_ID : mixSessionName;
import hivemall.utils.hadoop.HadoopUtils;
import javax.annotation.CheckForNull;
import javax.annotation.Nonnull;
public static final String DUMMY_JOB_ID = "__DUMMY_JOB_ID__";
private String groupID;
public MixClient(@Nonnull MixEventName event, @CheckForNull String groupID, @Nonnull String connectURIs, boolean ssl, int mixThreshold, @Nonnull PredictionModel model) {
replaceGroupIDIfRequired();
private void replaceGroupIDIfRequired() {
if(groupID.startsWith(DUMMY_JOB_ID)) {
String jobId = HadoopUtils.getJobId();
this.groupID = groupID.replace(DUMMY_JOB_ID, jobId);
}
}
String queryId = conf.get("hive.query.id");
if(queryId != null) {
return queryId;
import hivemall.io.SynchronizedModelWrapper;
import hivemall.mix.MixMessage.MixEventName;
import hivemall.mix.client.MixClient;
import hivemall.utils.io.IOUtils;
import org.apache.hadoop.hive.ql.metadata.HiveException;
protected String mixConnectInfo;
protected String mixSessionName;
protected int mixThreshold;
protected boolean ssl;
protected MixClient mixClient;
opts.addOption("mix", "mix_servers", true, "Comma separated list of MIX servers");
opts.addOption("mix_session", "mix_session_name", true, "Mix session name [default: ${mapred.job.id}]");
opts.addOption("mix_threshold", true, "Threshold to mix local updates in range (0,127] [default: 3]");
opts.addOption("ssl", false, "Use SSL for the communication with mix servers");
String mixConnectInfo = null;
String mixSessionName = null;
int mixThreshold = -1;
boolean ssl = false;
mixConnectInfo = cl.getOptionValue("mix");
mixSessionName = cl.getOptionValue("mix_session");
mixThreshold = Primitives.parseInt(cl.getOptionValue("mix_threshold"), 3);
if(mixThreshold > Byte.MAX_VALUE) {
throw new UDFArgumentException("mix_threshold must be in range (0,127]: "
mixThreshold);
}
ssl = cl.hasOption("ssl");
this.mixConnectInfo = mixConnectInfo;
this.mixSessionName = mixSessionName;
this.mixThreshold = mixThreshold;
this.ssl = ssl;
return createModel(null);
}
protected PredictionModel createModel(String label) {
PredictionModel model;
final boolean useCovar = useCovariance();
model = new SpaceEfficientDenseModel(model_dims, useCovar);
model = new DenseModel(model_dims, useCovar);
model = new SparseModel(initModelSize, useCovar);
if(mixConnectInfo != null) {
model.configureClock();
model = new SynchronizedModelWrapper(model);
MixClient client = configureMixClient(mixConnectInfo, label, model);
model.setUpdateHandler(client);
this.mixClient = client;
}
assert (model != null);
return model;
}
protected MixClient configureMixClient(String connectURIs, String label, PredictionModel model) {
assert (connectURIs != null);
assert (model != null);
String jobId = (mixSessionName == null) ? MixClient.DUMMY_JOB_ID : mixSessionName;
if(label != null) {
}
MixEventName event = useCovariance() ? MixEventName.argminKLD : MixEventName.average;
MixClient client = new MixClient(event, jobId, connectURIs, ssl, mixThreshold, model);
return client;
@Override
public void close() throws HiveException {
if(mixClient != null) {
IOUtils.closeQuietly(mixClient);
this.mixClient = null;
}
}
protected final List<FeatureValue> parseFeatures(final List<?> features, final ObjectInspector featureInspector, final boolean parseFeature) {
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
IWeightValue new_w = getNewWeight(old_w, v, y, alpha, beta);
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float y, final float alpha, final float beta) {
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
public final void close() throws HiveException {
super.close();
final IMapIterator<Object, IWeightValue> itor = model.entries();
final IMapIterator<Object, IWeightValue> itor = model.entries();
int numMixed = model.getNumMixed();
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
IWeightValue new_w = getNewWeight(old_w, v, coeff, alpha, phi);
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float coeff, final float alpha, final float phi) {
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
IWeightValue new_w = getNewWeight(old_w, v, y, alpha, beta);
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float y, final float alpha, final float beta) {
import hivemall.io.IWeightValue;
IWeightValue old_correctclass_w = model2add.get(k);
IWeightValue new_correctclass_w = getNewWeight(old_correctclass_w, v, alpha, beta, true);
IWeightValue old_wrongclass_w = model2sub.get(k);
IWeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, beta, false);
private static IWeightValue getNewWeight(final IWeightValue old, final float v, final float alpha, final float beta, final boolean positive) {
import hivemall.io.IWeightValue;
IWeightValue old_correctclass_w = model2add.get(k);
IWeightValue new_correctclass_w = getNewWeight(old_correctclass_w, v, alpha, phi, true);
IWeightValue old_wrongclass_w = model2sub.get(k);
IWeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, phi, false);
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float alpha, final float phi, final boolean positive) {
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
public final void close() throws HiveException {
super.close();
long numMixed = 0L;
IMapIterator<Object, IWeightValue> itor = model.entries();
IMapIterator<Object, IWeightValue> itor = model.entries();
import hivemall.io.IWeightValue;
IWeightValue old_correctclass_w = model2add.get(k);
IWeightValue new_correctclass_w = getNewWeight(old_correctclass_w, v, alpha, beta, true);
IWeightValue old_wrongclass_w = model2sub.get(k);
IWeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, beta, false);
private static IWeightValue getNewWeight(final IWeightValue old, final float v, final float alpha, final float beta, final boolean positive) {
public final class DenseModel extends AbstractPredictionModel {
private short[] clocks;
private byte[] deltaUpdates;
super();
this.clocks = null;
this.deltaUpdates = null;
}
@Override
public boolean hasCovariance() {
return covars != null;
}
@Override
public void configureClock() {
if(clocks == null) {
this.clocks = new short[size];
this.deltaUpdates = new byte[size];
}
}
@Override
public boolean hasClock() {
return clocks != null;
}
@Override
public void resetDeltaUpdates(int feature) {
deltaUpdates[feature] = 0;
if(clocks != null) {
this.clocks = Arrays.copyOf(clocks, newSize);
this.deltaUpdates = Arrays.copyOf(deltaUpdates, newSize);
}
public <T extends IWeightValue> T get(Object feature) {
final int i = HiveUtils.parseInt(feature);
public <T extends IWeightValue> void set(Object feature, T value) {
float covar = 1.f;
covar = value.getCovariance();
short clock = 0;
int delta = 0;
if(clocks != null && value.isTouched()) {
clocks[i] = clock;
assert (delta > 0) : delta;
deltaUpdates[i] = (byte) delta;
}
onUpdate(i, weight, covar, clock, delta);
public void _set(Object feature, float weight, short clock) {
clocks[i] = clock;
deltaUpdates[i] = 0;
public void _set(Object feature, float weight, float covar, short clock) {
clocks[i] = clock;
deltaUpdates[i] = 0;
public <K, V extends IWeightValue> IMapIterator<K, V> entries() {
private final class Itr implements IMapIterator<Number, IWeightValue> {
public IWeightValue getValue() {
public <T extends Copyable<IWeightValue>> void getValue(T probe) {
tmpWeight.setCovariance(cov);
ModelUpdateHandler getUpdateHandler();
void setUpdateHandler(ModelUpdateHandler handler);
int getNumMixed();
boolean hasCovariance();
void configureClock();
boolean hasClock();
void resetDeltaUpdates(int feature);
int size();
boolean contains(Object feature);
<T extends IWeightValue> T get(Object feature);
<T extends IWeightValue> void set(Object feature, T value);
float getWeight(Object feature);
float getCovariance(Object feature);
void _set(Object feature, float weight, short clock);
void _set(Object feature, float weight, float covar, short clock);
<K, V extends IWeightValue> IMapIterator<K, V> entries();
}
public final class SpaceEfficientDenseModel extends AbstractPredictionModel {
private short[] clocks;
private byte[] deltaUpdates;
super();
this.clocks = null;
this.deltaUpdates = null;
}
@Override
public boolean hasCovariance() {
return covars != null;
}
@Override
public void configureClock() {
if(clocks == null) {
this.clocks = new short[size];
this.deltaUpdates = new byte[size];
}
}
@Override
public boolean hasClock() {
return clocks != null;
}
@Override
public void resetDeltaUpdates(int feature) {
deltaUpdates[feature] = 0;
if(clocks != null) {
this.clocks = Arrays.copyOf(clocks, newSize);
this.deltaUpdates = Arrays.copyOf(deltaUpdates, newSize);
}
public <T extends IWeightValue> T get(Object feature) {
final int i = HiveUtils.parseInt(feature);
public <T extends IWeightValue> void set(Object feature, T value) {
float covar = 1.f;
covar = value.getCovariance();
short clock = 0;
int delta = 0;
if(clocks != null && value.isTouched()) {
clocks[i] = clock;
assert (delta > 0) : delta;
deltaUpdates[i] = (byte) delta;
}
onUpdate(i, weight, covar, clock, delta);
public void _set(Object feature, float weight, short clock) {
clocks[i] = clock;
deltaUpdates[i] = 0;
public void _set(Object feature, float weight, float covar, short clock) {
clocks[i] = clock;
deltaUpdates[i] = 0;
public <K, V extends IWeightValue> IMapIterator<K, V> entries() {
private final class Itr implements IMapIterator<Number, IWeightValue> {
public IWeightValue getValue() {
public <T extends Copyable<IWeightValue>> void getValue(T probe) {
tmpWeight.setCovariance(cov);
import hivemall.io.WeightValueWithClock.WeightValueWithCovarClock;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
public final class SparseModel extends AbstractPredictionModel {
private static final Log logger = LogFactory.getLog(SparseModel.class);
private final OpenHashMap<Object, IWeightValue> weights;
private final boolean hasCovar;
private boolean clockEnabled;
public SparseModel(int size, boolean hasCovar) {
super();
this.weights = new OpenHashMap<Object, IWeightValue>(size);
this.hasCovar = hasCovar;
this.clockEnabled = false;
@Override
public boolean hasCovariance() {
return hasCovar;
}
@Override
public void configureClock() {
this.clockEnabled = true;
}
@Override
public boolean hasClock() {
return clockEnabled;
public <T extends IWeightValue> T get(final Object feature) {
public <T extends IWeightValue> void set(final Object feature, final T value) {
assert (feature != null);
assert (value != null);
final IWeightValue wrapperValue = wrapIfRequired(value);
if(clockEnabled && value.isTouched()) {
IWeightValue old = weights.get(feature);
if(old != null) {
wrapperValue.setClock(newclock);
wrapperValue.setDeltaUpdates((byte) newDelta);
}
}
weights.put(feature, wrapperValue);
onUpdate(feature, wrapperValue);
}
private IWeightValue wrapIfRequired(final IWeightValue value) {
if(clockEnabled) {
if(value.hasCovariance()) {
return new WeightValueWithCovarClock(value);
} else {
return new WeightValueWithClock(value);
}
} else {
return value;
}
public float getWeight(final Object feature) {
IWeightValue v = weights.get(feature);
return v == null ? 0.f : v.get();
public float getCovariance(final Object feature) {
IWeightValue v = weights.get(feature);
return v == null ? 1.f : v.getCovariance();
public void _set(final Object feature, final float weight, final short clock) {
final IWeightValue w = weights.get(feature);
if(w == null) {
}
w.set(weight);
w.setClock(clock);
w.setDeltaUpdates(BYTE0);
public void _set(final Object feature, final float weight, final float covar, final short clock) {
final IWeightValue w = weights.get(feature);
if(w == null) {
}
w.set(weight);
w.setCovariance(covar);
w.setClock(clock);
w.setDeltaUpdates(BYTE0);
public boolean contains(final Object feature) {
public <K, V extends IWeightValue> IMapIterator<K, V> entries() {
public class WeightValue implements IWeightValue {
@Override
public final float get() {
@Override
public final void set(float weight) {
@Override
@Override
@Override
@Override
public final boolean isTouched() {
@Override
public final void setTouched(boolean touched) {
public final short getClock() {
throw new UnsupportedOperationException();
public final void setClock(short clock) {
throw new UnsupportedOperationException();
}
@Override
public final byte getDeltaUpdates() {
throw new UnsupportedOperationException();
}
@Override
public final void setDeltaUpdates(byte deltaUpdates) {
throw new UnsupportedOperationException();
}
@Override
public void copyTo(IWeightValue another) {
another.set(value);
another.setTouched(touched);
}
@Override
public void copyFrom(IWeightValue another) {
this.value = another.get();
this.touched = another.isTouched();
private float covariance;
public void copyTo(IWeightValue another) {
another.setCovariance(covariance);
public void copyFrom(IWeightValue another) {
this.covariance = another.getCovariance();
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
IWeightValue new_w = getNewWeight(old_w, v, coeff, beta);
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float coeff, final float beta) {
import hivemall.io.IWeightValue;
IWeightValue old_w = model.get(k);
public final void close() throws HiveException {
super.close();
final IMapIterator<Object, IWeightValue> itor = model.entries();
final IMapIterator<Object, IWeightValue> itor = model.entries();
int numMixed = model.getNumMixed();
import java.util.Iterator;
import java.util.Map.Entry;
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.TaskID;
private static final class BufferedReaderExt extends BufferedReader {
@Nonnull
public static String getJobId() {
MapredContext ctx = MapredContextAccessor.get();
if(ctx == null) {
throw new IllegalStateException("MapredContext is not set");
}
JobConf conf = ctx.getJobConf();
if(conf == null) {
throw new IllegalStateException("JobConf is not set");
}
String jobId = conf.get("mapred.job.id");
if(jobId == null) {
jobId = conf.get("mapreduce.job.id");
if(jobId == null) {
String queryId = conf.get("hive.query.id");
if(queryId != null) {
return queryId;
}
String taskidStr = conf.get("mapred.task.id");
if(taskidStr == null) {
}
jobId = getJobIdFromTaskId(taskidStr);
}
}
return jobId;
}
@Nonnull
public static String getJobIdFromTaskId(@Nonnull String taskidStr) {
taskidStr = taskidStr.replace("task", "task_");
taskidStr = taskidStr.substring(0, taskidStr.lastIndexOf('_'));
}
TaskID taskId = TaskID.forName(taskidStr);
JobID jobId = taskId.getJobID();
return jobId.toString();
}
if(ctx == null) {
throw new IllegalStateException("MapredContext is not set");
}
JobConf jobconf = ctx.getJobConf();
if(jobconf == null) {
throw new IllegalStateException("JobConf is not set");
}
int taskid = jobconf.getInt("mapred.task.partition", -1);
taskid = jobconf.getInt("mapreduce.task.partition", -1);
if(taskid == -1) {
throw new IllegalStateException("Both mapred.task.partition and mapreduce.task.partition are not set: "
toString(jobconf));
}
@Nonnull
public static String toString(@Nonnull JobConf jobconf) {
return toString(jobconf, null);
}
@Nonnull
public static String toString(@Nonnull JobConf jobconf, @Nullable String regexKey) {
final Iterator<Entry<String, String>> itor = jobconf.iterator();
boolean hasNext = itor.hasNext();
if(!hasNext) {
return "";
}
final StringBuilder buf = new StringBuilder(1024);
do {
Entry<String, String> e = itor.next();
hasNext = itor.hasNext();
String k = e.getKey();
if(k == null) {
continue;
}
if(regexKey == null || k.matches(regexKey)) {
String v = e.getValue();
buf.append(k).append('=').append(v);
if(hasNext) {
buf.append(',');
}
}
} while(hasNext);
return buf.toString();
}
public static int toUnsignedShort(final short v) {
}
public static short parseShort(final String s, final short defaultValue) {
if(s == null) {
return defaultValue;
}
return Short.parseShort(s);
}
public static int parseInt(final String s, final int defaultValue) {
public static long parseLong(final String s, final long defaultValue) {
if(s == null) {
return defaultValue;
}
return Long.parseLong(s);
}
public static float parseFloat(final String s, final float defaultValue) {
public static boolean parseBoolean(final String s, final boolean defaultValue) {
public static int compare(final int x, final int y) {
public static void putChar(final byte[] b, final int off, final char val) {
b[off] = (byte) (val >>> 8);
}
case argminKLD: {
}
case closeGroup: {
closeGroup(msg);
break;
}
private void closeGroup(@Nonnull MixMessage msg) {
String groupId = msg.getGroupID();
if(groupId == null) {
return;
}
sessionStore.remove(groupId);
}
private static final Log logger = LogFactory.getLog(SessionStore.class);
public void remove(@Nonnull String groupID) {
sessions.remove(groupID);
}
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
@Nullable
public static FeatureValue parse(final Object o) throws IllegalArgumentException {
@Nullable
public static FeatureValue parse(@Nonnull final String s) throws IllegalArgumentException {
assert (s != null);
final int pos = s.indexOf(':');
if(pos == 0) {
final Text feature;
final float weight;
if(pos > 0) {
String s1 = s.substring(0, pos);
feature = new Text(s1);
weight = Float.parseFloat(s2);
} else {
feature = new Text(s);
weight = 1.f;
}
return new FeatureValue(feature, weight);
@Nonnull
public static FeatureValue parseFeatureAsString(@Nonnull final String s)
throws IllegalArgumentException {
assert (s != null);
final int pos = s.indexOf(':');
if(pos == 0) {
final String feature;
final float weight;
if(pos > 0) {
feature = s.substring(0, pos);
weight = Float.parseFloat(s2);
} else {
feature = s;
weight = 1.f;
}
return new FeatureValue(feature, weight);
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
@Nullable
public static FeatureValue parse(final Object o) throws IllegalArgumentException {
@Nullable
public static FeatureValue parse(@Nonnull final String s) throws IllegalArgumentException {
assert (s != null);
final int pos = s.indexOf(':');
if(pos == 0) {
final Text feature;
final float weight;
if(pos > 0) {
String s1 = s.substring(0, pos);
feature = new Text(s1);
weight = Float.parseFloat(s2);
} else {
feature = new Text(s);
weight = 1.f;
}
return new FeatureValue(feature, weight);
@Nonnull
public static FeatureValue parseFeatureAsString(@Nonnull final String s)
throws IllegalArgumentException {
assert (s != null);
final int pos = s.indexOf(':');
if(pos == 0) {
final String feature;
final float weight;
if(pos > 0) {
feature = s.substring(0, pos);
weight = Float.parseFloat(s2);
} else {
feature = s;
weight = 1.f;
}
return new FeatureValue(feature, weight);
case argminKLD: {
}
case closeGroup: {
closeGroup(msg);
break;
}
private void closeGroup(@Nonnull MixMessage msg) {
String groupId = msg.getGroupID();
if(groupId == null) {
return;
}
sessionStore.remove(groupId);
}
private static final Log logger = LogFactory.getLog(SessionStore.class);
public void remove(@Nonnull String groupID) {
sessions.remove(groupID);
}
public static final class SimpleEtaEstimator implements EtaEstimator {
public float eta(final int t) {
public static final class InvscalingEtaEstimator implements EtaEstimator {
public float eta(final int t) {
public static final class SimpleEtaEstimator implements EtaEstimator {
public float eta(final int t) {
public static final class InvscalingEtaEstimator implements EtaEstimator {
public float eta(final int t) {
private static float computeLoss(float target, float predicted) {
float d = computeUpdate(target, predicted);
protected float computeUpdate(float target, float predicted) {
import hivemall.common.LossFunctions;
public final class LogressUDTF extends OnlineRegressionUDTF {
this.etaEstimator = EtaEstimator.get(cl);
protected void checkTargetValue(final float target) throws UDFArgumentException {
protected float computeUpdate(final float target, final float predicted) {
float eta = etaEstimator.eta(count);
float gradient = LossFunctions.logisticLoss(target, predicted);
return eta * gradient;
import javax.annotation.Nonnull;
import org.apache.commons.cli.CommandLine;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
public abstract class EtaEstimator {
public abstract float eta(int t);
public static final class SimpleEtaEstimator extends EtaEstimator {
public static final class InvscalingEtaEstimator extends EtaEstimator {
@Nonnull
public static EtaEstimator get(@Nonnull CommandLine cl) throws UDFArgumentException {
if(cl == null) {
return new InvscalingEtaEstimator(0.1f, 0.1f);
}
float eta0 = Float.parseFloat(cl.getOptionValue("eta0", "0.1"));
if(cl.hasOption("t")) {
int t = Integer.parseInt(cl.getOptionValue("t"));
return new SimpleEtaEstimator(eta0, t);
}
float power_t = Float.parseFloat(cl.getOptionValue("power_t", "0.1"));
return new InvscalingEtaEstimator(eta0, power_t);
}
import hivemall.utils.math.MathUtils;
public static float logisticLoss(final float target, final float predicted) {
if(-100.d < predicted) {
return target - (float) MathUtils.sigmoid(predicted);
} else {
return target;
}
}
import hivemall.io.WeightValue.WeightValueWithGt;
private float[] sum_of_squared_gradients;
this.sum_of_squared_gradients = null;
public void configurParams(boolean sumOfSquaredGradients) {
if(sumOfSquaredGradients) {
this.sum_of_squared_gradients = new float[size];
}
}
@Override
if(sum_of_squared_gradients != null) {
this.sum_of_squared_gradients = Arrays.copyOf(sum_of_squared_gradients, newSize);
}
if(sum_of_squared_gradients != null) {
return (T) new WeightValueWithGt(weights[i], sum_of_squared_gradients[i]);
} else if(covars != null) {
} else {
return (T) new WeightValue(weights[i]);
if(sum_of_squared_gradients != null) {
sum_of_squared_gradients[i] = value.getSumOfSquaredGradients();
}
int delta = 0;
public enum WeightValueType {
WeightValue, WeightValueWithGt, WeightValueWithCovar;
}
WeightValueType getType();
float getSumOfSquaredGradients();
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
void configurParams(boolean sumOfSquaredGradients);
boolean contains(@Nonnull Object feature);
@Nullable
<T extends IWeightValue> T get(@Nonnull Object feature);
<T extends IWeightValue> void set(@Nonnull Object feature, T value);
float getWeight(@Nonnull Object feature);
float getCovariance(@Nonnull Object feature);
void _set(@Nonnull Object feature, float weight, short clock);
void _set(@Nonnull Object feature, float weight, float covar, short clock);
import hivemall.io.WeightValue.WeightValueWithGt;
private float[] sum_of_squared_gradients;
this.sum_of_squared_gradients = null;
public void configurParams(boolean sumOfSquaredGradients) {
if(sumOfSquaredGradients) {
this.sum_of_squared_gradients = new float[size];
}
}
@Override
if(sum_of_squared_gradients != null) {
this.sum_of_squared_gradients = Arrays.copyOf(sum_of_squared_gradients, newSize);
}
if(sum_of_squared_gradients != null) {
return (T) new WeightValueWithGt(getWeight(i), sum_of_squared_gradients[i]);
} else if(covars != null) {
} else {
return (T) new WeightValue(getWeight(i));
if(sum_of_squared_gradients != null) {
sum_of_squared_gradients[i] = value.getSumOfSquaredGradients();
}
import hivemall.io.WeightValueWithClock.WeightValueWithGtClock;
public void configurParams(boolean sumOfSquaredGradients) {}
@Override
final IWeightValue wrapper;
switch(value.getType()) {
case WeightValue:
wrapper = new WeightValueWithClock(value);
break;
case WeightValueWithCovar:
wrapper = new WeightValueWithCovarClock(value);
break;
case WeightValueWithGt:
wrapper = new WeightValueWithGtClock(value);
break;
default:
wrapper = value;
return wrapper;
public void configurParams(boolean sumOfSquaredGradients) {
model.configurParams(sumOfSquaredGradients);
}
@Override
public WeightValueType getType() {
return WeightValueType.WeightValue;
}
@Override
@Override
public float getSumOfSquaredGradients() {
return 0.f;
}
public static final class WeightValueWithGt extends WeightValue {
private final float sum_of_squared_gradients;
public WeightValueWithGt(float weight, float sum_of_squared_gradients) {
super(weight);
this.sum_of_squared_gradients = sum_of_squared_gradients;
}
@Override
public WeightValueType getType() {
return WeightValueType.WeightValueWithGt;
}
@Override
public float getSumOfSquaredGradients() {
return sum_of_squared_gradients;
}
}
public WeightValueType getType() {
return WeightValueType.WeightValueWithCovar;
}
@Override
@Override
public WeightValueType getType() {
return WeightValueType.WeightValue;
}
@Override
public float getSumOfSquaredGradients() {
return 0.f;
}
public static final class WeightValueWithGtClock extends WeightValueWithClock {
private final float sum_of_squared_gradients;
public WeightValueWithGtClock(IWeightValue src) {
super(src);
this.sum_of_squared_gradients = src.getSumOfSquaredGradients();
}
@Override
public WeightValueType getType() {
return WeightValueType.WeightValueWithGt;
}
@Override
public float getSumOfSquaredGradients() {
return sum_of_squared_gradients;
}
}
public WeightValueType getType() {
return WeightValueType.WeightValueWithCovar;
}
@Override
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
@Nullable
protected CommandLine processOptions(@Nonnull ObjectInspector[] argOIs) throws UDFArgumentException {
if(cl == null) {
this.eta0 = 0.1f;
this.eps = 1.f;
this.scaling = 100f;
} else {
this.eta0 = Primitives.parseFloat(cl.getOptionValue("eta0"), 0.1f);
this.eps = Primitives.parseFloat(cl.getOptionValue("eps"), 1.f);
this.scaling = Primitives.parseFloat(cl.getOptionValue("scale"), 100f);
}
@Override
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
model.configurParams(true, false);
IWeightValue old_w = model.get(x);
IWeightValue new_w = getNewWeight(old_w, xi, gradient, g_g);
model.set(x, new_w);
@Nonnull
protected IWeightValue getNewWeight(@Nullable final IWeightValue old, final float xi, final float gradient, final float g_g) {
float old_w = 0.f;
float scaled_sum_sqgrad = 0.f;
if(old != null) {
old_w = old.get();
scaled_sum_sqgrad = old.getSumOfSquaredGradients();
}
float coeff = eta(scaled_sum_sqgrad) * gradient;
return new WeightValueWithGt(new_w, scaled_sum_sqgrad);
}
import hivemall.io.WeightValue.WeightValueWithGtXt;
private float[] sum_of_squared_delta_x;
this.sum_of_squared_delta_x = null;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x) {
if(sum_of_squared_gradients) {
if(sum_of_squared_delta_x) {
this.sum_of_squared_delta_x = new float[size];
}
if(sum_of_squared_delta_x != null) {
this.sum_of_squared_delta_x = Arrays.copyOf(sum_of_squared_delta_x, newSize);
}
if(sum_of_squared_delta_x == null) {
return (T) new WeightValueWithGt(weights[i], sum_of_squared_gradients[i]);
} else {
return (T) new WeightValueWithGtXt(weights[i], sum_of_squared_gradients[i], sum_of_squared_delta_x[i]);
}
if(sum_of_squared_delta_x != null) {
sum_of_squared_delta_x[i] = value.getSumOfSquaredDeltaX();
}
int delta = 0;
WeightValue, WeightValueWithGt, WeightValueWithGtXt, WeightValueWithCovar;
float getSumOfSquaredDeltaX();
void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x);
import hivemall.io.WeightValue.WeightValueWithGtXt;
private float[] sum_of_squared_delta_x;
this.sum_of_squared_delta_x = null;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x) {
if(sum_of_squared_gradients) {
if(sum_of_squared_delta_x) {
this.sum_of_squared_delta_x = new float[size];
}
if(sum_of_squared_delta_x != null) {
this.sum_of_squared_delta_x = Arrays.copyOf(sum_of_squared_delta_x, newSize);
}
if(sum_of_squared_delta_x == null) {
return (T) new WeightValueWithGt(getWeight(i), sum_of_squared_gradients[i]);
} else {
return (T) new WeightValueWithGtXt(getWeight(i), sum_of_squared_gradients[i], sum_of_squared_delta_x[i]);
}
if(sum_of_squared_delta_x != null) {
sum_of_squared_delta_x[i] = value.getSumOfSquaredDeltaX();
}
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x) {}
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x) {
model.configurParams(sum_of_squared_gradients, sum_of_squared_delta_x);
@Override
public float getSumOfSquaredDeltaX() {
return 0.f;
}
public final float getSumOfSquaredGradients() {
return sum_of_squared_gradients;
}
}
public static final class WeightValueWithGtXt extends WeightValue {
private final float sum_of_squared_gradients;
private final float sum_of_squared_delta_x;
public WeightValueWithGtXt(float weight, float sum_of_squared_gradients, float sum_of_squared_delta_x) {
super(weight);
this.sum_of_squared_gradients = sum_of_squared_gradients;
this.sum_of_squared_delta_x = sum_of_squared_delta_x;
}
@Override
public WeightValueType getType() {
return WeightValueType.WeightValueWithGt;
}
@Override
public final float getSumOfSquaredGradients() {
@Override
public final float getSumOfSquaredDeltaX() {
return sum_of_squared_delta_x;
}
@Override
public float getSumOfSquaredDeltaX() {
return 0.f;
}
public static final class WeightValueWithGtXtClock extends WeightValueWithClock {
private final float sum_of_squared_gradients;
private final float sum_of_squared_delta_x;
public WeightValueWithGtXtClock(IWeightValue src) {
super(src);
this.sum_of_squared_gradients = src.getSumOfSquaredGradients();
this.sum_of_squared_delta_x = src.getSumOfSquaredGradients();
}
@Override
public WeightValueType getType() {
return WeightValueType.WeightValueWithGtXt;
}
@Override
public float getSumOfSquaredGradients() {
return sum_of_squared_gradients;
}
@Override
public float getSumOfSquaredDeltaX() {
return sum_of_squared_delta_x;
}
}
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
@Nullable
protected CommandLine processOptions(@Nonnull ObjectInspector[] argOIs) throws UDFArgumentException {
import javax.annotation.Nonnull;
import org.apache.commons.cli.CommandLine;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
public abstract class EtaEstimator {
public abstract float eta(int t);
public static final class SimpleEtaEstimator extends EtaEstimator {
public static final class InvscalingEtaEstimator extends EtaEstimator {
@Nonnull
public static EtaEstimator get(@Nonnull CommandLine cl) throws UDFArgumentException {
if(cl == null) {
return new InvscalingEtaEstimator(0.1f, 0.1f);
}
float eta0 = Float.parseFloat(cl.getOptionValue("eta0", "0.1"));
if(cl.hasOption("t")) {
int t = Integer.parseInt(cl.getOptionValue("t"));
return new SimpleEtaEstimator(eta0, t);
}
float power_t = Float.parseFloat(cl.getOptionValue("power_t", "0.1"));
return new InvscalingEtaEstimator(eta0, power_t);
}
import hivemall.utils.math.MathUtils;
public static float logisticLoss(final float target, final float predicted) {
if(-100.d < predicted) {
return target - (float) MathUtils.sigmoid(predicted);
} else {
return target;
}
}
import hivemall.io.WeightValue.WeightValueWithGt;
import hivemall.io.WeightValue.WeightValueWithGtXt;
private float[] sum_of_squared_gradients;
private float[] sum_of_squared_delta_x;
this.sum_of_squared_gradients = null;
this.sum_of_squared_delta_x = null;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x) {
if(sum_of_squared_gradients) {
this.sum_of_squared_gradients = new float[size];
}
if(sum_of_squared_delta_x) {
this.sum_of_squared_delta_x = new float[size];
}
}
@Override
if(sum_of_squared_gradients != null) {
this.sum_of_squared_gradients = Arrays.copyOf(sum_of_squared_gradients, newSize);
}
if(sum_of_squared_delta_x != null) {
this.sum_of_squared_delta_x = Arrays.copyOf(sum_of_squared_delta_x, newSize);
}
if(sum_of_squared_gradients != null) {
if(sum_of_squared_delta_x == null) {
return (T) new WeightValueWithGt(weights[i], sum_of_squared_gradients[i]);
} else {
return (T) new WeightValueWithGtXt(weights[i], sum_of_squared_gradients[i], sum_of_squared_delta_x[i]);
}
} else if(covars != null) {
} else {
return (T) new WeightValue(weights[i]);
if(sum_of_squared_gradients != null) {
sum_of_squared_gradients[i] = value.getSumOfSquaredGradients();
}
if(sum_of_squared_delta_x != null) {
sum_of_squared_delta_x[i] = value.getSumOfSquaredDeltaX();
}
public enum WeightValueType {
WeightValue, WeightValueWithGt, WeightValueWithGtXt, WeightValueWithCovar;
}
WeightValueType getType();
float getSumOfSquaredGradients();
float getSumOfSquaredDeltaX();
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x);
boolean contains(@Nonnull Object feature);
@Nullable
<T extends IWeightValue> T get(@Nonnull Object feature);
<T extends IWeightValue> void set(@Nonnull Object feature, T value);
float getWeight(@Nonnull Object feature);
float getCovariance(@Nonnull Object feature);
void _set(@Nonnull Object feature, float weight, short clock);
void _set(@Nonnull Object feature, float weight, float covar, short clock);
import hivemall.io.WeightValue.WeightValueWithGt;
import hivemall.io.WeightValue.WeightValueWithGtXt;
private float[] sum_of_squared_gradients;
private float[] sum_of_squared_delta_x;
this.sum_of_squared_gradients = null;
this.sum_of_squared_delta_x = null;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x) {
if(sum_of_squared_gradients) {
this.sum_of_squared_gradients = new float[size];
}
if(sum_of_squared_delta_x) {
this.sum_of_squared_delta_x = new float[size];
}
}
@Override
if(sum_of_squared_gradients != null) {
this.sum_of_squared_gradients = Arrays.copyOf(sum_of_squared_gradients, newSize);
}
if(sum_of_squared_delta_x != null) {
this.sum_of_squared_delta_x = Arrays.copyOf(sum_of_squared_delta_x, newSize);
}
if(sum_of_squared_gradients != null) {
if(sum_of_squared_delta_x == null) {
return (T) new WeightValueWithGt(getWeight(i), sum_of_squared_gradients[i]);
} else {
return (T) new WeightValueWithGtXt(getWeight(i), sum_of_squared_gradients[i], sum_of_squared_delta_x[i]);
}
} else if(covars != null) {
} else {
return (T) new WeightValue(getWeight(i));
if(sum_of_squared_gradients != null) {
sum_of_squared_gradients[i] = value.getSumOfSquaredGradients();
}
if(sum_of_squared_delta_x != null) {
sum_of_squared_delta_x[i] = value.getSumOfSquaredDeltaX();
}
import hivemall.io.WeightValueWithClock.WeightValueWithGtClock;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x) {}
@Override
final IWeightValue wrapper;
switch(value.getType()) {
case WeightValue:
wrapper = new WeightValueWithClock(value);
break;
case WeightValueWithCovar:
wrapper = new WeightValueWithCovarClock(value);
break;
case WeightValueWithGt:
wrapper = new WeightValueWithGtClock(value);
break;
default:
wrapper = value;
return wrapper;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x) {
model.configurParams(sum_of_squared_gradients, sum_of_squared_delta_x);
}
@Override
public WeightValueType getType() {
return WeightValueType.WeightValue;
}
@Override
@Override
public float getSumOfSquaredGradients() {
return 0.f;
}
@Override
public float getSumOfSquaredDeltaX() {
return 0.f;
}
public static final class WeightValueWithGt extends WeightValue {
private final float sum_of_squared_gradients;
public WeightValueWithGt(float weight, float sum_of_squared_gradients) {
super(weight);
this.sum_of_squared_gradients = sum_of_squared_gradients;
}
@Override
public WeightValueType getType() {
return WeightValueType.WeightValueWithGt;
}
@Override
public final float getSumOfSquaredGradients() {
return sum_of_squared_gradients;
}
}
public static final class WeightValueWithGtXt extends WeightValue {
private final float sum_of_squared_gradients;
private final float sum_of_squared_delta_x;
public WeightValueWithGtXt(float weight, float sum_of_squared_gradients, float sum_of_squared_delta_x) {
super(weight);
this.sum_of_squared_gradients = sum_of_squared_gradients;
this.sum_of_squared_delta_x = sum_of_squared_delta_x;
}
@Override
public WeightValueType getType() {
return WeightValueType.WeightValueWithGt;
}
@Override
public final float getSumOfSquaredGradients() {
return sum_of_squared_gradients;
}
@Override
public final float getSumOfSquaredDeltaX() {
return sum_of_squared_delta_x;
}
}
public WeightValueType getType() {
return WeightValueType.WeightValueWithCovar;
}
@Override
@Override
public WeightValueType getType() {
return WeightValueType.WeightValue;
}
@Override
public float getSumOfSquaredGradients() {
return 0.f;
}
@Override
public float getSumOfSquaredDeltaX() {
return 0.f;
}
public static final class WeightValueWithGtClock extends WeightValueWithClock {
private final float sum_of_squared_gradients;
public WeightValueWithGtClock(IWeightValue src) {
super(src);
this.sum_of_squared_gradients = src.getSumOfSquaredGradients();
}
@Override
public WeightValueType getType() {
return WeightValueType.WeightValueWithGt;
}
@Override
public float getSumOfSquaredGradients() {
return sum_of_squared_gradients;
}
}
public static final class WeightValueWithGtXtClock extends WeightValueWithClock {
private final float sum_of_squared_gradients;
private final float sum_of_squared_delta_x;
public WeightValueWithGtXtClock(IWeightValue src) {
super(src);
this.sum_of_squared_gradients = src.getSumOfSquaredGradients();
this.sum_of_squared_delta_x = src.getSumOfSquaredGradients();
}
@Override
public WeightValueType getType() {
return WeightValueType.WeightValueWithGtXt;
}
@Override
public float getSumOfSquaredGradients() {
return sum_of_squared_gradients;
}
@Override
public float getSumOfSquaredDeltaX() {
return sum_of_squared_delta_x;
}
}
public WeightValueType getType() {
return WeightValueType.WeightValueWithCovar;
}
@Override
import hivemall.common.LossFunctions;
public final class LogressUDTF extends OnlineRegressionUDTF {
this.etaEstimator = EtaEstimator.get(cl);
protected void checkTargetValue(final float target) throws UDFArgumentException {
protected float computeUpdate(final float target, final float predicted) {
float eta = etaEstimator.eta(count);
float gradient = LossFunctions.logisticLoss(target, predicted);
return eta * gradient;
float d = computeUpdate(target, predicted);
protected float computeUpdate(float target, float predicted) {
import hivemall.io.WeightValue.WeightValueParamsF1;
import hivemall.io.WeightValue.WeightValueParamsF2;
private float[] sum_of_gradients;
this.sum_of_gradients = null;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
if(sum_of_gradients) {
this.sum_of_gradients = new float[size];
}
if(sum_of_gradients != null) {
this.sum_of_gradients = Arrays.copyOf(sum_of_gradients, newSize);
}
if(sum_of_squared_delta_x != null) {
return (T) new WeightValueParamsF2(weights[i], sum_of_squared_gradients[i], sum_of_squared_delta_x[i]);
} else if(sum_of_gradients != null) {
return (T) new WeightValueParamsF2(weights[i], sum_of_squared_gradients[i], sum_of_gradients[i]);
return (T) new WeightValueParamsF1(weights[i], sum_of_squared_gradients[i]);
if(sum_of_gradients != null) {
sum_of_gradients[i] = value.getSumOfGradients();
}
import javax.annotation.Nonnegative;
NoParams, ParamsF1, ParamsF2, ParamsCovar;
float getFloatParams(@Nonnegative int i);
float getSumOfGradients();
void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients);
import hivemall.io.WeightValue.WeightValueParamsF1;
import hivemall.io.WeightValue.WeightValueParamsF2;
private float[] sum_of_gradients;
this.sum_of_gradients = null;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
if(sum_of_gradients) {
this.sum_of_gradients = new float[size];
}
if(sum_of_gradients != null) {
this.sum_of_gradients = Arrays.copyOf(sum_of_gradients, newSize);
}
if(sum_of_squared_delta_x != null) {
return (T) new WeightValueParamsF2(getWeight(i), sum_of_squared_gradients[i], sum_of_squared_delta_x[i]);
} else if(sum_of_gradients != null) {
return (T) new WeightValueParamsF2(getWeight(i), sum_of_squared_gradients[i], sum_of_gradients[i]);
return (T) new WeightValueParamsF1(getWeight(i), sum_of_squared_gradients[i]);
if(sum_of_gradients != null) {
sum_of_squared_gradients[i] = value.getSumOfGradients();
}
import hivemall.io.WeightValueWithClock.WeightValueParamsF1Clock;
import hivemall.io.WeightValueWithClock.WeightValueParamsF2Clock;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {}
case NoParams:
case ParamsCovar:
case ParamsF1:
wrapper = new WeightValueParamsF1Clock(value);
break;
case ParamsF2:
wrapper = new WeightValueParamsF2Clock(value);
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
model.configurParams(sum_of_squared_gradients, sum_of_squared_delta_x, sum_of_gradients);
import javax.annotation.Nonnegative;
return WeightValueType.NoParams;
}
@Override
public float getFloatParams(@Nonnegative int i) {
throw new UnsupportedOperationException("getFloatParams(int) should not be called");
@Override
public float getSumOfGradients() {
return 0.f;
}
public static final class WeightValueParamsF1 extends WeightValue {
private final float f1;
public WeightValueParamsF1(float weight, float f1) {
this.f1 = f1;
return WeightValueType.ParamsF1;
}
@Override
public float getFloatParams(@Nonnegative final int i) {
if(i == 1) {
return f1;
}
return f1;
public static final class WeightValueParamsF2 extends WeightValue {
private final float f1;
private final float f2;
public WeightValueParamsF2(float weight, float f1, float f2) {
this.f1 = f1;
this.f2 = f2;
return WeightValueType.ParamsF2;
}
@Override
public float getFloatParams(@Nonnegative final int i) {
if(i == 1) {
return f1;
} else if(i == 2) {
return f2;
}
return f1;
return f2;
@Override
public float getSumOfGradients() {
return f2;
}
return WeightValueType.ParamsCovar;
import javax.annotation.Nonnegative;
return WeightValueType.NoParams;
}
@Override
public float getFloatParams(@Nonnegative int i) {
throw new UnsupportedOperationException("getFloatParams(int) should not be called");
@Override
public float getSumOfGradients() {
return 0.f;
}
public static final class WeightValueParamsF1Clock extends WeightValueWithClock {
private final float f1;
public WeightValueParamsF1Clock(IWeightValue src) {
this.f1 = src.getFloatParams(1);
return WeightValueType.ParamsF1;
}
@Override
public float getFloatParams(@Nonnegative final int i) {
if(i == 1) {
return f1;
}
return f1;
public static final class WeightValueParamsF2Clock extends WeightValueWithClock {
private final float f1;
private final float f2;
public WeightValueParamsF2Clock(IWeightValue src) {
this.f1 = src.getFloatParams(1);
this.f2 = src.getFloatParams(2);
return WeightValueType.ParamsF2;
}
@Override
public float getFloatParams(@Nonnegative final int i) {
if(i == 1) {
return f1;
} else if(i == 2) {
return f2;
}
return f1;
return f2;
}
@Override
public float getSumOfGradients() {
return f2;
return WeightValueType.ParamsCovar;
import hivemall.io.WeightValue.WeightValueParamsF2;
model.configurParams(true, true, false);
return new WeightValueParamsF2(new_w, new_scaled_sum_sq_grad, new_sum_squared_delta_x);
import hivemall.io.WeightValue.WeightValueParamsF1;
model.configurParams(true, false, false);
return new WeightValueParamsF1(new_w, scaled_sum_sqgrad);
private float eta;
opts.addOption("eta", "eta0", true, "The initial learning rate [default 0.1]");
this.eta = 0.1f;
this.eta = Primitives.parseFloat(cl.getOptionValue("eta"), 0.1f);
import javax.annotation.Nonnull;
Arrays.fill(covars, oldSize, newSize, 1.f);
public void delete(@Nonnull Object feature) {
final int i = HiveUtils.parseInt(feature);
if(i >= size) {
return;
}
weights[i] = 0.f;
if(covars != null) {
covars[i] = 1.f;
}
if(sum_of_squared_gradients != null) {
sum_of_squared_gradients[i] = 0.f;
}
if(sum_of_squared_delta_x != null) {
sum_of_squared_delta_x[i] = 0.f;
}
if(sum_of_gradients != null) {
sum_of_gradients[i] = 0.f;
}
}
@Override
void delete(@Nonnull Object feature);
<T extends IWeightValue> void set(@Nonnull Object feature, @Nonnull T value);
import javax.annotation.Nonnull;
public void delete(@Nonnull Object feature) {
final int i = HiveUtils.parseInt(feature);
if(i >= size) {
return;
}
setWeight(i, 0.f);
if(covars != null) {
setCovar(i, 1.f);
}
if(sum_of_squared_gradients != null) {
sum_of_squared_gradients[i] = 0.f;
}
if(sum_of_squared_delta_x != null) {
sum_of_squared_delta_x[i] = 0.f;
}
if(sum_of_gradients != null) {
sum_of_gradients[i] = 0.f;
}
}
@Override
import javax.annotation.Nonnull;
@Override
public void delete(@Nonnull Object feature) {
weights.remove(feature);
}
import javax.annotation.Nonnull;
public void delete(@Nonnull Object feature) {
try {
lock.lock();
model.delete(feature);
} finally {
lock.unlock();
}
}
@Override
StructObjectInspector oi = super.initialize(argOIs);
model.configurParams(true, false, true);
return oi;
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
public final class SigmodUDF extends UDF {
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
public final class SplitWordsUDF extends UDF {
opts.addOption("eta", "eta0", true, "The initial learning rate [default 1.0]");
this.eta = 1.f;
this.eta = Primitives.parseFloat(cl.getOptionValue("eta"), 1.f);
import hivemall.io.WeightValue.WeightValueParamsF1;
import hivemall.io.WeightValue.WeightValueParamsF2;
import javax.annotation.Nonnull;
private float[] sum_of_gradients;
this.sum_of_gradients = null;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
if(sum_of_gradients) {
this.sum_of_gradients = new float[size];
}
Arrays.fill(covars, oldSize, newSize, 1.f);
if(sum_of_gradients != null) {
this.sum_of_gradients = Arrays.copyOf(sum_of_gradients, newSize);
}
if(sum_of_squared_delta_x != null) {
return (T) new WeightValueParamsF2(weights[i], sum_of_squared_gradients[i], sum_of_squared_delta_x[i]);
} else if(sum_of_gradients != null) {
return (T) new WeightValueParamsF2(weights[i], sum_of_squared_gradients[i], sum_of_gradients[i]);
return (T) new WeightValueParamsF1(weights[i], sum_of_squared_gradients[i]);
if(sum_of_gradients != null) {
sum_of_gradients[i] = value.getSumOfGradients();
}
public void delete(@Nonnull Object feature) {
final int i = HiveUtils.parseInt(feature);
if(i >= size) {
return;
}
weights[i] = 0.f;
if(covars != null) {
covars[i] = 1.f;
}
if(sum_of_squared_gradients != null) {
sum_of_squared_gradients[i] = 0.f;
}
if(sum_of_squared_delta_x != null) {
sum_of_squared_delta_x[i] = 0.f;
}
if(sum_of_gradients != null) {
sum_of_gradients[i] = 0.f;
}
}
@Override
import javax.annotation.Nonnegative;
NoParams, ParamsF1, ParamsF2, ParamsCovar;
float getFloatParams(@Nonnegative int i);
float getSumOfGradients();
void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients);
void delete(@Nonnull Object feature);
<T extends IWeightValue> void set(@Nonnull Object feature, @Nonnull T value);
import hivemall.io.WeightValue.WeightValueParamsF1;
import hivemall.io.WeightValue.WeightValueParamsF2;
import javax.annotation.Nonnull;
private float[] sum_of_gradients;
this.sum_of_gradients = null;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
if(sum_of_gradients) {
this.sum_of_gradients = new float[size];
}
if(sum_of_gradients != null) {
this.sum_of_gradients = Arrays.copyOf(sum_of_gradients, newSize);
}
if(sum_of_squared_delta_x != null) {
return (T) new WeightValueParamsF2(getWeight(i), sum_of_squared_gradients[i], sum_of_squared_delta_x[i]);
} else if(sum_of_gradients != null) {
return (T) new WeightValueParamsF2(getWeight(i), sum_of_squared_gradients[i], sum_of_gradients[i]);
return (T) new WeightValueParamsF1(getWeight(i), sum_of_squared_gradients[i]);
if(sum_of_gradients != null) {
sum_of_squared_gradients[i] = value.getSumOfGradients();
}
public void delete(@Nonnull Object feature) {
final int i = HiveUtils.parseInt(feature);
if(i >= size) {
return;
}
setWeight(i, 0.f);
if(covars != null) {
setCovar(i, 1.f);
}
if(sum_of_squared_gradients != null) {
sum_of_squared_gradients[i] = 0.f;
}
if(sum_of_squared_delta_x != null) {
sum_of_squared_delta_x[i] = 0.f;
}
if(sum_of_gradients != null) {
sum_of_gradients[i] = 0.f;
}
}
@Override
import hivemall.io.WeightValueWithClock.WeightValueParamsF1Clock;
import hivemall.io.WeightValueWithClock.WeightValueParamsF2Clock;
import javax.annotation.Nonnull;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {}
@Override
public void delete(@Nonnull Object feature) {
weights.remove(feature);
}
case NoParams:
case ParamsCovar:
case ParamsF1:
wrapper = new WeightValueParamsF1Clock(value);
break;
case ParamsF2:
wrapper = new WeightValueParamsF2Clock(value);
import javax.annotation.Nonnull;
public void configurParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
model.configurParams(sum_of_squared_gradients, sum_of_squared_delta_x, sum_of_gradients);
public void delete(@Nonnull Object feature) {
try {
lock.lock();
model.delete(feature);
} finally {
lock.unlock();
}
}
@Override
import javax.annotation.Nonnegative;
return WeightValueType.NoParams;
}
@Override
public float getFloatParams(@Nonnegative int i) {
throw new UnsupportedOperationException("getFloatParams(int) should not be called");
@Override
public float getSumOfGradients() {
return 0.f;
}
public static final class WeightValueParamsF1 extends WeightValue {
private final float f1;
public WeightValueParamsF1(float weight, float f1) {
this.f1 = f1;
return WeightValueType.ParamsF1;
}
@Override
public float getFloatParams(@Nonnegative final int i) {
if(i == 1) {
return f1;
}
return f1;
public static final class WeightValueParamsF2 extends WeightValue {
private final float f1;
private final float f2;
public WeightValueParamsF2(float weight, float f1, float f2) {
this.f1 = f1;
this.f2 = f2;
return WeightValueType.ParamsF2;
}
@Override
public float getFloatParams(@Nonnegative final int i) {
if(i == 1) {
return f1;
} else if(i == 2) {
return f2;
}
return f1;
return f2;
@Override
public float getSumOfGradients() {
return f2;
}
return WeightValueType.ParamsCovar;
import javax.annotation.Nonnegative;
return WeightValueType.NoParams;
}
@Override
public float getFloatParams(@Nonnegative int i) {
throw new UnsupportedOperationException("getFloatParams(int) should not be called");
@Override
public float getSumOfGradients() {
return 0.f;
}
public static final class WeightValueParamsF1Clock extends WeightValueWithClock {
private final float f1;
public WeightValueParamsF1Clock(IWeightValue src) {
this.f1 = src.getFloatParams(1);
return WeightValueType.ParamsF1;
}
@Override
public float getFloatParams(@Nonnegative final int i) {
if(i == 1) {
return f1;
}
return f1;
public static final class WeightValueParamsF2Clock extends WeightValueWithClock {
private final float f1;
private final float f2;
public WeightValueParamsF2Clock(IWeightValue src) {
this.f1 = src.getFloatParams(1);
this.f2 = src.getFloatParams(2);
return WeightValueType.ParamsF2;
}
@Override
public float getFloatParams(@Nonnegative final int i) {
if(i == 1) {
return f1;
} else if(i == 2) {
return f2;
}
return f1;
return f2;
}
@Override
public float getSumOfGradients() {
return f2;
return WeightValueType.ParamsCovar;
import hivemall.io.WeightValue.WeightValueParamsF2;
model.configurParams(true, true, false);
return new WeightValueParamsF2(new_w, new_scaled_sum_sq_grad, new_sum_squared_delta_x);
import hivemall.io.WeightValue.WeightValueParamsF1;
private float eta;
model.configurParams(true, false, false);
opts.addOption("eta", "eta0", true, "The initial learning rate [default 1.0]");
this.eta = 1.f;
this.eta = Primitives.parseFloat(cl.getOptionValue("eta"), 1.f);
return new WeightValueParamsF1(new_w, scaled_sum_sqgrad);
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
public final class SigmodUDF extends UDF {
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
public final class SplitWordsUDF extends UDF {
import hivemall.mix.metrics.MetricsRegistry;
import hivemall.mix.metrics.MixServerMetrics;
import hivemall.mix.metrics.ThroughputCounter;
private final boolean jmx;
this.jmx = cl.hasOption("jmx");
opts.addOption("jmx", "metrics", false, "Toggle this option to enable monitoring metrics using JMX [default: false]");
ThroughputCounter throughputCounter = null;
ScheduledExecutorService metricCollector = null;
if(jmx) {
metricCollector = Executors.newScheduledThreadPool(1);
MixServerMetrics metrics = new MixServerMetrics();
throughputCounter = new ThroughputCounter(metricCollector, 5000L, metrics);
MetricsRegistry.registerMBeans(metrics, port);
}
MixServerInitializer initializer = new MixServerInitializer(msgHandler, throughputCounter, sslCtx);
Runnable cleanSessionTask = new IdleSessionSweeper(sessionStore, sessionTTLinSec * 1000L);
ScheduledExecutorService idleSessionChecker = Executors.newScheduledThreadPool(1);
if(jmx) {
MetricsRegistry.unregisterMBeans(port);
metricCollector.shutdownNow();
}
import hivemall.mix.metrics.ThroughputCounter;
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
@Nonnull
@Nullable
private final ThroughputCounter throughputCounter;
@Nullable
public MixServerInitializer(@Nonnull MixServerHandler msgHandler, @Nullable ThroughputCounter throughputCounter, @Nullable SslContext sslCtx) {
this.throughputCounter = throughputCounter;
if(throughputCounter != null) {
pipeline.addLast(decoder, encoder, throughputCounter, requestHandler);
} else {
pipeline.addLast(decoder, encoder, requestHandler);
}
buf.append("Read Throughput: ").append(readThroughput / 1024L).append(" KB/sec, ");
buf.append("Write Throughput: ").append(writeThroughput / 1024).append(" KB/sec, ");
pipeline.addLast(throughputCounter, decoder, encoder, requestHandler);
import hivemall.mix.metrics.MetricsRegistry;
import hivemall.mix.metrics.MixServerMetrics;
import hivemall.mix.metrics.ThroughputCounter;
private final boolean jmx;
this.jmx = cl.hasOption("jmx");
opts.addOption("jmx", "metrics", false, "Toggle this option to enable monitoring metrics using JMX [default: false]");
ThroughputCounter throughputCounter = null;
ScheduledExecutorService metricCollector = null;
if(jmx) {
metricCollector = Executors.newScheduledThreadPool(1);
MixServerMetrics metrics = new MixServerMetrics();
throughputCounter = new ThroughputCounter(metricCollector, 5000L, metrics);
MetricsRegistry.registerMBeans(metrics, port);
}
MixServerInitializer initializer = new MixServerInitializer(msgHandler, throughputCounter, sslCtx);
Runnable cleanSessionTask = new IdleSessionSweeper(sessionStore, sessionTTLinSec * 1000L);
ScheduledExecutorService idleSessionChecker = Executors.newScheduledThreadPool(1);
if(jmx) {
MetricsRegistry.unregisterMBeans(port);
metricCollector.shutdownNow();
}
import hivemall.mix.metrics.ThroughputCounter;
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
@Nonnull
@Nullable
private final ThroughputCounter throughputCounter;
@Nullable
public MixServerInitializer(@Nonnull MixServerHandler msgHandler, @Nullable ThroughputCounter throughputCounter, @Nullable SslContext sslCtx) {
this.throughputCounter = throughputCounter;
if(throughputCounter != null) {
pipeline.addLast(throughputCounter, decoder, encoder, requestHandler);
} else {
pipeline.addLast(decoder, encoder, requestHandler);
}
public final class ExtractWeightUDF extends UDF {
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "sort_by_feature", value = "_FUNC_(map in map<int,float>) - Returns a sorted map")
@UDFType(deterministic = true, stateful = false)
public final class SortByFeatureUDF extends UDF {
public final class ExtractWeightUDF extends UDF {
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "sort_by_feature", value = "_FUNC_(map in map<int,float>) - Returns a sorted map")
@UDFType(deterministic = true, stateful = false)
public final class SortByFeatureUDF extends UDF {
return val(Arrays.binarySearch(stopwords, word) >= 0);
return val(Arrays.binarySearch(stopwords, word) >= 0);
package hivemall.mix.metrics;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelPromise;
import io.netty.handler.traffic.GlobalTrafficShapingHandler;
import io.netty.handler.traffic.TrafficCounter;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.atomic.AtomicLong;
import javax.annotation.Nonnull;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(ThroughputCounter.class);
if(logger.isInfoEnabled()) {
if(lastReads > 0 || lastWrites > 0) {
logger.info(toString());
}
}
buf.append(lastWrites).append(" msg/sec");
ScheduledExecutorService metricCollector = Executors.newScheduledThreadPool(1);
MixServerMetrics metrics = new MixServerMetrics();
ThroughputCounter throughputCounter = new ThroughputCounter(metricCollector, 5000L, metrics);
metricCollector.shutdownNow();
return new InvscalingEtaEstimator(0.2f, 0.1f);
float eta0 = Float.parseFloat(cl.getOptionValue("eta0", "0.2"));
model.configureParams(true, false, true);
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients);
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {}
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
model.configureParams(sum_of_squared_gradients, sum_of_squared_delta_x, sum_of_gradients);
model.configureParams(true, true, false);
model.configureParams(true, false, false);
model.configureParams(true, false, true);
return new InvscalingEtaEstimator(0.2f, 0.1f);
float eta0 = Float.parseFloat(cl.getOptionValue("eta0", "0.2"));
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients);
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {}
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x, boolean sum_of_gradients) {
model.configureParams(sum_of_squared_gradients, sum_of_squared_delta_x, sum_of_gradients);
package hivemall.mix.metrics;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelPromise;
import io.netty.handler.traffic.GlobalTrafficShapingHandler;
import io.netty.handler.traffic.TrafficCounter;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.atomic.AtomicLong;
import javax.annotation.Nonnull;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(ThroughputCounter.class);
if(logger.isInfoEnabled()) {
if(lastReads > 0 || lastWrites > 0) {
logger.info(toString());
}
}
buf.append(lastWrites).append(" msg/sec");
ScheduledExecutorService metricCollector = Executors.newScheduledThreadPool(1);
MixServerMetrics metrics = new MixServerMetrics();
ThroughputCounter throughputCounter = new ThroughputCounter(metricCollector, 5000L, metrics);
metricCollector.shutdownNow();
model.configureParams(true, true, false);
model.configureParams(true, false, false);
protected CommandLine processOptions(@Nonnull ObjectInspector[] argOIs)
throws UDFArgumentException {
BufferedReader reader = null;
reader = HadoopUtils.getBufferedReader(file);
IOUtils.closeQuietly(reader);
BufferedReader reader = null;
reader = HadoopUtils.getBufferedReader(file);
IOUtils.closeQuietly(reader);
import hivemall.utils.io.IOUtils;
BufferedReader reader = null;
reader = HadoopUtils.getBufferedReader(file);
IOUtils.closeQuietly(reader);
BufferedReader reader = null;
reader = HadoopUtils.getBufferedReader(file);
IOUtils.closeQuietly(reader);
import hivemall.utils.io.IOUtils;
BufferedReader reader = null;
reader = HadoopUtils.getBufferedReader(file);
IOUtils.closeQuietly(reader);
protected CommandLine processOptions(@Nonnull ObjectInspector[] argOIs)
throws UDFArgumentException {
BufferedReader reader = null;
reader = HadoopUtils.getBufferedReader(file);
IOUtils.closeQuietly(reader);
BufferedReader reader = null;
reader = HadoopUtils.getBufferedReader(file);
IOUtils.closeQuietly(reader);
import hivemall.utils.io.IOUtils;
BufferedReader reader = null;
reader = HadoopUtils.getBufferedReader(file);
IOUtils.closeQuietly(reader);
BufferedReader reader = null;
reader = HadoopUtils.getBufferedReader(file);
IOUtils.closeQuietly(reader);
import hivemall.utils.io.IOUtils;
BufferedReader reader = null;
reader = HadoopUtils.getBufferedReader(file);
IOUtils.closeQuietly(reader);
int index = (hashcode & Integer.MAX_VALUE) % numNodes;
public OpenHashMap() {}
int index = (hashcode & Integer.MAX_VALUE) % numNodes;
public OpenHashMap() {}
return compare(value, other.value);
}
private static int compare(final int x, final int y) {
return (x < y) ? -1 : ((x == y) ? 0 : 1);
return compare(value, other.value);
}
private static int compare(final int x, final int y) {
return (x < y) ? -1 : ((x == y) ? 0 : 1);
private volatile ServerState state;
this.state = ServerState.INITIALIZING;
public ServerState getState() {
return state;
}
private void acceptConnections(@Nonnull MixServerInitializer initializer, int port)
this.state = ServerState.RUNNING;
this.state = ServerState.STOPPING;
public enum ServerState {
INITIALIZING, RUNNING, STOPPING,
}
import java.io.IOException;
import java.net.ServerSocket;
import java.util.NoSuchElementException;
public static int getAvailablePort() {
try {
ServerSocket s = new ServerSocket(0);
s.setReuseAddress(true);
s.close();
return s.getLocalPort();
} catch (IOException e) {
throw new IllegalStateException("Failed to find an available port", e);
}
}
public static int getAvialablePort(final int basePort) {
if(basePort == 0) {
return getAvailablePort();
}
if(basePort < 0 || basePort > 65535) {
}
if(isPortAvailable(i)) {
return i;
}
}
throw new NoSuchElementException("Could not find available port greater than or equals to "
basePort);
}
public static boolean isPortAvailable(final int port) {
ServerSocket s = null;
try {
s = new ServerSocket(port);
s.setReuseAddress(true);
return true;
} catch (IOException e) {
return false;
} finally {
if(s != null) {
try {
s.close();
} catch (IOException e) {
;
}
}
}
}
private volatile ServerState state;
this.state = ServerState.INITIALIZING;
public ServerState getState() {
return state;
}
private void acceptConnections(@Nonnull MixServerInitializer initializer, int port)
this.state = ServerState.RUNNING;
this.state = ServerState.STOPPING;
public enum ServerState {
INITIALIZING, RUNNING, STOPPING,
}
import java.io.IOException;
import java.net.ServerSocket;
import java.util.NoSuchElementException;
public static int getAvailablePort() {
try {
ServerSocket s = new ServerSocket(0);
s.setReuseAddress(true);
s.close();
return s.getLocalPort();
} catch (IOException e) {
throw new IllegalStateException("Failed to find an available port", e);
}
}
public static int getAvialablePort(final int basePort) {
if(basePort == 0) {
return getAvailablePort();
}
if(basePort < 0 || basePort > 65535) {
}
if(isPortAvailable(i)) {
return i;
}
}
throw new NoSuchElementException("Could not find available port greater than or equals to "
basePort);
}
public static boolean isPortAvailable(final int port) {
ServerSocket s = null;
try {
s = new ServerSocket(port);
s.setReuseAddress(true);
return true;
} catch (IOException e) {
return false;
} finally {
if(s != null) {
try {
s.close();
} catch (IOException e) {
;
}
}
}
}
public static int getAvailablePort(final int basePort) {
public static int getAvailablePort(final int basePort) {
import javax.annotation.Nonnull;
@Override
public void set(@Nonnull Object feature, float weight, float covar, short clock) {
if(hasCovariance()) {
_set(feature, weight, covar, clock);
} else {
_set(feature, weight, clock);
}
}
protected abstract void _set(@Nonnull Object feature, float weight, short clock);
protected abstract void _set(@Nonnull Object feature, float weight, float covar, short clock);
protected void _set(Object feature, float weight, short clock) {
protected void _set(Object feature, float weight, float covar, short clock) {
import hivemall.mix.MixedModel;
public interface PredictionModel extends MixedModel {
protected void _set(Object feature, float weight, short clock) {
protected void _set(Object feature, float weight, float covar, short clock) {
protected void _set(final Object feature, final float weight, final short clock) {
protected void _set(final Object feature, final float weight, final float covar, final short clock) {
public void set(@Nonnull Object feature, float weight, float covar, short clock) {
model.set(feature, weight, covar, clock);
import hivemall.mix.MixedModel;
public MixClient(@Nonnull MixEventName event, @CheckForNull String groupID, @Nonnull String connectURIs, boolean ssl, int mixThreshold, @Nonnull MixedModel model) {
import hivemall.mix.MixedModel;
private final MixedModel model;
public MixClientHandler(MixedModel model) {
float covar = msg.getCovariance();
model.set(feature, weight, covar, clock);
partial = new PartialAverage();
partial = new PartialArgminKLD();
partial.add(weight, covar, clock, deltaUpdates, scale);
float averagedWeight = partial.getWeight(scale);
float meanCovar = partial.getCovariance(scale);
private double sum_mean_div_covar;
@GuardedBy("lock()")
private int num_updates;
this.num_updates = 0;
public float getCovariance(float scale) {
assert (num_updates > 0) : num_updates;
}
@Override
public void add(float localWeight, float covar, short clock, int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
addWeight(localWeight, covar, scale);
private void addWeight(float localWeight, float covar, float scale) {
assert (covar != 0.f);
num_updates;
public float getWeight(float scale) {
return (float) (sum_mean_div_covar / sum_inv_covar);
private double scaledSumWeights;
private int totalUpdates;
this.scaledSumWeights = 0.d;
public float getCovariance(float scale) {
return 1.f;
}
@Override
public void add(float localWeight, float covar, short clock, @Nonnegative int deltaUpdates, float scale) {
addWeight(localWeight, deltaUpdates, scale);
protected void addWeight(float localWeight, int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
public float getWeight(float scale) {
return (float) (scaledSumWeights / totalUpdates) * scale;
this.totalClock = 0;
public abstract void add(float localWeight, float covar, short clock, @Nonnegative int deltaUpdates, float scale);
public abstract float getWeight(float scale);
public abstract float getCovariance(float scale);
import javax.annotation.Nonnull;
@Override
public void set(@Nonnull Object feature, float weight, float covar, short clock) {
if(hasCovariance()) {
_set(feature, weight, covar, clock);
} else {
_set(feature, weight, clock);
}
}
protected abstract void _set(@Nonnull Object feature, float weight, short clock);
protected abstract void _set(@Nonnull Object feature, float weight, float covar, short clock);
protected void _set(Object feature, float weight, short clock) {
protected void _set(Object feature, float weight, float covar, short clock) {
import hivemall.mix.MixedModel;
public interface PredictionModel extends MixedModel {
protected void _set(Object feature, float weight, short clock) {
protected void _set(Object feature, float weight, float covar, short clock) {
protected void _set(final Object feature, final float weight, final short clock) {
protected void _set(final Object feature, final float weight, final float covar, final short clock) {
public void set(@Nonnull Object feature, float weight, float covar, short clock) {
model.set(feature, weight, covar, clock);
import hivemall.mix.MixedModel;
public MixClient(@Nonnull MixEventName event, @CheckForNull String groupID, @Nonnull String connectURIs, boolean ssl, int mixThreshold, @Nonnull MixedModel model) {
import hivemall.mix.MixedModel;
private final MixedModel model;
public MixClientHandler(MixedModel model) {
float covar = msg.getCovariance();
model.set(feature, weight, covar, clock);
partial = new PartialAverage();
partial = new PartialArgminKLD();
partial.add(weight, covar, clock, deltaUpdates, scale);
float averagedWeight = partial.getWeight(scale);
float meanCovar = partial.getCovariance(scale);
private double sum_mean_div_covar;
@GuardedBy("lock()")
private int num_updates;
this.num_updates = 0;
public float getCovariance(float scale) {
assert (num_updates > 0) : num_updates;
}
@Override
public void add(float localWeight, float covar, short clock, int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
addWeight(localWeight, covar, scale);
private void addWeight(float localWeight, float covar, float scale) {
assert (covar != 0.f);
num_updates;
public float getWeight(float scale) {
return (float) (sum_mean_div_covar / sum_inv_covar);
private double scaledSumWeights;
private int totalUpdates;
this.scaledSumWeights = 0.d;
public float getCovariance(float scale) {
return 1.f;
}
@Override
public void add(float localWeight, float covar, short clock, @Nonnegative int deltaUpdates, float scale) {
addWeight(localWeight, deltaUpdates, scale);
protected void addWeight(float localWeight, int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
public float getWeight(float scale) {
return (float) (scaledSumWeights / totalUpdates) * scale;
this.totalClock = 0;
public abstract void add(float localWeight, float covar, short clock, @Nonnegative int deltaUpdates, float scale);
public abstract float getWeight(float scale);
public abstract float getCovariance(float scale);
return 1.f / (sum_inv_covar * scale);
import hivemall.mix.store.SessionObject;
SessionObject session = getSession(msg);
PartialResult partial = getPartialResult(msg, session);
mix(ctx, msg, partial, session);
private SessionObject getSession(@Nonnull MixMessage msg) {
SessionObject session = sessionStore.get(groupID);
session.incrRequest();
return session;
}
@Nonnull
private PartialResult getPartialResult(@Nonnull MixMessage msg, @Nonnull SessionObject session) {
final ConcurrentMap<Object, PartialResult> map = session.get();
private void mix(final ChannelHandlerContext ctx, final MixMessage requestMsg, final PartialResult partial, final SessionObject session) {
session.incrResponse();
import java.util.concurrent.atomic.AtomicLong;
private final AtomicLong num_requests;
private final AtomicLong num_responses;
this.num_requests = new AtomicLong(0L);
this.num_responses = new AtomicLong(0L);
public void incrRequest() {
num_requests.getAndIncrement();
}
public void incrResponse() {
num_responses.getAndIncrement();
}
public long getRequests() {
return num_requests.get();
}
public long getResponses() {
return num_responses.get();
}
public String getSessionInfo() {
long requests = num_requests.get();
long responses = num_responses.get();
float percentage = ((float) ((double) responses / requests)) * 100.f;
public SessionObject get(@Nonnull String groupID) {
return sessionObj;
SessionObject removedSession = sessions.remove(groupID);
if(removedSession != null) {
removedSession.getSessionInfo());
}
SessionObject removedSession = sessions.remove(key);
if(removedSession != null) {
removedSession.getSessionInfo());
}
import hivemall.mix.store.SessionObject;
SessionObject session = getSession(msg);
PartialResult partial = getPartialResult(msg, session);
mix(ctx, msg, partial, session);
private SessionObject getSession(@Nonnull MixMessage msg) {
SessionObject session = sessionStore.get(groupID);
session.incrRequest();
return session;
}
@Nonnull
private PartialResult getPartialResult(@Nonnull MixMessage msg, @Nonnull SessionObject session) {
final ConcurrentMap<Object, PartialResult> map = session.get();
private void mix(final ChannelHandlerContext ctx, final MixMessage requestMsg, final PartialResult partial, final SessionObject session) {
session.incrResponse();
return 1.f / (sum_inv_covar * scale);
import java.util.concurrent.atomic.AtomicLong;
private final AtomicLong num_requests;
private final AtomicLong num_responses;
this.num_requests = new AtomicLong(0L);
this.num_responses = new AtomicLong(0L);
public void incrRequest() {
num_requests.getAndIncrement();
}
public void incrResponse() {
num_responses.getAndIncrement();
}
public long getRequests() {
return num_requests.get();
}
public long getResponses() {
return num_responses.get();
}
public String getSessionInfo() {
long requests = num_requests.get();
long responses = num_responses.get();
float percentage = ((float) ((double) responses / requests)) * 100.f;
public SessionObject get(@Nonnull String groupID) {
return sessionObj;
SessionObject removedSession = sessions.remove(groupID);
if(removedSession != null) {
removedSession.getSessionInfo());
}
SessionObject removedSession = sessions.remove(key);
if(removedSession != null) {
removedSession.getSessionInfo());
}
opts.addOption("eta0", true, "The initial learning rate [default 1.0]");
import org.apache.hadoop.hive.ql.exec.UDF;
public final class KLDivergenceUDF extends UDF {
ArrayUtils.shuffle(slots[x], position, randoms[x]);
}
}
public static final class FixedEtaEstimator extends EtaEstimator {
private final float eta;
public FixedEtaEstimator(float eta) {
this.eta = eta;
}
@Override
public float eta(int t) {
return eta;
}
}
String etaValue = cl.getOptionValue("eta");
if(etaValue != null) {
float eta = Float.parseFloat(etaValue);
return new FixedEtaEstimator(eta);
}
float eta0 = Float.parseFloat(cl.getOptionValue("eta0", "0.2"));
public final class ArgminKLDistanceUDAF extends UDAF {
public final class MaxRowUDAF extends AbstractGenericUDAFResolver {
public final class MaxValueLabelUDAF extends UDAF {
public final class VotedAvgUDAF extends UDAF {
public final class WeightVotedAvgUDAF extends UDAF {
if(deltaUpdates < 1) {
return;
}
public WeightValueWithClock(float value) {
this.value = value;
this.clock = 0;
this.deltaUpdates = 0;
}
public WeightValueParamsF1Clock(float value, float f1) {
super(value);
this.f1 = f1;
}
public WeightValueParamsF2Clock(float value, float f1, float f2) {
super(value);
this.f1 = f1;
this.f2 = f2;
}
public WeightValueWithCovarClock(float value, float covar) {
super(value);
this.covariance = covar;
}
public void addValue(float o) {
}
public void addValue(int o) {
}
import javax.annotation.Nonnull;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
import org.apache.hadoop.io.FloatWritable;
public static IntObjectInspector asIntOI(@Nonnull ObjectInspector argOI)
throws UDFArgumentException {
if(!INT_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (IntObjectInspector) argOI;
}
public static PrimitiveObjectInspector asDoubleCompatibleOI(@Nonnull ObjectInspector argOI)
throws UDFArgumentTypeException {
if(argOI.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0, "Only primitive type arguments are accepted but "
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch(oi.getPrimitiveCategory()) {
case BYTE:
case SHORT:
case INT:
case LONG:
case FLOAT:
case DOUBLE:
case STRING:
case TIMESTAMP:
break;
default:
throw new UDFArgumentTypeException(0, "Only numeric or string type arguments are accepted but "
}
return oi;
}
@Nonnull
public static FloatWritable[] newFloatArray(final int size, final float defaultVal) {
final FloatWritable[] array = new FloatWritable[size];
array[i] = new FloatWritable(defaultVal);
}
return array;
}
public static Integer[] toObject(final int[] array) {
final Integer[] result = new Integer[array.length];
result[i] = array[i];
}
return result;
}
public static List<Integer> toList(final int[] array) {
Integer[] v = toObject(array);
return Arrays.asList(v);
}
public static Long[] toObject(final long[] array) {
final Long[] result = new Long[array.length];
result[i] = array[i];
}
return result;
}
public static List<Long> toList(final long[] array) {
Long[] v = toObject(array);
return Arrays.asList(v);
}
public static Double[] toObject(final double[] array) {
final Double[] result = new Double[array.length];
result[i] = array[i];
}
return result;
}
public static List<Double> toList(final double[] array) {
Double[] v = toObject(array);
return Arrays.asList(v);
}
public static void fill(final float[] a, final Random rand) {
a[i] = rand.nextFloat();
}
}
opts.addOption("eta", "eta0", true, "The initial learning rate [default 1.0]");
public static final class FixedEtaEstimator extends EtaEstimator {
private final float eta;
public FixedEtaEstimator(float eta) {
this.eta = eta;
}
@Override
public float eta(int t) {
return eta;
}
}
String etaValue = cl.getOptionValue("eta");
if(etaValue != null) {
float eta = Float.parseFloat(etaValue);
return new FixedEtaEstimator(eta);
}
float eta0 = Float.parseFloat(cl.getOptionValue("eta0", "0.2"));
ArrayUtils.shuffle(slots[x], position, randoms[x]);
}
}
public final class ArgminKLDistanceUDAF extends UDAF {
public final class MaxRowUDAF extends AbstractGenericUDAFResolver {
public final class MaxValueLabelUDAF extends UDAF {
public final class VotedAvgUDAF extends UDAF {
public final class WeightVotedAvgUDAF extends UDAF {
if(deltaUpdates < 1) {
return;
}
public WeightValueWithClock(float value) {
this.value = value;
this.clock = 0;
this.deltaUpdates = 0;
}
public WeightValueParamsF1Clock(float value, float f1) {
super(value);
this.f1 = f1;
}
public WeightValueParamsF2Clock(float value, float f1, float f2) {
super(value);
this.f1 = f1;
this.f2 = f2;
}
public WeightValueWithCovarClock(float value, float covar) {
super(value);
this.covariance = covar;
}
import javax.annotation.Nonnull;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
import org.apache.hadoop.io.FloatWritable;
public static IntObjectInspector asIntOI(@Nonnull ObjectInspector argOI)
throws UDFArgumentException {
if(!INT_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (IntObjectInspector) argOI;
}
public static PrimitiveObjectInspector asDoubleCompatibleOI(@Nonnull ObjectInspector argOI)
throws UDFArgumentTypeException {
if(argOI.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0, "Only primitive type arguments are accepted but "
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch(oi.getPrimitiveCategory()) {
case BYTE:
case SHORT:
case INT:
case LONG:
case FLOAT:
case DOUBLE:
case STRING:
case TIMESTAMP:
break;
default:
throw new UDFArgumentTypeException(0, "Only numeric or string type arguments are accepted but "
}
return oi;
}
@Nonnull
public static FloatWritable[] newFloatArray(final int size, final float defaultVal) {
final FloatWritable[] array = new FloatWritable[size];
array[i] = new FloatWritable(defaultVal);
}
return array;
}
public static Integer[] toObject(final int[] array) {
final Integer[] result = new Integer[array.length];
result[i] = array[i];
}
return result;
}
public static List<Integer> toList(final int[] array) {
Integer[] v = toObject(array);
return Arrays.asList(v);
}
public static Long[] toObject(final long[] array) {
final Long[] result = new Long[array.length];
result[i] = array[i];
}
return result;
}
public static List<Long> toList(final long[] array) {
Long[] v = toObject(array);
return Arrays.asList(v);
}
public static Double[] toObject(final double[] array) {
final Double[] result = new Double[array.length];
result[i] = array[i];
}
return result;
}
public static List<Double> toList(final double[] array) {
Double[] v = toObject(array);
return Arrays.asList(v);
}
public static void fill(final float[] a, final Random rand) {
a[i] = rand.nextFloat();
}
}
public void addValue(float o) {
}
public void addValue(int o) {
}
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "f1score", value = "_FUNC_(array[int], array[int]) - Return a F-measure/F1 score")
import javax.annotation.Nullable;
public static EtaEstimator get(@Nullable CommandLine cl) throws UDFArgumentException {
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public boolean iterate(DoubleWritable predicted, DoubleWritable actual)
throws HiveException {
return true;
}
partial.iterate(predicted.get(), actual.get());
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public boolean iterate(DoubleWritable predicted, DoubleWritable actual)
throws HiveException {
return true;
}
partial.iterate(predicted.get(), actual.get());
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public boolean iterate(DoubleWritable predicted, DoubleWritable actual)
throws HiveException {
return true;
}
partial.iterate(predicted.get(), actual.get());
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "mf_predict", value = "_FUNC_(List<Float> Pu, List<Float> Qi[, double Bu, double Bi[, double mu]]) - Returns the prediction value")
if(Pu == null || Qi == null) {
if(Pu == null && Qi == null) {
return new FloatWritable(ret);
} else if(Qi == null) {
return new FloatWritable(ret);
import hivemall.common.RatingInitilizer;
import hivemall.utils.math.MathUtils;
import javax.annotation.Nonnegative;
import javax.annotation.Nonnull;
@Nonnull
private final RatingInitilizer ratingInitializer;
@Nonnegative
@Nonnegative
private final float maxInitValue;
@Nonnegative
private final double initStdDev;
@Nonnull
private Rating meanRating;
private IntOpenHashMap<Rating> userBias;
private IntOpenHashMap<Rating> itemBias;
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor, float meanRating, boolean randInit, @Nonnegative float maxInitValue, @Nonnegative double initStdDev) {
this(ratingInitializer, factor, meanRating, randInit, maxInitValue, initStdDev, 136861);
}
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor, float meanRating, boolean randInit, @Nonnegative float maxInitValue, @Nonnegative double initStdDev, int expectedSize) {
this.ratingInitializer = ratingInitializer;
this.randInit = randInit;
this.maxInitValue = maxInitValue;
this.initStdDev = initStdDev;
this.meanRating = ratingInitializer.newRating(meanRating);
this.userBias = new IntOpenHashMap<Rating>(expectedSize);
this.itemBias = new IntOpenHashMap<Rating>(expectedSize);
@Nonnull
public Rating meanRating() {
public float getMeanRating() {
return meanRating.getWeight();
}
meanRating.setWeight(rating);
uniformFill(v, randU, maxInitValue, ratingInitializer);
} else {
gaussianFill(v, randU, initStdDev, ratingInitializer);
uniformFill(v, randI, maxInitValue, ratingInitializer);
} else {
gaussianFill(v, randI, initStdDev, ratingInitializer);
@Nonnull
public Rating userBias(int u) {
Rating b = userBias.get(u);
if(b == null) {
userBias.put(u, b);
}
return b;
}
Rating b = userBias.get(u);
if(b == null) {
return 0.f;
}
return b.getWeight();
Rating b = userBias.get(u);
if(b == null) {
b = ratingInitializer.newRating(value);
userBias.put(u, b);
}
b.setWeight(value);
@Nonnull
public Rating itemBias(int i) {
Rating b = itemBias.get(i);
if(b == null) {
itemBias.put(i, b);
}
return b;
}
@Nullable
public Rating getItemBiasObject(int i) {
public float getItemBias(int i) {
Rating b = itemBias.get(i);
if(b == null) {
return 0.f;
}
return b.getWeight();
public void setItemBias(int i, float value) {
Rating b = itemBias.get(i);
if(b == null) {
b = ratingInitializer.newRating(value);
itemBias.put(i, b);
}
b.setWeight(value);
}
private static void uniformFill(final Rating[] a, final Random rand, final float maxInitValue, final RatingInitilizer init) {
float v = rand.nextFloat() * maxInitValue / len;
a[i] = init.newRating(v);
private static void gaussianFill(final Rating[] a, final Random rand, final double stddev, final RatingInitilizer init) {
float v = (float) MathUtils.gaussian(0.d, stddev, rand);
a[i] = init.newRating(v);
}
}
import hivemall.common.RatingInitilizer;
public abstract class OnlineMatrixFactorizationUDTF extends UDTFWithOptions
implements RatingInitilizer {
protected boolean randInit;
protected float maxInitValue;
protected double initStdDev;
this.maxInitValue = 1.f;
opts.addOption("mu", "mean_rating", true, "The mean rating [default: 0.0]");
opts.addOption("update_mean", false, "Whether update (and return) the mean rating or not");
opts.addOption("rand_init", false, "Perform random initialization of rank matrix [default: false]");
opts.addOption("maxval", "max_init_value", true, "The maximum initial value in the rank matrix [default: 1.0]");
opts.addOption("min_init_stddev", true, "The minimum standard deviation of initial rank matrix [default: 0.1]");
this.meanRating = Primitives.parseFloat(cl.getOptionValue("mu"), 0.f);
this.randInit = cl.hasOption("rand_init");
this.maxInitValue = Primitives.parseFloat(cl.getOptionValue("max_init_value"), 1.f);
this.initStdDev = Primitives.parseDouble(cl.getOptionValue("min_init_stddev"), 0.1d);
this.initStdDev = Math.max(initStdDev, 1.0d / factor);
this.model = new FactorizedModel(this, factor, meanRating, randInit, maxInitValue, initStdDev);
public Rating newRating(float v) {
return new Rating(v);
}
@Override
public static double parseDouble(final String s, final double defaultValue) {
if(s == null) {
return defaultValue;
}
return Double.parseDouble(s);
}
import java.util.Random;
import javax.annotation.Nonnull;
public static double gaussian(final double mean, final double stddev, @Nonnull final Random rnd) {
}
public static double lognormal(final double mean, final double stddev, @Nonnull final Random rnd) {
return Math.exp(gaussian(mean, stddev, rnd));
}
package hivemall.common;
import hivemall.io.Rating;
public interface RatingInitilizer {
public Rating newRating(float v);
public double getSumOfSquaredGradients() {
throw new UnsupportedOperationException();
}
public void setSumOfSquaredGradients(double sqgrad) {
throw new UnsupportedOperationException();
}
public static class RatingWithSquaredGrad extends Rating {
private double sumSquaredGrads;
public RatingWithSquaredGrad(float weight) {
this(weight, 0.d);
}
public RatingWithSquaredGrad(float weight, double sqgrad) {
super(weight);
this.sumSquaredGrads = sqgrad;
}
@Override
public double getSumOfSquaredGradients() {
return sumSquaredGrads;
}
@Override
public void setSumOfSquaredGradients(double sqgrad) {
this.sumSquaredGrads = sqgrad;
}
}
import javax.annotation.Nullable;
public static EtaEstimator get(@Nullable CommandLine cl) throws UDFArgumentException {
package hivemall.common;
import hivemall.io.Rating;
public interface RatingInitilizer {
public Rating newRating(float v);
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "f1score", value = "_FUNC_(array[int], array[int]) - Return a F-measure/F1 score")
import hivemall.common.RatingInitilizer;
import hivemall.utils.math.MathUtils;
import javax.annotation.Nonnegative;
import javax.annotation.Nonnull;
@Nonnull
private final RatingInitilizer ratingInitializer;
@Nonnegative
@Nonnegative
private final float maxInitValue;
@Nonnegative
private final double initStdDev;
@Nonnull
private Rating meanRating;
private IntOpenHashMap<Rating> userBias;
private IntOpenHashMap<Rating> itemBias;
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor, float meanRating, boolean randInit, @Nonnegative float maxInitValue, @Nonnegative double initStdDev) {
this(ratingInitializer, factor, meanRating, randInit, maxInitValue, initStdDev, 136861);
}
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor, float meanRating, boolean randInit, @Nonnegative float maxInitValue, @Nonnegative double initStdDev, int expectedSize) {
this.ratingInitializer = ratingInitializer;
this.randInit = randInit;
this.maxInitValue = maxInitValue;
this.initStdDev = initStdDev;
this.meanRating = ratingInitializer.newRating(meanRating);
this.userBias = new IntOpenHashMap<Rating>(expectedSize);
this.itemBias = new IntOpenHashMap<Rating>(expectedSize);
@Nonnull
public Rating meanRating() {
public float getMeanRating() {
return meanRating.getWeight();
}
meanRating.setWeight(rating);
uniformFill(v, randU, maxInitValue, ratingInitializer);
} else {
gaussianFill(v, randU, initStdDev, ratingInitializer);
uniformFill(v, randI, maxInitValue, ratingInitializer);
} else {
gaussianFill(v, randI, initStdDev, ratingInitializer);
@Nonnull
public Rating userBias(int u) {
Rating b = userBias.get(u);
if(b == null) {
userBias.put(u, b);
}
return b;
}
Rating b = userBias.get(u);
if(b == null) {
return 0.f;
}
return b.getWeight();
Rating b = userBias.get(u);
if(b == null) {
b = ratingInitializer.newRating(value);
userBias.put(u, b);
}
b.setWeight(value);
@Nonnull
public Rating itemBias(int i) {
Rating b = itemBias.get(i);
if(b == null) {
itemBias.put(i, b);
}
return b;
}
@Nullable
public Rating getItemBiasObject(int i) {
public float getItemBias(int i) {
Rating b = itemBias.get(i);
if(b == null) {
return 0.f;
}
return b.getWeight();
public void setItemBias(int i, float value) {
Rating b = itemBias.get(i);
if(b == null) {
b = ratingInitializer.newRating(value);
itemBias.put(i, b);
}
b.setWeight(value);
}
private static void uniformFill(final Rating[] a, final Random rand, final float maxInitValue, final RatingInitilizer init) {
float v = rand.nextFloat() * maxInitValue / len;
a[i] = init.newRating(v);
private static void gaussianFill(final Rating[] a, final Random rand, final double stddev, final RatingInitilizer init) {
float v = (float) MathUtils.gaussian(0.d, stddev, rand);
a[i] = init.newRating(v);
}
}
public double getSumOfSquaredGradients() {
throw new UnsupportedOperationException();
}
public void setSumOfSquaredGradients(double sqgrad) {
throw new UnsupportedOperationException();
}
public static class RatingWithSquaredGrad extends Rating {
private double sumSquaredGrads;
public RatingWithSquaredGrad(float weight) {
this(weight, 0.d);
}
public RatingWithSquaredGrad(float weight, double sqgrad) {
super(weight);
this.sumSquaredGrads = sqgrad;
}
@Override
public double getSumOfSquaredGradients() {
return sumSquaredGrads;
}
@Override
public void setSumOfSquaredGradients(double sqgrad) {
this.sumSquaredGrads = sqgrad;
}
}
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "mf_predict", value = "_FUNC_(List<Float> Pu, List<Float> Qi[, double Bu, double Bi[, double mu]]) - Returns the prediction value")
if(Pu == null || Qi == null) {
if(Pu == null && Qi == null) {
return new FloatWritable(ret);
} else if(Qi == null) {
return new FloatWritable(ret);
import hivemall.common.RatingInitilizer;
public abstract class OnlineMatrixFactorizationUDTF extends UDTFWithOptions
implements RatingInitilizer {
protected boolean randInit;
protected float maxInitValue;
protected double initStdDev;
this.maxInitValue = 1.f;
opts.addOption("mu", "mean_rating", true, "The mean rating [default: 0.0]");
opts.addOption("update_mean", false, "Whether update (and return) the mean rating or not");
opts.addOption("rand_init", false, "Perform random initialization of rank matrix [default: false]");
opts.addOption("maxval", "max_init_value", true, "The maximum initial value in the rank matrix [default: 1.0]");
opts.addOption("min_init_stddev", true, "The minimum standard deviation of initial rank matrix [default: 0.1]");
this.meanRating = Primitives.parseFloat(cl.getOptionValue("mu"), 0.f);
this.randInit = cl.hasOption("rand_init");
this.maxInitValue = Primitives.parseFloat(cl.getOptionValue("max_init_value"), 1.f);
this.initStdDev = Primitives.parseDouble(cl.getOptionValue("min_init_stddev"), 0.1d);
this.initStdDev = Math.max(initStdDev, 1.0d / factor);
this.model = new FactorizedModel(this, factor, meanRating, randInit, maxInitValue, initStdDev);
public Rating newRating(float v) {
return new Rating(v);
}
@Override
public static double parseDouble(final String s, final double defaultValue) {
if(s == null) {
return defaultValue;
}
return Double.parseDouble(s);
}
import java.util.Random;
import javax.annotation.Nonnull;
public static double gaussian(final double mean, final double stddev, @Nonnull final Random rnd) {
}
public static double lognormal(final double mean, final double stddev, @Nonnull final Random rnd) {
return Math.exp(gaussian(mean, stddev, rnd));
}
private final Random[] randU, randI;
this.randU = newRandoms(factor, 31L);
this.randI = newRandoms(factor, 41L);
}
private static Random[] newRandoms(@Nonnull final int size, final long seed) {
final Random[] rand = new Random[size];
}
return rand;
uniformFill(v, randU[0], maxInitValue, ratingInitializer);
uniformFill(v, randI[0], maxInitValue, ratingInitializer);
private static void gaussianFill(final Rating[] a, final Random[] rand, final double stddev, final RatingInitilizer init) {
float v = (float) MathUtils.gaussian(0.d, stddev, rand[i]);
onUpdate(user, item, users, items, err);
protected void onUpdate(final int user, final int item, final Rating[] users, final Rating[] items, final double err) {}
private final RankInitScheme initScheme;
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor, float meanRating, @Nonnull RankInitScheme initScheme) {
this(ratingInitializer, factor, meanRating, initScheme, 136861);
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor, float meanRating, @Nonnull RankInitScheme initScheme, int expectedSize) {
this.initScheme = initScheme;
public enum RankInitScheme {
@Nonnegative
private float maxInitValue;
@Nonnegative
private double initStdDev;
@Nonnull
public static RankInitScheme resolve(@Nullable String opt) {
if(opt == null) {
return random_vcol;
} else if("random_vcol".equalsIgnoreCase(opt)) {
return random_vcol;
} else if("gaussian".equalsIgnoreCase(opt)) {
return gaussian;
} else if("random".equalsIgnoreCase(opt)) {
return random;
}
return random_vcol;
}
public void setMaxInitValue(float maxInitValue) {
this.maxInitValue = maxInitValue;
}
public void setInitStdDev(double initStdDev) {
this.initStdDev = initStdDev;
}
}
switch(initScheme) {
case random_vcol:
randomVcolFill(v, randU[0], 10, initScheme.maxInitValue, ratingInitializer);
break;
case random:
uniformFill(v, randU[0], initScheme.maxInitValue, ratingInitializer);
break;
case gaussian:
gaussianFill(v, randU, initScheme.initStdDev, ratingInitializer);
break;
default:
throw new IllegalStateException("Unsupported rank initialization scheme: "
initScheme);
switch(initScheme) {
case random_vcol:
randomVcolFill(v, randI[0], 10, initScheme.maxInitValue, ratingInitializer);
break;
case random:
uniformFill(v, randI[0], initScheme.maxInitValue, ratingInitializer);
break;
case gaussian:
gaussianFill(v, randI, initScheme.initStdDev, ratingInitializer);
break;
default:
throw new IllegalStateException("Unsupported rank initialization scheme: "
initScheme);
private static void randomVcolFill(final Rating[] a, final Random rand, final int k, final float maxInitValue, final RatingInitilizer init) {
float v = avg(rand, k) * maxInitValue / len;
a[i] = init.newRating(v);
}
}
private static float avg(final Random rand, final int k) {
float total = 0.f;
}
return total / k;
}
import hivemall.io.FactorizedModel.RankInitScheme;
protected RankInitScheme rankInit;
opts.addOption("rankinit", true, "Initialization strategy of rank matrix [default: random_vcol, random, guassian]");
String rankInitOpt = null;
float maxInitValue = 1.f;
double initStdDev = 0.1d;
rankInitOpt = cl.getOptionValue("rankinit");
maxInitValue = Primitives.parseFloat(cl.getOptionValue("max_init_value"), 1.f);
initStdDev = Primitives.parseDouble(cl.getOptionValue("min_init_stddev"), 0.1d);
this.rankInit = RankInitScheme.resolve(rankInitOpt);
rankInit.setMaxInitValue(maxInitValue);
initStdDev = Math.max(initStdDev, 1.0d / factor);
rankInit.setInitStdDev(initStdDev);
this.model = new FactorizedModel(this, factor, meanRating, rankInit);
return gaussian;
return gaussian;
opts.addOption("rankinit", true, "Initialization strategy of rank matrix [default: guassian, random]");
private final RankInitScheme initScheme;
private final Random[] randU, randI;
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor, float meanRating, @Nonnull RankInitScheme initScheme) {
this(ratingInitializer, factor, meanRating, initScheme, 136861);
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor, float meanRating, @Nonnull RankInitScheme initScheme, int expectedSize) {
this.initScheme = initScheme;
this.randU = newRandoms(factor, 31L);
this.randI = newRandoms(factor, 41L);
}
public enum RankInitScheme {
@Nonnegative
private float maxInitValue;
@Nonnegative
private double initStdDev;
@Nonnull
public static RankInitScheme resolve(@Nullable String opt) {
if(opt == null) {
return gaussian;
} else if("gaussian".equalsIgnoreCase(opt)) {
return gaussian;
} else if("random".equalsIgnoreCase(opt)) {
return random;
}
return gaussian;
}
public void setMaxInitValue(float maxInitValue) {
this.maxInitValue = maxInitValue;
}
public void setInitStdDev(double initStdDev) {
this.initStdDev = initStdDev;
}
}
private static Random[] newRandoms(@Nonnull final int size, final long seed) {
final Random[] rand = new Random[size];
}
return rand;
switch(initScheme) {
case random:
uniformFill(v, randU[0], initScheme.maxInitValue, ratingInitializer);
break;
case gaussian:
gaussianFill(v, randU, initScheme.initStdDev, ratingInitializer);
break;
default:
throw new IllegalStateException("Unsupported rank initialization scheme: "
initScheme);
switch(initScheme) {
case random:
uniformFill(v, randI[0], initScheme.maxInitValue, ratingInitializer);
break;
case gaussian:
gaussianFill(v, randI, initScheme.initStdDev, ratingInitializer);
break;
default:
throw new IllegalStateException("Unsupported rank initialization scheme: "
initScheme);
private static void gaussianFill(final Rating[] a, final Random[] rand, final double stddev, final RatingInitilizer init) {
float v = (float) MathUtils.gaussian(0.d, stddev, rand[i]);
import hivemall.io.FactorizedModel.RankInitScheme;
protected RankInitScheme rankInit;
opts.addOption("rankinit", true, "Initialization strategy of rank matrix [default: guassian, random]");
String rankInitOpt = null;
float maxInitValue = 1.f;
double initStdDev = 0.1d;
rankInitOpt = cl.getOptionValue("rankinit");
maxInitValue = Primitives.parseFloat(cl.getOptionValue("max_init_value"), 1.f);
initStdDev = Primitives.parseDouble(cl.getOptionValue("min_init_stddev"), 0.1d);
this.rankInit = RankInitScheme.resolve(rankInitOpt);
rankInit.setMaxInitValue(maxInitValue);
initStdDev = Math.max(initStdDev, 1.0d / factor);
rankInit.setInitStdDev(initStdDev);
this.model = new FactorizedModel(this, factor, meanRating, rankInit);
onUpdate(user, item, users, items, err);
protected void onUpdate(final int user, final int item, final Rating[] users, final Rating[] items, final double err) {}
return random;
return random;
double Gu = err - lambda * Bu;
updateRating(ratingBu, Bu, Gu);
double Gi = err - lambda * Bi;
updateRating(ratingBi, Bi, Gi);
protected double errors;
this.lambda = 0.03f;
opts.addOption("r", "lambda", true, "The regularization factor [default: 0.03]");
this.lambda = Primitives.parseFloat(cl.getOptionValue("lambda"), 0.03f);
this.errors = 0.d;
final double err = rating - predict(user, item, userProbe, itemProbe);
protected float eta() {
}
double Gu = err - lambda * Bu;
double Gi = err - lambda * Bi;
return random;
return random;
double Gu = err - lambda * Bu;
updateRating(ratingBu, Bu, Gu);
double Gi = err - lambda * Bi;
updateRating(ratingBi, Bi, Gi);
protected double errors;
this.lambda = 0.03f;
opts.addOption("r", "lambda", true, "The regularization factor [default: 0.03]");
this.lambda = Primitives.parseFloat(cl.getOptionValue("lambda"), 0.03f);
this.errors = 0.d;
final double err = rating - predict(user, item, userProbe, itemProbe);
protected float eta() {
}
double Gu = err - lambda * Bu;
double Gi = err - lambda * Bi;
public abstract float eta(long t);
public float eta(long t) {
private final double eta0;
private final float finalEta;
private final double total_steps;
public SimpleEtaEstimator(double eta0, long total_steps) {
this.finalEta = (float) (eta0 / 2.d);
public float eta(final long t) {
return finalEta;
private final double eta0;
public InvscalingEtaEstimator(double eta0, double power_t) {
public float eta(final long t) {
return (float) (eta0 / Math.pow(t, power_t));
double eta0 = Double.parseDouble(cl.getOptionValue("eta0", "0.2"));
long t = Long.parseLong(cl.getOptionValue("t"));
double power_t = Double.parseDouble(cl.getOptionValue("power_t", "0.1"));
import hivemall.utils.io.FileUtils;
import hivemall.utils.io.NioFixedSegment;
import hivemall.utils.io.Segments;
import hivemall.utils.lang.NumberUtils;
import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import javax.annotation.Nonnegative;
import org.apache.hadoop.hive.ql.exec.MapredContext;
protected MapredContext mapredContext;
protected int iterations;
protected Segments fileIO;
protected ByteBuffer inputBuf;
private long lastWritePos;
protected long count;
this.mapredContext = null;
this.iterations = 1;
opts.addOption("update_mean", "update_mu", false, "Whether update (and return) the mean rating or not");
opts.addOption("iter", "iterations", true, "The number of iterations [default: 1]");
this.iterations = Primitives.parseInt(cl.getOptionValue("iterations"), 1);
if(iterations < 1) {
throw new UDFArgumentException("'-iterations' must be greater than or equals to 1: "
iterations);
}
public void configure(MapredContext mapredContext) {
this.mapredContext = mapredContext;
}
@Override
this.count = 0L;
this.lastWritePos = 0L;
if(mapredContext != null && iterations > 1) {
final File file;
try {
file = File.createTempFile("hivemall_mf", ".sgmt");
file.deleteOnExit();
if(!file.canWrite()) {
throw new UDFArgumentException("Cannot write a temporary file: "
file.getAbsolutePath());
}
} catch (IOException ioe) {
throw new UDFArgumentException(ioe);
} catch (Throwable e) {
throw new UDFArgumentException(e);
}
this.fileIO = new NioFixedSegment(file, RECORD_BYTES, false);
}
beforeTrain(count, user, item, rating);
protected void train(final int user, final int item, final double rating) throws HiveException {
private void beforeTrain(final long rowNum, final int user, final int item, final double rating)
throws HiveException {
if(inputBuf != null) {
assert (fileIO != null);
final ByteBuffer buf = inputBuf;
int remain = buf.remaining();
if(remain < RECORD_BYTES) {
writeBuffer(buf, fileIO, lastWritePos);
this.lastWritePos = rowNum;
}
buf.putInt(user);
buf.putInt(item);
buf.putDouble(rating);
}
}
private static void writeBuffer(@Nonnull final ByteBuffer srcBuf, @Nonnull final Segments dst, final long lastWritePos)
throws HiveException {
srcBuf.flip();
try {
dst.write(lastWritePos, srcBuf);
} catch (IOException e) {
}
srcBuf.clear();
}
protected void onUpdate(final int user, final int item, final Rating[] users, final Rating[] items, final double err)
throws HiveException {}
if(iterations > 1) {
runIterativeTraining(iterations);
}
protected final void runIterativeTraining(@Nonnegative final int iterations)
throws HiveException {
assert (inputBuf != null);
assert (fileIO != null);
try {
if(inputBuf.position() == 0) {
}
inputBuf.flip();
while(inputBuf.remaining() > 0) {
int user = inputBuf.getInt();
int item = inputBuf.getInt();
double rating = inputBuf.getDouble();
count;
train(user, item, rating);
}
inputBuf.rewind();
}
if(inputBuf.position() > 0) {
writeBuffer(inputBuf, fileIO, lastWritePos);
} else if(lastWritePos == 0) {
}
try {
fileIO.flush();
} catch (IOException e) {
throw new HiveException("Failed to flush a file: "
fileIO.getFile().getAbsolutePath(), e);
}
final long numTrainingExamples = count;
if(logger.isInfoEnabled()) {
File tmpFile = fileIO.getFile();
" records to a temporary file for iterative training: "
")");
}
inputBuf.clear();
long seekPos = 0L;
while(true) {
final int bytesRead;
try {
bytesRead = fileIO.directRead(seekPos, inputBuf);
} catch (IOException e) {
throw new HiveException("Failed to read a file: "
fileIO.getFile().getAbsolutePath(), e);
}
break;
}
assert (bytesRead > 0) : bytesRead;
inputBuf.flip();
int remain = inputBuf.remaining();
assert (remain > 0) : remain;
for(; remain >= RECORD_BYTES; remain -= RECORD_BYTES) {
int user = inputBuf.getInt();
int item = inputBuf.getInt();
double rating = inputBuf.getDouble();
count;
train(user, item, rating);
}
inputBuf.compact();
}
}
NumberUtils.formatNumber(numTrainingExamples)
" training updates in total)");
}
} finally {
try {
fileIO.close(true);
} catch (IOException e) {
throw new HiveException("Failed to close a file: "
fileIO.getFile().getAbsolutePath(), e);
}
this.inputBuf = null;
this.fileIO = null;
}
}
import java.text.DecimalFormat;
public static String formatNumber(final long number) {
DecimalFormat f = new DecimalFormat("#,###");
return f.format(number);
}
public static String prettySize(long size) {
if(size < 0) {
return "N/A";
} else {
if(size < 1024) {
} else {
float kb = size / 1024f;
if(kb < 1024f) {
return String.format("%.1f KiB", kb);
} else {
float mb = kb / 1024f;
if(mb < 1024f) {
return String.format("%.1f MiB", mb);
} else {
float gb = mb / 1024f;
return String.format("%.2f GiB", gb);
}
}
}
}
}
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.JobConf;
@Nullable
@Nonnull
public static MapredContext create(boolean isMap, @Nullable JobConf jobConf) {
return MapredContext.init(isMap, jobConf);
}
}
public abstract float eta(long t);
public float eta(long t) {
private final double eta0;
private final float finalEta;
private final double total_steps;
public SimpleEtaEstimator(double eta0, long total_steps) {
this.finalEta = (float) (eta0 / 2.d);
public float eta(final long t) {
return finalEta;
private final double eta0;
public InvscalingEtaEstimator(double eta0, double power_t) {
public float eta(final long t) {
return (float) (eta0 / Math.pow(t, power_t));
double eta0 = Double.parseDouble(cl.getOptionValue("eta0", "0.2"));
long t = Long.parseLong(cl.getOptionValue("t"));
double power_t = Double.parseDouble(cl.getOptionValue("power_t", "0.1"));
import hivemall.utils.io.FileUtils;
import hivemall.utils.io.NioFixedSegment;
import hivemall.utils.io.Segments;
import hivemall.utils.lang.NumberUtils;
import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import javax.annotation.Nonnegative;
import org.apache.hadoop.hive.ql.exec.MapredContext;
protected MapredContext mapredContext;
protected int iterations;
protected Segments fileIO;
protected ByteBuffer inputBuf;
private long lastWritePos;
protected long count;
this.mapredContext = null;
this.iterations = 1;
opts.addOption("update_mean", "update_mu", false, "Whether update (and return) the mean rating or not");
opts.addOption("iter", "iterations", true, "The number of iterations [default: 1]");
this.iterations = Primitives.parseInt(cl.getOptionValue("iterations"), 1);
if(iterations < 1) {
throw new UDFArgumentException("'-iterations' must be greater than or equals to 1: "
iterations);
}
public void configure(MapredContext mapredContext) {
this.mapredContext = mapredContext;
}
@Override
this.count = 0L;
this.lastWritePos = 0L;
if(mapredContext != null && iterations > 1) {
final File file;
try {
file = File.createTempFile("hivemall_mf", ".sgmt");
file.deleteOnExit();
if(!file.canWrite()) {
throw new UDFArgumentException("Cannot write a temporary file: "
file.getAbsolutePath());
}
} catch (IOException ioe) {
throw new UDFArgumentException(ioe);
} catch (Throwable e) {
throw new UDFArgumentException(e);
}
this.fileIO = new NioFixedSegment(file, RECORD_BYTES, false);
}
beforeTrain(count, user, item, rating);
protected void train(final int user, final int item, final double rating) throws HiveException {
private void beforeTrain(final long rowNum, final int user, final int item, final double rating)
throws HiveException {
if(inputBuf != null) {
assert (fileIO != null);
final ByteBuffer buf = inputBuf;
int remain = buf.remaining();
if(remain < RECORD_BYTES) {
writeBuffer(buf, fileIO, lastWritePos);
this.lastWritePos = rowNum;
}
buf.putInt(user);
buf.putInt(item);
buf.putDouble(rating);
}
}
private static void writeBuffer(@Nonnull final ByteBuffer srcBuf, @Nonnull final Segments dst, final long lastWritePos)
throws HiveException {
srcBuf.flip();
try {
dst.write(lastWritePos, srcBuf);
} catch (IOException e) {
}
srcBuf.clear();
}
protected void onUpdate(final int user, final int item, final Rating[] users, final Rating[] items, final double err)
throws HiveException {}
if(iterations > 1) {
runIterativeTraining(iterations);
}
protected final void runIterativeTraining(@Nonnegative final int iterations)
throws HiveException {
assert (inputBuf != null);
assert (fileIO != null);
try {
if(inputBuf.position() == 0) {
}
inputBuf.flip();
while(inputBuf.remaining() > 0) {
int user = inputBuf.getInt();
int item = inputBuf.getInt();
double rating = inputBuf.getDouble();
count;
train(user, item, rating);
}
inputBuf.rewind();
}
if(inputBuf.position() > 0) {
writeBuffer(inputBuf, fileIO, lastWritePos);
} else if(lastWritePos == 0) {
}
try {
fileIO.flush();
} catch (IOException e) {
throw new HiveException("Failed to flush a file: "
fileIO.getFile().getAbsolutePath(), e);
}
final long numTrainingExamples = count;
if(logger.isInfoEnabled()) {
File tmpFile = fileIO.getFile();
" records to a temporary file for iterative training: "
")");
}
inputBuf.clear();
long seekPos = 0L;
while(true) {
final int bytesRead;
try {
bytesRead = fileIO.directRead(seekPos, inputBuf);
} catch (IOException e) {
throw new HiveException("Failed to read a file: "
fileIO.getFile().getAbsolutePath(), e);
}
break;
}
assert (bytesRead > 0) : bytesRead;
inputBuf.flip();
int remain = inputBuf.remaining();
assert (remain > 0) : remain;
for(; remain >= RECORD_BYTES; remain -= RECORD_BYTES) {
int user = inputBuf.getInt();
int item = inputBuf.getInt();
double rating = inputBuf.getDouble();
count;
train(user, item, rating);
}
inputBuf.compact();
}
}
NumberUtils.formatNumber(numTrainingExamples)
" training updates in total)");
}
} finally {
try {
fileIO.close(true);
} catch (IOException e) {
throw new HiveException("Failed to close a file: "
fileIO.getFile().getAbsolutePath(), e);
}
this.inputBuf = null;
this.fileIO = null;
}
}
import java.text.DecimalFormat;
public static String formatNumber(final long number) {
DecimalFormat f = new DecimalFormat("#,###");
return f.format(number);
}
public static String prettySize(long size) {
if(size < 0) {
return "N/A";
} else {
if(size < 1024) {
} else {
float kb = size / 1024f;
if(kb < 1024f) {
return String.format("%.1f KiB", kb);
} else {
float mb = kb / 1024f;
if(mb < 1024f) {
return String.format("%.1f MiB", mb);
} else {
float gb = mb / 1024f;
return String.format("%.2f GiB", gb);
}
}
}
}
}
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.JobConf;
@Nullable
@Nonnull
public static MapredContext create(boolean isMap, @Nullable JobConf jobConf) {
return MapredContext.init(isMap, jobConf);
}
return new InvscalingEtaEstimator(0.1f, 0.1f);
double eta0 = Double.parseDouble(cl.getOptionValue("eta0", "0.1"));
protected boolean conversionCheck;
protected double convergenceRate;
protected RankInitScheme rankInit;
protected FactorizedModel model;
protected long count;
protected double totalErrors;
protected double currLosses, prevLosses;
protected IntObjectInspector userOI;
protected IntObjectInspector itemOI;
protected PrimitiveObjectInspector ratingOI;
protected MapredContext mapredContext;
this.conversionCheck = true;
this.convergenceRate = 0.01d;
opts.addOption("disable_cv", "disable_cvtest", false, "Whether to disable convergence check [default: enabled]");
opts.addOption("cv_rate", "convergence_rate", true, "Threshold to determine convergence [default: 0.01]");
this.conversionCheck = !cl.hasOption("disable_cvtest");
this.convergenceRate = Primitives.parseDouble(cl.getOptionValue("cv_rate"), 0.01d);
public final void configure(MapredContext mapredContext) {
this.totalErrors = 0.d;
this.currLosses = 0.d;
this.prevLosses = Double.POSITIVE_INFINITY;
public final void process(Object[] args) throws HiveException {
protected void beforeTrain(final long rowNum, final int user, final int item, final double rating)
protected static void writeBuffer(@Nonnull final ByteBuffer srcBuf, @Nonnull final Segments dst, final long lastWritePos)
throws HiveException {
srcBuf.flip();
try {
dst.write(lastWritePos, srcBuf);
} catch (IOException e) {
}
srcBuf.clear();
}
final ByteBuffer inputBuf = this.inputBuf;
final Segments fileIO = this.fileIO;
this.currLosses /= 2.0d;
break;
}
private boolean isConverged(final int iter) {
assert conversionCheck;
return false;
}
final double changeRate = (prevLosses - currLosses) / prevLosses;
if(changeRate < convergenceRate) {
return true;
} else {
if(logger.isDebugEnabled()) {
}
}
this.prevLosses = currLosses;
this.currLosses = 0.d;
return false;
}
return new InvscalingEtaEstimator(0.1f, 0.1f);
double eta0 = Double.parseDouble(cl.getOptionValue("eta0", "0.1"));
protected boolean conversionCheck;
protected double convergenceRate;
protected RankInitScheme rankInit;
protected FactorizedModel model;
protected long count;
protected double totalErrors;
protected double currLosses, prevLosses;
protected IntObjectInspector userOI;
protected IntObjectInspector itemOI;
protected PrimitiveObjectInspector ratingOI;
protected MapredContext mapredContext;
this.conversionCheck = true;
this.convergenceRate = 0.01d;
opts.addOption("disable_cv", "disable_cvtest", false, "Whether to disable convergence check [default: enabled]");
opts.addOption("cv_rate", "convergence_rate", true, "Threshold to determine convergence [default: 0.01]");
this.conversionCheck = !cl.hasOption("disable_cvtest");
this.convergenceRate = Primitives.parseDouble(cl.getOptionValue("cv_rate"), 0.01d);
public final void configure(MapredContext mapredContext) {
this.totalErrors = 0.d;
this.currLosses = 0.d;
this.prevLosses = Double.POSITIVE_INFINITY;
public final void process(Object[] args) throws HiveException {
protected void beforeTrain(final long rowNum, final int user, final int item, final double rating)
protected static void writeBuffer(@Nonnull final ByteBuffer srcBuf, @Nonnull final Segments dst, final long lastWritePos)
throws HiveException {
srcBuf.flip();
try {
dst.write(lastWritePos, srcBuf);
} catch (IOException e) {
}
srcBuf.clear();
}
final ByteBuffer inputBuf = this.inputBuf;
final Segments fileIO = this.fileIO;
this.currLosses /= 2.0d;
break;
}
private boolean isConverged(final int iter) {
assert conversionCheck;
return false;
}
final double changeRate = (prevLosses - currLosses) / prevLosses;
if(changeRate < convergenceRate) {
return true;
} else {
if(logger.isDebugEnabled()) {
}
}
this.prevLosses = currLosses;
this.currLosses = 0.d;
return false;
}
import hivemall.mix.MixedWeight;
import hivemall.mix.MixedWeight.WeightWithCovar;
import hivemall.mix.MixedWeight.WeightWithDelta;
import hivemall.utils.collections.IntOpenHashMap;
import hivemall.utils.collections.OpenHashMap;
private IntOpenHashMap<MixedWeight> mixedRequests_i;
private OpenHashMap<Object, MixedWeight> mixedRequests_o;
protected abstract boolean isDenseModel();
if(isDenseModel()) {
this.mixedRequests_i = new IntOpenHashMap<MixedWeight>(16384);
} else {
this.mixedRequests_o = new OpenHashMap<Object, MixedWeight>(16384);
}
protected final void onUpdate(final int feature, final float weight, final float covar, final short clock, final int deltaUpdates, final boolean hasCovar) {
final boolean requestSent;
requestSent = handler.onUpdate(feature, weight, covar, clock, deltaUpdates);
if(requestSent) {
if(hasCovar) {
MixedWeight prevMixed = mixedRequests_i.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithCovar(weight, covar);
mixedRequests_i.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setCovar(covar);
}
} else {
MixedWeight prevMixed = mixedRequests_i.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithDelta(weight, deltaUpdates);
mixedRequests_i.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setDeltaUpdates(deltaUpdates);
}
}
final boolean requestSent;
requestSent = handler.onUpdate(feature, weight, covar, clock, deltaUpdates);
if(requestSent) {
MixedWeight prevMixed = mixedRequests_o.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithCovar(weight, covar);
mixedRequests_o.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setCovar(covar);
}
value.setDeltaUpdates(BYTE0);
}
requestSent = handler.onUpdate(feature, weight, 1.f, clock, deltaUpdates);
if(requestSent) {
MixedWeight prevMixed = mixedRequests_o.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithDelta(weight, deltaUpdates);
mixedRequests_o.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setDeltaUpdates(deltaUpdates);
}
value.setDeltaUpdates(BYTE0);
}
protected boolean isDenseModel() {
return true;
}
@Override
boolean hasCovar = value.hasCovariance();
if(hasCovar) {
onUpdate(i, weight, covar, clock, delta, hasCovar);
int i = ((Integer) feature).intValue();
int i = ((Integer) feature).intValue();
import hivemall.mix.MixedWeight;
import javax.annotation.Nonnull;
boolean onUpdate(@Nonnull Object feature, float weight, float covar, short clock, int deltaUpdates)
void sendCancelRequest(@Nonnull Object feature, @Nonnull MixedWeight mixed) throws Exception;
protected boolean isDenseModel() {
return true;
}
@Override
boolean hasCovar = value.hasCovariance();
if(hasCovar) {
onUpdate(i, weight, covar, clock, delta, hasCovar);
int i = ((Integer) feature).intValue();
int i = ((Integer) feature).intValue();
protected boolean isDenseModel() {
return false;
}
@Override
private boolean cancelRequest;
this(event, feature, weight, 0.f, clock, deltaUpdates, false);
this(event, feature, weight, covariance, clock, deltaUpdates, false);
}
public MixMessage(MixEventName event, Object feature, float weight, float covariance, int deltaUpdates, boolean cancelRequest) {
}
MixMessage(MixEventName event, Object feature, float weight, float covariance, short clock, int deltaUpdates, boolean cancelRequest) {
this.cancelRequest = cancelRequest;
public boolean isCancelRequest() {
return cancelRequest;
}
out.writeBoolean(cancelRequest);
this.cancelRequest = in.readBoolean();
import hivemall.mix.MixedWeight;
@Override
public void sendCancelRequest(@Nonnull Object feature, @Nonnull MixedWeight mixed)
throws Exception {
if(!initialized) {
throw new IllegalStateException("Initilize() is not called");
}
float weight = mixed.getWeight();
float covar = mixed.getCovar();
int deltaUpdates = mixed.getDeltaUpdates();
MixMessage msg = new MixMessage(event, feature, weight, covar, deltaUpdates, true);
assert (groupID != null);
msg.setGroupID(groupID);
NodeInfo server = router.selectNode(msg);
Channel ch = channelMap.get(server);
SocketAddress remoteAddr = server.getSocketAddress();
ch.connect(remoteAddr).sync();
}
}
final MixEventName event = requestMsg.getEvent();
final Object feature = requestMsg.getFeature();
final float weight = requestMsg.getWeight();
final float covar = requestMsg.getCovariance();
final short clock = requestMsg.getClock();
final int deltaUpdates = requestMsg.getDeltaUpdates();
final boolean cancelRequest = requestMsg.isCancelRequest();
if(cancelRequest) {
partial.subtract(weight, covar, deltaUpdates, scale);
} else {
int diffClock = partial.diffClock(clock);
partial.add(weight, covar, clock, deltaUpdates, scale);
float averagedWeight = partial.getWeight(scale);
float meanCovar = partial.getCovariance(scale);
short totalClock = partial.getClock();
}
import javax.annotation.Nonnegative;
public void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale) {
this.sum_mean_div_covar -= (localWeight / covar) / scale;
this.sum_inv_covar -= (1.f / covar) / scale;
}
@Override
private void addWeight(float localWeight, int deltaUpdates, float scale) {
public void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
scaledSumWeights -= ((localWeight / scale) * deltaUpdates);
assert (totalUpdates > 0) : totalUpdates;
}
@Override
public abstract void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale);
protected boolean mixCancel;
opts.addOption("mix_cancel", "enable_mix_canceling", false, "Enable mix cancel requests");
boolean mixCancel = false;
mixCancel = cl.hasOption("mix_cancel");
this.mixCancel = mixCancel;
model.configureMix(client, mixCancel);
import hivemall.utils.collections.OpenHashTable;
private boolean cancelMixRequest;
private OpenHashTable<Object, MixedWeight> mixedRequests_o;
this.cancelMixRequest = false;
public void configureMix(ModelUpdateHandler handler, boolean cancelMixRequest) {
this.cancelMixRequest = cancelMixRequest;
if(cancelMixRequest) {
if(isDenseModel()) {
this.mixedRequests_i = new IntOpenHashMap<MixedWeight>(327680);
} else {
this.mixedRequests_o = new OpenHashTable<Object, MixedWeight>(327680);
}
if(cancelMixRequest) {
if(hasCovar) {
MixedWeight prevMixed = mixedRequests_i.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithCovar(weight, covar);
mixedRequests_i.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setCovar(covar);
MixedWeight prevMixed = mixedRequests_i.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithDelta(weight, deltaUpdates);
mixedRequests_i.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setDeltaUpdates(deltaUpdates);
if(cancelMixRequest) {
MixedWeight prevMixed = mixedRequests_o.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithCovar(weight, covar);
mixedRequests_o.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setCovar(covar);
if(cancelMixRequest) {
MixedWeight prevMixed = mixedRequests_o.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithDelta(weight, deltaUpdates);
mixedRequests_o.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setDeltaUpdates(deltaUpdates);
void configureMix(ModelUpdateHandler handler, boolean cancelMixRequest);
public void configureMix(ModelUpdateHandler handler, boolean cancelMixRequest) {
model.configureMix(handler, cancelMixRequest);
public final class OpenHashMap<K, V> implements Map<K, V>, Externalizable {
import hivemall.utils.collections.OpenHashMap;
private OpenHashMap<Object, MixedWeight> mixedRequests_o;
this.mixedRequests_o = new OpenHashMap<Object, MixedWeight>(327680);
protected boolean mixCancel;
opts.addOption("mix_cancel", "enable_mix_canceling", false, "Enable mix cancel requests");
boolean mixCancel = false;
mixCancel = cl.hasOption("mix_cancel");
this.mixCancel = mixCancel;
model.configureMix(client, mixCancel);
import hivemall.mix.MixedWeight;
import hivemall.mix.MixedWeight.WeightWithCovar;
import hivemall.mix.MixedWeight.WeightWithDelta;
import hivemall.utils.collections.IntOpenHashMap;
import hivemall.utils.collections.OpenHashMap;
private boolean cancelMixRequest;
private IntOpenHashMap<MixedWeight> mixedRequests_i;
private OpenHashMap<Object, MixedWeight> mixedRequests_o;
this.cancelMixRequest = false;
protected abstract boolean isDenseModel();
public void configureMix(ModelUpdateHandler handler, boolean cancelMixRequest) {
this.cancelMixRequest = cancelMixRequest;
if(cancelMixRequest) {
if(isDenseModel()) {
this.mixedRequests_i = new IntOpenHashMap<MixedWeight>(327680);
} else {
this.mixedRequests_o = new OpenHashMap<Object, MixedWeight>(327680);
}
}
protected final void onUpdate(final int feature, final float weight, final float covar, final short clock, final int deltaUpdates, final boolean hasCovar) {
final boolean requestSent;
requestSent = handler.onUpdate(feature, weight, covar, clock, deltaUpdates);
if(requestSent) {
if(cancelMixRequest) {
if(hasCovar) {
MixedWeight prevMixed = mixedRequests_i.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithCovar(weight, covar);
mixedRequests_i.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setCovar(covar);
}
} else {
MixedWeight prevMixed = mixedRequests_i.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithDelta(weight, deltaUpdates);
mixedRequests_i.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setDeltaUpdates(deltaUpdates);
}
}
}
final boolean requestSent;
requestSent = handler.onUpdate(feature, weight, covar, clock, deltaUpdates);
if(requestSent) {
if(cancelMixRequest) {
MixedWeight prevMixed = mixedRequests_o.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithCovar(weight, covar);
mixedRequests_o.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setCovar(covar);
}
}
value.setDeltaUpdates(BYTE0);
}
requestSent = handler.onUpdate(feature, weight, 1.f, clock, deltaUpdates);
if(requestSent) {
if(cancelMixRequest) {
MixedWeight prevMixed = mixedRequests_o.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithDelta(weight, deltaUpdates);
mixedRequests_o.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setDeltaUpdates(deltaUpdates);
}
}
value.setDeltaUpdates(BYTE0);
}
protected boolean isDenseModel() {
return true;
}
@Override
boolean hasCovar = value.hasCovariance();
if(hasCovar) {
onUpdate(i, weight, covar, clock, delta, hasCovar);
int i = ((Integer) feature).intValue();
int i = ((Integer) feature).intValue();
import hivemall.mix.MixedWeight;
import javax.annotation.Nonnull;
boolean onUpdate(@Nonnull Object feature, float weight, float covar, short clock, int deltaUpdates)
void sendCancelRequest(@Nonnull Object feature, @Nonnull MixedWeight mixed) throws Exception;
void configureMix(ModelUpdateHandler handler, boolean cancelMixRequest);
protected boolean isDenseModel() {
return true;
}
@Override
boolean hasCovar = value.hasCovariance();
if(hasCovar) {
onUpdate(i, weight, covar, clock, delta, hasCovar);
int i = ((Integer) feature).intValue();
int i = ((Integer) feature).intValue();
protected boolean isDenseModel() {
return false;
}
@Override
public void configureMix(ModelUpdateHandler handler, boolean cancelMixRequest) {
model.configureMix(handler, cancelMixRequest);
private boolean cancelRequest;
this(event, feature, weight, 0.f, clock, deltaUpdates, false);
this(event, feature, weight, covariance, clock, deltaUpdates, false);
}
public MixMessage(MixEventName event, Object feature, float weight, float covariance, int deltaUpdates, boolean cancelRequest) {
}
MixMessage(MixEventName event, Object feature, float weight, float covariance, short clock, int deltaUpdates, boolean cancelRequest) {
this.cancelRequest = cancelRequest;
public boolean isCancelRequest() {
return cancelRequest;
}
out.writeBoolean(cancelRequest);
this.cancelRequest = in.readBoolean();
import hivemall.mix.MixedWeight;
@Override
public void sendCancelRequest(@Nonnull Object feature, @Nonnull MixedWeight mixed)
throws Exception {
if(!initialized) {
throw new IllegalStateException("Initilize() is not called");
}
float weight = mixed.getWeight();
float covar = mixed.getCovar();
int deltaUpdates = mixed.getDeltaUpdates();
MixMessage msg = new MixMessage(event, feature, weight, covar, deltaUpdates, true);
assert (groupID != null);
msg.setGroupID(groupID);
NodeInfo server = router.selectNode(msg);
Channel ch = channelMap.get(server);
SocketAddress remoteAddr = server.getSocketAddress();
ch.connect(remoteAddr).sync();
}
}
final MixEventName event = requestMsg.getEvent();
final Object feature = requestMsg.getFeature();
final float weight = requestMsg.getWeight();
final float covar = requestMsg.getCovariance();
final short clock = requestMsg.getClock();
final int deltaUpdates = requestMsg.getDeltaUpdates();
final boolean cancelRequest = requestMsg.isCancelRequest();
if(cancelRequest) {
partial.subtract(weight, covar, deltaUpdates, scale);
} else {
int diffClock = partial.diffClock(clock);
partial.add(weight, covar, clock, deltaUpdates, scale);
float averagedWeight = partial.getWeight(scale);
float meanCovar = partial.getCovariance(scale);
short totalClock = partial.getClock();
}
import javax.annotation.Nonnegative;
public void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale) {
this.sum_mean_div_covar -= (localWeight / covar) / scale;
this.sum_inv_covar -= (1.f / covar) / scale;
}
@Override
private void addWeight(float localWeight, int deltaUpdates, float scale) {
public void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
scaledSumWeights -= ((localWeight / scale) * deltaUpdates);
assert (totalUpdates > 0) : totalUpdates;
}
@Override
public abstract void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale);
public final class OpenHashMap<K, V> implements Map<K, V>, Externalizable {
final boolean requestSent;
final boolean requestSent;
boolean cancelRequest = frame.readBoolean();
MixMessage msg = new MixMessage(event, feature, weight, covariance, clock, deltaUpdates, cancelRequest);
boolean cancelRequest = msg.isCancelRequest();
out.writeBoolean(cancelRequest);
assert (initialized);
final boolean requestSent;
final boolean requestSent;
boolean cancelRequest = frame.readBoolean();
MixMessage msg = new MixMessage(event, feature, weight, covariance, clock, deltaUpdates, cancelRequest);
boolean cancelRequest = msg.isCancelRequest();
out.writeBoolean(cancelRequest);
assert (initialized);
protected boolean readyToFinishIterations;
this.convergenceRate = 0.005d;
this.readyToFinishIterations = false;
opts.addOption("cv_rate", "convergence_rate", true, "Threshold to determine convergence [default: 0.005]");
this.convergenceRate = Primitives.parseDouble(cl.getOptionValue("cv_rate"), 0.005d);
int i = 1;
if(currLosses > prevLosses) {
if(logger.isInfoEnabled()) {
}
this.prevLosses = currLosses;
this.currLosses = 0.d;
this.readyToFinishIterations = false;
if(readyToFinishIterations) {
"]");
return true;
} else {
this.readyToFinishIterations = true;
}
this.readyToFinishIterations = false;
protected boolean readyToFinishIterations;
this.convergenceRate = 0.005d;
this.readyToFinishIterations = false;
opts.addOption("cv_rate", "convergence_rate", true, "Threshold to determine convergence [default: 0.005]");
this.convergenceRate = Primitives.parseDouble(cl.getOptionValue("cv_rate"), 0.005d);
int i = 1;
if(currLosses > prevLosses) {
if(logger.isInfoEnabled()) {
}
this.prevLosses = currLosses;
this.currLosses = 0.d;
this.readyToFinishIterations = false;
if(readyToFinishIterations) {
"]");
return true;
} else {
this.readyToFinishIterations = true;
}
this.readyToFinishIterations = false;
protected boolean mixCancel;
opts.addOption("mix_cancel", "enable_mix_canceling", false, "Enable mix cancel requests");
boolean mixCancel = false;
mixCancel = cl.hasOption("mix_cancel");
this.mixCancel = mixCancel;
model.configureMix(client, mixCancel);
import javax.annotation.Nullable;
public abstract float eta(long t);
public static final class FixedEtaEstimator extends EtaEstimator {
private final float eta;
public FixedEtaEstimator(float eta) {
this.eta = eta;
}
@Override
public float eta(long t) {
return eta;
}
}
private final double eta0;
private final float finalEta;
private final double total_steps;
public SimpleEtaEstimator(double eta0, long total_steps) {
this.finalEta = (float) (eta0 / 2.d);
public float eta(final long t) {
return finalEta;
private final double eta0;
public InvscalingEtaEstimator(double eta0, double power_t) {
public float eta(final long t) {
return (float) (eta0 / Math.pow(t, power_t));
public static EtaEstimator get(@Nullable CommandLine cl) throws UDFArgumentException {
return new InvscalingEtaEstimator(0.1f, 0.1f);
String etaValue = cl.getOptionValue("eta");
if(etaValue != null) {
float eta = Float.parseFloat(etaValue);
return new FixedEtaEstimator(eta);
}
double eta0 = Double.parseDouble(cl.getOptionValue("eta0", "0.1"));
long t = Long.parseLong(cl.getOptionValue("t"));
double power_t = Double.parseDouble(cl.getOptionValue("power_t", "0.1"));
ArrayUtils.shuffle(slots[x], position, randoms[x]);
}
}
public final class ArgminKLDistanceUDAF extends UDAF {
public final class MaxRowUDAF extends AbstractGenericUDAFResolver {
public final class MaxValueLabelUDAF extends UDAF {
public final class VotedAvgUDAF extends UDAF {
public final class WeightVotedAvgUDAF extends UDAF {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "f1score", value = "_FUNC_(array[int], array[int]) - Return a F-measure/F1 score")
import hivemall.mix.MixedWeight;
import hivemall.mix.MixedWeight.WeightWithCovar;
import hivemall.mix.MixedWeight.WeightWithDelta;
import hivemall.utils.collections.IntOpenHashMap;
import hivemall.utils.collections.OpenHashMap;
private boolean cancelMixRequest;
private IntOpenHashMap<MixedWeight> mixedRequests_i;
private OpenHashMap<Object, MixedWeight> mixedRequests_o;
this.cancelMixRequest = false;
protected abstract boolean isDenseModel();
public void configureMix(ModelUpdateHandler handler, boolean cancelMixRequest) {
this.cancelMixRequest = cancelMixRequest;
if(cancelMixRequest) {
if(isDenseModel()) {
this.mixedRequests_i = new IntOpenHashMap<MixedWeight>(327680);
} else {
this.mixedRequests_o = new OpenHashMap<Object, MixedWeight>(327680);
}
}
protected final void onUpdate(final int feature, final float weight, final float covar, final short clock, final int deltaUpdates, final boolean hasCovar) {
if(deltaUpdates < 1) {
return;
}
final boolean requestSent;
requestSent = handler.onUpdate(feature, weight, covar, clock, deltaUpdates);
if(requestSent) {
if(cancelMixRequest) {
if(hasCovar) {
MixedWeight prevMixed = mixedRequests_i.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithCovar(weight, covar);
mixedRequests_i.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setCovar(covar);
}
} else {
MixedWeight prevMixed = mixedRequests_i.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithDelta(weight, deltaUpdates);
mixedRequests_i.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setDeltaUpdates(deltaUpdates);
}
}
}
final boolean requestSent;
requestSent = handler.onUpdate(feature, weight, covar, clock, deltaUpdates);
if(requestSent) {
if(cancelMixRequest) {
MixedWeight prevMixed = mixedRequests_o.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithCovar(weight, covar);
mixedRequests_o.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setCovar(covar);
}
}
value.setDeltaUpdates(BYTE0);
}
final boolean requestSent;
requestSent = handler.onUpdate(feature, weight, 1.f, clock, deltaUpdates);
if(requestSent) {
if(cancelMixRequest) {
MixedWeight prevMixed = mixedRequests_o.get(feature);
if(prevMixed == null) {
prevMixed = new WeightWithDelta(weight, deltaUpdates);
mixedRequests_o.put(feature, prevMixed);
} else {
try {
handler.sendCancelRequest(feature, prevMixed);
} catch (Exception e) {
throw new RuntimeException(e);
}
prevMixed.setWeight(weight);
prevMixed.setDeltaUpdates(deltaUpdates);
}
}
value.setDeltaUpdates(BYTE0);
}
protected boolean isDenseModel() {
return true;
}
@Override
boolean hasCovar = value.hasCovariance();
if(hasCovar) {
onUpdate(i, weight, covar, clock, delta, hasCovar);
int i = ((Integer) feature).intValue();
int i = ((Integer) feature).intValue();
import hivemall.mix.MixedWeight;
import javax.annotation.Nonnull;
boolean onUpdate(@Nonnull Object feature, float weight, float covar, short clock, int deltaUpdates)
void sendCancelRequest(@Nonnull Object feature, @Nonnull MixedWeight mixed) throws Exception;
void configureMix(ModelUpdateHandler handler, boolean cancelMixRequest);
protected boolean isDenseModel() {
return true;
}
@Override
boolean hasCovar = value.hasCovariance();
if(hasCovar) {
onUpdate(i, weight, covar, clock, delta, hasCovar);
int i = ((Integer) feature).intValue();
int i = ((Integer) feature).intValue();
protected boolean isDenseModel() {
return false;
}
@Override
public void configureMix(ModelUpdateHandler handler, boolean cancelMixRequest) {
model.configureMix(handler, cancelMixRequest);
public WeightValueWithClock(float value) {
this.value = value;
this.clock = 0;
this.deltaUpdates = 0;
}
public WeightValueParamsF1Clock(float value, float f1) {
super(value);
this.f1 = f1;
}
public WeightValueParamsF2Clock(float value, float f1, float f2) {
super(value);
this.f1 = f1;
this.f2 = f2;
}
public WeightValueWithCovarClock(float value, float covar) {
super(value);
this.covariance = covar;
}
private boolean cancelRequest;
this(event, feature, weight, 0.f, clock, deltaUpdates, false);
this(event, feature, weight, covariance, clock, deltaUpdates, false);
}
public MixMessage(MixEventName event, Object feature, float weight, float covariance, int deltaUpdates, boolean cancelRequest) {
}
MixMessage(MixEventName event, Object feature, float weight, float covariance, short clock, int deltaUpdates, boolean cancelRequest) {
this.cancelRequest = cancelRequest;
public boolean isCancelRequest() {
return cancelRequest;
}
out.writeBoolean(cancelRequest);
this.cancelRequest = in.readBoolean();
boolean cancelRequest = frame.readBoolean();
MixMessage msg = new MixMessage(event, feature, weight, covariance, clock, deltaUpdates, cancelRequest);
boolean cancelRequest = msg.isCancelRequest();
out.writeBoolean(cancelRequest);
import hivemall.mix.MixedWeight;
@Override
public void sendCancelRequest(@Nonnull Object feature, @Nonnull MixedWeight mixed)
throws Exception {
assert (initialized);
float weight = mixed.getWeight();
float covar = mixed.getCovar();
int deltaUpdates = mixed.getDeltaUpdates();
MixMessage msg = new MixMessage(event, feature, weight, covar, deltaUpdates, true);
assert (groupID != null);
msg.setGroupID(groupID);
NodeInfo server = router.selectNode(msg);
Channel ch = channelMap.get(server);
SocketAddress remoteAddr = server.getSocketAddress();
ch.connect(remoteAddr).sync();
}
}
final MixEventName event = requestMsg.getEvent();
final Object feature = requestMsg.getFeature();
final float weight = requestMsg.getWeight();
final float covar = requestMsg.getCovariance();
final short clock = requestMsg.getClock();
final int deltaUpdates = requestMsg.getDeltaUpdates();
final boolean cancelRequest = requestMsg.isCancelRequest();
if(cancelRequest) {
partial.subtract(weight, covar, deltaUpdates, scale);
} else {
int diffClock = partial.diffClock(clock);
partial.add(weight, covar, clock, deltaUpdates, scale);
float averagedWeight = partial.getWeight(scale);
float meanCovar = partial.getCovariance(scale);
short totalClock = partial.getClock();
}
import javax.annotation.Nonnegative;
public void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale) {
this.sum_mean_div_covar -= (localWeight / covar) / scale;
this.sum_inv_covar -= (1.f / covar) / scale;
}
@Override
private void addWeight(float localWeight, int deltaUpdates, float scale) {
public void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
scaledSumWeights -= ((localWeight / scale) * deltaUpdates);
assert (totalUpdates > 0) : totalUpdates;
}
@Override
public abstract void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale);
public final class OpenHashMap<K, V> implements Map<K, V>, Externalizable {
import javax.annotation.Nonnull;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
import org.apache.hadoop.io.FloatWritable;
public static IntObjectInspector asIntOI(@Nonnull ObjectInspector argOI)
throws UDFArgumentException {
if(!INT_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (IntObjectInspector) argOI;
}
public static PrimitiveObjectInspector asDoubleCompatibleOI(@Nonnull ObjectInspector argOI)
throws UDFArgumentTypeException {
if(argOI.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0, "Only primitive type arguments are accepted but "
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch(oi.getPrimitiveCategory()) {
case BYTE:
case SHORT:
case INT:
case LONG:
case FLOAT:
case DOUBLE:
case STRING:
case TIMESTAMP:
break;
default:
throw new UDFArgumentTypeException(0, "Only numeric or string type arguments are accepted but "
}
return oi;
}
@Nonnull
public static FloatWritable[] newFloatArray(final int size, final float defaultVal) {
final FloatWritable[] array = new FloatWritable[size];
array[i] = new FloatWritable(defaultVal);
}
return array;
}
public static Integer[] toObject(final int[] array) {
final Integer[] result = new Integer[array.length];
result[i] = array[i];
}
return result;
}
public static List<Integer> toList(final int[] array) {
Integer[] v = toObject(array);
return Arrays.asList(v);
}
public static Long[] toObject(final long[] array) {
final Long[] result = new Long[array.length];
result[i] = array[i];
}
return result;
}
public static List<Long> toList(final long[] array) {
Long[] v = toObject(array);
return Arrays.asList(v);
}
public static Double[] toObject(final double[] array) {
final Double[] result = new Double[array.length];
result[i] = array[i];
}
return result;
}
public static List<Double> toList(final double[] array) {
Double[] v = toObject(array);
return Arrays.asList(v);
}
public static void fill(final float[] a, final Random rand) {
a[i] = rand.nextFloat();
}
}
import java.text.DecimalFormat;
public static String formatNumber(final long number) {
DecimalFormat f = new DecimalFormat("#,###");
return f.format(number);
}
public static String prettySize(long size) {
if(size < 0) {
return "N/A";
} else {
if(size < 1024) {
} else {
float kb = size / 1024f;
if(kb < 1024f) {
return String.format("%.1f KiB", kb);
} else {
float mb = kb / 1024f;
if(mb < 1024f) {
return String.format("%.1f MiB", mb);
} else {
float gb = mb / 1024f;
return String.format("%.2f GiB", gb);
}
}
}
}
}
public static double parseDouble(final String s, final double defaultValue) {
if(s == null) {
return defaultValue;
}
return Double.parseDouble(s);
}
public void addValue(float o) {
}
public void addValue(int o) {
}
import java.util.Random;
import javax.annotation.Nonnull;
public static double gaussian(final double mean, final double stddev, @Nonnull final Random rnd) {
}
public static double lognormal(final double mean, final double stddev, @Nonnull final Random rnd) {
return Math.exp(gaussian(mean, stddev, rnd));
}
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.JobConf;
@Nullable
@Nonnull
public static MapredContext create(boolean isMap, @Nullable JobConf jobConf) {
return MapredContext.init(isMap, jobConf);
}
StructObjectInspector inputStructOI;
this.inputStructOI = inputStructOI;
otherObjects = inputStructOI.getStructFieldsDataAsList(partial);
import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;
import org.apache.hadoop.io.IntWritable;
if(!(argOIs[0] instanceof ConstantObjectInspector)) {
this.xtimes = ((IntWritable)((ConstantObjectInspector) argOIs[0]).getWritableConstantValue()).get();
import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;
import org.apache.hadoop.io.IntWritable;
if(!(argOIs[0] instanceof ConstantObjectInspector)) {
int xtimes = ((IntWritable)((ConstantObjectInspector) argOIs[0]).getWritableConstantValue()).get();
if(!(argOIs[1] instanceof ConstantObjectInspector)) {
int numBuffers = ((IntWritable)((ConstantObjectInspector) argOIs[1]).getWritableConstantValue()).get();
ObjectInspector retOI = ObjectInspectorUtils.getStandardObjectInspector(rawOI, ObjectInspectorCopyOption.JAVA);
public ArrayAvgUDAF() {}
import org.apache.hadoop.hive.serde2.io.ByteWritable;
import org.apache.hadoop.hive.serde2.io.ShortWritable;
ConstantObjectInspector stringOI = (ConstantObjectInspector) oi;
ConstantObjectInspector longOI = (ConstantObjectInspector) numberOI;
return ((LongWritable)longOI.getWritableConstantValue()).get();
ConstantObjectInspector intOI = (ConstantObjectInspector) numberOI;
return ((IntWritable) intOI.getWritableConstantValue()).get();
ConstantObjectInspector shortOI = (ConstantObjectInspector) numberOI;
return (long) ((ShortWritable)shortOI.getWritableConstantValue()).get();
ConstantObjectInspector byteOI = (ConstantObjectInspector) numberOI;
return (long) ((ByteWritable)byteOI.getWritableConstantValue()).get();
import org.apache.hadoop.io.FloatWritable;
public List<FloatWritable> terminate() {
final FloatWritable[] ary = new FloatWritable[size];
ary[i] = new FloatWritable(avg);
import javax.annotation.Nonnull;
@Nonnull
public static List<LongWritable> newLongList(int size) {
final LongWritable[] array = new LongWritable[size];
array[i] = new LongWritable(0L);
}
return Arrays.asList(array);
}
@Nonnull
public static List<DoubleWritable> newDoubleList(int size) {
final DoubleWritable[] array = new DoubleWritable[size];
array[i] = new DoubleWritable(0.d);
}
return Arrays.asList(array);
}
@Nonnull
public static List<LongWritable> toWritableList(@Nonnull final long[] src) {
final LongWritable[] writables = new LongWritable[src.length];
writables[i] = new LongWritable(src[i]);
}
return Arrays.asList(writables);
}
@Nonnull
public static List<DoubleWritable> toWritableList(@Nonnull final double[] src) {
final DoubleWritable[] writables = new DoubleWritable[src.length];
writables[i] = new DoubleWritable(src[i]);
}
return Arrays.asList(writables);
}
import org.apache.hadoop.io.FloatWritable;
public List<FloatWritable> terminate() {
final FloatWritable[] ary = new FloatWritable[size];
ary[i] = new FloatWritable(avg);
import javax.annotation.Nonnull;
@Nonnull
public static List<LongWritable> newLongList(int size) {
final LongWritable[] array = new LongWritable[size];
array[i] = new LongWritable(0L);
}
return Arrays.asList(array);
}
@Nonnull
public static List<DoubleWritable> newDoubleList(int size) {
final DoubleWritable[] array = new DoubleWritable[size];
array[i] = new DoubleWritable(0.d);
}
return Arrays.asList(array);
}
@Nonnull
public static List<LongWritable> toWritableList(@Nonnull final long[] src) {
final LongWritable[] writables = new LongWritable[src.length];
writables[i] = new LongWritable(src[i]);
}
return Arrays.asList(writables);
}
@Nonnull
public static List<DoubleWritable> toWritableList(@Nonnull final double[] src) {
final DoubleWritable[] writables = new DoubleWritable[src.length];
writables[i] = new DoubleWritable(src[i]);
}
return Arrays.asList(writables);
}
StructObjectInspector inputStructOI;
otherObjects = inputStructOI.getStructFieldsDataAsList(partial);
} else if(inputStructOI != null) {
} else {
import javax.annotation.Nullable;
import org.apache.hadoop.io.BooleanWritable;
import org.apache.hadoop.io.Writable;
public static int parseInt(@Nonnull final Object o) {
public static Text asText(@Nullable final Object o) {
public static int asJavaInt(@Nullable final Object o, final int nullValue) {
public static int asJavaInt(@Nullable final Object o) {
public static boolean isStringOI(@Nonnull final ObjectInspector oi) {
return STRING_TYPE_NAME.equals(typeName);
public static boolean isIntOI(@Nonnull final ObjectInspector oi) {
String typeName = oi.getTypeName();
return INT_TYPE_NAME.equals(typeName);
}
public static boolean isBigIntOI(@Nonnull final ObjectInspector oi) {
public static boolean isBooleanOI(@Nonnull final ObjectInspector oi) {
String typeName = oi.getTypeName();
return BOOLEAN_TYPE_NAME.equals(typeName);
}
@SuppressWarnings("unchecked")
@Nullable
public static <T extends Writable> T getConstValue(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
ConstantObjectInspector constOI = (ConstantObjectInspector) oi;
Object v = constOI.getWritableConstantValue();
return (T) v;
public static String getConstString(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
if(!isStringOI(oi)) {
throw new UDFArgumentException("argument must be a Text value: "
Text v = getConstValue(oi);
return v == null ? null : v.toString();
public static boolean getConstBoolean(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
if(!isBooleanOI(oi)) {
throw new UDFArgumentException("argument must be a Boolean value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
BooleanWritable v = getConstValue(oi);
return v == null ? null : v.get();
}
public static int getConstInt(@Nonnull final ObjectInspector oi) throws UDFArgumentException {
if(!isIntOI(oi)) {
throw new UDFArgumentException("argument must be a Int value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
IntWritable v = getConstValue(oi);
return v == null ? null : v.get();
}
public static long getConstLong(@Nonnull final ObjectInspector oi) throws UDFArgumentException {
if(!isBigIntOI(oi)) {
throw new UDFArgumentException("argument must be a BigInt value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
LongWritable v = getConstValue(oi);
return v == null ? null : v.get();
}
public static long getAsConstLong(@Nonnull final ObjectInspector numberOI)
throws UDFArgumentException {
final String typeName = numberOI.getTypeName();
LongWritable v = getConstValue(numberOI);
return v.get();
IntWritable v = getConstValue(numberOI);
return v.get();
ShortWritable v = getConstValue(numberOI);
return v.get();
ByteWritable v = getConstValue(numberOI);
return v.get();
@Nonnull
public static ConstantObjectInspector asConstantObjectInspector(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
return (ConstantObjectInspector) oi;
public static PrimitiveObjectInspector asPrimitiveObjectInspector(@Nonnull final ObjectInspector oi)
public static IntObjectInspector asIntOI(@Nonnull final ObjectInspector argOI)
public static PrimitiveObjectInspector asDoubleCompatibleOI(@Nonnull final ObjectInspector argOI)
public static LazySimpleSerDe getKeyValueLineSerde(@Nonnull final PrimitiveObjectInspector keyOI, @Nonnull final PrimitiveObjectInspector valueOI)
throws SerDeException {
LazySimpleSerDe serde = new LazySimpleSerDe();
Configuration conf = new Configuration();
Properties tbl = new Properties();
tbl.setProperty("columns", "key,value");
serde.initialize(conf, tbl);
return serde;
}
public static LazySimpleSerDe getLineSerde(@Nonnull final PrimitiveObjectInspector... OIs)
throws SerDeException {
if(OIs.length == 0) {
throw new IllegalArgumentException("OIs must be specified");
}
LazySimpleSerDe serde = new LazySimpleSerDe();
Configuration conf = new Configuration();
Properties tbl = new Properties();
StringBuilder columnNames = new StringBuilder();
StringBuilder columnTypes = new StringBuilder();
columnTypes.append(OIs[i].getTypeName()).append(',');
}
columnNames.deleteCharAt(columnNames.length() - 1);
columnTypes.deleteCharAt(columnTypes.length() - 1);
tbl.setProperty("columns", columnNames.toString());
tbl.setProperty("columns.types", columnTypes.toString());
serde.initialize(conf, tbl);
return serde;
}
import hivemall.utils.hadoop.HiveUtils;
this.xtimes = HiveUtils.getConstInt(argOIs[0]);
import hivemall.utils.hadoop.HiveUtils;
int xtimes = HiveUtils.getConstInt(argOIs[0]);
int numBuffers = HiveUtils.getConstInt(argOIs[1]);
this.useBigInt = HiveUtils.isBigIntOI(argOIs[1]);
import hivemall.utils.hadoop.HiveUtils;
String rawArgs = HiveUtils.getConstString(argOIs[2]);
import org.apache.hadoop.io.FloatWritable;
public List<FloatWritable> terminate() {
final FloatWritable[] ary = new FloatWritable[size];
ary[i] = new FloatWritable(avg);
return v.get();
return v.get();
return v.get();
ObjectInspector retOI = ObjectInspectorUtils.getStandardObjectInspector(rawOI, ObjectInspectorCopyOption.DEFAULT);
row[i - 2] = ObjectInspectorUtils.copyToStandardObject(arg, argOI, ObjectInspectorCopyOption.DEFAULT);
StructObjectInspector inputStructOI;
this.inputStructOI = inputStructOI;
} else if(inputStructOI != null) {
otherObjects = inputStructOI.getStructFieldsDataAsList(partial);
import hivemall.utils.hadoop.HiveUtils;
this.xtimes = HiveUtils.getConstInt(argOIs[0]);
import hivemall.utils.hadoop.HiveUtils;
int xtimes = HiveUtils.getConstInt(argOIs[0]);
int numBuffers = HiveUtils.getConstInt(argOIs[1]);
ObjectInspector retOI = ObjectInspectorUtils.getStandardObjectInspector(rawOI, ObjectInspectorCopyOption.DEFAULT);
row[i - 2] = ObjectInspectorUtils.copyToStandardObject(arg, argOI, ObjectInspectorCopyOption.DEFAULT);
import hivemall.utils.hadoop.HiveUtils;
String rawArgs = HiveUtils.getConstString(argOIs[2]);
this.useBigInt = HiveUtils.isBigIntOI(argOIs[1]);
public ArrayAvgUDAF() {}
import javax.annotation.Nullable;
import org.apache.hadoop.hive.serde2.io.ByteWritable;
import org.apache.hadoop.hive.serde2.io.ShortWritable;
import org.apache.hadoop.io.BooleanWritable;
import org.apache.hadoop.io.Writable;
public static int parseInt(@Nonnull final Object o) {
public static Text asText(@Nullable final Object o) {
public static int asJavaInt(@Nullable final Object o, final int nullValue) {
public static int asJavaInt(@Nullable final Object o) {
public static boolean isStringOI(@Nonnull final ObjectInspector oi) {
return STRING_TYPE_NAME.equals(typeName);
public static boolean isIntOI(@Nonnull final ObjectInspector oi) {
String typeName = oi.getTypeName();
return INT_TYPE_NAME.equals(typeName);
}
public static boolean isBigIntOI(@Nonnull final ObjectInspector oi) {
public static boolean isBooleanOI(@Nonnull final ObjectInspector oi) {
String typeName = oi.getTypeName();
return BOOLEAN_TYPE_NAME.equals(typeName);
}
@SuppressWarnings("unchecked")
@Nullable
public static <T extends Writable> T getConstValue(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
ConstantObjectInspector constOI = (ConstantObjectInspector) oi;
Object v = constOI.getWritableConstantValue();
return (T) v;
public static String getConstString(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
if(!isStringOI(oi)) {
throw new UDFArgumentException("argument must be a Text value: "
Text v = getConstValue(oi);
return v == null ? null : v.toString();
public static boolean getConstBoolean(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
if(!isBooleanOI(oi)) {
throw new UDFArgumentException("argument must be a Boolean value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
BooleanWritable v = getConstValue(oi);
return v.get();
}
public static int getConstInt(@Nonnull final ObjectInspector oi) throws UDFArgumentException {
if(!isIntOI(oi)) {
throw new UDFArgumentException("argument must be a Int value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
IntWritable v = getConstValue(oi);
return v.get();
}
public static long getConstLong(@Nonnull final ObjectInspector oi) throws UDFArgumentException {
if(!isBigIntOI(oi)) {
throw new UDFArgumentException("argument must be a BigInt value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
LongWritable v = getConstValue(oi);
return v.get();
}
public static long getAsConstLong(@Nonnull final ObjectInspector numberOI)
throws UDFArgumentException {
final String typeName = numberOI.getTypeName();
LongWritable v = getConstValue(numberOI);
return v.get();
IntWritable v = getConstValue(numberOI);
return v.get();
ShortWritable v = getConstValue(numberOI);
return v.get();
ByteWritable v = getConstValue(numberOI);
return v.get();
@Nonnull
public static ConstantObjectInspector asConstantObjectInspector(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
return (ConstantObjectInspector) oi;
public static PrimitiveObjectInspector asPrimitiveObjectInspector(@Nonnull final ObjectInspector oi)
public static IntObjectInspector asIntOI(@Nonnull final ObjectInspector argOI)
public static PrimitiveObjectInspector asDoubleCompatibleOI(@Nonnull final ObjectInspector argOI)
public static LazySimpleSerDe getKeyValueLineSerde(@Nonnull final PrimitiveObjectInspector keyOI, @Nonnull final PrimitiveObjectInspector valueOI)
throws SerDeException {
LazySimpleSerDe serde = new LazySimpleSerDe();
Configuration conf = new Configuration();
Properties tbl = new Properties();
tbl.setProperty("columns", "key,value");
serde.initialize(conf, tbl);
return serde;
}
public static LazySimpleSerDe getLineSerde(@Nonnull final PrimitiveObjectInspector... OIs)
throws SerDeException {
if(OIs.length == 0) {
throw new IllegalArgumentException("OIs must be specified");
}
LazySimpleSerDe serde = new LazySimpleSerDe();
Configuration conf = new Configuration();
Properties tbl = new Properties();
StringBuilder columnNames = new StringBuilder();
StringBuilder columnTypes = new StringBuilder();
columnTypes.append(OIs[i].getTypeName()).append(',');
}
columnNames.deleteCharAt(columnNames.length() - 1);
columnTypes.deleteCharAt(columnTypes.length() - 1);
tbl.setProperty("columns", columnNames.toString());
tbl.setProperty("columns.types", columnTypes.toString());
serde.initialize(conf, tbl);
return serde;
}
package hivemall.knn.lsh;
package hivemall.knn.lsh;
package hivemall.knn.lsh;
package hivemall.knn.lsh;
if(keyIdx < 0) {
sum_of_gradients[i] = value.getSumOfGradients();
if(keyIdx < 0) {
import java.util.Iterator;
import java.util.Set;
final Set<Map.Entry<String, SessionObject>> entries = sessions.entrySet();
final Iterator<Map.Entry<String, SessionObject>> itor = entries.iterator();
while(itor.hasNext()) {
Map.Entry<String, SessionObject> e = itor.next();
itor.remove();
if(logger.isInfoEnabled()) {
sessionObj.getSessionInfo());
float jaccard = countMatches / (float) k;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
protected PrimitiveObjectInspector targetOI;
this.targetOI = HiveUtils.asDoubleCompatibleOI(argOIs[1]);
float target = PrimitiveObjectInspectorUtils.getFloat(args[1], targetOI);
sum_of_gradients[i] = value.getSumOfGradients();
float jaccard = countMatches / (float) k;
import java.util.Iterator;
import java.util.Set;
final Set<Map.Entry<String, SessionObject>> entries = sessions.entrySet();
final Iterator<Map.Entry<String, SessionObject>> itor = entries.iterator();
while(itor.hasNext()) {
Map.Entry<String, SessionObject> e = itor.next();
itor.remove();
if(logger.isInfoEnabled()) {
sessionObj.getSessionInfo());
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
protected PrimitiveObjectInspector targetOI;
this.targetOI = HiveUtils.asDoubleCompatibleOI(argOIs[1]);
float target = PrimitiveObjectInspectorUtils.getFloat(args[1], targetOI);
if(keyIdx < 0) {
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, int label) {
protected void update(@Nonnull final FeatureValue[] features, final float y, final float alpha, final float beta) {
for(FeatureValue f : features) {
final Object k = f.getFeature();
final float v = f.getValue();
protected void train(@Nonnull final FeatureValue[] features, int label) {
protected void train(@Nonnull final FeatureValue[] features, final int label) {
protected void update(@Nonnull final FeatureValue[] features, final float y, final int t) {
Object x = f.getFeature();
float xi = f.getValue();
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
private ListObjectInspector featureListOI;
private IntObjectInspector labelOI;
private boolean parseFeature;
this.labelOI = HiveUtils.asIntOI(argOIs[1]);
FeatureValue[] featureVector = parseFeatures(features);
if(featureVector == null) {
int label = labelOI.get(args[1]);
train(featureVector, label);
}
@Nullable
protected final FeatureValue[] parseFeatures(@Nonnull final List<?> features) {
final int size = features.size();
if(size == 0) {
return null;
}
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final FeatureValue[] featureVector = new FeatureValue[size];
Object f = features.get(i);
if(f == null) {
continue;
}
final FeatureValue fv;
if(parseFeature) {
fv = FeatureValue.parse(f);
} else {
Object k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
fv = new FeatureValue(k, 1.f);
}
featureVector[i] = fv;
}
return featureVector;
void train(List<?> features, int label) {
FeatureValue[] featureVector = parseFeatures(features);
train(featureVector, label);
}
protected void train(@Nonnull final FeatureValue[] features, final int label) {
protected float predict(@Nonnull final FeatureValue[] features) {
float score = 0.f;
Object k = f.getFeature();
if(old_w != 0.f) {
float v = f.getValue();
@Nonnull
protected PredictionResult calcScoreAndNorm(@Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
@Nonnull
protected PredictionResult calcScoreAndVariance(@Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
protected void update(@Nonnull final FeatureValue[] features, float y, float p) {
throw new IllegalStateException("update() should not be called");
protected void update(@Nonnull final FeatureValue[] features, final float coeff) {
final Object k = f.getFeature();
final float v = f.getValue();
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, int label) {
@Override
protected void update(@Nonnull final FeatureValue[] features, final float coeff, final float alpha) {
for(FeatureValue f : features) {
final Object k = f.getFeature();
final float v = f.getValue();
import hivemall.io.FeatureValue;
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, final int label) {
final float y = label > 0 ? 1.f : -1.f;
import hivemall.io.FeatureValue;
import javax.annotation.Nonnull;
protected void update(@Nonnull final FeatureValue[] features, float y, float p) {
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, int label) {
final float y = label > 0 ? 1.f : -1.f;
protected void update(@Nonnull final FeatureValue[] features, final float y, final float alpha, final float beta) {
for(FeatureValue f : features) {
final Object k = f.getFeature();
final float v = f.getValue();
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, @Nonnull Object actual_label) {
protected void update(@Nonnull final FeatureValue[] features, final Object actual_label, final Object missed_label, final float alpha, final float beta) {
final Object k = f.getFeature();
final float v = f.getValue();
protected void train(@Nonnull final FeatureValue[] features, @Nonnull Object actual_label) {
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, @Nonnull Object actual_label) {
protected void update(@Nonnull final FeatureValue[] features, float alpha, Object actual_label, Object missed_label) {
final Object k = f.getFeature();
final float v = f.getValue();
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
private ListObjectInspector featureListOI;
private boolean parseFeature;
private PrimitiveObjectInspector labelInputOI;
FeatureValue[] featureVector = parseFeatures(features);
if(featureVector == null) {
train(featureVector, label);
@Nullable
protected final FeatureValue[] parseFeatures(@Nonnull final List<?> features) {
final int size = features.size();
if(size == 0) {
return null;
}
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final FeatureValue[] featureVector = new FeatureValue[size];
Object f = features.get(i);
if(f == null) {
continue;
}
final FeatureValue fv;
if(parseFeature) {
fv = FeatureValue.parse(f);
} else {
Object k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
fv = new FeatureValue(k, 1.f);
}
featureVector[i] = fv;
}
return featureVector;
}
protected abstract void train(@Nonnull final FeatureValue[] features, @Nonnull final Object actual_label);
protected final PredictionResult classify(@Nonnull final FeatureValue[] features) {
protected Margin getMargin(@Nonnull final FeatureValue[] features, final Object actual_label) {
protected Margin getMarginAndVariance(@Nonnull final FeatureValue[] features, final Object actual_label) {
protected Margin getMarginAndVariance(@Nonnull final FeatureValue[] features, final Object actual_label, boolean nonZeroVariance) {
protected final float squaredNorm(@Nonnull final FeatureValue[] features) {
final float v = f.getValue();
protected final float calcScore(@Nonnull final PredictionModel model, @Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
protected final float calcVariance(@Nonnull final FeatureValue[] features) {
float v = f.getValue();
protected final PredictionResult calcScoreAndVariance(@Nonnull final PredictionModel model, @Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
protected void update(@Nonnull final FeatureValue[] features, float coeff, Object actual_label, Object missed_label) {
final Object k = f.getFeature();
final float v = f.getValue();
import hivemall.io.FeatureValue;
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, @Nonnull Object actual_label) {
import hivemall.io.FeatureValue;
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, @Nonnull final Object actual_label) {
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, @Nonnull Object actual_label) {
protected void update(@Nonnull final FeatureValue[] features, final Object actual_label, final Object missed_label, final float alpha, final float beta) {
final Object k = f.getFeature();
final float v = f.getValue();
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, float target) {
protected void update(@Nonnull final FeatureValue[] features, final float coeff, final float beta) {
for(FeatureValue f : features) {
Object k = f.getFeature();
float v = f.getValue();
protected void train(@Nonnull final FeatureValue[] features, float target) {
protected void update(@Nonnull final FeatureValue[] features, float target, float predicted) {
protected void update(@Nonnull final FeatureValue[] features, float gradient) {
Object x = f.getFeature();
float xi = f.getValue();
protected void update(@Nonnull final FeatureValue[] features, float target, float predicted) {
protected void update(@Nonnull final FeatureValue[] features, float gradient) {
Object x = f.getFeature();
float xi = f.getValue();
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
private ListObjectInspector featureListOI;
private PrimitiveObjectInspector featureInputOI;
private PrimitiveObjectInspector targetOI;
private boolean parseFeature;
FeatureValue[] featureVector = parseFeatures(features);
if(featureVector == null) {
train(featureVector, target);
}
@Nullable
protected final FeatureValue[] parseFeatures(@Nonnull final List<?> features) {
final int size = features.size();
if(size == 0) {
return null;
}
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final FeatureValue[] featureVector = new FeatureValue[size];
Object f = features.get(i);
if(f == null) {
continue;
}
final FeatureValue fv;
if(parseFeature) {
fv = FeatureValue.parse(f);
} else {
Object k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
fv = new FeatureValue(k, 1.f);
}
featureVector[i] = fv;
}
return featureVector;
protected void train(@Nonnull final FeatureValue[] features, final float target) {
protected float predict(@Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
@Nonnull
protected PredictionResult calcScoreAndNorm(@Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
@Nonnull
protected PredictionResult calcScoreAndVariance(@Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
protected void update(@Nonnull final FeatureValue[] features, float target, float predicted) {
protected void update(@Nonnull final FeatureValue[] features, float coeff) {
final Object x = f.getFeature();
final float xi = f.getValue();
import hivemall.io.FeatureValue;
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, float target) {
final String[] splits = ftvec.split(":");
if(splits.length == 2) {
float f = Float.valueOf(splits[1]);
return new FloatWritable(f);
} else {
return new FloatWritable(1.f);
final String[] splits = ftvec.split(":");
if(splits.length == 2) {
float f = Float.valueOf(splits[1]);
return new FloatWritable(f);
} else {
return new FloatWritable(1.f);
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
protected PrimitiveObjectInspector labelOI;
public StructObjectInspector initialize(ObjectInspector[] argOIs)
throws UDFArgumentException {
this.labelOI = HiveUtils.asIntCompatibleOI(argOIs[1]);
PrimitiveObjectInspector featureOutputOI = dense_model
? PrimitiveObjectInspectorFactory.javaIntObjectInspector
if(!STRING_TYPE_NAME.equals(keyTypeName)
&& !INT_TYPE_NAME.equals(keyTypeName)
int label = PrimitiveObjectInspectorUtils.getInt(args[1], labelOI);
" training examples"
" rows");
public static int getConstInt(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
public static long getConstLong(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
throw new UDFArgumentException("Argument type must be INT: "
argOI.getTypeName());
public static PrimitiveObjectInspector asIntCompatibleOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if(argOI.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0, "Only primitive type arguments are accepted but "
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case INT:
case SHORT:
case LONG:
case FLOAT:
case DOUBLE:
case BOOLEAN:
case BYTE:
case STRING:
case DECIMAL:
break;
default:
throw new UDFArgumentTypeException(0, "Unxpected type '"
}
return oi;
}
switch (oi.getPrimitiveCategory()) {
valueOI.getTypeName());
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
protected PrimitiveObjectInspector labelOI;
public StructObjectInspector initialize(ObjectInspector[] argOIs)
throws UDFArgumentException {
this.labelOI = HiveUtils.asIntCompatibleOI(argOIs[1]);
PrimitiveObjectInspector featureOutputOI = dense_model
? PrimitiveObjectInspectorFactory.javaIntObjectInspector
if(!STRING_TYPE_NAME.equals(keyTypeName)
&& !INT_TYPE_NAME.equals(keyTypeName)
int label = PrimitiveObjectInspectorUtils.getInt(args[1], labelOI);
" training examples"
" rows");
public static int getConstInt(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
public static long getConstLong(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
throw new UDFArgumentException("Argument type must be INT: "
argOI.getTypeName());
public static PrimitiveObjectInspector asIntCompatibleOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if(argOI.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0, "Only primitive type arguments are accepted but "
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case INT:
case SHORT:
case LONG:
case FLOAT:
case DOUBLE:
case BOOLEAN:
case BYTE:
case STRING:
case DECIMAL:
break;
default:
throw new UDFArgumentTypeException(0, "Unxpected type '"
}
return oi;
}
switch (oi.getPrimitiveCategory()) {
valueOI.getTypeName());
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, int label) {
protected void update(@Nonnull final FeatureValue[] features, final float y, final float alpha, final float beta) {
for(FeatureValue f : features) {
final Object k = f.getFeature();
final float v = f.getValue();
protected void train(@Nonnull final FeatureValue[] features, int label) {
protected void train(@Nonnull final FeatureValue[] features, final int label) {
protected void update(@Nonnull final FeatureValue[] features, final float y, final int t) {
Object x = f.getFeature();
float xi = f.getValue();
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
private ListObjectInspector featureListOI;
private PrimitiveObjectInspector labelOI;
private boolean parseFeature;
FeatureValue[] featureVector = parseFeatures(features);
if(featureVector == null) {
train(featureVector, label);
}
@Nullable
protected final FeatureValue[] parseFeatures(@Nonnull final List<?> features) {
final int size = features.size();
if(size == 0) {
return null;
}
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final FeatureValue[] featureVector = new FeatureValue[size];
Object f = features.get(i);
if(f == null) {
continue;
}
final FeatureValue fv;
if(parseFeature) {
fv = FeatureValue.parse(f);
} else {
Object k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
fv = new FeatureValue(k, 1.f);
}
featureVector[i] = fv;
}
return featureVector;
void train(List<?> features, int label) {
FeatureValue[] featureVector = parseFeatures(features);
train(featureVector, label);
}
protected void train(@Nonnull final FeatureValue[] features, final int label) {
protected float predict(@Nonnull final FeatureValue[] features) {
float score = 0.f;
Object k = f.getFeature();
if(old_w != 0.f) {
float v = f.getValue();
@Nonnull
protected PredictionResult calcScoreAndNorm(@Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
@Nonnull
protected PredictionResult calcScoreAndVariance(@Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
protected void update(@Nonnull final FeatureValue[] features, float y, float p) {
throw new IllegalStateException("update() should not be called");
protected void update(@Nonnull final FeatureValue[] features, final float coeff) {
final Object k = f.getFeature();
final float v = f.getValue();
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, int label) {
@Override
protected void update(@Nonnull final FeatureValue[] features, final float coeff, final float alpha) {
for(FeatureValue f : features) {
final Object k = f.getFeature();
final float v = f.getValue();
import hivemall.io.FeatureValue;
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, final int label) {
final float y = label > 0 ? 1.f : -1.f;
import hivemall.io.FeatureValue;
import javax.annotation.Nonnull;
protected void update(@Nonnull final FeatureValue[] features, float y, float p) {
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, int label) {
final float y = label > 0 ? 1.f : -1.f;
protected void update(@Nonnull final FeatureValue[] features, final float y, final float alpha, final float beta) {
for(FeatureValue f : features) {
final Object k = f.getFeature();
final float v = f.getValue();
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, @Nonnull Object actual_label) {
protected void update(@Nonnull final FeatureValue[] features, final Object actual_label, final Object missed_label, final float alpha, final float beta) {
final Object k = f.getFeature();
final float v = f.getValue();
protected void train(@Nonnull final FeatureValue[] features, @Nonnull Object actual_label) {
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, @Nonnull Object actual_label) {
protected void update(@Nonnull final FeatureValue[] features, float alpha, Object actual_label, Object missed_label) {
final Object k = f.getFeature();
final float v = f.getValue();
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
private ListObjectInspector featureListOI;
private boolean parseFeature;
private PrimitiveObjectInspector labelInputOI;
FeatureValue[] featureVector = parseFeatures(features);
if(featureVector == null) {
train(featureVector, label);
@Nullable
protected final FeatureValue[] parseFeatures(@Nonnull final List<?> features) {
final int size = features.size();
if(size == 0) {
return null;
}
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final FeatureValue[] featureVector = new FeatureValue[size];
Object f = features.get(i);
if(f == null) {
continue;
}
final FeatureValue fv;
if(parseFeature) {
fv = FeatureValue.parse(f);
} else {
Object k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
fv = new FeatureValue(k, 1.f);
}
featureVector[i] = fv;
}
return featureVector;
}
protected abstract void train(@Nonnull final FeatureValue[] features, @Nonnull final Object actual_label);
protected final PredictionResult classify(@Nonnull final FeatureValue[] features) {
protected Margin getMargin(@Nonnull final FeatureValue[] features, final Object actual_label) {
protected Margin getMarginAndVariance(@Nonnull final FeatureValue[] features, final Object actual_label) {
protected Margin getMarginAndVariance(@Nonnull final FeatureValue[] features, final Object actual_label, boolean nonZeroVariance) {
protected final float squaredNorm(@Nonnull final FeatureValue[] features) {
final float v = f.getValue();
protected final float calcScore(@Nonnull final PredictionModel model, @Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
protected final float calcVariance(@Nonnull final FeatureValue[] features) {
float v = f.getValue();
protected final PredictionResult calcScoreAndVariance(@Nonnull final PredictionModel model, @Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
protected void update(@Nonnull final FeatureValue[] features, float coeff, Object actual_label, Object missed_label) {
final Object k = f.getFeature();
final float v = f.getValue();
import hivemall.io.FeatureValue;
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, @Nonnull Object actual_label) {
import hivemall.io.FeatureValue;
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, @Nonnull final Object actual_label) {
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, @Nonnull Object actual_label) {
protected void update(@Nonnull final FeatureValue[] features, final Object actual_label, final Object missed_label, final float alpha, final float beta) {
final Object k = f.getFeature();
final float v = f.getValue();
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, float target) {
protected void update(@Nonnull final FeatureValue[] features, final float coeff, final float beta) {
for(FeatureValue f : features) {
Object k = f.getFeature();
float v = f.getValue();
protected void train(@Nonnull final FeatureValue[] features, float target) {
protected void update(@Nonnull final FeatureValue[] features, float target, float predicted) {
protected void update(@Nonnull final FeatureValue[] features, float gradient) {
Object x = f.getFeature();
float xi = f.getValue();
protected void update(@Nonnull final FeatureValue[] features, float target, float predicted) {
protected void update(@Nonnull final FeatureValue[] features, float gradient) {
Object x = f.getFeature();
float xi = f.getValue();
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
private ListObjectInspector featureListOI;
private PrimitiveObjectInspector featureInputOI;
private PrimitiveObjectInspector targetOI;
private boolean parseFeature;
FeatureValue[] featureVector = parseFeatures(features);
if(featureVector == null) {
train(featureVector, target);
}
@Nullable
protected final FeatureValue[] parseFeatures(@Nonnull final List<?> features) {
final int size = features.size();
if(size == 0) {
return null;
}
final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();
final FeatureValue[] featureVector = new FeatureValue[size];
Object f = features.get(i);
if(f == null) {
continue;
}
final FeatureValue fv;
if(parseFeature) {
fv = FeatureValue.parse(f);
} else {
Object k = ObjectInspectorUtils.copyToStandardObject(f, featureInspector);
fv = new FeatureValue(k, 1.f);
}
featureVector[i] = fv;
}
return featureVector;
protected void train(@Nonnull final FeatureValue[] features, final float target) {
protected float predict(@Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
@Nonnull
protected PredictionResult calcScoreAndNorm(@Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
@Nonnull
protected PredictionResult calcScoreAndVariance(@Nonnull final FeatureValue[] features) {
final Object k = f.getFeature();
final float v = f.getValue();
protected void update(@Nonnull final FeatureValue[] features, float target, float predicted) {
protected void update(@Nonnull final FeatureValue[] features, float coeff) {
final Object x = f.getFeature();
final float xi = f.getValue();
import hivemall.io.FeatureValue;
import javax.annotation.Nonnull;
protected void train(@Nonnull final FeatureValue[] features, float target) {
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "float_array", value = "_FUNC_(nDims) - Returns an array<float> of nDims elements")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "array_avg", value = "_FUNC_(array) - Returns an array<double> in which "
"each element is the mean of a set of numbers")
public GenericUDAFEvaluator getEvaluator(TypeInfo[] typeInfo)
throws SemanticException {
public ObjectInspector init(Mode mode, ObjectInspector[] parameters)
throws HiveException {
public void iterate(AggregationBuffer aggr, Object[] parameters)
throws HiveException {
public Object terminatePartial(AggregationBuffer aggr)
throws HiveException {
public void merge(AggregationBuffer aggr, Object partial)
throws HiveException {
public List<FloatWritable> terminate(AggregationBuffer aggr)
throws HiveException {
import org.apache.hadoop.hive.ql.exec.Description;
@Deprecated
@Description(name = "array_avg", value = "_FUNC_(array) - Returns an array<double>"
" in which each element is the mean of a set of numbers")
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "array_remove", value = "_FUNC_(original, target) - Returns an array that the target is removed "
"from the original array")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "collect_all", value = "_FUNC_(x) - Retrurns a set of objects with duplicate elements eliminated")
public GenericUDAFEvaluator getEvaluator(TypeInfo[] tis)
throws SemanticException {
public ObjectInspector init(Mode m, ObjectInspector[] parameters)
throws HiveException {
public void iterate(AggregationBuffer ab, Object[] parameters)
throws HiveException {
public Object terminatePartial(AggregationBuffer ab)
throws HiveException {
}
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "concat_array", value = "_FUNC_(x1, x2, ..) - Returns a concatinated array")
@UDFType(deterministic = true, stateful = false)
public ObjectInspector initialize(ObjectInspector[] arguments)
throws UDFArgumentException {
switch (arguments[i].getCategory()) {
" of function CONCAT_ARRAY must be "
}
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "sort_and_uniq_array", value = "_FUNC_(x) - Takes an array of type int and "
"returns a sorted array with duplicate elementes eliminated")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "subarray_endwith", value = "_FUNC_(original, key) - Returns an array that ends with the specified key")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "subarray_startwith", value = "_FUNC_(original, key) - Returns an array that starts with the specified key")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "subarray", value = "_FUNC_(orignal, fromIndex, toIndex) - Returns a slice of the orignal array "
"between the specified fromIndex, inclusive, and toIndex, exclusive")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "map_get_sum", value = "_FUNC_(map<int,float> src, array<int> keys) - Returns sum of values "
"that are retrieved by keys")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "map_tail_n", value = "_FUNC_(map SRC, int N) - Returns the last N elements "
"from a sorted array of SRC")
@UDFType(deterministic = true, stateful = false)
public ObjectInspector initialize(ObjectInspector[] arguments)
throws UDFArgumentException {
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "split_words", value = "_FUNC_(query [, regex]) - Returns an array<text> containing splitted strings")
@Description(name = "generate_series", value = "_FUNC_(const int|bigint start, const int|bigint end) - "
"Generate a series of values, from start to end")
public GenericUDAFEvaluator getEvaluator(TypeInfo[] typeInfo) throws SemanticException {
public ObjectInspector init(Mode mode, ObjectInspector[] parameters) throws HiveException {
public void iterate(AggregationBuffer aggr, Object[] parameters) throws HiveException {
public Object terminatePartial(AggregationBuffer aggr) throws HiveException {
public void merge(AggregationBuffer aggr, Object partial) throws HiveException {
public List<FloatWritable> terminate(AggregationBuffer aggr) throws HiveException {
void doIterate(@Nonnull final Object tuple, @Nonnull ListObjectInspector listOI, @Nonnull PrimitiveObjectInspector elemOI) throws HiveException {
void merge(final int o_size, @Nonnull final Object o_sum, @Nonnull final Object o_count, @Nonnull final StandardListObjectInspector sumOI, @Nonnull final StandardListObjectInspector countOI) throws HiveException {
public GenericUDAFEvaluator getEvaluator(TypeInfo[] tis) throws SemanticException {
public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {
public void iterate(AggregationBuffer ab, Object[] parameters) throws HiveException {
public Object terminatePartial(AggregationBuffer ab) throws HiveException {
public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
}
}
switch (argOIs[1].getCategory()) {
private static void loadValues(OpenHashMap<Object, Object> map, File file, PrimitiveObjectInspector keyOI, PrimitiveObjectInspector valueOI) throws IOException, SerDeException {
@Description(name = "generate_series", value = "_FUNC_(const int|bigint start, const int|bigint end) - "
"Generate a series of values, from start to end")
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "float_array", value = "_FUNC_(nDims) - Returns an array<float> of nDims elements")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "array_avg", value = "_FUNC_(array) - Returns an array<double> in which "
"each element is the mean of a set of numbers")
void doIterate(@Nonnull final Object tuple, @Nonnull ListObjectInspector listOI, @Nonnull PrimitiveObjectInspector elemOI) throws HiveException {
void merge(final int o_size, @Nonnull final Object o_sum, @Nonnull final Object o_count, @Nonnull final StandardListObjectInspector sumOI, @Nonnull final StandardListObjectInspector countOI) throws HiveException {
import org.apache.hadoop.hive.ql.exec.Description;
@Deprecated
@Description(name = "array_avg", value = "_FUNC_(array) - Returns an array<double>"
" in which each element is the mean of a set of numbers")
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "array_remove", value = "_FUNC_(original, target) - Returns an array that the target is removed "
"from the original array")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "collect_all", value = "_FUNC_(x) - Retrurns a set of objects with duplicate elements eliminated")
}
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "concat_array", value = "_FUNC_(x1, x2, ..) - Returns a concatinated array")
@UDFType(deterministic = true, stateful = false)
switch (arguments[i].getCategory()) {
}
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "sort_and_uniq_array", value = "_FUNC_(x) - Takes an array of type int and "
"returns a sorted array with duplicate elementes eliminated")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "subarray_endwith", value = "_FUNC_(original, key) - Returns an array that ends with the specified key")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "subarray_startwith", value = "_FUNC_(original, key) - Returns an array that starts with the specified key")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "subarray", value = "_FUNC_(orignal, fromIndex, toIndex) - Returns a slice of the orignal array "
"between the specified fromIndex, inclusive, and toIndex, exclusive")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "map_get_sum", value = "_FUNC_(map<int,float> src, array<int> keys) - Returns sum of values "
"that are retrieved by keys")
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "map_tail_n", value = "_FUNC_(map SRC, int N) - Returns the last N elements "
"from a sorted array of SRC")
@UDFType(deterministic = true, stateful = false)
}
}
switch (argOIs[1].getCategory()) {
private static void loadValues(OpenHashMap<Object, Object> map, File file, PrimitiveObjectInspector keyOI, PrimitiveObjectInspector valueOI) throws IOException, SerDeException {
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "split_words", value = "_FUNC_(query [, regex]) - Returns an array<text> containing splitted strings")
public static final String VERSION = "0.3.1";
public static final String VERSION = "0.3.1";
public FloatWritable evaluate(final double value, final double min, final double max) {
return val(min_max_normalization(value, min, max));
String[] fv = s.split(":");
if(fv.length != 2) {
}
double v = Float.parseFloat(fv[1]);
float scaled_v = min_max_normalization(v, min, max);
return val(ret);
private static float min_max_normalization(final double value, final double min, final double max) {
return (float) ((value - min) / (max - min));
}
public FloatWritable evaluate(double value, double mean, double stddev) {
float v = (float) ((value - mean) / stddev);
return val(v);
opts.addOption("rankinit", true, "Initialization strategy of rank matrix [random, gaussian] (default: random)");
public FloatWritable evaluate(final double value, final double min, final double max) {
return val(min_max_normalization(value, min, max));
String[] fv = s.split(":");
if(fv.length != 2) {
}
double v = Float.parseFloat(fv[1]);
float scaled_v = min_max_normalization(v, min, max);
return val(ret);
private static float min_max_normalization(final double value, final double min, final double max) {
return (float) ((value - min) / (max - min));
}
public FloatWritable evaluate(double value, double mean, double stddev) {
float v = (float) ((value - mean) / stddev);
return val(v);
opts.addOption("rankinit", true, "Initialization strategy of rank matrix [random, gaussian] (default: random)");
return null;
return null;
return null;
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
public final class MinHashesUDF extends UDF {
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
public final class bBitMinHashUDF extends UDF {
@Nonnull
public static void parseFeatureAsString(@Nonnull final String s, @Nonnull final FeatureValue probe)
throws IllegalArgumentException {
assert (s != null);
assert (probe != null);
final int pos = s.indexOf(':');
if(pos == 0) {
}
if(pos > 0) {
probe.feature = s.substring(0, pos);
probe.value = Float.parseFloat(s2);
} else {
probe.feature = s;
probe.value = 1.f;
}
}
package hivemall.knn.similarity;
import hivemall.knn.distance.HammingDistanceUDF;
protected PrimitiveObjectInspector userOI;
protected PrimitiveObjectInspector itemOI;
this.userOI = HiveUtils.asIntCompatibleOI(argOIs[0]);
this.itemOI = HiveUtils.asIntCompatibleOI(argOIs[1]);
int user = PrimitiveObjectInspectorUtils.getInt(args[0], userOI);
int item = PrimitiveObjectInspectorUtils.getInt(args[1], itemOI);
if(ft == null) {
continue;
}
if(ft == null) {
continue;
}
import org.apache.hadoop.io.Text;
public DoubleWritable evaluate(final List<Text> ftvec1, final List<Text> ftvec2) {
for(Text ft : ftvec1) {
String s = ft.toString();
FeatureValue.parseFeatureAsString(s, probe);
for(Text ft : ftvec2) {
String s = ft.toString();
FeatureValue.parseFeatureAsString(s, probe);
@Nonnull
public static void parseFeatureAsString(@Nonnull final String s, @Nonnull final FeatureValue probe)
throws IllegalArgumentException {
assert (s != null);
assert (probe != null);
final int pos = s.indexOf(':');
if(pos == 0) {
}
if(pos > 0) {
probe.feature = s.substring(0, pos);
probe.value = Float.parseFloat(s2);
} else {
probe.feature = s;
probe.value = 1.f;
}
}
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
public final class MinHashesUDF extends UDF {
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = true, stateful = false)
public final class bBitMinHashUDF extends UDF {
package hivemall.knn.similarity;
import hivemall.knn.distance.HammingDistanceUDF;
protected PrimitiveObjectInspector userOI;
protected PrimitiveObjectInspector itemOI;
this.userOI = HiveUtils.asIntCompatibleOI(argOIs[0]);
this.itemOI = HiveUtils.asIntCompatibleOI(argOIs[1]);
int user = PrimitiveObjectInspectorUtils.getInt(args[0], userOI);
int item = PrimitiveObjectInspectorUtils.getInt(args[1], itemOI);
@Description(name = "cosine_similarity", value = "_FUNC_(ftvec1, ftvec2) - Returns a cosine similarity of the given two vectors")
return new FloatWritable(cosineSimilarity(ftvec1, ftvec2));
}
public static float cosineSimilarity(final List<String> ftvec1, List<String> ftvec2) {
return 0.f;
return 0.f;
return (float) (dotp / denom);
public static final String VERSION = "0.3.2";
public static FeatureValue parseFeatureAsString(@Nonnull final Text t) {
String s = t.toString();
return parseFeatureAsString(s);
}
@Nonnull
public static void parseFeatureAsString(@Nonnull final Text t, @Nonnull final FeatureValue probe) {
assert (t != null);
String s = t.toString();
parseFeatureAsString(s, probe);
}
public static List<Text> val(final String... v) {
public static final String VERSION = "0.3.2";
public static FeatureValue parseFeatureAsString(@Nonnull final Text t) {
String s = t.toString();
return parseFeatureAsString(s);
}
@Nonnull
public static void parseFeatureAsString(@Nonnull final Text t, @Nonnull final FeatureValue probe) {
assert (t != null);
String s = t.toString();
parseFeatureAsString(s, probe);
}
@Description(name = "cosine_similarity", value = "_FUNC_(ftvec1, ftvec2) - Returns a cosine similarity of the given two vectors")
return new FloatWritable(cosineSimilarity(ftvec1, ftvec2));
}
public static float cosineSimilarity(final List<String> ftvec1, List<String> ftvec2) {
return 0.f;
return 0.f;
return (float) (dotp / denom);
public static List<Text> val(final String... v) {
import java.util.List;
@Nonnull
public static String[] getConstStringArray(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
ConstantObjectInspector constOI = (ConstantObjectInspector) oi;
final List<?> lst = (List<?>) constOI.getWritableConstantValue();
final int size = lst.size();
final String[] ary = new String[size];
Object o = lst.get(i);
if(o != null) {
ary[i] = o.toString();
}
}
return ary;
}
public static int getConstInt(@Nonnull final ObjectInspector oi) throws UDFArgumentException {
public static long getConstLong(@Nonnull final ObjectInspector oi) throws UDFArgumentException {
"' is passed.");
public static boolean isNumber(final String str) {
if(str == null || str.length() == 0) {
return false;
}
char[] chars = str.toCharArray();
int sz = chars.length;
boolean hasExp = false;
boolean hasDecPoint = false;
boolean allowSigns = false;
boolean foundDigit = false;
int start = (chars[0] == '-') ? 1 : 0;
if(i == sz) {
}
if((chars[i] < '0' || chars[i] > '9') && (chars[i] < 'a' || chars[i] > 'f')
&& (chars[i] < 'A' || chars[i] > 'F')) {
return false;
}
}
return true;
}
}
int i = start;
if(chars[i] >= '0' && chars[i] <= '9') {
foundDigit = true;
allowSigns = false;
} else if(chars[i] == '.') {
if(hasDecPoint || hasExp) {
return false;
}
hasDecPoint = true;
} else if(chars[i] == 'e' || chars[i] == 'E') {
if(hasExp) {
return false;
}
if(!foundDigit) {
return false;
}
hasExp = true;
allowSigns = true;
if(!allowSigns) {
return false;
}
allowSigns = false;
} else {
return false;
}
}
if(i < chars.length) {
if(chars[i] >= '0' && chars[i] <= '9') {
return true;
}
if(chars[i] == 'e' || chars[i] == 'E') {
return false;
}
if(!allowSigns
&& (chars[i] == 'd' || chars[i] == 'D' || chars[i] == 'f' || chars[i] == 'F')) {
return foundDigit;
}
if(chars[i] == 'l' || chars[i] == 'L') {
return foundDigit && !hasExp;
}
return false;
}
return !allowSigns && foundDigit;
}
import java.util.List;
@Nonnull
public static String[] getConstStringArray(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
if(!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
throw new UDFArgumentException("argument must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
ConstantObjectInspector constOI = (ConstantObjectInspector) oi;
final List<?> lst = (List<?>) constOI.getWritableConstantValue();
final int size = lst.size();
final String[] ary = new String[size];
Object o = lst.get(i);
if(o != null) {
ary[i] = o.toString();
}
}
return ary;
}
public static int getConstInt(@Nonnull final ObjectInspector oi) throws UDFArgumentException {
public static long getConstLong(@Nonnull final ObjectInspector oi) throws UDFArgumentException {
"' is passed.");
public static boolean isNumber(final String str) {
if(str == null || str.length() == 0) {
return false;
}
char[] chars = str.toCharArray();
int sz = chars.length;
boolean hasExp = false;
boolean hasDecPoint = false;
boolean allowSigns = false;
boolean foundDigit = false;
int start = (chars[0] == '-') ? 1 : 0;
if(i == sz) {
}
if((chars[i] < '0' || chars[i] > '9') && (chars[i] < 'a' || chars[i] > 'f')
&& (chars[i] < 'A' || chars[i] > 'F')) {
return false;
}
}
return true;
}
}
int i = start;
if(chars[i] >= '0' && chars[i] <= '9') {
foundDigit = true;
allowSigns = false;
} else if(chars[i] == '.') {
if(hasDecPoint || hasExp) {
return false;
}
hasDecPoint = true;
} else if(chars[i] == 'e' || chars[i] == 'E') {
if(hasExp) {
return false;
}
if(!foundDigit) {
return false;
}
hasExp = true;
allowSigns = true;
if(!allowSigns) {
return false;
}
allowSigns = false;
} else {
return false;
}
}
if(i < chars.length) {
if(chars[i] >= '0' && chars[i] <= '9') {
return true;
}
if(chars[i] == 'e' || chars[i] == 'E') {
return false;
}
if(!allowSigns
&& (chars[i] == 'd' || chars[i] == 'D' || chars[i] == 'f' || chars[i] == 'F')) {
return foundDigit;
}
if(chars[i] == 'l' || chars[i] == 'L') {
return foundDigit && !hasExp;
}
return false;
}
return !allowSigns && foundDigit;
}
@Description(name = "powered_features", value = "_FUNC_(feature_vector in array<string>, int degree [, boolean truncate])"
" - Returns a feature vector having a powered feature space")
return evaluate(ftvec, degree, true);
}
public List<Text> evaluate(final List<Text> ftvec, final int degree, final boolean truncate)
throws HiveException {
if(truncate && (v == 0.f || v == 1.f)) {
continue;
}
final String f = probe.getFeature();
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "jaccard", value = "_FUNC_(A, B [,int k]) - Returns Jaccard similarity coefficient of A and B")
private final Set<Object> union = new HashSet<Object>();
private final Set<Object> intersect = new HashSet<Object>();
public FloatWritable evaluate(final List<String> a, final List<String> b) {
if(a == null || b == null) {
return new FloatWritable(0.f);
}
if(a.size() == 0 || b.size() == 0) {
return new FloatWritable(0.f);
}
union.addAll(a);
union.addAll(b);
float unionSize = union.size();
union.clear();
intersect.addAll(a);
intersect.retainAll(b);
float intersectSize = intersect.size();
intersect.clear();
return new FloatWritable(intersectSize / unionSize);
}
@Description(name = "to_string_array", value = "_FUNC_(array<int>) - Returns an array of strings")
public List<Text> evaluate(@Nullable final List<Integer> inArray) {
@Description(name = "to_string_array", value = "_FUNC_(array<ANY>) - Returns an array of strings")
public List<Text> evaluate(@Nullable final List<Text> inArray) {
return inArray;
if(a == null && b == null) {
} else if(a == null || b == null) {
return new FloatWritable(1.f);
final int asize = a.size();
final int bsize = b.size();
if(asize == 0 && bsize == 0) {
} else if(asize == 0 || bsize == 0) {
return new FloatWritable(1.f);
if(a == null && b == null) {
return new FloatWritable(1.f);
} else if(a == null || b == null) {
final int asize = a.size();
final int bsize = b.size();
if(asize == 0 && bsize == 0) {
return new FloatWritable(1.f);
} else if(asize == 0 || bsize == 0) {
if(a == null && b == null) {
return new FloatWritable(1.f);
} else if(a == null || b == null) {
final int asize = a.size();
final int bsize = b.size();
if(asize == 0 && bsize == 0) {
return new FloatWritable(1.f);
} else if(asize == 0 || bsize == 0) {
this.xtimes = HiveUtils.getAsConstInt(argOIs[0]);
int xtimes = HiveUtils.getAsConstInt(argOIs[0]);
int numBuffers = HiveUtils.getAsConstInt(argOIs[1]);
public static int getAsConstInt(@Nonnull final ObjectInspector numberOI)
throws UDFArgumentException {
final String typeName = numberOI.getTypeName();
if(INT_TYPE_NAME.equals(typeName)) {
IntWritable v = getConstValue(numberOI);
return v.get();
} else if(BIGINT_TYPE_NAME.equals(typeName)) {
LongWritable v = getConstValue(numberOI);
return (int) v.get();
} else if(SMALLINT_TYPE_NAME.equals(typeName)) {
ShortWritable v = getConstValue(numberOI);
return v.get();
} else if(TINYINT_TYPE_NAME.equals(typeName)) {
ByteWritable v = getConstValue(numberOI);
return v.get();
}
throw new UDFArgumentException("Unexpected argument type to cast as INT: "
TypeInfoUtils.getTypeInfoFromObjectInspector(numberOI));
}
public static final String VERSION = "0.3.2-1";
public static final String VERSION = "0.3.2-1";
this.xtimes = HiveUtils.getAsConstInt(argOIs[0]);
int xtimes = HiveUtils.getAsConstInt(argOIs[0]);
int numBuffers = HiveUtils.getAsConstInt(argOIs[1]);
public static int getAsConstInt(@Nonnull final ObjectInspector numberOI)
throws UDFArgumentException {
final String typeName = numberOI.getTypeName();
if(INT_TYPE_NAME.equals(typeName)) {
IntWritable v = getConstValue(numberOI);
return v.get();
} else if(BIGINT_TYPE_NAME.equals(typeName)) {
LongWritable v = getConstValue(numberOI);
return (int) v.get();
} else if(SMALLINT_TYPE_NAME.equals(typeName)) {
ShortWritable v = getConstValue(numberOI);
return v.get();
} else if(TINYINT_TYPE_NAME.equals(typeName)) {
ByteWritable v = getConstValue(numberOI);
return v.get();
}
throw new UDFArgumentException("Unexpected argument type to cast as INT: "
TypeInfoUtils.getTypeInfoFromObjectInspector(numberOI));
}
result.add(f);
} else if(v == 0.f || v == -1.f) {
String featureName = featureNames[i];
if(!STRING_TYPE_NAME.equals(labelTypeName) && !INT_TYPE_NAME.equals(labelTypeName)
&& !BIGINT_TYPE_NAME.equals(labelTypeName)) {
throw new UDFArgumentTypeException(0, "label must be a type [Int|BigInt|Text]: "
PrimitiveObjectInspector featureOutputOI = dense_model
? PrimitiveObjectInspectorFactory.javaIntObjectInspector : featureInputOI;
" takes 2 arguments: List<Int|BigInt|Text> features, {Int|BitInt|Text} label [, constant text options]");
return evaluate(Pu, Qi, 0.d);
}
public FloatWritable evaluate(List<Float> Pu, List<Float> Qi, double mu) throws HiveException {
float ret = (float) mu;
protected boolean useBiasClause;
this.useBiasClause = true;
opts.addOption("disable_bias", "no_bias", false, "Turn off bias clause");
boolean noBias = cl.hasOption("no_bias");
this.useBiasClause = !noBias;
if(noBias && updateMeanRating) {
throw new UDFArgumentException("Cannot set both `update_mean` and `no_bias` option");
}
if(useBiasClause) {
fieldNames.add("Bu");
fieldNames.add("Bi");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
if(updateMeanRating) {
fieldNames.add("mu");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
}
if(useBiasClause) {
updateBias(user, item, err, eta);
if(updateMeanRating) {
updateMeanRating(err, eta);
}
if(useBiasClause == false) {
return model.getMeanRating();
}
assert useBiasClause;
assert useBiasClause;
if(useBiasClause) {
forwardObj = new Object[] { idx, Pu, Qi, Bu, Bi };
} else {
forwardObj = new Object[] { idx, Pu, Qi };
}
if(useBiasClause) {
Bu.set(model.getUserBias(i));
Bi.set(model.getItemBias(i));
}
public static final String VERSION = "0.3.2-2";
public static final String VERSION = "0.3.2-2";
return evaluate(Pu, Qi, 0.d);
}
public FloatWritable evaluate(List<Float> Pu, List<Float> Qi, double mu) throws HiveException {
float ret = (float) mu;
protected boolean useBiasClause;
this.useBiasClause = true;
opts.addOption("disable_bias", "no_bias", false, "Turn off bias clause");
boolean noBias = cl.hasOption("no_bias");
this.useBiasClause = !noBias;
if(noBias && updateMeanRating) {
throw new UDFArgumentException("Cannot set both `update_mean` and `no_bias` option");
}
if(useBiasClause) {
fieldNames.add("Bu");
fieldNames.add("Bi");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
if(updateMeanRating) {
fieldNames.add("mu");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
}
if(useBiasClause) {
updateBias(user, item, err, eta);
if(updateMeanRating) {
updateMeanRating(err, eta);
}
if(useBiasClause == false) {
return model.getMeanRating();
}
assert useBiasClause;
assert useBiasClause;
if(useBiasClause) {
forwardObj = new Object[] { idx, Pu, Qi, Bu, Bi };
} else {
forwardObj = new Object[] { idx, Pu, Qi };
}
if(useBiasClause) {
Bu.set(model.getUserBias(i));
Bi.set(model.getItemBias(i));
}
@Description(name = "jaccard_similarity", value = "_FUNC_(A, B [,int k]) - Returns Jaccard similarity coefficient of A and B")
import hivemall.utils.hadoop.HiveUtils;
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
public final class AngularDistanceUDF extends GenericUDF {
private ListObjectInspector arg0ListOI, arg1ListOI;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if(argOIs.length != 2) {
throw new UDFArgumentException("angular_distance takes 2 arguments");
}
this.arg0ListOI = HiveUtils.asListOI(argOIs[0]);
this.arg1ListOI = HiveUtils.asListOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
}
@Override
public FloatWritable evaluate(DeferredObject[] arguments) throws HiveException {
List<String> ftvec1 = HiveUtils.asStringList(arguments[0], arg0ListOI);
List<String> ftvec2 = HiveUtils.asStringList(arguments[1], arg1ListOI);
float d = 1.f - AngularSimilarityUDF.angularSimilarity(ftvec1, ftvec2);
return new FloatWritable(d);
}
@Deprecated
@Override
public String getDisplayString(String[] children) {
}
import hivemall.utils.hadoop.HiveUtils;
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
public final class CosineDistanceUDF extends GenericUDF {
private ListObjectInspector arg0ListOI, arg1ListOI;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if(argOIs.length != 2) {
throw new UDFArgumentException("cosine_distance takes 2 arguments");
}
this.arg0ListOI = HiveUtils.asListOI(argOIs[0]);
this.arg1ListOI = HiveUtils.asListOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
}
@Override
public FloatWritable evaluate(DeferredObject[] arguments) throws HiveException {
List<String> ftvec1 = HiveUtils.asStringList(arguments[0], arg0ListOI);
List<String> ftvec2 = HiveUtils.asStringList(arguments[1], arg1ListOI);
float d = 1.f - CosineSimilarityUDF.cosineSimilarity(ftvec1, ftvec2);
return new FloatWritable(d);
}
@Deprecated
@Override
public String getDisplayString(String[] children) {
}
import hivemall.utils.hadoop.HiveUtils;
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.apache.hadoop.io.FloatWritable;
public final class EuclidDistanceUDF extends GenericUDF {
private ListObjectInspector arg0ListOI, arg1ListOI;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if(argOIs.length != 2) {
throw new UDFArgumentException("euclid_distance takes 2 arguments");
}
this.arg0ListOI = HiveUtils.asListOI(argOIs[0]);
this.arg1ListOI = HiveUtils.asListOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
}
@Override
public FloatWritable evaluate(DeferredObject[] arguments) throws HiveException {
List<String> ftvec1 = HiveUtils.asStringList(arguments[0], arg0ListOI);
List<String> ftvec2 = HiveUtils.asStringList(arguments[1], arg1ListOI);
return evaluate(ftvec1, ftvec2);
}
public FloatWritable evaluate(final List<String> ftvec1, final List<String> ftvec2) {
for(String ft : ftvec1) {
FeatureValue.parseFeatureAsString(ft, probe);
for(String ft : ftvec2) {
FeatureValue.parseFeatureAsString(ft, probe);
return new FloatWritable((float) Math.sqrt(d));
}
@Override
public String getDisplayString(String[] children) {
import hivemall.utils.hadoop.HiveUtils;
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
public final class AngularSimilarityUDF extends GenericUDF {
private ListObjectInspector arg0ListOI, arg1ListOI;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if(argOIs.length != 2) {
throw new UDFArgumentException("angular_similarity takes 2 arguments");
}
this.arg0ListOI = HiveUtils.asListOI(argOIs[0]);
this.arg1ListOI = HiveUtils.asListOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
}
@Override
public FloatWritable evaluate(DeferredObject[] arguments) throws HiveException {
List<String> ftvec1 = HiveUtils.asStringList(arguments[0], arg0ListOI);
List<String> ftvec2 = HiveUtils.asStringList(arguments[1], arg1ListOI);
float similarity = angularSimilarity(ftvec1, ftvec2);
return new FloatWritable(similarity);
@Override
public String getDisplayString(String[] children) {
}
import hivemall.utils.hadoop.HiveUtils;
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
public final class CosineSimilarityUDF extends GenericUDF {
private ListObjectInspector arg0ListOI, arg1ListOI;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if(argOIs.length != 2) {
throw new UDFArgumentException("cosine_similarity takes 2 arguments");
}
this.arg0ListOI = HiveUtils.asListOI(argOIs[0]);
this.arg1ListOI = HiveUtils.asListOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
}
@Deprecated
@Override
public FloatWritable evaluate(DeferredObject[] arguments) throws HiveException {
List<String> ftvec1 = HiveUtils.asStringList(arguments[0], arg0ListOI);
List<String> ftvec2 = HiveUtils.asStringList(arguments[1], arg1ListOI);
float similarity = cosineSimilarity(ftvec1, ftvec2);
return new FloatWritable(similarity);
}
public static float cosineSimilarity(final List<String> ftvec1, final List<String> ftvec2) {
final double denom = l1norm1 * l1norm2;
@Override
public String getDisplayString(String[] children) {
}
import java.util.Arrays;
import java.util.Collections;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredObject;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
@Nullable
public static List<String> asStringList(@Nonnull final DeferredObject arg, @Nonnull final ListObjectInspector listOI)
throws HiveException {
Object argObj = arg.get();
if(argObj == null) {
return null;
}
List<?> data = listOI.getList(argObj);
int size = data.size();
if(size == 0) {
return Collections.emptyList();
}
final String[] ary = new String[size];
Object o = data.get(i);
if(o != null) {
ary[i] = o.toString();
}
}
return Arrays.asList(ary);
}
public static ListObjectInspector asListOI(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
Category category = oi.getCategory();
if(category != Category.LIST) {
}
return (ListObjectInspector) oi;
}
@Nonnull
import hivemall.utils.hadoop.HiveUtils;
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
public final class AngularDistanceUDF extends GenericUDF {
private ListObjectInspector arg0ListOI, arg1ListOI;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if(argOIs.length != 2) {
throw new UDFArgumentException("angular_distance takes 2 arguments");
}
this.arg0ListOI = HiveUtils.asListOI(argOIs[0]);
this.arg1ListOI = HiveUtils.asListOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
}
@Override
public FloatWritable evaluate(DeferredObject[] arguments) throws HiveException {
List<String> ftvec1 = HiveUtils.asStringList(arguments[0], arg0ListOI);
List<String> ftvec2 = HiveUtils.asStringList(arguments[1], arg1ListOI);
float d = 1.f - AngularSimilarityUDF.angularSimilarity(ftvec1, ftvec2);
return new FloatWritable(d);
}
@Deprecated
@Override
public String getDisplayString(String[] children) {
}
import hivemall.utils.hadoop.HiveUtils;
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
public final class CosineDistanceUDF extends GenericUDF {
private ListObjectInspector arg0ListOI, arg1ListOI;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if(argOIs.length != 2) {
throw new UDFArgumentException("cosine_distance takes 2 arguments");
}
this.arg0ListOI = HiveUtils.asListOI(argOIs[0]);
this.arg1ListOI = HiveUtils.asListOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
}
@Override
public FloatWritable evaluate(DeferredObject[] arguments) throws HiveException {
List<String> ftvec1 = HiveUtils.asStringList(arguments[0], arg0ListOI);
List<String> ftvec2 = HiveUtils.asStringList(arguments[1], arg1ListOI);
float d = 1.f - CosineSimilarityUDF.cosineSimilarity(ftvec1, ftvec2);
return new FloatWritable(d);
}
@Deprecated
@Override
public String getDisplayString(String[] children) {
}
import hivemall.utils.hadoop.HiveUtils;
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.apache.hadoop.io.FloatWritable;
public final class EuclidDistanceUDF extends GenericUDF {
private ListObjectInspector arg0ListOI, arg1ListOI;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if(argOIs.length != 2) {
throw new UDFArgumentException("euclid_distance takes 2 arguments");
}
this.arg0ListOI = HiveUtils.asListOI(argOIs[0]);
this.arg1ListOI = HiveUtils.asListOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
}
@Override
public FloatWritable evaluate(DeferredObject[] arguments) throws HiveException {
List<String> ftvec1 = HiveUtils.asStringList(arguments[0], arg0ListOI);
List<String> ftvec2 = HiveUtils.asStringList(arguments[1], arg1ListOI);
return evaluate(ftvec1, ftvec2);
}
public FloatWritable evaluate(final List<String> ftvec1, final List<String> ftvec2) {
for(String ft : ftvec1) {
FeatureValue.parseFeatureAsString(ft, probe);
for(String ft : ftvec2) {
FeatureValue.parseFeatureAsString(ft, probe);
return new FloatWritable((float) Math.sqrt(d));
}
@Override
public String getDisplayString(String[] children) {
import hivemall.utils.hadoop.HiveUtils;
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
public final class AngularSimilarityUDF extends GenericUDF {
private ListObjectInspector arg0ListOI, arg1ListOI;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if(argOIs.length != 2) {
throw new UDFArgumentException("angular_similarity takes 2 arguments");
}
this.arg0ListOI = HiveUtils.asListOI(argOIs[0]);
this.arg1ListOI = HiveUtils.asListOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
}
@Override
public FloatWritable evaluate(DeferredObject[] arguments) throws HiveException {
List<String> ftvec1 = HiveUtils.asStringList(arguments[0], arg0ListOI);
List<String> ftvec2 = HiveUtils.asStringList(arguments[1], arg1ListOI);
float similarity = angularSimilarity(ftvec1, ftvec2);
return new FloatWritable(similarity);
@Override
public String getDisplayString(String[] children) {
}
import hivemall.utils.hadoop.HiveUtils;
import java.util.Arrays;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
public final class CosineSimilarityUDF extends GenericUDF {
private ListObjectInspector arg0ListOI, arg1ListOI;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if(argOIs.length != 2) {
throw new UDFArgumentException("cosine_similarity takes 2 arguments");
}
this.arg0ListOI = HiveUtils.asListOI(argOIs[0]);
this.arg1ListOI = HiveUtils.asListOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
}
@Deprecated
@Override
public FloatWritable evaluate(DeferredObject[] arguments) throws HiveException {
List<String> ftvec1 = HiveUtils.asStringList(arguments[0], arg0ListOI);
List<String> ftvec2 = HiveUtils.asStringList(arguments[1], arg1ListOI);
float similarity = cosineSimilarity(ftvec1, ftvec2);
return new FloatWritable(similarity);
}
public static float cosineSimilarity(final List<String> ftvec1, final List<String> ftvec2) {
final double denom = l1norm1 * l1norm2;
@Override
public String getDisplayString(String[] children) {
}
@Description(name = "jaccard_similarity", value = "_FUNC_(A, B [,int k]) - Returns Jaccard similarity coefficient of A and B")
import java.util.Arrays;
import java.util.Collections;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredObject;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
@Nullable
public static List<String> asStringList(@Nonnull final DeferredObject arg, @Nonnull final ListObjectInspector listOI)
throws HiveException {
Object argObj = arg.get();
if(argObj == null) {
return null;
}
List<?> data = listOI.getList(argObj);
int size = data.size();
if(size == 0) {
return Collections.emptyList();
}
final String[] ary = new String[size];
Object o = data.get(i);
if(o != null) {
ary[i] = o.toString();
}
}
return Arrays.asList(ary);
}
public static ListObjectInspector asListOI(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
Category category = oi.getCategory();
if(category != Category.LIST) {
}
return (ListObjectInspector) oi;
}
@Nonnull
import static hivemall.HivemallConstants.DOUBLE_TYPE_NAME;
import static hivemall.HivemallConstants.FLOAT_TYPE_NAME;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public static double getAsConstDouble(@Nonnull final ObjectInspector numberOI)
throws UDFArgumentException {
final String typeName = numberOI.getTypeName();
if(DOUBLE_TYPE_NAME.equals(typeName)) {
DoubleWritable v = getConstValue(numberOI);
return v.get();
} else if(FLOAT_TYPE_NAME.equals(typeName)) {
FloatWritable v = getConstValue(numberOI);
return v.get();
} else if(INT_TYPE_NAME.equals(typeName)) {
IntWritable v = getConstValue(numberOI);
return v.get();
} else if(BIGINT_TYPE_NAME.equals(typeName)) {
LongWritable v = getConstValue(numberOI);
return v.get();
} else if(SMALLINT_TYPE_NAME.equals(typeName)) {
ShortWritable v = getConstValue(numberOI);
return v.get();
} else if(TINYINT_TYPE_NAME.equals(typeName)) {
ByteWritable v = getConstValue(numberOI);
return v.get();
}
throw new UDFArgumentException("Unexpected argument type to cast as double: "
TypeInfoUtils.getTypeInfoFromObjectInspector(numberOI));
}
private final Comparator<E> comparator;
public BoundedPriorityQueue(int size, @Nonnull Comparator<E> comparator) {
import static hivemall.HivemallConstants.DOUBLE_TYPE_NAME;
import static hivemall.HivemallConstants.FLOAT_TYPE_NAME;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public static double getAsConstDouble(@Nonnull final ObjectInspector numberOI)
throws UDFArgumentException {
final String typeName = numberOI.getTypeName();
if(DOUBLE_TYPE_NAME.equals(typeName)) {
DoubleWritable v = getConstValue(numberOI);
return v.get();
} else if(FLOAT_TYPE_NAME.equals(typeName)) {
FloatWritable v = getConstValue(numberOI);
return v.get();
} else if(INT_TYPE_NAME.equals(typeName)) {
IntWritable v = getConstValue(numberOI);
return v.get();
} else if(BIGINT_TYPE_NAME.equals(typeName)) {
LongWritable v = getConstValue(numberOI);
return v.get();
} else if(SMALLINT_TYPE_NAME.equals(typeName)) {
ShortWritable v = getConstValue(numberOI);
return v.get();
} else if(TINYINT_TYPE_NAME.equals(typeName)) {
ByteWritable v = getConstValue(numberOI);
return v.get();
}
throw new UDFArgumentException("Unexpected argument type to cast as double: "
TypeInfoUtils.getTypeInfoFromObjectInspector(numberOI));
}
float d = (float) euclidDistance(ftvec1, ftvec2);
return new FloatWritable(d);
public static double euclidDistance(final List<String> ftvec1, final List<String> ftvec2) {
return Math.sqrt(d);
public static final String VERSION = "0.3.2-3";
@Description(name = "euclid_distance", value = "_FUNC_(ftvec1, ftvec2) - Returns the square root of the sum of the squared differences"
": sqrt(sum((x - y)^2))")
public static final String VERSION = "0.3.2-3";
@Description(name = "euclid_distance", value = "_FUNC_(ftvec1, ftvec2) - Returns the square root of the sum of the squared differences"
": sqrt(sum((x - y)^2))")
float d = (float) euclidDistance(ftvec1, ftvec2);
return new FloatWritable(d);
public static double euclidDistance(final List<String> ftvec1, final List<String> ftvec2) {
return Math.sqrt(d);
import org.apache.hadoop.hive.ql.metadata.HiveException;
public Text evaluate() throws HiveException {
public Text evaluate(@Nullable final String regexKey) throws HiveException {
throw new HiveException("MapredContext is not set");
throw new HiveException("JobConf is not set");
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
public static double[] asDoubleArray(@Nullable Object argObj, @Nonnull ListObjectInspector listOI, @Nonnull PrimitiveObjectInspector elemOI) {
if(argObj == null) {
return null;
}
final int length = listOI.getListLength(argObj);
final double[] ary = new double[length];
Object o = listOI.getListElement(argObj, i);
if(o == null) {
continue;
}
double d = PrimitiveObjectInspectorUtils.getDouble(o, elemOI);
ary[i] = d;
}
return ary;
}
@Nonnull
public static final int INDEX_NOT_FOUND = -1;
public static int indexOf(final int[] array, final int valueToFind, int startIndex, int endIndex) {
if(array == null) {
return INDEX_NOT_FOUND;
}
final int til = Math.min(endIndex, array.length);
if(startIndex < 0 || startIndex > til) {
}
if(valueToFind == array[i]) {
return i;
}
}
return INDEX_NOT_FOUND;
}
import hivemall.smile.vm.Operation;
public int opcodegen(ArrayList<String> scripts, int depth) {
int selfDepth=0;
StringBuilder buf = new StringBuilder();
if(trueChild == null && falseChild == null) {
buf.append("push ").append(output);
scripts.add(buf.toString());
buf.setLength(0);
buf.append("goto last");
scripts.add(buf.toString());
} else {
if(attributes[splitFeature].type == Attribute.Type.NOMINAL) {
buf.append("push ").append("x[").append(splitFeature).append("]");
scripts.add(buf.toString());
buf.setLength(0);
buf.append("push ").append(splitValue);
scripts.add(buf.toString());
buf.setLength(0);
buf.append("ifeq ");
scripts.add(buf.toString());
int trueDepth = trueChild.opcodegen(scripts, depth);
} else if(attributes[splitFeature].type == Attribute.Type.NUMERIC) {
buf.append("push ").append("x[").append(splitFeature).append("]");
scripts.add(buf.toString());
buf.setLength(0);
buf.append("push ").append(splitValue);
scripts.add(buf.toString());
buf.setLength(0);
buf.append("ifgr ");
scripts.add(buf.toString());
int trueDepth = trueChild.opcodegen(scripts, depth);
} else {
throw new IllegalStateException("Unsupported attribute type: "
attributes[splitFeature].type);
}
}
return selfDepth;
}
public ArrayList<String> predictOpCodegen() {
ArrayList<String> scripts = new ArrayList<String>();
root.opcodegen(scripts, 0);
scripts.add("call end");
return scripts;
}
int selfDepth = 0;
import hivemall.smile.vm.VMRuntimeException;
import java.io.IOException;
import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.WeakHashMap;
import javax.annotation.Nonnull;
import javax.script.Compilable;
import javax.script.CompiledScript;
import javax.script.ScriptEngine;
import javax.script.ScriptEngineManager;
List<String> scriptList = Arrays.asList(script.split("n"));
public Writable evaluate(@Nonnull final List<String> script, @Nonnull final double[] features, final boolean classification)
try {
sm.run(script, features);
} catch (VMRuntimeException e) {
throw new HiveException("failed to run StackMachine", e);
}
return new IntWritable(result.intValue());
import javax.annotation.Nonnull;
if(J < 2) {
if(J < 2) {
if(numFeatures <= 0) {
throw new IllegalArgumentException("Invalid number of sparse binary features: "
numFeatures);
if(J < 2) {
if(J < 2) {
if(numFeatures <= 0) {
if(trueChild == null && falseChild == null) {
if(attributes[splitFeature].type == Attribute.Type.NOMINAL) {
if(Math.equals(x[splitFeature], splitValue)) {
} else if(attributes[splitFeature].type == Attribute.Type.NUMERIC) {
if(x[splitFeature] <= splitValue) {
throw new IllegalStateException("Unsupported attribute type: "
attributes[splitFeature].type);
if(trueChild == null && falseChild == null) {
} else if(x[splitFeature] == (int) splitValue) {
int selfDepth = 0;
if(node.trueChild == null && node.falseChild == null) {
if(trueChild != null) {
if(falseChild != null) {
for(int s : samples) {
if(n <= S) {
if(M < p) {
synchronized(RegressionTree.class) {
if(split.splitScore > node.splitScore) {
for(Node split : MulticoreExecutor.run(tasks)) {
if(split.splitScore > node.splitScore) {
if(split.splitScore > node.splitScore) {
* node.
this.sum = sum;
if(attributes[j].type == Attribute.Type.NOMINAL) {
if(samples[i] > 0) {
if(tc == 0 || fc == 0) {
* split.output * split.output;
if(gain > split.splitScore) {
} else if(attributes[j].type == Attribute.Type.NUMERIC) {
for(int i : order[j]) {
if(samples[i] > 0) {
if(Double.isNaN(prevx) || x[i][j] == prevx) {
if(trueCount == 0 || falseCount == 0) {
* falseMean)
- n * split.output * split.output;
if(gain > split.splitScore) {
if(node.splitFeature < 0) {
if(attributes[node.splitFeature].type == Attribute.Type.NOMINAL) {
if(samples[i] > 0) {
if(x[i][node.splitFeature] == node.splitValue) {
falseSamples[i] = samples[i];
} else if(attributes[node.splitFeature].type == Attribute.Type.NUMERIC) {
if(samples[i] > 0) {
if(x[i][node.splitFeature] <= node.splitValue) {
falseSamples[i] = samples[i];
throw new IllegalStateException("Unsupported attribute type: "
attributes[node.splitFeature].type);
if(tc == 0 || fc == 0) {
if(tc > S && trueChild.findBestSplit()) {
if(nextSplits != null) {
if(fc > S && falseChild.findBestSplit()) {
if(nextSplits != null) {
if(node.trueChild != null || node.falseChild != null) {
if(samples[i] == 0) {
if((tc < 2) || (fc < 2)) {
* node.output * node.output;
if(gain > node.splitScore) {
if(node.splitFeature < 0) {
if(node.trueChild != null || node.falseChild != null) {
if(samples[i] > 0) {
if(x[i][node.splitFeature] == (int) node.splitValue) {
if(tc > S && trueChild.findBestSplit()) {
if(nextSplits != null) {
if(fc > S && falseChild.findBestSplit()) {
if(nextSplits != null) {
if(node.trueChild == null && node.falseChild == null) {
if(trueChild != null) {
if(falseChild != null) {
this(attributes, x, y, J, null, null, null);
if(x.length != y.length) {
if(J < 2) {
if(attributes == null) {
if(order != null) {
if(attributes[j] instanceof NumericAttribute) {
if(samples == null) {
if(trainRoot.findBestSplit()) {
if(node == null) {
if(output != null) {
if(x.length != y.length) {
if(M <= 0 || M > x[0].length) {
throw new IllegalArgumentException("Invalid number of variables to split on at a node of the tree: "
M);
if(S <= 0) {
throw new IllegalArgumentException("Invalid mimum number of instances in leaf nodes: "
S);
if(samples == null) {
if(attributes == null) {
if(trainRoot.findBestSplit()) {
this(numFeatures, x, y, J, null, null);
if(x.length != y.length) {
if(J < 2) {
if(samples == null) {
if(trainRoot.findBestSplit()) {
if(node == null) {
if(output != null) {
import javax.annotation.Nonnull;
public final class Operation {
final OperationEnum op;
final String operand;
Operation(@Nonnull OperationEnum op, String operand) {
Operation(@Nonnull OperationEnum op) {
import hivemall.utils.lang.StringUtils;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Stack;
public final class StackMachine {
private int IP;
private int SP = 0;
private int codeLength;
public StackMachine() {}
public void run(List<String> script, double[] features) throws VMRuntimeException {
for(String line : script) {
if(ops.length == 2) {
} else {
this.valuesMap = new HashMap<String, Double>();
this.jumpMap = new HashMap<String, Integer>();
int size = script.size();
this.codeLength = size - 1;
this.done = new boolean[size];
public void execute(int entryPoint) throws VMRuntimeException {
while(IP < code.size()) {
if(done[IP]) {
if(!executeOperation(currentOperation)) {
public void bind(final double[] features) {
final StringBuilder buf = new StringBuilder();
String bindKey = buf.append("x[").append(String.valueOf(i)).append("]").toString();
valuesMap.put(bindKey, features[i]);
StringUtils.clear(buf);
private boolean executeOperation(Operation currentOperation) throws VMRuntimeException {
if(IP < 0)
if(isInt(currentOperation.operand))
if(candidateIP < 0) {
if(a == b || Math.abs(a - b) <= Math.min(absa, absb) * 2.2204460492503131e-16)
if(isInt(currentOperation.operand))
if(upper > lower)
if(isInt(currentOperation.operand))
if(isDouble(currentOperation.operand))
else {
Double v = valuesMap.get(currentOperation.operand);
if(v == null) {
throw new VMRuntimeException("value is not binded: "
currentOperation.operand);
}
push(v);
}
throw new IllegalArgumentException("Machine code has wrong opcode :"
currentOperation.op);
if(name.equals("end"))
private static boolean isInt(String i) {
private static boolean isDouble(String i) {
import javax.annotation.Nonnull;
public static void clear(@Nonnull final StringBuilder buf) {
buf.setLength(0);
}
@Description(name = "js_tree_predict", value = "_FUNC_(string script, array<double> features [, const boolean classification]) - Returns a prediction result of a random forest")
public final class TreePredictByJavascriptUDF extends GenericUDF {
throw new UDFArgumentException("js_tree_predict takes 2 or 3 arguments");
@Description(name = "vm_tree_predict", value = "_FUNC_(string script, array<double> features [, const boolean classification]) - Returns a prediction result of a random forest")
public final class TreePredictByStackMachineUDF extends GenericUDF {
throw new UDFArgumentException("vm_tree_predict takes 2 or 3 arguments");
final StackMachine vm = new StackMachine();
vm.run(script, features);
Double result = vm.getResult();
public void close() throws IOException {}
final StringBuilder buf = new StringBuilder();
import java.util.Map;
import javax.annotation.Nullable;
private final Map<String, Double> valuesMap;
private final Map<String, Integer> jumpMap;
private final List<Operation> code;
private final Stack<Double> programStack;
private int SP;
private Double result;
public StackMachine() {
this.valuesMap = new HashMap<String, Double>();
this.jumpMap = new HashMap<String, Integer>();
this.code = new ArrayList<Operation>();
this.programStack = new Stack<Double>();
this.SP = 0;
this.result = null;
}
public void run(List<String> scripts, double[] features) throws VMRuntimeException {
for(String line : scripts) {
int size = scripts.size();
throw new VMRuntimeException("There is a infinite loop in the Machine code.");
Operation currentOperation = code.get(IP);
String bindKey = buf.append("x[").append(i).append("]").toString();
@Nullable
if(IP < 0) {
}
case GOTO: {
if(isInt(currentOperation.operand)) {
} else {
}
}
case CALL: {
}
case IFEQ: {
if(a == b || Math.abs(a - b) <= Math.min(absa, absb) * 2.2204460492503131e-16) {
if(isInt(currentOperation.operand)) {
} else {
}
} else {
}
}
case IFGR: {
if(upper > lower) {
if(isInt(currentOperation.operand)) {
} else {
}
} else {
}
}
case POP: {
}
case PUSH: {
}
throw new VMRuntimeException("Machine code has wrong opcode :"
private void evaluateBuiltinByName(String name) throws VMRuntimeException {
if(name.equals("end")) {
this.result = pop();
} else {
}
public int opcodegen(final List<String> scripts, int depth) {
final StringBuilder buf = new StringBuilder();
public int opcodegen(final List<String> scripts, int depth) {
if(smile.math.Math.equals(a, b)) {
import javax.annotation.Nonnull;
import java.util.ArrayList;
import java.util.List;
import java.util.PriorityQueue;
import java.util.concurrent.Callable;
buf.append("ifle ");
buf.append("ifle ");
case IFGE: {
if(upper >= lower) {
IP;
} else {
}
break;
}
case IFGT: {
double lower = pop();
double upper = pop();
if(upper > lower) {
} else {
if(isInt(currentOperation.operand)) {
IP = Integer.parseInt(currentOperation.operand);
} else {
IP = jumpMap.get(currentOperation.operand);
}
}
break;
}
case IFLE: {
double lower = pop();
double upper = pop();
if(upper <= lower) {
IP;
} else {
if(isInt(currentOperation.operand)) {
IP = Integer.parseInt(currentOperation.operand);
} else {
IP = jumpMap.get(currentOperation.operand);
}
}
break;
}
case IFLT: {
double lower = pop();
double upper = pop();
if(upper < lower) {
IP;
} else {
if(isInt(currentOperation.operand)) {
IP = Integer.parseInt(currentOperation.operand);
} else {
IP = jumpMap.get(currentOperation.operand);
}
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
public final class SmileExtUtils extends smile.util.SmileUtils {
private SmileExtUtils() {}
public static Attribute[] resolveAttributes(@Nullable final String opt)
throws UDFArgumentException {
import hivemall.smile.SmileExtUtils;
import hivemall.smile.SmileTaskExecutor;
import hivemall.utils.lang.Primitives;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hive.ql.exec.MapredContext;
import org.apache.hadoop.hive.ql.exec.MapredContextAccessor;
private static final Log logger = LogFactory.getLog(RandomForestClassifierUDTF.class);
private List<double[]> featuresList;
private int numTrees;
private int numVars;
private long seed;
private OutputType outputType;
opts.addOption("trees", "num_trees", true, "The number of trees for each task [default: 50]");
opts.addOption("vars", "num_variables", true, "The number of random selected features [default: floor(sqrt(dim))]");
opts.addOption("seed", true, "seed value in long [default: -1 (random)]");
opts.addOption("attrs", "attribute_types", true, "Comma separated attribute types "
opts.addOption("output", "output_type", true, "The output type (opscode/vm or javascript/js) [default: opscode]");
Attribute[] attrs = null;
long seed = -1L;
String output = "opscode";
T = Primitives.parseInt(cl.getOptionValue("num_trees"), T);
if(T < 1) {
}
M = Primitives.parseInt(cl.getOptionValue("num_variables"), M);
attrs = SmileExtUtils.resolveAttributes(cl.getOptionValue("attribute_types"));
seed = Primitives.parseLong(cl.getOptionValue("seed"), seed);
output = cl.getOptionValue("output", output);
this.numTrees = T;
this.numVars = M;
this.seed = seed;
this.attributes = attrs;
this.outputType = OutputType.resolve(output);
public enum OutputType {
opscode, javascript;
public static OutputType resolve(String name) {
if("opscode".equalsIgnoreCase(name) || "vm".equalsIgnoreCase(name)) {
return opscode;
} else if("javascript".equalsIgnoreCase(name) || "js".equalsIgnoreCase(name)) {
return javascript;
}
}
}
fieldNames.add("oob_errors");
fieldNames.add("oob_tests");
train(x, y, attributes, numTrees, numVars, seed);
private void train(@Nonnull final double[][] x, @Nonnull final int[] y, final Attribute[] attrs, final int numTrees, final int numVars, final long seed)
int[] labels = SmileExtUtils.classLables(y);
Attribute[] attributes = SmileExtUtils.attributeTypes(attrs, x);
int numInputVars = (numVars == -1) ? (int) Math.floor(Math.sqrt(x[0].length)) : numVars;
int[][] order = SmileExtUtils.sort(attributes, x);
AtomicInteger remainingTasks = new AtomicInteger(numTrees);
tasks.add(new TrainingTask(this, attributes, x, y, numInputVars, order, prediction, seed
i, remainingTasks));
MapredContext mapredContext = MapredContextAccessor.get();
final SmileTaskExecutor executor = new SmileTaskExecutor(mapredContext);
executor.run(tasks);
} finally {
executor.shotdown();
private static final class TrainingTask implements Callable<Integer> {
private final int numVars;
private final long seed;
TrainingTask(RandomForestClassifierUDTF udtf, Attribute[] attributes, double[][] x, int[] y, int M, int[][] order, int[][] prediction, long seed, AtomicInteger remainingTasks) {
this.numVars = M;
this.seed = seed;
public Integer call() throws HiveException {
long s = (this.seed == -1L) ? Thread.currentThread().getId()
* System.currentTimeMillis() : this.seed;
final Random random = new Random(s);
final int n = x.length;
DecisionTree tree = new DecisionTree(attributes, x, y, numVars, samples, order);
final int p = tree.predict(x[i]);
String model = getModel(tree, udtf.outputType);
return Integer.valueOf(remain);
private String getModel(DecisionTree tree, OutputType outputType) {
final String model;
switch (outputType) {
case opscode: {
model = tree.predictCodegen();
break;
}
case javascript: {
model = tree.predictCodegen();
break;
}
default: {
". Use javascript for the output instead");
model = tree.predictCodegen();
break;
}
}
return model;
}
import javax.annotation.Nonnull;
public static void clear(@Nonnull final StringBuilder buf) {
buf.setLength(0);
}
public int opcodegen(final List<String> scripts, int depth) {
int selfDepth = 0;
final StringBuilder buf = new StringBuilder();
if(trueChild == null && falseChild == null) {
buf.append("push ").append(output);
scripts.add(buf.toString());
buf.setLength(0);
buf.append("goto last");
scripts.add(buf.toString());
} else {
if(attributes[splitFeature].type == Attribute.Type.NOMINAL) {
buf.append("push ").append("x[").append(splitFeature).append("]");
scripts.add(buf.toString());
buf.setLength(0);
buf.append("push ").append(splitValue);
scripts.add(buf.toString());
buf.setLength(0);
buf.append("ifeq ");
scripts.add(buf.toString());
int trueDepth = trueChild.opcodegen(scripts, depth);
} else if(attributes[splitFeature].type == Attribute.Type.NUMERIC) {
buf.append("push ").append("x[").append(splitFeature).append("]");
scripts.add(buf.toString());
buf.setLength(0);
buf.append("push ").append(splitValue);
scripts.add(buf.toString());
buf.setLength(0);
buf.append("ifle ");
scripts.add(buf.toString());
int trueDepth = trueChild.opcodegen(scripts, depth);
} else {
throw new IllegalStateException("Unsupported attribute type: "
attributes[splitFeature].type);
}
}
return selfDepth;
}
public ArrayList<String> predictOpCodegen() {
ArrayList<String> scripts = new ArrayList<String>();
root.opcodegen(scripts, 0);
scripts.add("call end");
return scripts;
}
@Description(name = "js_tree_predict", value = "_FUNC_(string script, array<double> features [, const boolean classification]) - Returns a prediction result of a random forest")
public final class TreePredictByJavascriptUDF extends GenericUDF {
throw new UDFArgumentException("js_tree_predict takes 2 or 3 arguments");
import hivemall.utils.lang.StringUtils;
public String predictOpCodegen(String sep) {
List<String> opslist = new ArrayList<String>();
root.opcodegen(opslist, 0);
opslist.add("call end");
String scripts = StringUtils.concat(opslist, sep);
import hivemall.smile.utils.SmileExtUtils;
import hivemall.smile.utils.SmileTaskExecutor;
import hivemall.smile.vm.StackMachine;
model = tree.predictOpCodegen(StackMachine.SEP);
} catch (ScriptException se) {
} catch (Throwable e) {
if(!(result instanceof Number)) {
}
Number casted = (Number) result;
Number casted = (Number) result;
return evaluate(script, features, classification);
public static Writable evaluate(@Nonnull final String scripts, @Nonnull final double[] features, final boolean classification)
vm.run(scripts, features);
} catch (VMRuntimeException vme) {
throw new HiveException("failed to run StackMachine", vme);
} catch (Throwable e) {
package hivemall.smile.utils;
package hivemall.smile.utils;
import javax.annotation.Nullable;
public Operation(@Nonnull OperationEnum op) {
this(op, null);
public Operation(@Nonnull OperationEnum op, @Nullable String operand) {
this.operand = operand;
import java.util.Arrays;
import javax.annotation.Nonnull;
public static final String SEP = "; ";
@Nonnull
@Nonnull
@Nonnull
@Nonnull
@SuppressWarnings("unused")
public void run(@Nonnull String scripts, @Nonnull double[] features) throws VMRuntimeException {
List<String> opslist = Arrays.asList(scripts.split(SEP));
run(opslist, features);
}
public void run(@Nonnull List<String> opslist, @Nonnull double[] features)
throws VMRuntimeException {
for(String line : opslist) {
int size = opslist.size();
private void execute(int entryPoint) throws VMRuntimeException {
private void bind(final double[] features) {
if(StringUtils.isInt(currentOperation.operand)) {
if(StringUtils.isInt(currentOperation.operand)) {
if(StringUtils.isInt(currentOperation.operand)) {
if(StringUtils.isInt(currentOperation.operand)) {
if(StringUtils.isInt(currentOperation.operand)) {
if(StringUtils.isInt(currentOperation.operand)) {
if(StringUtils.isDouble(currentOperation.operand))
import java.util.List;
public static boolean isInt(@Nonnull final String i) {
try {
Integer.parseInt(i);
return true;
} catch (NumberFormatException nfe) {
return false;
}
}
public static boolean isDouble(@Nonnull final String i) {
try {
Double.parseDouble(i);
return true;
} catch (NumberFormatException nfe) {
return false;
}
}
public static String concat(@Nonnull final List<String> list, @Nonnull final String sep) {
final StringBuilder buf = new StringBuilder(128);
for(String s : list) {
if(s == null) {
continue;
}
buf.append(s);
buf.append(sep);
}
return buf.toString();
}
import hivemall.utils.lang.StringUtils;
import java.util.ArrayList;
import java.util.List;
import java.util.PriorityQueue;
import java.util.concurrent.Callable;
import javax.annotation.Nonnull;
public String predictOpCodegen() {
List<String> opslist = new ArrayList<String>();
root.opcodegen(opslist, 0);
opslist.add("call end");
String scripts = StringUtils.concat(opslist, "\n");
protected Attribute[] attributes;
public DecisionTree(Attribute[] attributes, double[][] x, int[] y, int J, int[] samples, int[][] order, SplitRule rule) {
public DecisionTree(Attribute[] attributes, double[][] x, int[] y, int M, int[] samples, int[][] order) {
import hivemall.smile.utils.SmileExtUtils;
import javax.annotation.Nullable;
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y, int J) {
this(attributes, x, y, x[0].length, J, null, null, SplitRule.GINI);
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y, int M, int J, @Nullable int[] samples, @Nullable int[][] order, @Nonnull SplitRule rule) {
if(J < 2) {
this.attributes = SmileExtUtils.attributeTypes(attributes, x);
this.rule = rule;
this.order = (order == null) ? SmileExtUtils.sort(attributes, x) : order;
this.importance = new double[attributes.length];
if(samples == null) {
samples = new int[n];
samples[i] = 1;
}
} else {
}
this.root = new Node(Math.whichMax(count));
if(J == Integer.MAX_VALUE) {
if(trainRoot.findBestSplit()) {
trainRoot.split(null);
}
} else {
PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
if(trainRoot.findBestSplit()) {
nextSplits.add(trainRoot);
}
TrainNode node = nextSplits.poll();
if(node == null) {
break;
}
}
import hivemall.smile.classification.DecisionTree.SplitRule;
import javax.annotation.Nullable;
"Returns a relation consists of <string pred_model, double[] var_importance, int oob_errors, int oob_tests>")
private int maxLeafNodes;
private SplitRule splitRule;
opts.addOption("leafs", "max_leaf_nodes", true, "The maximum number of leaf nodes [default: Integer.MAX_VALUE]");
opts.addOption("split", "split_rule", true, "Split algorithm [default: GINI, ENTROPY]");
int T = 50, M = -1, J = Integer.MAX_VALUE;
SplitRule splitRule = SplitRule.GINI;
J = Primitives.parseInt(cl.getOptionValue("max_leaf_nodes"), J);
attrs = SmileExtUtils.resolveAttributes(cl.getOptionValue("attribute_types"));
splitRule = SmileExtUtils.resolveSplitRule(cl.getOptionValue("split_rule", "GINI"));
this.maxLeafNodes = J;
this.splitRule = splitRule;
train(x, y, attributes, splitRule, numTrees, numVars, maxLeafNodes, seed);
private void train(@Nonnull final double[][] x, @Nonnull final int[] y, @Nullable final Attribute[] attrs, @Nonnull final SplitRule splitRule, final int numTrees, final int numVars, final int maxLeafs, final long seed)
tasks.add(new TrainingTask(this, attributes, x, y, numInputVars, maxLeafs, order, prediction, splitRule, seed
private final SplitRule splitRule;
private final int numLeafs;
TrainingTask(RandomForestClassifierUDTF udtf, Attribute[] attributes, double[][] x, int[] y, int M, int J, int[][] order, int[][] prediction, SplitRule splitRule, long seed, AtomicInteger remainingTasks) {
this.splitRule = splitRule;
this.numLeafs = J;
DecisionTree tree = new DecisionTree(attributes, x, y, numVars, numLeafs, samples, order, splitRule);
private String getModel(@Nonnull final DecisionTree tree, @Nonnull final OutputType outputType) {
import hivemall.smile.utils.SmileExtUtils;
import javax.annotation.Nullable;
private Attribute[] attributes;
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int J) {
this(attributes, x, y, x[0].length, 5, J, null, null);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int M, int S, int J, @Nullable int[][] order, @Nullable int[] samples) {
this(attributes, x, y, M, S, J, order, samples, null);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int M, int S, int J, @Nullable int[][] order, @Nullable int[] samples, @Nullable NodeOutput output) {
if(M <= 0 || M > x[0].length) {
throw new IllegalArgumentException("Invalid number of variables to split on at a node of the tree: "
M);
}
if(S <= 0) {
throw new IllegalArgumentException("Invalid mimum number of instances in leaf nodes: "
S);
}
this.attributes = SmileExtUtils.attributeTypes(attributes, x);
this.M = M;
this.S = S;
this.order = (order == null) ? SmileExtUtils.sort(attributes, x) : order;
this.importance = new double[attributes.length];
this.root = new Node(sum / n);
TrainNode trainRoot = new TrainNode(root, x, y, samples);
if(J == Integer.MAX_VALUE) {
if(trainRoot.findBestSplit()) {
trainRoot.split(null);
} else {
PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
if(trainRoot.findBestSplit()) {
nextSplits.add(trainRoot);
}
TrainNode node = nextSplits.poll();
if(node == null) {
break;
}
}
public String predictOpCodegen(@Nonnull String sep) {
String scripts = StringUtils.concat(opslist, sep);
package hivemall.smile.tools;
package hivemall.smile.tools;
import hivemall.smile.classification.DecisionTree.SplitRule;
@Nonnull
@Nonnull
public static SplitRule resolveSplitRule(@Nullable String ruleName) {
if("gini".equalsIgnoreCase(ruleName)) {
return SplitRule.GINI;
} else if("entropy".equalsIgnoreCase(ruleName)) {
return SplitRule.ENTROPY;
} else {
return SplitRule.GINI;
}
}
import java.io.PrintWriter;
import java.io.StringWriter;
import org.apache.commons.cli.HelpFormatter;
import org.apache.hadoop.hive.ql.exec.Description;
opts.addOption("help", false, "Show function help");
CommandLine cl = CommandLineUtils.parseOptions(args, opts);
if(cl.hasOption("help")) {
Description funcDesc = getClass().getAnnotation(Description.class);
final String cmdLineSyntax;
if(funcDesc == null) {
cmdLineSyntax = getClass().getSimpleName();
} else {
String funcName = funcDesc.name();
cmdLineSyntax = funcName == null ? getClass().getSimpleName()
: funcDesc.value().replace("_FUNC_", funcDesc.name());
}
StringWriter sw = new StringWriter();
sw.write('\n');
PrintWriter pw = new PrintWriter(sw);
HelpFormatter formatter = new HelpFormatter();
formatter.printHelp(pw, HelpFormatter.DEFAULT_WIDTH, cmdLineSyntax, null, opts, HelpFormatter.DEFAULT_LEFT_PAD, HelpFormatter.DEFAULT_DESC_PAD, null, true);
pw.flush();
String helpMsg = sw.toString();
throw new UDFArgumentException(helpMsg);
}
return cl;
public static boolean isNumberOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if(argOI.getCategory() != Category.PRIMITIVE) {
return false;
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case SHORT:
case INT:
case LONG:
case FLOAT:
case DOUBLE:
return true;
default:
return false;
}
}
public int increment(E key) {
return increment(key, 1);
public int increment(E key, int amount) {
return 0;
int old = count.intValue();
return old;
package hivemall.ftvec.conv;
package hivemall.ftvec.conv;
package hivemall.ftvec.conv;
package hivemall.ftvec.trans;
package hivemall.ftvec.trans;
package hivemall.ftvec.trans;
case BYTE:
case TIMESTAMP:
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
@Description(name = "quantify", value = "_FUNC_(boolean outout, col1, col2, ...) - Returns an identified features")
private BooleanObjectInspector boolOI;
if(size < 2) {
throw new UDFArgumentException("quantified_features takes at least two arguments: "
size);
}
this.boolOI = HiveUtils.asBooleanOI(argOIs[0]);
int outputSize = size - 1;
this.forwardObjs = new Object[outputSize];
this.forwardIntObjs = new IntWritable[outputSize];
this.identifiers = new Identifier[outputSize];
final ArrayList<String> fieldNames = new ArrayList<String>(outputSize);
final ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(outputSize);
if(HiveUtils.isNumberOI(argOI)) {
fieldOIs.add(argOI);
boolean outputRow = boolOI.get(args[0]);
if(outputRow) {
final Object[] forwardObjs = this.forwardObjs;
Identifier<String> identifier = identifiers[i];
if(identifier == null) {
forwardObjs[i] = arg;
if(arg == null) {
forwardObjs[i] = null;
} else {
String k = arg.toString();
int id = identifier.valueOf(k);
IntWritable o = forwardIntObjs[i];
o.set(id);
forwardObjs[i] = o;
}
forward(forwardObjs);
Identifier<String> identifier = identifiers[i];
if(identifier != null) {
if(arg != null) {
String k = arg.toString();
identifier.valueOf(k);
}
}
}
this.boolOI = null;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
@Description(name = "quantified_features", value = "_FUNC_(boolean output, col1, col2, ...) - Returns an identified features in a dence array<double>")
private BooleanObjectInspector boolOI;
if(size < 2) {
throw new UDFArgumentException("quantified_features takes at least two arguments: "
size);
}
this.boolOI = HiveUtils.asBooleanOI(argOIs[0]);
int outputSize = size - 1;
this.doubleOIs = new PrimitiveObjectInspector[outputSize];
this.columnValues = new DoubleWritable[outputSize];
this.identifiers = new Identifier[outputSize];
if(HiveUtils.isNumberOI(argOI)) {
doubleOIs[i] = HiveUtils.asDoubleCompatibleOI(argOI);
ArrayList<String> fieldNames = new ArrayList<String>(outputSize);
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(outputSize);
boolean outputRow = boolOI.get(args[0]);
if(outputRow) {
final DoubleWritable[] values = this.columnValues;
Identifier<String> identifier = identifiers[i];
if(identifier == null) {
double v = PrimitiveObjectInspectorUtils.getDouble(arg, doubleOIs[i]);
values[i].set(v);
if(arg == null) {
} else {
String k = arg.toString();
int id = identifier.valueOf(k);
values[i].set(id);
}
forward(fowardObjs);
Identifier<String> identifier = identifiers[i];
if(identifier != null) {
if(arg != null) {
String k = arg.toString();
identifier.valueOf(k);
}
}
}
this.boolOI = null;
this.fowardObjs = null;
import java.util.Arrays;
if(attributes.length != x[0].length) {
throw new IllegalArgumentException("-attrs option is invliad: "
Arrays.toString(attributes));
}
if(numExamples > 0) {
double[][] x = featuresList.toArray(new double[numExamples][]);
this.featuresList = null;
double[] y = targets.toArray();
this.targets = null;
train(x, y, attributes, numTrees, numVars, nodeCapacity, maxLeafNodes, seed);
}
RegressionTree tree = new RegressionTree(attributes, x, y, numVars, nodeCapacity, numLeafs, order, samples);
import java.util.Arrays;
if(attributes.length != x[0].length) {
throw new IllegalArgumentException("-attrs option is invliad: "
Arrays.toString(attributes));
}
import hivemall.smile.data.NominalAttribute2;
} else {
int size = attributes.length;
Attribute attr = attributes[j];
if(attr instanceof NominalAttribute2) {
int max_x = 0;
int x_ij = (int) x[i][j];
if(x_ij > max_x) {
max_x = x_ij;
}
}
}
}
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
public static BooleanObjectInspector asBooleanOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentException {
if(!BOOLEAN_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (BooleanObjectInspector) argOI;
}
Node split = findBestSplit(n, count, falseCount, impurity, variables[j]);
if(split.splitScore > node.splitScore) {
node.splitFeature = split.splitFeature;
node.splitValue = split.splitValue;
node.splitScore = split.splitScore;
node.trueChildOutput = split.trueChildOutput;
node.falseChildOutput = split.falseChildOutput;
Node split = findBestSplit(n, sum, variables[j]);
if(split.splitScore > node.splitScore) {
node.splitFeature = split.splitFeature;
node.splitValue = split.splitValue;
node.splitScore = split.splitScore;
node.trueChildOutput = split.trueChildOutput;
node.falseChildOutput = split.falseChildOutput;
import hivemall.utils.collections.IntArrayList;
import java.util.Arrays;
import java.util.List;
public boolean iterate(Integer k) {
if(k == null) {
partial.increment(k);
public Result terminate() {
return new Result(partial);
}
}
public static final class Result {
@SuppressWarnings("unused")
private Integer predicted_class;
@SuppressWarnings("unused")
private Double probability;
@SuppressWarnings("unused")
private List<Double> probabilities;
Result(Counter<Integer> partial) {
final Map<Integer, Integer> counts = partial.getMap();
int size = counts.size();
assert (size > 0) : size;
IntArrayList keyList = new IntArrayList(size);
long totalCnt = 0L;
Integer maxKey = null;
int maxCnt = Integer.MIN_VALUE;
for(Map.Entry<Integer, Integer> e : counts.entrySet()) {
Integer key = e.getKey();
keyList.add(key);
int cnt = e.getValue().intValue();
if(cnt >= maxCnt) {
maxCnt = cnt;
maxKey = key;
}
int[] keyArray = keyList.toArray();
Arrays.sort(keyArray);
int last = keyArray[keyArray.length - 1];
double totalCnt_d = (double) totalCnt;
final Integer cnt = counts.get(Integer.valueOf(i));
if(cnt == null) {
probabilities[i] = Double.valueOf(0d);
} else {
probabilities[i] = Double.valueOf(cnt.intValue() / totalCnt_d);
}
}
this.predicted_class = maxKey;
this.probability = Double.valueOf(maxCnt / totalCnt_d);
this.probabilities = Arrays.asList(probabilities);
private String prevScripts;
private StackMachine prevVM;
String scripts = arg0.toString();
return evaluate(scripts, features, classification);
public Writable evaluate(@Nonnull final String scripts, @Nonnull final double[] features, final boolean classification)
final StackMachine vm;
if(scripts.equals(prevScripts)) {
vm = prevVM;
} else {
vm = new StackMachine();
try {
vm.compile(scripts);
} catch (VMRuntimeException e) {
throw new HiveException("failed to compile StackMachine", e);
}
this.prevScripts = scripts;
this.prevVM = vm;
try {
vm.eval(features);
} catch (VMRuntimeException vme) {
throw new HiveException("failed to eval StackMachine", vme);
} catch (Throwable e) {
throw new HiveException("failed to eval StackMachine", e);
}
Double result = vm.getResult();
public void close() throws IOException {
this.featureElemOI = null;
this.featureListOI = null;
this.prevScripts = null;
this.prevVM = null;
}
private final List<Operation> code;
@Nonnull
this.code = new ArrayList<Operation>();
compile(scripts);
eval(features);
compile(opslist);
eval(features);
}
public void compile(@Nonnull String scripts) throws VMRuntimeException {
List<String> opslist = Arrays.asList(scripts.split(SEP));
compile(opslist);
}
public void compile(@Nonnull List<String> opslist) throws VMRuntimeException {
}
public void eval(final double[] features) throws VMRuntimeException {
init();
private void init() {
valuesMap.clear();
jumpMap.clear();
programStack.clear();
this.SP = 0;
this.result = null;
Arrays.fill(done, false);
}
private void bind(final double[] features) {
final StringBuilder buf = new StringBuilder();
String bindKey = buf.append("x[").append(i).append("]").toString();
valuesMap.put(bindKey, features[i]);
StringUtils.clear(buf);
}
}
if(StringUtils.isDouble(currentOperation.operand)) {
} else {
private ScriptEngine scriptEngine = null;
private Compilable compilableEngine = null;
private Map<String, CompiledScript> cache = null;
if(scriptEngine == null) {
ScriptEngineManager manager = new ScriptEngineManager();
ScriptEngine engine = manager.getEngineByExtension("js");
if(!(engine instanceof Compilable)) {
throw new UDFArgumentException("ScriptEngine was not compilable: "
engine.getFactory().getEngineVersion());
}
this.scriptEngine = engine;
this.compilableEngine = (Compilable) engine;
this.cache = new WeakHashMap<String, CompiledScript>();
}
private String prevScripts = null;
private StackMachine prevVM = null;
import hivemall.utils.math.MathUtils;
import smile.math.Random;
private final Attribute[] attributes;
private final double[] importance;
private final Node root;
private final SplitRule rule;
private final int k;
private final int M;
private final int[][] order;
private final Random rnd;
SmileExtUtils.permutate(variables, rnd);
}
Node split = findBestSplit(n, count, falseCount, impurity, variables[j]);
if(split.splitScore > node.splitScore) {
node.splitFeature = split.splitFeature;
node.splitValue = split.splitValue;
node.splitScore = split.splitScore;
node.trueChildOutput = split.trueChildOutput;
node.falseChildOutput = split.falseChildOutput;
this(attributes, x, y, x[0].length, J, null, null, SplitRule.GINI, SmileExtUtils.generateSeed());
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y, int M, int J, @Nullable int[] samples, @Nullable int[][] order, @Nonnull SplitRule rule, long seed) {
this.rnd = new Random(seed);
long s = (this.seed == -1L) ? SmileExtUtils.generateSeed() : this.seed;
DecisionTree tree = new DecisionTree(attributes, x, y, numVars, numLeafs, samples, order, splitRule, s);
RegressionTree tree = new RegressionTree(attributes, x, y, numVars, nodeCapacity, numLeafs, order, samples, s);
import smile.math.Random;
import smile.util.SmileUtils;
private final Attribute[] attributes;
private final double[] importance;
private final Node root;
private final int S;
private final int J;
private final int M;
private final int[][] order;
private final Random rnd;
SmileExtUtils.permutate(variables, rnd);
}
Node split = findBestSplit(n, sum, variables[j]);
if(split.splitScore > node.splitScore) {
node.splitFeature = split.splitFeature;
node.splitValue = split.splitValue;
node.splitScore = split.splitScore;
node.trueChildOutput = split.trueChildOutput;
node.falseChildOutput = split.falseChildOutput;
this(attributes, x, y, x[0].length, 5, J, null, null, SmileExtUtils.generateSeed());
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int M, int S, int J, @Nullable int[][] order, @Nullable int[] samples, long seed) {
this(attributes, x, y, M, S, J, order, samples, null, seed);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int M, int S, int J, @Nullable int[][] order, @Nullable int[] samples, @Nullable NodeOutput output, long seed) {
this.rnd = new Random(seed);
public static long generateSeed() {
return Thread.currentThread().getId() * System.currentTimeMillis();
}
public static void permutate(@Nonnull final int[] x, @Nonnull final smile.math.Random rnd) {
Math.swap(x, i, j);
}
}
import java.io.InputStream;
import java.io.OutputStream;
import javax.annotation.Nonnull;
private static final int DEFAULT_BUFFER_SIZE = 1024 * 4;
public static String toString(@Nonnull final InputStream input) throws IOException {
FastMultiByteArrayOutputStream output = new FastMultiByteArrayOutputStream();
copy(input, output);
return output.toString();
}
public static int copy(@Nonnull final InputStream input, @Nonnull final OutputStream output)
throws IOException {
final byte[] buffer = new byte[DEFAULT_BUFFER_SIZE];
int count = 0;
int n = 0;
while(-1 != (n = input.read(buffer))) {
output.write(buffer, 0, n);
}
return count;
}
buf.append("ifne ");
case IFEQ: {
double a = pop();
double b = pop();
if(a==b) {
if(StringUtils.isInt(currentOperation.operand)) {
IP = Integer.parseInt(currentOperation.operand);
} else {
IP = jumpMap.get(currentOperation.operand);
}
} else {
IP;
}
break;
}
IP;
} else {
IP;
} else {
buf.append("ifne ");
case IFEQ: {
if(a==b) {
IP;
} else {
}
break;
}
double a = pop();
double b = pop();
if(smile.math.Math.equals(a, b)) {
} else {
if(StringUtils.isInt(currentOperation.operand)) {
IP = Integer.parseInt(currentOperation.operand);
} else {
IP = jumpMap.get(currentOperation.operand);
}
private Integer label;
this.label = maxKey;
if(logger.isInfoEnabled()) {
}
int pred = smile.math.Math.whichMax(prediction[i]);
final smile.math.Random rand = new smile.math.Random(s);
int[] samples = new int[n];
if(logger.isInfoEnabled()) {
}
final Random rand = new Random(s);
int[] samples = new int[n];
public static void shuffle(@Nonnull final double[][] x, final int[] y, @Nonnull final smile.math.Random rnd) {
if(x.length != y.length) {
}
for(int i = x.length; i > 1; i--) {
int j = rnd.nextInt(i);
swap(x, i - 1, j);
Math.swap(y, i - 1, j);
}
}
public static void shuffle(@Nonnull final double[][] x, final double[] y, @Nonnull final smile.math.Random rnd) {
if(x.length != y.length) {
}
for(int i = x.length; i > 1; i--) {
int j = rnd.nextInt(i);
swap(x, i - 1, j);
Math.swap(y, i - 1, j);
}
}
public static void swap(final double[][] x, final int i, final int j) {
double[] s = x[i];
x[i] = x[j];
x[j] = s;
}
import javax.annotation.Nonnull;
public static <T> void shuffle(@Nonnull final T[] array, final int size, @Nonnull final Random rnd) {
public static void shuffle(@Nonnull final int[] array, @Nonnull final Random rnd) {
for(int i = array.length; i > 1; i--) {
int randomPosition = rnd.nextInt(i);
swap(array, i - 1, randomPosition);
}
}
public static void swap(@Nonnull final Object[] arr, final int i, final int j) {
public static void swap(@Nonnull final int[] arr, final int i, final int j) {
int tmp = arr[i];
arr[i] = arr[j];
arr[j] = tmp;
}
SmileExtUtils.shuffle(variables, rnd);
SmileExtUtils.shuffle(x, y, seed);
SmileExtUtils.shuffle(x, y, seed);
long s = (this.seed == -1L) ? SmileExtUtils.generateSeed() : this.seed;
SmileExtUtils.shuffle(variables, rnd);
public static void shuffle(@Nonnull final int[] x, @Nonnull final smile.math.Random rnd) {
for(int i = x.length; i > 1; i--) {
int j = rnd.nextInt(i);
Math.swap(x, i - 1, j);
public static void shuffle(@Nonnull final double[][] x, final int[] y, @Nonnull long seed) {
if(seed == -1L) {
seed = generateSeed();
}
final smile.math.Random rnd = new smile.math.Random(seed);
public static void shuffle(@Nonnull final double[][] x, final double[] y, @Nonnull long seed) {
if(seed == -1L) {
seed = generateSeed();
}
final smile.math.Random rnd = new smile.math.Random(seed);
private final int S;
if(tc > S && trueChild.findBestSplit()) {
if(fc > S && falseChild.findBestSplit()) {
public DecisionTree(@Nonnull double[][] x, @Nonnull int[] y, int J, @Nonnull SplitRule rule) {
this(null, x, y, x[0].length, J, 2, null, null, rule, SmileExtUtils.generateSeed());
}
this(attributes, x, y, x[0].length, J, 2, null, null, SplitRule.GINI, SmileExtUtils.generateSeed());
}
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y, int J, @Nonnull SplitRule rule) {
this(attributes, x, y, x[0].length, J, 2, null, null, rule, SmileExtUtils.generateSeed());
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y, int M, int J, int S, @Nullable int[] samples, @Nullable int[][] order, @Nonnull SplitRule rule, long seed) {
this.S = S;
private int minSamplesSplit;
opts.addOption("split", "min_samples_split", true, "S number of instances in a node below which the tree will not split [default: 2]");
int T = 50, M = -1, J = Integer.MAX_VALUE, S = 2;
S = Primitives.parseInt(cl.getOptionValue("min_samples_split"), S);
this.minSamplesSplit = S;
train(x, y, attributes, splitRule, numTrees, numVars, maxLeafNodes, minSamplesSplit, seed);
private void train(@Nonnull final double[][] x, @Nonnull final int[] y, @Nullable final Attribute[] attrs, @Nonnull final SplitRule splitRule, final int numTrees, final int numVars, final int maxLeafs, final int minSamplesSplit, final long seed)
if(minSamplesSplit <= 0) {
}
tasks.add(new TrainingTask(this, attributes, x, y, numInputVars, maxLeafs, minSamplesSplit, order, prediction, splitRule, s, remainingTasks));
private final int minSamplesSplit;
TrainingTask(RandomForestClassifierUDTF udtf, Attribute[] attributes, double[][] x, int[] y, int M, int J, int S, int[][] order, int[][] prediction, SplitRule splitRule, long seed, AtomicInteger remainingTasks) {
this.minSamplesSplit = S;
DecisionTree tree = new DecisionTree(attributes, x, y, numVars, numLeafs, minSamplesSplit, samples, order, splitRule, s);
private int minSamplesSplit;
opts.addOption("split", "min_samples_split", true, "S number of instances in a node below which the tree will not split [default: 5]");
S = Primitives.parseInt(cl.getOptionValue("min_samples_split"), S);
this.minSamplesSplit = S;
train(x, y, attributes, numTrees, numVars, maxLeafNodes, minSamplesSplit, seed);
private void train(@Nonnull final double[][] x, @Nonnull final double[] y, @Nullable final Attribute[] attrs, final int numTrees, final int numVars, final int maxLeafs, final int minSamplesSplit, final long seed)
if(minSamplesSplit <= 0) {
tasks.add(new TrainingTask(this, attributes, x, y, numInputVars, maxLeafs, minSamplesSplit, order, prediction, oob, s, remainingTasks));
private final int minSamplesSplit;
TrainingTask(RandomForestRegressionUDTF udtf, Attribute[] attributes, double[][] x, double[] y, int numVars, int numLeafs, int minSamplesSplit, int[][] order, double[] prediction, int[] oob, long seed, AtomicInteger remainingTasks) {
this.minSamplesSplit = minSamplesSplit;
RegressionTree tree = new RegressionTree(attributes, x, y, numVars, numLeafs, minSamplesSplit, order, samples, s);
this(attributes, x, y, x[0].length, J, 5, null, null, SmileExtUtils.generateSeed());
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int M, int J, int S, @Nullable int[][] order, @Nullable int[] samples, long seed) {
this(attributes, x, y, M, J, S, order, samples, null, seed);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int M, int J, int S, @Nullable int[][] order, @Nullable int[] samples, @Nullable NodeOutput output, long seed) {
if(scriptEngine == null) {
ScriptEngineManager manager = new ScriptEngineManager();
ScriptEngine engine = manager.getEngineByExtension("js");
if(!(engine instanceof Compilable)) {
throw new UDFArgumentException("ScriptEngine was not compilable: "
engine.getFactory().getEngineVersion());
}
this.scriptEngine = engine;
this.compilableEngine = (Compilable) engine;
this.cache = new WeakHashMap<String, CompiledScript>();
}
if(a == b) {
if(x[splitFeature] == splitValue) {
buf.append("ifeq ");
private final Attribute[] _attributes;
private final double[] _importance;
private final Node _root;
private final SplitRule _rule;
private final int _k;
private final int _M;
private final int _S;
private final int[][] _order;
private final Random _rnd;
if(_attributes[splitFeature].type == Attribute.Type.NOMINAL) {
} else if(_attributes[splitFeature].type == Attribute.Type.NUMERIC) {
_attributes[splitFeature].type);
if(_attributes[splitFeature].type == Attribute.Type.NOMINAL) {
} else if(_attributes[splitFeature].type == Attribute.Type.NUMERIC) {
_attributes[splitFeature].type);
if(_attributes[splitFeature].type == Attribute.Type.NOMINAL) {
} else if(_attributes[splitFeature].type == Attribute.Type.NUMERIC) {
_attributes[splitFeature].type);
int[] count = new int[_k];
int[] falseCount = new int[_k];
int p = _attributes.length;
if(_M < p) {
SmileExtUtils.shuffle(variables, _rnd);
if(_attributes[j].type == Attribute.Type.NOMINAL) {
int m = ((NominalAttribute) _attributes[j]).size();
int[][] trueCount = new int[m][_k];
} else if(_attributes[j].type == Attribute.Type.NUMERIC) {
int[] trueCount = new int[_k];
for(int i : _order[j]) {
if(_attributes[node.splitFeature].type == Attribute.Type.NOMINAL) {
} else if(_attributes[node.splitFeature].type == Attribute.Type.NUMERIC) {
_attributes[node.splitFeature].type);
if(tc > _S && trueChild.findBestSplit()) {
if(fc > _S && falseChild.findBestSplit()) {
switch (_rule) {
this(null, x, y, x[0].length, J, 2, null, null, rule, null);
this(attributes, x, y, x[0].length, J, 2, null, null, SplitRule.GINI, null);
this(attributes, x, y, x[0].length, J, 2, null, null, rule, null);
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y, int M, int J, int S, @Nullable int[] samples, @Nullable int[][] order, @Nonnull SplitRule rule, @Nullable smile.math.Random rand) {
if(_k < 2) {
this._attributes = SmileExtUtils.attributeTypes(attributes, x);
this._M = M;
this._S = S;
this._rule = rule;
this._order = (order == null) ? SmileExtUtils.sort(attributes, x) : order;
this._importance = new double[attributes.length];
this._rnd = (rand == null) ? new smile.math.Random() : rand;
int[] count = new int[_k];
this._root = new Node(Math.whichMax(count));
TrainNode trainRoot = new TrainNode(_root, x, y, samples);
return _importance;
return _root.predict(x);
_root.codegen(buf, 0);
_root.opcodegen(opslist, 0);
long s = (this.seed == -1L) ? SmileExtUtils.generateSeed()
: new smile.math.Random(seed).nextLong();
final smile.math.Random rnd1 = new smile.math.Random(s);
final smile.math.Random rnd2 = new smile.math.Random(rnd1.nextLong());
final int[] samples = new int[n];
DecisionTree tree = new DecisionTree(attributes, x, y, numVars, numLeafs, minSamplesSplit, samples, order, splitRule, rnd2);
long s = (this.seed == -1L) ? SmileExtUtils.generateSeed()
: new smile.math.Random(seed).nextLong();
final smile.math.Random rnd1 = new smile.math.Random(s);
final smile.math.Random rnd2 = new smile.math.Random(rnd1.nextLong());
RegressionTree tree = new RegressionTree(attributes, x, y, numVars, numLeafs, minSamplesSplit, order, samples, rnd2);
private final Attribute[] _attributes;
private final double[] _importance;
private final Node _root;
private final int _S;
private final int _M;
private final int[][] _order;
private final Random _rnd;
if(_attributes[splitFeature].type == Attribute.Type.NOMINAL) {
} else if(_attributes[splitFeature].type == Attribute.Type.NUMERIC) {
_attributes[splitFeature].type);
if(_attributes[splitFeature].type == Attribute.Type.NOMINAL) {
} else if(_attributes[splitFeature].type == Attribute.Type.NUMERIC) {
_attributes[splitFeature].type);
if(_attributes[splitFeature].type == Attribute.Type.NOMINAL) {
} else if(_attributes[splitFeature].type == Attribute.Type.NUMERIC) {
_attributes[splitFeature].type);
if(n <= _S) {
int p = _attributes.length;
if(_M < p) {
SmileExtUtils.shuffle(variables, _rnd);
if(_attributes[j].type == Attribute.Type.NOMINAL) {
int m = ((NominalAttribute) _attributes[j]).size();
} else if(_attributes[j].type == Attribute.Type.NUMERIC) {
for(int i : _order[j]) {
if(_attributes[node.splitFeature].type == Attribute.Type.NOMINAL) {
} else if(_attributes[node.splitFeature].type == Attribute.Type.NUMERIC) {
_attributes[node.splitFeature].type);
if(tc > _S && trueChild.findBestSplit()) {
if(fc > _S && falseChild.findBestSplit()) {
this(attributes, x, y, x[0].length, J, 5, null, null, null);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int M, int J, int S, @Nullable int[][] order, @Nullable int[] samples, @Nullable smile.math.Random rand) {
this(attributes, x, y, M, J, S, order, samples, null, rand);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int M, int J, int S, @Nullable int[][] order, @Nullable int[] samples, @Nullable NodeOutput output, @Nullable smile.math.Random rand) {
this._attributes = SmileExtUtils.attributeTypes(attributes, x);
this._M = M;
this._S = S;
this._order = (order == null) ? SmileExtUtils.sort(attributes, x) : order;
this._importance = new double[attributes.length];
this._rnd = (rand == null) ? new smile.math.Random() : rand;
this._root = new Node(sum / n);
TrainNode trainRoot = new TrainNode(_root, x, y, samples);
return _importance;
return _root.predict(x);
_root.codegen(buf, 0);
_root.opcodegen(opslist, 0);
float sign = (label <= prob_one) ? 1.f : 0.f;
float sign = (label <= prob_one) ? 1.f : 0.f;
float sign = (label <= prob_one) ? 1.f : 0.f;
throw new IllegalStateException("Unsupported attribute type: "
_attributes[j].type);
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y, int numVars, int numLeafs, int minSplits, @Nullable int[] samples, @Nullable int[][] order, @Nonnull SplitRule rule, @Nullable smile.math.Random rand) {
if(numVars <= 0 || numVars > x[0].length) {
numVars);
if(numLeafs < 2) {
this._M = numVars;
this._S = minSplits;
this._rnd = (rand == null) ? new smile.math.Random() : rand;
if(numLeafs == Integer.MAX_VALUE) {
private final Attribute[] _attributes;
private final double[][] _x;
private final int[] _y;
private final int[][] _order;
private final SplitRule _splitRule;
private final int _numVars;
private final int _maxLeafs;
private final int _minSamplesSplit;
private final int[][] _prediction;
private final RandomForestClassifierUDTF _udtf;
private final long _seed;
private final AtomicInteger _remainingTasks;
TrainingTask(RandomForestClassifierUDTF udtf, Attribute[] attributes, double[][] x, int[] y, int numVars, int maxLeafs, int minSamplesSplit, int[][] order, int[][] prediction, SplitRule splitRule, long seed, AtomicInteger remainingTasks) {
this._udtf = udtf;
this._attributes = attributes;
this._x = x;
this._y = y;
this._order = order;
this._splitRule = splitRule;
this._numVars = numVars;
this._maxLeafs = maxLeafs;
this._minSamplesSplit = minSamplesSplit;
this._prediction = prediction;
this._seed = seed;
this._remainingTasks = remainingTasks;
long s = (this._seed == -1L) ? SmileExtUtils.generateSeed()
: new smile.math.Random(_seed).nextLong();
final int n = _x.length;
DecisionTree tree = new DecisionTree(_attributes, _x, _y, _numVars, _maxLeafs, _minSamplesSplit, samples, _order, _splitRule, rnd2);
final int p = tree.predict(_x[i]);
synchronized(_prediction[i]) {
String model = getModel(tree, _udtf.outputType);
int remain = _remainingTasks.decrementAndGet();
_udtf.forward(model, importance, _y, _prediction, lastTask);
private final int maxLeafs;
TrainingTask(RandomForestRegressionUDTF udtf, Attribute[] attributes, double[][] x, double[] y, int numVars, int maxLeafs, int minSamplesSplit, int[][] order, double[] prediction, int[] oob, long seed, AtomicInteger remainingTasks) {
this.maxLeafs = maxLeafs;
RegressionTree tree = new RegressionTree(attributes, x, y, numVars, maxLeafs, minSamplesSplit, order, samples, rnd2);
throw new IllegalStateException("Unsupported attribute type: "
_attributes[j].type);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int maxLeafs) {
this(attributes, x, y, x[0].length, maxLeafs, 5, null, null, null);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int numVars, int maxLeafs, int minSplits, @Nullable int[][] order, @Nullable int[] samples, @Nullable smile.math.Random rand) {
this(attributes, x, y, numVars, maxLeafs, minSplits, order, samples, null, rand);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int numVars, int maxLeafs, int minSplits, @Nullable int[][] order, @Nullable int[] samples, @Nullable NodeOutput output, @Nullable smile.math.Random rand) {
if(numVars <= 0 || numVars > x[0].length) {
numVars);
if(minSplits <= 0) {
minSplits);
if(maxLeafs < 2) {
this._M = numVars;
this._S = minSplits;
if(maxLeafs == Integer.MAX_VALUE) {
opts.addOption("splits", "min_samples_split", true, "S number of instances in a node below which the tree will not split [default: 2]");
opts.addOption("rule", "split_rule", true, "Split algorithm [default: GINI, ENTROPY]");
opts.addOption("vars", "num_variables", true, "The number of random selected features [default: floor(max(sqrt(x[0].length),x[0].length/3.0))]");
int numInputVars = (numVars <= 0)
? (int) Math.floor(Math.max(Math.sqrt(x[0].length), x[0].length / 3.d)) : numVars;
opts.addOption("vars", "num_variables", true, "The number of random selected features [default: floor(max(sqrt(x[0].length),x[0].length/3.0))]");
int numInputVars = (numVars <= 0)
? (int) Math.floor(Math.max(Math.sqrt(x[0].length), x[0].length / 3.d)) : numVars;
if(tc >= _S && trueChild.findBestSplit()) {
if(fc >= _S && falseChild.findBestSplit()) {
case GINI: {
}
case ENTROPY: {
}
opts.addOption("splits", "min_split", true, "A node that has greater than or equals to `min_split` examples will split [default: 2]");
S = Primitives.parseInt(cl.getOptionValue("min_split"), S);
opts.addOption("split", "min_split", true, "A node that has greater than or equals to `min_split` examples will split [default: 5]");
S = Primitives.parseInt(cl.getOptionValue("min_split"), S);
if(tc >= _S && trueChild.findBestSplit()) {
if(fc >= _S && falseChild.findBestSplit()) {
private int _numTrees;
private float _numVars;
private int _maxLeafNodes;
private int _minSamplesSplit;
private long _seed;
private Attribute[] _attributes;
private OutputType _outputType;
private SplitRule _splitRule;
opts.addOption("vars", "num_variables", true, "The number of random selected features [default: round(max(sqrt(x[0].length),x[0].length/3.0))]."
" If a floating number is specified, int(num_variables * x[0].length) is considered if num_variable is (0,1]");
int T = 50, J = Integer.MAX_VALUE, S = 2;
float M = -1.f;
M = Primitives.parseFloat(cl.getOptionValue("num_variables"), M);
this._numTrees = T;
this._numVars = M;
this._maxLeafNodes = J;
this._minSamplesSplit = S;
this._seed = seed;
this._attributes = attrs;
this._outputType = OutputType.resolve(output);
this._splitRule = splitRule;
if(numExamples > 0) {
double[][] x = featuresList.toArray(new double[numExamples][]);
this.featuresList = null;
int[] y = labels.toArray();
this.labels = null;
train(x, y);
}
this._attributes = null;
private void train(@Nonnull final double[][] x, @Nonnull final int[] y) throws HiveException {
if(_minSamplesSplit <= 0) {
SmileExtUtils.shuffle(x, y, _seed);
Attribute[] attributes = SmileExtUtils.attributeTypes(_attributes, x);
int numInputVars = SmileExtUtils.computeNumInputVars(_numVars, x);
AtomicInteger remainingTasks = new AtomicInteger(_numTrees);
tasks.add(new TrainingTask(this, attributes, x, y, numInputVars, _maxLeafNodes, _minSamplesSplit, order, prediction, _splitRule, s, remainingTasks));
String model = getModel(tree, _udtf._outputType);
private int _numTrees;
private float _numVars;
private int _maxLeafNodes;
private int _minSamplesSplit;
private long _seed;
private Attribute[] _attributes;
private OutputType _outputType;
opts.addOption("vars", "num_variables", true, "The number of random selected features [default: round(max(sqrt(x[0].length),x[0].length/3.0))]."
" If a floating number is specified, int(num_variables * x[0].length) is considered if num_variable is (0,1]");
int T = 50, J = Integer.MAX_VALUE, S = 5;
float M = -1.f;
M = Primitives.parseFloat(cl.getOptionValue("num_variables"), M);
this._numTrees = T;
this._numVars = M;
this._maxLeafNodes = J;
this._minSamplesSplit = S;
this._seed = seed;
this._attributes = attrs;
this._outputType = OutputType.resolve(output);
train(x, y);
this._attributes = null;
private void train(@Nonnull final double[][] x, @Nonnull final double[] y) throws HiveException {
if(_minSamplesSplit <= 0) {
SmileExtUtils.shuffle(x, y, _seed);
Attribute[] attributes = SmileExtUtils.attributeTypes(_attributes, x);
int numInputVars = SmileExtUtils.computeNumInputVars(_numVars, x);
AtomicInteger remainingTasks = new AtomicInteger(_numTrees);
tasks.add(new TrainingTask(this, attributes, x, y, numInputVars, _maxLeafNodes, _minSamplesSplit, order, prediction, oob, s, remainingTasks));
String model = getModel(tree, udtf._outputType);
public static int computeNumInputVars(final float numVars, final double[][] x) {
final int numInputVars;
if(numVars <= 0.f) {
int dims = x[0].length;
numInputVars = (int) Math.round(Math.max(Math.sqrt(dims), dims / 3.0d));
} else if(numVars > 0.f && numVars <= 1.f) {
numInputVars = (int) (numVars * x[0].length);
} else {
numInputVars = (int) numVars;
}
return numInputVars;
}
threads = Primitives.parseInt(tdHivemallNprocs, 1);
private static String getModel(@Nonnull final DecisionTree tree, @Nonnull final OutputType outputType) {
private static String getModel(@Nonnull final RegressionTree tree, @Nonnull final OutputType outputType) {
synchronized void forward(@Nonnull final String model, @Nonnull final double[] importance, final int[] y, final int[][] prediction, final boolean lastTask)
synchronized void forward(@Nonnull final String model, @Nonnull final double[] importance, final double[] y, final double[] prediction, final int[] oob, final boolean lastTask)
private int _maxDepth;
opts.addOption("depth", "max_depth", true, "The maximum number of the tree depth [default: Integer.MAX_VALUE]");
int trees = 50, maxDepth = Integer.MAX_VALUE, maxLeafs = Integer.MAX_VALUE, minSplit = 5;
float numVars = -1.f;
trees = Primitives.parseInt(cl.getOptionValue("num_trees"), trees);
if(trees < 1) {
numVars = Primitives.parseFloat(cl.getOptionValue("num_variables"), numVars);
maxDepth = Primitives.parseInt(cl.getOptionValue("max_depth"), maxDepth);
maxLeafs = Primitives.parseInt(cl.getOptionValue("max_leaf_nodes"), maxLeafs);
minSplit = Primitives.parseInt(cl.getOptionValue("min_split"), minSplit);
this._numTrees = trees;
this._numVars = numVars;
this._maxDepth = maxDepth;
this._maxLeafNodes = maxLeafs;
this._minSamplesSplit = minSplit;
tasks.add(new TrainingTask(this, attributes, x, y, numInputVars, _maxDepth, _maxLeafNodes, _minSamplesSplit, order, prediction, oob, s, remainingTasks));
private final int maxDepth;
TrainingTask(RandomForestRegressionUDTF udtf, Attribute[] attributes, double[][] x, double[] y, int numVars, int maxDepth, int maxLeafs, int minSamplesSplit, int[][] order, double[] prediction, int[] oob, long seed, AtomicInteger remainingTasks) {
this.maxDepth = maxDepth;
RegressionTree tree = new RegressionTree(attributes, x, y, numVars, maxDepth, maxLeafs, minSamplesSplit, order, samples, rnd2);
private final int _maxDepth;
private final int _minSplit;
private final int _numVars;
final Node node;
final double[][] x;
final double[] y;
final int[] samples;
final int depth;
public TrainNode(Node node, double[][] x, double[] y, int[] samples, int depth) {
this.depth = depth;
if(depth >= _maxDepth) {
return false;
}
if(n <= _minSplit) {
if(_numVars < p) {
if(tc >= _minSplit && trueChild.findBestSplit()) {
if(fc >= _minSplit && falseChild.findBestSplit()) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, maxLeafs, 5, null, null, null);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int numVars, int maxDepth, int maxLeafs, int minSplits, @Nullable int[][] order, @Nullable int[] samples, @Nullable smile.math.Random rand) {
this(attributes, x, y, numVars, maxDepth, maxLeafs, minSplits, order, samples, null, rand);
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull double[] y, int numVars, int maxDepth, int maxLeafs, int minSplits, @Nullable int[][] order, @Nullable int[] samples, @Nullable NodeOutput output, @Nullable smile.math.Random rand) {
this._numVars = numVars;
this._maxDepth = maxDepth;
this._minSplit = minSplits;
TrainNode trainRoot = new TrainNode(_root, x, y, samples, 0);
final int pred = (h[i] > 0.d) ? 1 : 0;
opts.addOption("depth", "max_depth", true, "The maximum number of the tree depth [default: 12]");
int trees = 500, maxDepth = 12, maxLeafs = Integer.MAX_VALUE, minSplit = 5;
if(_maxDepth < 1) {
}
private void checkOptions() throws HiveException {
if(_minSamplesSplit <= 0) {
}
}
checkOptions();
private void checkOptions() throws HiveException {
if(_minSamplesSplit <= 0) {
}
if(_maxDepth < 1) {
}
}
checkOptions();
TrainNode trainRoot = new TrainNode(_root, x, y, samples, 1);
private final int _numVars;
private final int _minSplit;
final Node node;
final double[][] x;
final int[] y;
final int[] samples;
if(_numVars < p) {
if(tc >= _minSplit && trueChild.findBestSplit()) {
if(fc >= _minSplit && falseChild.findBestSplit()) {
this._numVars = numVars;
this._minSplit = minSplits;
partial.map.put(new Text(term), new MutableInt(1));
map.put(new Text(term), new MutableInt(1));
public Text evaluate(int feature, int weight) {
}
public Text evaluate(int feature, long weight) {
}
public Text evaluate(long feature, int weight) {
}
public Text evaluate(long feature, long weight) {
}
public Text evaluate(String feature, int weight) {
if(feature == null) {
return null;
}
}
public Text evaluate(String feature, long weight) {
if(feature == null) {
return null;
}
}
double squaredSum = 0.d;
weights[i] = 1.f;
features[i] = ft[0];
if(norm == 0.f) {
String f = features[i];
}
} else {
String f = features[i];
float v = weights[i] / norm;
}
if(min == max) {
return 0.5f;
}
if(min == max) {
return 0.5f;
}
if(stddev == 0.d) {
return new FloatWritable(0.f);
}
return new FloatWritable(v);
if(stddev == 0.d) {
return new FloatWritable(0.f);
}
float v = (value - mean) / stddev;
return new FloatWritable(v);
if(stddev == 0.f) {
assert (i >= 0) : i;
return _w[i];
assert (i >= 0) : i;
import hivemall.utils.collections.Int2FloatOpenHash;
private float _w0;
private Int2FloatOpenHash _w;
this._w0 = 0.f;
this._w = new Int2FloatOpenHash(DEFAULT_MAPSIZE);
_w.defaultReturnValue(0.f);
public float getW(final int i) {
assert (i >= 1) : i;
assert (i >= 0) : i;
assert (i >= 0) : i;
_w.put(idx, 0.f);
@Nonnull
float prevW0 = getW(0);
setW(0, nextW0);
double ret = getW(0);
public float getV(int i, int f) {
if(i < 1 || i > _p) {
}
return _V[i - 1][f];
}
@Override
if(i < 1 || i > _p) {
}
_V[i - 1][f] = nextVif;
protected void setW(int i, float nextWi) {
assert (i >= 0) : i;
_w.put(i, nextWi);
}
@Override
assert (i >= 1) : i;
assert (i >= 1) : i;
public abstract float getV(int i, int f);
public final double dlossMultiplier(@Nonnull Feature[] x, double y) {
final double ret;
double p = predict(x);
if(_classification) {
ret = (MathUtils.sigmoid(p * y) - 1.d) * y;
p = Math.min(p, _max_target);
p = Math.max(p, _min_target);
ret = 2.d * (p - y);
}
return ret;
protected final double predict(@Nonnull final Feature[] x) {
double vx = vjf * xj;
public final void updateW0(@Nonnull Feature[] x, double dlossMultiplier, float eta) {
float gradW0 = (float) dlossMultiplier;
float prevW0 = getW(0);
setW(0, nextW0);
}
public final void updateWi(@Nonnull Feature[] x, double dlossMultiplier, int i, double xi, float eta) {
float gradWi = (float) (dlossMultiplier * xi);
public final void updateV(@Nonnull Feature[] x, double dlossMultiplier, int i, int f, float eta) {
float Vif = getV(i, f);
float gradV = (float) (dlossMultiplier * gradV(x, i, Vif));
private static double gradV(@Nonnull final Feature[] x, final int i, final float Vif) {
xi = xj;
return ret;
double y = PrimitiveObjectInspectorUtils.getDouble(args[1], _yOI);
if(_classification) {
y = (y > 0.d) ? 1.d : -1.d;
}
final double dlossMultiplier = _model.dlossMultiplier(x, y);
_model.updateW0(x, dlossMultiplier, eta);
_model.updateWi(x, dlossMultiplier, i, xi, eta);
_model.updateV(x, dlossMultiplier, i, f, eta);
final int P = _model.getSize();
final FloatWritable[] Vi = HiveUtils.newFloatArray(_factor, 0.f);
final Object[] forwardObjs = new Object[3];
forwardObjs[2] = null;
forwardObjs[2] = Arrays.asList(Vi);
float w = _model.getW(i);
Wi.set(w);
float v = _model.getV(i, f);
Vi[f].set(v);
ret = (p - y);
public final double dloss(@Nonnull Feature[] x, double y) {
public final class FactorizationMachineUDTF extends UDTFWithOptions {
final double dlossMultiplier = _model.dloss(x, y);
public static int writeFully(@Nonnull final FileChannel dst, @Nonnull final ByteBuffer src, @Nonnegative final long position)
int count = 0;
return count;
}
public static int writeFully(@Nonnull final FileChannel dst, @Nonnull final ByteBuffer src)
throws IOException {
int count = 0;
while(src.remaining() > 0) {
}
return count;
protected NioFixedSegment fileIO;
protected static void writeBuffer(@Nonnull final ByteBuffer srcBuf, @Nonnull final NioFixedSegment dst, final long lastWritePos)
final NioFixedSegment fileIO = this.fileIO;
public final class NioFixedSegment extends NioSegment {
super(file, recordLength, readOnly);
public File getFile();
public int directWrite(long filePos, @Nonnull ByteBuffer buf) throws IOException;
public void flush() throws IOException;
super(file, readOnly);
public NioSegment(@Nonnull File file) {
this(file, false);
public NioSegment(@Nonnull File file, boolean readOnly) {
dst.writeRecords(lastWritePos, srcBuf);
bytesRead = fileIO.read(seekPos, inputBuf);
public int readRecords(final long idx, final ByteBuffer buf) throws IOException {
public long writeRecords(final long idx, final ByteBuffer buf) throws IOException {
public int read(long filePos, @Nonnull ByteBuffer buf) throws IOException {
public int write(final long filePos, @Nonnull final ByteBuffer buf) throws IOException {
public int read(long filePos, @Nonnull ByteBuffer buf) throws IOException;
public int write(long filePos, @Nonnull ByteBuffer buf) throws IOException;
public void resetPosition() {
this.curPos = 0L;
}
import javax.annotation.concurrent.NotThreadSafe;
@NotThreadSafe
import hivemall.common.ConversionState;
import javax.annotation.Nullable;
@Nullable
protected ConversionState cvState;
boolean conversionCheck = true;
double convergenceRate = 0.005d;
conversionCheck = !cl.hasOption("disable_cvtest");
convergenceRate = Primitives.parseDouble(cl.getOptionValue("cv_rate"), convergenceRate);
rankInit.setInitStdDev(initStdDev);
this.cvState = new ConversionState(conversionCheck, convergenceRate);
cvState.incrError(Math.abs(err));
cvState.incrLoss(err * err);
cvState.incrLoss(lambda * Qi * Qi);
cvState.incrLoss(lambda * Pu * Pu);
cvState.incrLoss(lambda * Bu * Bu);
cvState.incrLoss(lambda * Bi * Bi);
cvState.multiplyLoss(0.5d);
public long getPosition() {
public void setPosition(long pos) {
package hivemall.mf;
import hivemall.mf.Rating.RatingWithSquaredGrad;
import hivemall.mf.FactorizedModel.RankInitScheme;
package hivemall.mf;
package hivemall.mf;
rankInit.setInitStdDev(initStdDev);
import hivemall.common.ConversionState;
import hivemall.utils.io.FileUtils;
import hivemall.utils.io.NioStatefullSegment;
import hivemall.utils.lang.NumberUtils;
import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(FactorizationMachineUDTF.class);
private static final int INT_BYTES = Integer.SIZE / 8;
private ConversionState _cvState;
private ByteBuffer _inputBuf;
private NioStatefullSegment _fileIO;
opts.addOption("disable_cv", "disable_cvtest", false, "Whether to disable convergence check [default: enabled]");
opts.addOption("cv_rate", "convergence_rate", true, "Threshold to determine convergence [default: 0.005]");
boolean conversionCheck = true;
double convergenceRate = 0.005d;
conversionCheck = !cl.hasOption("disable_cvtest");
convergenceRate = Primitives.parseDouble(cl.getOptionValue("cv_rate"), convergenceRate);
this._cvState = new ConversionState(conversionCheck, convergenceRate);
recordTrain(x, y);
protected void recordTrain(@Nonnull final Feature[] x, final double y) throws HiveException {
if(_iterations <= 1) {
return;
}
ByteBuffer inputBuf = _inputBuf;
NioStatefullSegment dst = _fileIO;
if(inputBuf == null) {
final File file;
try {
file = File.createTempFile("hivemall_fm", ".sgmt");
file.deleteOnExit();
if(!file.canWrite()) {
throw new UDFArgumentException("Cannot write a temporary file: "
file.getAbsolutePath());
}
} catch (IOException ioe) {
throw new UDFArgumentException(ioe);
} catch (Throwable e) {
throw new UDFArgumentException(e);
}
this._fileIO = dst = new NioStatefullSegment(file, false);
}
int xBytes = requiredBytes(x);
int remain = inputBuf.remaining();
if(remain < requiredBytes) {
writeBuffer(inputBuf, dst);
}
inputBuf.putInt(recordBytes);
inputBuf.putInt(x.length);
for(Feature f : x) {
f.writeTo(inputBuf);
}
inputBuf.putDouble(y);
}
private static void writeBuffer(@Nonnull ByteBuffer srcBuf, @Nonnull NioStatefullSegment dst)
throws HiveException {
srcBuf.flip();
try {
dst.write(srcBuf);
} catch (IOException e) {
throw new HiveException("Exception causes while writing a buffer to file", e);
}
srcBuf.clear();
}
final double dloss = _model.dloss(x, y);
_model.updateW0(x, dloss, eta);
_model.updateWi(x, dloss, i, xi, eta);
_model.updateV(x, dloss, i, f, eta);
if(_t == 0) {
this._model = null;
return;
}
if(_iterations > 1) {
runTrainingIteration(_iterations);
}
protected void runTrainingIteration(int iterations) throws HiveException {
final ByteBuffer inputBuf = this._inputBuf;
final NioStatefullSegment fileIO = this._fileIO;
assert (inputBuf != null);
assert (fileIO != null);
try {
if(inputBuf.position() == 0) {
}
inputBuf.flip();
while(inputBuf.remaining() > 0) {
int bytes = inputBuf.getInt();
assert (bytes > 0) : bytes;
int xLength = inputBuf.getInt();
final Feature[] x = new Feature[xLength];
x[j] = new Feature(inputBuf);
}
double y = inputBuf.getDouble();
_t;
train(x, y);
}
inputBuf.rewind();
}
if(inputBuf.remaining() > 0) {
writeBuffer(inputBuf, fileIO);
}
try {
fileIO.flush();
} catch (IOException e) {
throw new HiveException("Failed to flush a file: "
fileIO.getFile().getAbsolutePath(), e);
}
final long numTrainingExamples = _t;
if(logger.isInfoEnabled()) {
File tmpFile = fileIO.getFile();
" records to a temporary file for iterative training: "
")");
}
int i = 1;
_cvState.multiplyLoss(0.5d);
break;
}
inputBuf.clear();
fileIO.resetPosition();
while(true) {
final int bytesRead;
try {
bytesRead = fileIO.read(inputBuf);
} catch (IOException e) {
throw new HiveException("Failed to read a file: "
fileIO.getFile().getAbsolutePath(), e);
}
break;
}
assert (bytesRead > 0) : bytesRead;
inputBuf.flip();
int remain = inputBuf.remaining();
if(remain < INT_BYTES) {
throw new HiveException("Illegal file format was detected");
}
while(remain >= INT_BYTES) {
int recordBytes = inputBuf.getInt();
remain -= INT_BYTES;
if(remain < recordBytes) {
break;
}
final int xLength = inputBuf.getInt();
final Feature[] x = new Feature[xLength];
x[j] = new Feature(inputBuf);
}
double y = inputBuf.getDouble();
_t;
train(x, y);
remain -= recordBytes;
}
inputBuf.compact();
}
}
NumberUtils.formatNumber(numTrainingExamples)
" training updates in total)");
}
} finally {
try {
fileIO.close(true);
} catch (IOException e) {
throw new HiveException("Failed to close a file: "
fileIO.getFile().getAbsolutePath(), e);
}
this._inputBuf = null;
this._fileIO = null;
}
}
int j = 0;
ary[j] = f;
if(j == length) {
return ary;
} else {
Feature[] dst = new Feature[j];
System.arraycopy(ary, 0, dst, 0, j);
return dst;
}
public Feature(@Nonnull ByteBuffer src) {
readFrom(src);
}
int bytes() {
}
void writeTo(@Nonnull final ByteBuffer dst) {
dst.putInt(index);
dst.putDouble(value);
}
void readFrom(@Nonnull final ByteBuffer src) {
this.index = src.getInt();
this.value = src.getDouble();
}
private static int requiredBytes(@Nonnull final Feature[] x) {
int ret = 0;
for(Feature f : x) {
assert (f != null);
}
return ret;
}
cvState.incrLoss(lambda * Qi * Qi);
cvState.incrLoss(lambda * Pu * Pu);
cvState.incrLoss(lambda * Bu * Bu);
cvState.incrLoss(lambda * Bi * Bi);
switch (type) {
public double loss(double p, double y);
protected static void checkTarget(double y) {
if(!(y == 1.d || y == -1.d)) {
}
}
final float z = p - y;
return z * z * 0.5f;
}
@Override
public double loss(double p, double y) {
final double z = p - y;
return z * z * 0.5d;
final float z = y * p;
public double loss(double p, double y) {
checkTarget(y);
final double z = y * p;
if(z > 18.d) {
return Math.exp(-z);
}
if(z < -18.d) {
return -z;
}
}
@Override
public double loss(double p, double y) {
double loss = hingeLoss(p, y, threshold);
return (loss > 0.d) ? loss : 0.d;
}
@Override
public double loss(double p, double y) {
return squaredHingeLoss(p, y);
}
@Override
public double loss(double p, double y) {
double e = y - p;
if(e > 0.d) {
return tau * e;
} else {
return -(1.d - tau) * e;
}
}
@Override
public double loss(double p, double y) {
double loss = Math.abs(y - p) - epsilon;
return (loss > 0.d) ? loss : 0.d;
}
@Override
public static float logLoss(final float p, final float y) {
BinaryLoss.checkTarget(y);
final float z = y * p;
if(z > 18.f) {
return (float) Math.exp(-z);
}
if(z < -18.f) {
return -z;
}
}
public static double logLoss(final double p, final double y) {
BinaryLoss.checkTarget(y);
final double z = y * p;
if(z > 18.d) {
return Math.exp(-z);
}
if(z < -18.d) {
return -z;
}
}
public static float squaredLoss(float p, float y) {
final float z = p - y;
return z * z * 0.5f;
}
public static double squaredLoss(double p, double y) {
final double z = p - y;
return z * z * 0.5d;
}
public static double hingeLoss(final double p, final double y, final double threshold) {
BinaryLoss.checkTarget(y);
double z = y * p;
return threshold - z;
}
public static double hingeLoss(double p, double y) {
return hingeLoss(p, y, 1.d);
}
public static double squaredHingeLoss(final double p, final double y) {
BinaryLoss.checkTarget(y);
double z = y * p;
double d = 1.d - z;
return (d > 0.d) ? d * d : 0.d;
}
public final double dloss(@Nonnull final Feature[] x, final double y) {
return dloss(p, y);
}
public final double dloss(double p, final double y) {
final double ret;
import hivemall.common.LossFunctions;
import hivemall.common.LossFunctions.LossFunction;
import hivemall.common.LossFunctions.LossType;
private LossFunction _lossFunction;
this._lossFunction = classication ? LossFunctions.getLossFunction(LossType.LogLoss)
: LossFunctions.getLossFunction(LossType.SquaredLoss);
final double p = _model.predict(x);
final double lossGrad = _model.dloss(p, y);
double loss = _lossFunction.loss(p, y);
_cvState.incrLoss(loss);
_model.updateW0(x, lossGrad, eta);
_model.updateWi(x, lossGrad, i, xi, eta);
_model.updateV(x, lossGrad, i, f, eta);
public int getMinIndex() {
return 1;
}
@Override
public int getMaxIndex() {
return _p - 1;
}
@Override
public float[] getV(int i) {
if(i < 1 || i > _p) {
}
return _V[i - 1];
}
@Override
private int minIndex, maxIndex;
this.minIndex = 0;
this.maxIndex = 0;
public int getMinIndex() {
return minIndex;
}
@Override
public int getMaxIndex() {
return maxIndex;
}
@Override
public float[] getV(int i) {
assert (i >= 1) : i;
return _V.get(i);
}
@Override
this.maxIndex = Math.max(maxIndex, idx);
this.minIndex = Math.min(minIndex, idx);
import javax.annotation.Nullable;
public abstract int getMinIndex();
public abstract int getMaxIndex();
@Nullable
public abstract float[] getV(int i);
import java.util.Random;
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "train_fm", value = "_FUNC_(array<string> x, double y) - Returns a prediction value")
@Nullable
private Random _va_rand;
private float _validationRatio;
@Nullable
private Feature[] probes;
opts.addOption("disable_adareg", "disable_adaptive_regularizaion", false, "Whether to disable adaptive regularization [default: enabled]");
opts.addOption("va_ratio", "validation_ratio", true, "Ratio of training data used for validation [default: 0.05f]");
boolean adaptiveReglarization = true;
float validationRatio = 0.05f;
adaptiveReglarization = !cl.hasOption("disable_adaptive_regularizaion");
validationRatio = Primitives.parseFloat(cl.getOptionValue("validation_ratio"), validationRatio);
if(adaptiveReglarization) {
}
this._validationRatio = validationRatio;
if(_validationRatio < 0.f || _validationRatio >= 1.f) {
throw new UDFArgumentException("validation_ratio should be in range [0, 1): "
_validationRatio);
}
Feature[] x = Feature.parseFeatures(args[0], _xOI, probes);
this.probes = x;
int xBytes = Feature.requiredBytes(x);
trainTheta(x, y);
} else {
float rnd = _va_rand.nextFloat();
if(rnd < _validationRatio) {
} else {
trainTheta(x, y);
}
}
}
protected void trainTheta(final Feature[] x, final double y) throws HiveException {
protected void trainLambda(final Feature[] x, final double y) throws HiveException {
}
return;
final IntWritable f_idx = new IntWritable(0);
final FloatWritable f_Wi = new FloatWritable(0.f);
final FloatWritable[] f_Vi = HiveUtils.newFloatArray(_factor, 0.f);
forwardObjs[0] = f_idx;
forwardObjs[1] = f_Wi;
f_idx.set(0);
f_Wi.set(_model.getW(0));
forwardObjs[2] = Arrays.asList(f_Vi);
float[] vi = _model.getV(i);
if(vi == null) {
continue;
}
f_idx.set(i);
f_Wi.set(w);
float v = vi[f];
f_Vi[f].set(v);
final long numTrainingExamples = count;
int i = 1;
cvState.multiplyLoss(0.5d);
break;
}
NumberUtils.formatNumber(numTrainingExamples)
" training updates in total) ");
cvState.multiplyLoss(0.5d);
break;
}
" training examples using a secondary storage (thus "
private final float[] _w;
private final float[][] _V;
this._V = new float[p][factor];
private final Int2FloatOpenHash _w;
private final IntOpenHashMap<float[]> _V;
private int _minIndex, _maxIndex;
this._minIndex = 0;
this._maxIndex = 0;
return _minIndex;
return _maxIndex;
this._maxIndex = Math.max(_maxIndex, idx);
this._minIndex = Math.min(_minIndex, idx);
protected float _lambdaW0;
protected float _lambdaW;
private final float[] _lambdaV;
this._lambdaV = new float[factor];
Arrays.fill(_lambdaV, lambda0);
protected void initLearningParams() {}
float getLambdaV(int f) {
return _lambdaV[f];
}
final double dloss(@Nonnull final Feature[] x, final double y) {
final double dloss(double p, final double y) {
final double predict(@Nonnull final Feature[] x) {
final void updateW0(final double dloss, final float eta) {
float gradW0 = (float) dloss;
final void updateWi(final double dloss, final int i, final double xi, final float eta) {
float gradWi = (float) (dloss * xi);
final void updateV(@Nonnull final Feature[] x, final double dloss, final int i, final int f, final float eta) {
float gradV = (float) (dloss * gradV(x, i, Vif));
final void updateLambdaW0(final double dloss, final float eta) {
float lambda_w_grad = -2.f * eta * getW(0);
float lambdaW0 = _lambdaW0 - (float) (eta * dloss * lambda_w_grad);
this._lambdaW0 = Math.max(0.f, lambdaW0);
}
final void updateLambdaW(@Nonnull Feature[] x, double dloss, float eta) {
double sumWX = 0.d;
for(Feature e : x) {
if(e == null) {
continue;
}
int i = e.index;
double xi = e.value;
}
double lambda_w_grad = -2.f * eta * sumWX;
float lambdaW = _lambdaW - (float) (eta * dloss * lambda_w_grad);
this._lambdaW = Math.max(0.f, lambdaW);
}
final void updateLambdaV(@Nonnull final Feature[] x, final double dloss, final float eta) {
double sum_f_dash = 0.d, sum_f = 0.d, sum_f_dash_f = 0.d;
for(Feature e : x) {
assert (e != null) : Arrays.toString(x);
int j = e.index;
double x_j = e.value;
float v_jf = getV(j, f);
}
double lambda_v_grad = -2.f * eta * (sum_f_dash * sum_f - sum_f_dash_f);
float lambdaVf = _lambdaV[f] - (float) (eta * dloss * lambda_v_grad);
_lambdaV[f] = Math.max(0.f, lambdaVf);
}
}
boolean adaptiveRegularization = (_va_rand != null) && _t >= 1000;
train(x, y, adaptiveRegularization);
public void train(@Nonnull final Feature[] x, final double y, final boolean adaptiveRegularization)
throws HiveException {
if(adaptiveRegularization) {
assert (_va_rand != null);
final float rnd = _va_rand.nextFloat();
} else {
trainTheta(x, y);
_model.updateW0(lossGrad, eta);
_model.updateWi(lossGrad, i, xi, eta);
final float eta = _etaEstimator.eta(_t);
final double p = _model.predict(x);
final double lossGrad = _model.dloss(p, y);
_model.updateLambdaW0(lossGrad, eta);
_model.updateLambdaW(x, lossGrad, eta);
_model.updateLambdaV(x, lossGrad, eta);
final long numTrainingExamples = _t;
int i = 1;
train(x, y, true);
}
break;
NumberUtils.formatNumber(numTrainingExamples)
" training updates in total) ");
train(x, y, true);
break;
}
" training examples on a secondary storage (thus "
float lambdaVf = getLambdaV(f);
lambdaVf -= eta * dloss * lambda_v_grad;
@Override
public String toString() {
}
FactorizationMachineModel getModel() {
return _model;
}
processOptions(argOIs);
private int _validationThreshold;
opts.addOption("va_threshold", "validation_threshold", true, "Threshold to start validation. At least N training examples are used before validation [default: 1000]");
int validationThreshold = 1000;
validationThreshold = Primitives.parseInt(cl.getOptionValue("validation_threshold"), validationThreshold);
this._validationThreshold = validationThreshold;
boolean adaptiveRegularization = (_va_rand != null) && _t >= _validationThreshold;
i;
int pos = inputBuf.position();
inputBuf.position(pos);
i;
ret = p - y;
assert (!Double.isNaN(ret));
assert (!Double.isNaN(ret));
double h = gradV(x, i, f);
float gradV = (float) (dloss * h);
float LambdaVf = getLambdaV(f);
private double gradV(@Nonnull final Feature[] x, final int i, final int f) {
float Vjf = getV(j, f);
double min_target = Double.MIN_VALUE, max_target = Double.MAX_VALUE;
min_target = Primitives.parseDouble(cl.getOptionValue("min_target"), min_target);
max_target = Primitives.parseDouble(cl.getOptionValue("max_target"), max_target);
public float getW0() {
return _w0;
}
@Override
if(i == 0) {
this._w0 = nextWi;
} else {
assert (i >= 1) : i;
_w.put(i, nextWi);
}
public float getW0() {
return _w[0];
}
@Override
public abstract float getW0();
double ret = getW0();
float prevW0 = getW0();
float lambda_w_grad = -2.f * eta * getW0();
f_Wi.set(_model.getW0());
ret = 2.d * (p - y);
final double sumVfX = sumVfX(x, f);
double gradV = gradV(x_j, v_jf, sumVfX);
private double sumVfX(@Nonnull final Feature[] x, final int f) {
double ret = 0.d;
for(Feature e : x) {
int j = e.index;
double xj = e.value;
float Vjf = getV(j, f);
}
return ret;
}
private double gradV(@Nonnull final double Xj, final float Vjf, final double sumVfX) {
return Xj * (sumVfX - Vjf * Xj);
}
@Deprecated
final void updateV(final double dloss, final int i, final int f, final double Xi, final double sumViX, final float eta) {
float Vif = getV(i, f);
double h = gradV(Xi, Vif, sumViX);
float gradV = (float) (dloss * h);
float LambdaVf = getLambdaV(f);
setV(i, f, nextVif);
}
@Deprecated
double[] sumVfX(@Nonnull final Feature[] x) {
final int k = _factor;
final double[] ret = new double[k];
ret[f] = sumVfX(x, f);
}
return ret;
}
final double[] sumVfx = _model.sumVfX(x);
final int i = e.index;
final double xi = e.value;
_model.updateV(lossGrad, i, f, xi, sumVfx[f], eta);
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public DoubleWritable evaluate(String featureVector) throws UDFArgumentException {
public List<DoubleWritable> evaluate(List<String> featureVectors) throws UDFArgumentException {
final DoubleWritable[] output = new DoubleWritable[size];
private static DoubleWritable extractWeights(String ftvec) throws UDFArgumentException {
double d = Double.parseDouble(splits[1]);
return new DoubleWritable(d);
return new DoubleWritable(1.d);
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public boolean iterate(@Nullable FloatWritable Wj, @Nullable List<FloatWritable> Vjf, @Nullable DoubleWritable Xj)
public DoubleWritable terminate() {
double result = partial.getPrediction();
return new DoubleWritable(result);
void iterate(@Nullable FloatWritable Wj, @Nullable List<FloatWritable> Vjf, @Nullable DoubleWritable Xj)
final double x = Xj.get();
double vx = v.get() * x;
double getPrediction() {
import java.util.Arrays;
import org.apache.hadoop.hive.ql.metadata.HiveException;
protected int getMinIndex() {
protected int getMaxIndex() {
protected void setW0(float nextW0) {
_w[0] = nextW0;
}
@Override
protected float getW(int i) {
assert (i >= 1) : i;
return _w[i];
}
@Override
public float getW(@Nonnull final Feature x) {
int i = x.getIndex();
protected void setW(@Nonnull Feature x, float nextWi) {
int i = x.getIndex();
protected float[] getV(int i) {
public float getV(@Nonnull final Feature x, int f) {
final int i = x.getIndex();
protected void setV(@Nonnull Feature x, int f, float nextVif) {
final int i = x.getIndex();
@Override
public void check(@Nonnull Feature[] x) throws HiveException {
for(Feature e : x) {
if(e != null && e.getIndex() < 1) {
throw new HiveException("Index of x should be greater than or equals to 1: "
Arrays.toString(x));
}
}
}
public final class FMIntFeatureMapModel extends FactorizationMachineModel {
private static final int DEFAULT_MAPSIZE = 4096;
public FMIntFeatureMapModel(boolean classification, int factor, float lambda0, double sigma, long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta) {
protected int getMinIndex() {
protected int getMaxIndex() {
protected void setW0(float nextW0) {
this._w0 = nextW0;
}
@Override
protected float getW(final int i) {
assert (i >= 1) : i;
return _w.get(i);
}
@Override
public float getW(@Nonnull final Feature x) {
final int i = x.getIndex();
protected void setW(@Nonnull Feature x, float nextWi) {
final int i = x.getIndex();
protected float[] getV(int i) {
public float getV(@Nonnull final Feature x, final int f) {
int i = x.getIndex();
protected void setV(@Nonnull Feature x, int f, float nextVif) {
final int i = x.getIndex();
final int idx = e.getIndex();
protected int getMinIndex() {
throw new UnsupportedOperationException();
}
protected int getMaxIndex() {
throw new UnsupportedOperationException();
}
protected abstract void setW0(float nextW0);
protected float getW(int i) {
throw new UnsupportedOperationException();
}
public abstract float getW(@Nonnull Feature x);
protected abstract void setW(@Nonnull Feature x, float nextWi);
protected float[] getV(int i) {
throw new UnsupportedOperationException();
}
public abstract float getV(@Nonnull Feature x, int f);
protected abstract void setV(@Nonnull Feature x, int f, float nextVif);
double xj = e.getValue();
double xj = e.getValue();
float vjf = getV(e, f);
setW0(nextW0);
final void updateWi(final double dloss, @Nonnull final Feature x, final float eta) {
final double Xi = x.getValue();
float gradWi = (float) (dloss * Xi);
float wi = getW(x);
setW(x, nextWi);
final void updateV(final double dloss, @Nonnull final Feature x, final int f, final double sumViX, final float eta) {
final double Xi = x.getValue();
float Vif = getV(x, f);
setV(x, f, nextVif);
assert (e != null) : Arrays.toString(x);
double xi = e.getValue();
double x_j = e.getValue();
float v_jf = getV(e, f);
double xj = e.getValue();
float Vjf = getV(e, f);
public void check(@Nonnull Feature[] x) throws HiveException {}
import hivemall.fm.FMStringFeatureMapModel.Entry;
import hivemall.utils.collections.IMapIterator;
import org.apache.hadoop.io.Text;
private Feature[] _probes;
private boolean _parseFeatureAsInt;
opts.addOption("disable_cv", "disable_cvtest", false, "Whether to disable convergence check [default: OFF]");
opts.addOption("disable_adareg", "disable_adaptive_regularizaion", false, "Whether to disable adaptive regularization [default: OFF]");
opts.addOption("int_feature", "feature_as_integer", false, "Parse a feature as integer [default: OFF, ON if -p option is specified]");
boolean parseFeatureAsInt = false;
if(p == -1) {
parseFeatureAsInt = cl.hasOption("feature_as_integer");
} else {
parseFeatureAsInt = true;
}
this._parseFeatureAsInt = parseFeatureAsInt;
if(_parseFeatureAsInt) {
if(_p == -1) {
this._model = new FMIntFeatureMapModel(_classification, _factor, _lambda0, _sigma, _seed, _min_target, _max_target, _etaEstimator);
} else {
this._model = new FMArrayModel(_classification, _factor, _lambda0, _sigma, _p, _seed, _min_target, _max_target, _etaEstimator);
}
this._model = new FMStringFeatureMapModel(_classification, _factor, _lambda0, _sigma, _seed, _min_target, _max_target, _etaEstimator);
fieldNames.add("feature");
if(_parseFeatureAsInt) {
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
} else {
fieldOIs.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
}
Feature[] x = Feature.parseFeatures(args[0], _xOI, _probes, _parseFeatureAsInt);
this._probes = x;
_model.updateWi(lossGrad, e, eta);
_model.updateV(lossGrad, e, f, sumVfx[f], eta);
if(_parseFeatureAsInt) {
forwardAsIntFeature();
} else {
forwardAsStringFeature();
}
}
private void forwardAsIntFeature() throws HiveException {
final float[] vi = _model.getV(i);
final float w = _model.getW(i);
private void forwardAsStringFeature() throws HiveException {
final FMStringFeatureMapModel model = (FMStringFeatureMapModel) _model;
final Text feature = new Text();
final FloatWritable f_Wi = new FloatWritable(0.f);
final FloatWritable[] f_Vi = HiveUtils.newFloatArray(_factor, 0.f);
final Object[] forwardObjs = new Object[3];
forwardObjs[0] = feature;
forwardObjs[1] = f_Wi;
forwardObjs[2] = null;
feature.set("0");
f_Wi.set(_model.getW0());
forward(forwardObjs);
forwardObjs[2] = Arrays.asList(f_Vi);
final IMapIterator<String, Entry> itor = model.entries();
while(itor.next() != -1) {
String i = itor.getKey();
assert (i != null);
feature.set(i);
Entry entry = itor.getValue();
f_Wi.set(entry.W);
final float[] Vi = entry.Vf;
float v = Vi[f];
f_Vi[f].set(v);
}
forward(forwardObjs);
}
}
x[j] = Feature.createInstance(inputBuf, _parseFeatureAsInt);
x[j] = Feature.createInstance(inputBuf, _parseFeatureAsInt);
public abstract class Feature {
protected double value;
public Feature() {}
public Feature(double value) {
public void setFeature(String f) {
throw new UnsupportedOperationException();
public String getFeature() {
throw new UnsupportedOperationException();
public void setIndex(int i) {
throw new UnsupportedOperationException();
public int getIndex() {
throw new UnsupportedOperationException();
public double getValue() {
return value;
public abstract int bytes();
public abstract void writeTo(@Nonnull ByteBuffer dst);
public abstract void readFrom(@Nonnull ByteBuffer src);
@Nullable
public static Feature[] parseFeatures(@Nonnull final Object arg, @Nonnull final ListObjectInspector listOI, @Nullable final Feature[] probes, final boolean asIntFeature)
throws HiveException {
if(arg == null) {
return null;
}
final int length = listOI.getListLength(arg);
final Feature[] ary;
if(probes != null && probes.length == length) {
ary = probes;
} else {
ary = new Feature[length];
}
int j = 0;
Object o = listOI.getListElement(arg, i);
if(o == null) {
continue;
}
String s = o.toString();
Feature f = ary[j];
if(f == null) {
f = parse(s, asIntFeature);
} else {
parse(s, f, asIntFeature);
}
ary[j] = f;
}
if(j == length) {
return ary;
} else {
Feature[] dst = new Feature[j];
System.arraycopy(ary, 0, dst, 0, j);
return dst;
}
@Nonnull
private static Feature parse(@Nonnull final String s, final boolean asIntFeature)
throws HiveException {
int pos = s.indexOf(":");
String s1 = s.substring(0, pos);
if(asIntFeature) {
int index = Integer.parseInt(s1);
if(index < 0) {
}
double value = Double.parseDouble(s2);
return new IntFeature(index, value);
} else {
double value = Double.parseDouble(s2);
return new StringFeature(s1, value);
}
}
private static void parse(@Nonnull final String s, @Nonnull final Feature probe, final boolean asIntFeature)
throws HiveException {
int pos = s.indexOf(":");
String s1 = s.substring(0, pos);
if(asIntFeature) {
int index = Integer.parseInt(s1);
if(index < 0) {
}
double value = Double.parseDouble(s2);
probe.setIndex(index);
probe.value = value;
} else {
probe.setFeature(s1);
probe.value = Double.parseDouble(s2);
}
}
@Nonnull
public static Feature createInstance(@Nonnull ByteBuffer src, boolean asIntFeature) {
if(asIntFeature) {
return new IntFeature(src);
} else {
return new StringFeature(src);
}
}
private static final int INT_BYTES = Integer.SIZE / 8;
private static final int CHAR_BYTES = Character.SIZE / 8;
public static int requiredBytes(@Nonnull final String s) {
int size = s.length();
}
public static void putString(@Nonnull final String s, @Nonnull final ByteBuffer dst) {
final char[] array = s.toCharArray();
final int size = array.length;
dst.putInt(size);
dst.putChar(array[i]);
}
}
public static String getString(@Nonnull final ByteBuffer src) {
final int size = src.getInt();
final char[] array = new char[size];
array[i] = src.getChar();
}
return new String(array);
}
import java.util.Arrays;
import org.apache.hadoop.hive.ql.metadata.HiveException;
protected int getMinIndex() {
protected int getMaxIndex() {
protected void setW0(float nextW0) {
_w[0] = nextW0;
}
@Override
protected float getW(int i) {
assert (i >= 1) : i;
return _w[i];
}
@Override
public float getW(@Nonnull final Feature x) {
int i = x.getIndex();
protected void setW(@Nonnull Feature x, float nextWi) {
int i = x.getIndex();
protected float[] getV(int i) {
public float getV(@Nonnull final Feature x, int f) {
final int i = x.getIndex();
protected void setV(@Nonnull Feature x, int f, float nextVif) {
final int i = x.getIndex();
@Override
public void check(@Nonnull Feature[] x) throws HiveException {
for(Feature e : x) {
if(e != null && e.getIndex() < 1) {
throw new HiveException("Index of x should be greater than or equals to 1: "
Arrays.toString(x));
}
}
}
public final class FMIntFeatureMapModel extends FactorizationMachineModel {
private static final int DEFAULT_MAPSIZE = 4096;
public FMIntFeatureMapModel(boolean classification, int factor, float lambda0, double sigma, long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta) {
protected int getMinIndex() {
protected int getMaxIndex() {
protected void setW0(float nextW0) {
this._w0 = nextW0;
}
@Override
protected float getW(final int i) {
assert (i >= 1) : i;
return _w.get(i);
}
@Override
public float getW(@Nonnull final Feature x) {
final int i = x.getIndex();
protected void setW(@Nonnull Feature x, float nextWi) {
final int i = x.getIndex();
protected float[] getV(int i) {
public float getV(@Nonnull final Feature x, final int f) {
int i = x.getIndex();
protected void setV(@Nonnull Feature x, int f, float nextVif) {
final int i = x.getIndex();
final int idx = e.getIndex();
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public boolean iterate(@Nullable FloatWritable Wj, @Nullable List<FloatWritable> Vjf, @Nullable DoubleWritable Xj)
public DoubleWritable terminate() {
double result = partial.getPrediction();
return new DoubleWritable(result);
void iterate(@Nullable FloatWritable Wj, @Nullable List<FloatWritable> Vjf, @Nullable DoubleWritable Xj)
final double x = Xj.get();
double vx = v.get() * x;
double getPrediction() {
protected int getMinIndex() {
throw new UnsupportedOperationException();
}
protected int getMaxIndex() {
throw new UnsupportedOperationException();
}
protected abstract void setW0(float nextW0);
protected float getW(int i) {
throw new UnsupportedOperationException();
}
public abstract float getW(@Nonnull Feature x);
protected abstract void setW(@Nonnull Feature x, float nextWi);
protected float[] getV(int i) {
throw new UnsupportedOperationException();
}
public abstract float getV(@Nonnull Feature x, int f);
protected abstract void setV(@Nonnull Feature x, int f, float nextVif);
double xj = e.getValue();
double xj = e.getValue();
float vjf = getV(e, f);
setW0(nextW0);
final void updateWi(final double dloss, @Nonnull final Feature x, final float eta) {
final double Xi = x.getValue();
float gradWi = (float) (dloss * Xi);
float wi = getW(x);
setW(x, nextWi);
final void updateV(final double dloss, @Nonnull final Feature x, final int f, final double sumViX, final float eta) {
final double Xi = x.getValue();
float Vif = getV(x, f);
setV(x, f, nextVif);
assert (e != null) : Arrays.toString(x);
double xi = e.getValue();
double x_j = e.getValue();
float v_jf = getV(e, f);
double xj = e.getValue();
float Vjf = getV(e, f);
public void check(@Nonnull Feature[] x) throws HiveException {}
import hivemall.fm.FMStringFeatureMapModel.Entry;
import hivemall.utils.collections.IMapIterator;
import org.apache.hadoop.io.Text;
private Feature[] _probes;
private boolean _parseFeatureAsInt;
opts.addOption("disable_cv", "disable_cvtest", false, "Whether to disable convergence check [default: OFF]");
opts.addOption("disable_adareg", "disable_adaptive_regularizaion", false, "Whether to disable adaptive regularization [default: OFF]");
opts.addOption("int_feature", "feature_as_integer", false, "Parse a feature as integer [default: OFF, ON if -p option is specified]");
boolean parseFeatureAsInt = false;
if(p == -1) {
parseFeatureAsInt = cl.hasOption("feature_as_integer");
} else {
parseFeatureAsInt = true;
}
this._parseFeatureAsInt = parseFeatureAsInt;
if(_parseFeatureAsInt) {
if(_p == -1) {
this._model = new FMIntFeatureMapModel(_classification, _factor, _lambda0, _sigma, _seed, _min_target, _max_target, _etaEstimator);
} else {
this._model = new FMArrayModel(_classification, _factor, _lambda0, _sigma, _p, _seed, _min_target, _max_target, _etaEstimator);
}
this._model = new FMStringFeatureMapModel(_classification, _factor, _lambda0, _sigma, _seed, _min_target, _max_target, _etaEstimator);
fieldNames.add("feature");
if(_parseFeatureAsInt) {
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
} else {
fieldOIs.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
}
Feature[] x = Feature.parseFeatures(args[0], _xOI, _probes, _parseFeatureAsInt);
this._probes = x;
_model.updateWi(lossGrad, e, eta);
_model.updateV(lossGrad, e, f, sumVfx[f], eta);
if(_parseFeatureAsInt) {
forwardAsIntFeature();
} else {
forwardAsStringFeature();
}
}
private void forwardAsIntFeature() throws HiveException {
final float[] vi = _model.getV(i);
final float w = _model.getW(i);
private void forwardAsStringFeature() throws HiveException {
final FMStringFeatureMapModel model = (FMStringFeatureMapModel) _model;
final Text feature = new Text();
final FloatWritable f_Wi = new FloatWritable(0.f);
final FloatWritable[] f_Vi = HiveUtils.newFloatArray(_factor, 0.f);
final Object[] forwardObjs = new Object[3];
forwardObjs[0] = feature;
forwardObjs[1] = f_Wi;
forwardObjs[2] = null;
feature.set("0");
f_Wi.set(_model.getW0());
forward(forwardObjs);
forwardObjs[2] = Arrays.asList(f_Vi);
final IMapIterator<String, Entry> itor = model.entries();
while(itor.next() != -1) {
String i = itor.getKey();
assert (i != null);
feature.set(i);
Entry entry = itor.getValue();
f_Wi.set(entry.W);
final float[] Vi = entry.Vf;
float v = Vi[f];
f_Vi[f].set(v);
}
forward(forwardObjs);
}
}
x[j] = Feature.createInstance(inputBuf, _parseFeatureAsInt);
x[j] = Feature.createInstance(inputBuf, _parseFeatureAsInt);
public abstract class Feature {
protected double value;
public Feature() {}
public Feature(double value) {
public void setFeature(String f) {
throw new UnsupportedOperationException();
public String getFeature() {
throw new UnsupportedOperationException();
public void setIndex(int i) {
throw new UnsupportedOperationException();
public int getIndex() {
throw new UnsupportedOperationException();
public double getValue() {
return value;
public abstract int bytes();
public abstract void writeTo(@Nonnull ByteBuffer dst);
public abstract void readFrom(@Nonnull ByteBuffer src);
@Nullable
public static Feature[] parseFeatures(@Nonnull final Object arg, @Nonnull final ListObjectInspector listOI, @Nullable final Feature[] probes, final boolean asIntFeature)
throws HiveException {
if(arg == null) {
return null;
}
final int length = listOI.getListLength(arg);
final Feature[] ary;
if(probes != null && probes.length == length) {
ary = probes;
} else {
ary = new Feature[length];
}
int j = 0;
Object o = listOI.getListElement(arg, i);
if(o == null) {
continue;
}
String s = o.toString();
Feature f = ary[j];
if(f == null) {
f = parse(s, asIntFeature);
} else {
parse(s, f, asIntFeature);
}
ary[j] = f;
}
if(j == length) {
return ary;
} else {
Feature[] dst = new Feature[j];
System.arraycopy(ary, 0, dst, 0, j);
return dst;
}
@Nonnull
private static Feature parse(@Nonnull final String s, final boolean asIntFeature)
throws HiveException {
int pos = s.indexOf(":");
String s1 = s.substring(0, pos);
if(asIntFeature) {
int index = Integer.parseInt(s1);
if(index < 0) {
}
double value = Double.parseDouble(s2);
return new IntFeature(index, value);
} else {
double value = Double.parseDouble(s2);
return new StringFeature(s1, value);
}
}
private static void parse(@Nonnull final String s, @Nonnull final Feature probe, final boolean asIntFeature)
throws HiveException {
int pos = s.indexOf(":");
String s1 = s.substring(0, pos);
if(asIntFeature) {
int index = Integer.parseInt(s1);
if(index < 0) {
}
double value = Double.parseDouble(s2);
probe.setIndex(index);
probe.value = value;
} else {
probe.setFeature(s1);
probe.value = Double.parseDouble(s2);
}
}
@Nonnull
public static Feature createInstance(@Nonnull ByteBuffer src, boolean asIntFeature) {
if(asIntFeature) {
return new IntFeature(src);
} else {
return new StringFeature(src);
}
}
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public DoubleWritable evaluate(String featureVector) throws UDFArgumentException {
public List<DoubleWritable> evaluate(List<String> featureVectors) throws UDFArgumentException {
final DoubleWritable[] output = new DoubleWritable[size];
private static DoubleWritable extractWeights(String ftvec) throws UDFArgumentException {
double d = Double.parseDouble(splits[1]);
return new DoubleWritable(d);
return new DoubleWritable(1.d);
private static final int INT_BYTES = Integer.SIZE / 8;
private static final int CHAR_BYTES = Character.SIZE / 8;
public static int requiredBytes(@Nonnull final String s) {
int size = s.length();
}
public static void putString(@Nonnull final String s, @Nonnull final ByteBuffer dst) {
final char[] array = s.toCharArray();
final int size = array.length;
dst.putInt(size);
dst.putChar(array[i]);
}
}
public static String getString(@Nonnull final ByteBuffer src) {
final int size = src.getInt();
final char[] array = new char[size];
array[i] = src.getChar();
}
return new String(array);
}
private final OpenHashTable<String, Entry> _map;
@Description(name = "train_fm", value = "_FUNC_(array<string> x, double y [, const string options]) - Returns a prediction value")
final int pos = s.indexOf(":");
if(pos == -1) {
if(asIntFeature) {
int index = Integer.parseInt(s);
if(index < 0) {
}
return new IntFeature(index, 1.d);
} else {
return new StringFeature(s, 1.d);
String s1 = s.substring(0, pos);
if(asIntFeature) {
int index = Integer.parseInt(s1);
if(index < 0) {
}
double value = Double.parseDouble(s2);
return new IntFeature(index, value);
} else {
double value = Double.parseDouble(s2);
return new StringFeature(s1, value);
}
final int pos = s.indexOf(":");
if(pos == -1) {
if(asIntFeature) {
int index = Integer.parseInt(s);
if(index < 0) {
}
probe.setIndex(index);
} else {
probe.setFeature(s);
probe.value = 1.d;
String s1 = s.substring(0, pos);
if(asIntFeature) {
int index = Integer.parseInt(s1);
if(index < 0) {
}
double value = Double.parseDouble(s2);
probe.setIndex(index);
probe.value = value;
} else {
probe.setFeature(s1);
probe.value = Double.parseDouble(s2);
}
String s = PrimitiveObjectInspectorUtils.getString(argument, oi);
String featureName = featureNames[i];
result.add(f);
final double v = PrimitiveObjectInspectorUtils.getDouble(argument, oi);
if(v != 0.d) {
public boolean iterate(@Nullable DoubleWritable Wj, @Nullable List<FloatWritable> Vjf, @Nullable DoubleWritable Xj)
void iterate(@Nullable DoubleWritable Wj, @Nullable List<FloatWritable> Vjf, @Nullable DoubleWritable Xj)
import java.util.Arrays;
import javax.annotation.Nonnull;
private List<Double> sumVjXj;
private List<Double> sumV2X2;
this.sumVjXj = null;
this.sumV2X2 = null;
if(sumVjXj == null) {
int factors = Vjf.size();
this.sumVjXj = Arrays.asList(new Double[factors]);
this.sumV2X2 = Arrays.asList(new Double[factors]);
}
sumVjXj.set(f, Double.valueOf(vx));
sumV2X2.set(f, Double.valueOf(vx * vx));
if(this.sumVjXj == null) {
this.sumVjXj = other.sumVjXj;
this.sumV2X2 = other.sumV2X2;
} else {
add(other.sumVjXj, sumVjXj);
add(other.sumV2X2, sumV2X2);
}
if(sumVjXj == null) {
return ret;
}
final int factor = sumVjXj.size();
Double v1 = sumVjXj.get(f);
if(v1 == null) {
}
double d1 = v1.doubleValue();
Double v2 = sumV2X2.get(f);
assert (v2 != null);
double d2 = v2.doubleValue();
}
return ret;
}
private static void add(@Nonnull final List<Double> src, @Nonnull final List<Double> dst) {
final Double v1 = src.get(i);
if(v1 != null) {
final Double v2 = dst.get(i);
if(v2 == null) {
dst.set(i, v1);
} else {
dst.set(i, Double.valueOf(new_v));
}
}
}
final boolean adaregr = _va_rand != null;
train(x, y, adaregr);
train(x, y, adaregr);
opts.addOption("min", "min_target", true, "The minimum value of target variable");
opts.addOption("max", "max_target", true, "The maximum value of target variable");
import hivemall.utils.lang.mutable.MutableDouble;
private List<MutableDouble> sumVjXj;
private List<MutableDouble> sumV2X2;
this.sumVjXj = Arrays.asList(MutableDouble.initArray(factors, 0.d));
this.sumV2X2 = Arrays.asList(MutableDouble.initArray(factors, 0.d));
MutableDouble sumVXf = sumVjXj.get(f);
sumVXf.addValue(vx);
MutableDouble sumVX2f = sumV2X2.get(f);
sumVX2f.addValue(vx * vx);
MutableDouble v1 = sumVjXj.get(f);
assert (v1 != null);
MutableDouble v2 = sumV2X2.get(f);
private static void add(@Nonnull final List<MutableDouble> src, @Nonnull final List<MutableDouble> dst) {
MutableDouble s = src.get(i);
assert (s != null);
MutableDouble d = dst.get(i);
assert (d != null);
d.addValue(s.getValue());
import javax.annotation.Nonnull;
@Nonnull
public static MutableDouble[] initArray(int size, double defaultValue) {
final MutableDouble[] array = new MutableDouble[size];
array[i] = new MutableDouble(0.d);
}
return array;
}
opts.addOption("adareg", "adaptive_regularizaion", false, "Whether to enable adaptive regularization [default: OFF]");
boolean adaptiveReglarization = false;
adaptiveReglarization = cl.hasOption("adaptive_regularizaion");
opts.addOption("adareg", "adaptive_regularizaion", false, "Whether to enable adaptive regularization [default: OFF]");
boolean adaptiveReglarization = false;
adaptiveReglarization = cl.hasOption("adaptive_regularizaion");
ret = p - y;
public FMArrayModel(boolean classification, int factor, float lambda0, double sigma, int p, long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta, @Nonnull VInitScheme vInit) {
super(classification, factor, lambda0, sigma, seed, minTarget, maxTarget, eta, vInit);
_V[i] = initV();
public FMIntFeatureMapModel(boolean classification, int factor, float lambda0, double sigma, long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta, @Nonnull VInitScheme vInit) {
super(classification, factor, lambda0, sigma, seed, minTarget, maxTarget, eta, vInit);
float[] tmp = initV();
public FMStringFeatureMapModel(boolean classification, int factor, float lambda0, double sigma, long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta, @Nonnull VInitScheme vInit) {
super(classification, factor, lambda0, sigma, seed, minTarget, maxTarget, eta, vInit);
float[] Vf = initV();
V = initV();
V = initV();
import javax.annotation.Nonnegative;
protected final VInitScheme _initScheme;
protected final float[] _lambdaV;
public FactorizationMachineModel(boolean classification, int factor, float lambda0, double sigma, long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta, @Nonnull VInitScheme vInit) {
this._initScheme = vInit;
public enum VInitScheme {
@Nonnegative
float maxInitValue;
@Nonnegative
double initStdDev;
Random[] rand;
@Nonnull
public static VInitScheme resolve(@Nullable String opt) {
if(opt == null) {
return random;
} else if("gaussian".equalsIgnoreCase(opt)) {
return gaussian;
} else if("random".equalsIgnoreCase(opt)) {
return random;
}
return random;
}
public void setMaxInitValue(float maxInitValue) {
this.maxInitValue = maxInitValue;
}
public void setInitStdDev(double initStdDev) {
this.initStdDev = initStdDev;
}
public void initRandom(int factor, long seed) {
int size = (this == random) ? 1 : factor;
this.rand = new Random[size];
}
}
}
@Nonnull
protected final float[] initV() {
final float[] ret = new float[_factor];
switch (_initScheme) {
case random:
uniformFill(ret, _initScheme.rand[0], _initScheme.maxInitValue);
break;
case gaussian:
gaussianFill(ret, _initScheme.rand, _initScheme.initStdDev);
break;
default:
throw new IllegalStateException("Unsupported V initialization scheme: "
_initScheme);
}
return ret;
}
protected static final void uniformFill(final float[] a, final Random rand, final float maxInitValue) {
float v = rand.nextFloat() * maxInitValue / len;
a[i] = v;
}
}
protected static final void gaussianFill(final float[] a, final Random[] rand, final double stddev) {
float v = (float) MathUtils.gaussian(0.d, stddev, rand[i]);
a[i] = v;
}
}
import hivemall.fm.FactorizationMachineModel.VInitScheme;
private VInitScheme _vInit;
opts.addOption("init_v", true, "Initialization strategy of matrix V [random, gaussian] (default: random)");
opts.addOption("maxval", "max_init_value", true, "The maximum initial value in the matrix V [default: 1.0]");
opts.addOption("min_init_stddev", true, "The minimum standard deviation of initial matrix V [default: 0.1]");
String vInitOpt = null;
float maxInitValue = 1.f;
double initStdDev = 0.1d;
vInitOpt = cl.getOptionValue("init_v");
maxInitValue = Primitives.parseFloat(cl.getOptionValue("max_init_value"), 1.f);
initStdDev = Primitives.parseDouble(cl.getOptionValue("min_init_stddev"), 0.1d);
this._vInit = VInitScheme.resolve(vInitOpt);
_vInit.setMaxInitValue(maxInitValue);
initStdDev = Math.max(initStdDev, 1.0d / factor);
_vInit.setInitStdDev(initStdDev);
_vInit.initRandom(factor, seed);
this._model = new FMIntFeatureMapModel(_classification, _factor, _lambda0, _sigma, _seed, _min_target, _max_target, _etaEstimator, _vInit);
this._model = new FMArrayModel(_classification, _factor, _lambda0, _sigma, _p, _seed, _min_target, _max_target, _etaEstimator, _vInit);
this._model = new FMStringFeatureMapModel(_classification, _factor, _lambda0, _sigma, _seed, _min_target, _max_target, _etaEstimator, _vInit);
switch (type) {
public double loss(double p, double y);
protected static void checkTarget(double y) {
if(!(y == 1.d || y == -1.d)) {
}
}
final float z = p - y;
return z * z * 0.5f;
}
@Override
public double loss(double p, double y) {
final double z = p - y;
return z * z * 0.5d;
final float z = y * p;
public double loss(double p, double y) {
checkTarget(y);
final double z = y * p;
if(z > 18.d) {
return Math.exp(-z);
}
if(z < -18.d) {
return -z;
}
}
@Override
public double loss(double p, double y) {
double loss = hingeLoss(p, y, threshold);
return (loss > 0.d) ? loss : 0.d;
}
@Override
public double loss(double p, double y) {
return squaredHingeLoss(p, y);
}
@Override
public double loss(double p, double y) {
double e = y - p;
if(e > 0.d) {
return tau * e;
} else {
return -(1.d - tau) * e;
}
}
@Override
public double loss(double p, double y) {
double loss = Math.abs(y - p) - epsilon;
return (loss > 0.d) ? loss : 0.d;
}
@Override
public static float logLoss(final float p, final float y) {
BinaryLoss.checkTarget(y);
final float z = y * p;
if(z > 18.f) {
return (float) Math.exp(-z);
}
if(z < -18.f) {
return -z;
}
}
public static double logLoss(final double p, final double y) {
BinaryLoss.checkTarget(y);
final double z = y * p;
if(z > 18.d) {
return Math.exp(-z);
}
if(z < -18.d) {
return -z;
}
}
public static float squaredLoss(float p, float y) {
final float z = p - y;
return z * z * 0.5f;
}
public static double squaredLoss(double p, double y) {
final double z = p - y;
return z * z * 0.5d;
}
public static double hingeLoss(final double p, final double y, final double threshold) {
BinaryLoss.checkTarget(y);
double z = y * p;
return threshold - z;
}
public static double hingeLoss(double p, double y) {
return hingeLoss(p, y, 1.d);
}
public static double squaredHingeLoss(final double p, final double y) {
BinaryLoss.checkTarget(y);
double z = y * p;
double d = 1.d - z;
return (d > 0.d) ? d * d : 0.d;
}
String s = PrimitiveObjectInspectorUtils.getString(argument, oi);
String featureName = featureNames[i];
result.add(f);
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public DoubleWritable evaluate(String featureVector) throws UDFArgumentException {
public List<DoubleWritable> evaluate(List<String> featureVectors) throws UDFArgumentException {
final DoubleWritable[] output = new DoubleWritable[size];
private static DoubleWritable extractWeights(String ftvec) throws UDFArgumentException {
double d = Double.parseDouble(splits[1]);
return new DoubleWritable(d);
return new DoubleWritable(1.d);
final double v = PrimitiveObjectInspectorUtils.getDouble(argument, oi);
if(v != 0.d) {
package hivemall.mf;
import hivemall.mf.Rating.RatingWithSquaredGrad;
cvState.incrLoss(lambda * Qi * Qi);
cvState.incrLoss(lambda * Pu * Pu);
cvState.incrLoss(lambda * Bu * Bu);
cvState.incrLoss(lambda * Bi * Bi);
import hivemall.common.ConversionState;
import hivemall.mf.FactorizedModel.RankInitScheme;
protected ConversionState cvState;
protected NioFixedSegment fileIO;
boolean conversionCheck = true;
double convergenceRate = 0.005d;
conversionCheck = !cl.hasOption("disable_cvtest");
convergenceRate = Primitives.parseDouble(cl.getOptionValue("cv_rate"), convergenceRate);
this.cvState = new ConversionState(conversionCheck, convergenceRate);
cvState.incrError(Math.abs(err));
cvState.incrLoss(err * err);
cvState.incrLoss(lambda * Qi * Qi);
cvState.incrLoss(lambda * Pu * Pu);
cvState.incrLoss(lambda * Bu * Bu);
cvState.incrLoss(lambda * Bi * Bi);
protected static void writeBuffer(@Nonnull final ByteBuffer srcBuf, @Nonnull final NioFixedSegment dst, final long lastWritePos)
dst.writeRecords(lastWritePos, srcBuf);
final NioFixedSegment fileIO = this.fileIO;
final long numTrainingExamples = count;
int i = 1;
cvState.multiplyLoss(0.5d);
break;
}
NumberUtils.formatNumber(numTrainingExamples)
" training updates in total) ");
bytesRead = fileIO.read(seekPos, inputBuf);
cvState.multiplyLoss(0.5d);
break;
}
" training examples using a secondary storage (thus "
package hivemall.mf;
package hivemall.mf;
private static final int INT_BYTES = Integer.SIZE / 8;
private static final int CHAR_BYTES = Character.SIZE / 8;
public static int requiredBytes(@Nonnull final String s) {
int size = s.length();
}
public static void putString(@Nonnull final String s, @Nonnull final ByteBuffer dst) {
final char[] array = s.toCharArray();
final int size = array.length;
dst.putInt(size);
dst.putChar(array[i]);
}
}
public static String getString(@Nonnull final ByteBuffer src) {
final int size = src.getInt();
final char[] array = new char[size];
array[i] = src.getChar();
}
return new String(array);
}
public static int writeFully(@Nonnull final FileChannel dst, @Nonnull final ByteBuffer src, @Nonnegative final long position)
int count = 0;
return count;
}
public static int writeFully(@Nonnull final FileChannel dst, @Nonnull final ByteBuffer src)
throws IOException {
int count = 0;
while(src.remaining() > 0) {
}
return count;
public final class NioFixedSegment extends NioSegment {
super(file, readOnly);
public int readRecords(final long idx, final ByteBuffer buf) throws IOException {
public long writeRecords(final long idx, final ByteBuffer buf) throws IOException {
public File getFile();
public int read(long filePos, @Nonnull ByteBuffer buf) throws IOException;
public int write(long filePos, @Nonnull ByteBuffer buf) throws IOException;
import javax.annotation.Nonnull;
@Nonnull
public static MutableDouble[] initArray(int size, double defaultValue) {
final MutableDouble[] array = new MutableDouble[size];
array[i] = new MutableDouble(0.d);
}
return array;
}
import javax.annotation.Nonnegative;
private final int numWorkers;
int procs = Runtime.getRuntime().availableProcessors();
int workers = Math.max(1, (int) Math.round(procs * 1.5f));
this.numWorkers = Primitives.parseInt(cl.getOptionValue("num_workers"), workers);
opts.addOption("workers", "num_workers", true, "The number of MIX workers [default: max(1, round(procs * 1.5))] ");
acceptConnections(initializer, port, numWorkers);
private void acceptConnections(@Nonnull MixServerInitializer initializer, int port, @Nonnegative int numWorkers)
final EventLoopGroup workerGroup = new NioEventLoopGroup(numWorkers);
import javax.annotation.Nonnegative;
private final int numWorkers;
int procs = Runtime.getRuntime().availableProcessors();
int workers = Math.max(1, (int) Math.round(procs * 1.5f));
this.numWorkers = Primitives.parseInt(cl.getOptionValue("num_workers"), workers);
opts.addOption("workers", "num_workers", true, "The number of MIX workers [default: max(1, round(procs * 1.5))] ");
acceptConnections(initializer, port, numWorkers);
private void acceptConnections(@Nonnull MixServerInitializer initializer, int port, @Nonnegative int numWorkers)
final EventLoopGroup workerGroup = new NioEventLoopGroup(numWorkers);
switch (event) {
switch (event) {
final short localClock = requestMsg.getClock();
short diffClock = partial.diffClock(localClock);
partial.add(weight, covar, deltaUpdates, scale);
short globalClock = partial.getClock();
public void add(float localWeight, float covar, int deltaUpdates, float scale) {
incrClock(deltaUpdates);
public void add(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
incrClock(deltaUpdates);
import hivemall.utils.math.MathUtils;
protected short globalClock;
this.globalClock = 0;
public abstract void add(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale);
return globalClock;
protected final void incrClock(int deltaUpdates) {
public final short diffClock(short localClock) {
int dist = globalClock - localClock;
if(dist < 0) {
dist = -dist;
}
final short ret;
if(MathUtils.sign(globalClock) == MathUtils.sign(localClock)) {
ret = (short) dist;
} else {
int diff;
if(globalClock < 0) {
diff = globalClock - Short.MIN_VALUE;
} else {
diff = Short.MAX_VALUE - globalClock;
}
if(localClock < 0) {
int tmp = localClock - Short.MIN_VALUE;
} else {
}
assert (diff >= 0) : diff;
if(dist < diff) {
ret = (short) dist;
} else {
ret = (short) diff;
}
}
return ret;
public static int sign(final short v) {
return v < 0 ? -1 : 1;
}
import java.io.PrintWriter;
import java.io.StringWriter;
import org.apache.commons.cli.HelpFormatter;
import org.apache.hadoop.hive.ql.exec.Description;
opts.addOption("help", false, "Show function help");
CommandLine cl = CommandLineUtils.parseOptions(args, opts);
if(cl.hasOption("help")) {
Description funcDesc = getClass().getAnnotation(Description.class);
final String cmdLineSyntax;
if(funcDesc == null) {
cmdLineSyntax = getClass().getSimpleName();
} else {
String funcName = funcDesc.name();
cmdLineSyntax = funcName == null ? getClass().getSimpleName()
: funcDesc.value().replace("_FUNC_", funcDesc.name());
}
StringWriter sw = new StringWriter();
sw.write('\n');
PrintWriter pw = new PrintWriter(sw);
HelpFormatter formatter = new HelpFormatter();
formatter.printHelp(pw, HelpFormatter.DEFAULT_WIDTH, cmdLineSyntax, null, opts, HelpFormatter.DEFAULT_LEFT_PAD, HelpFormatter.DEFAULT_DESC_PAD, null, true);
pw.flush();
String helpMsg = sw.toString();
throw new UDFArgumentException(helpMsg);
}
return cl;
public Text evaluate(int feature, int weight) {
}
public Text evaluate(int feature, long weight) {
}
public Text evaluate(long feature, int weight) {
}
public Text evaluate(long feature, long weight) {
}
public Text evaluate(String feature, int weight) {
if(feature == null) {
return null;
}
}
public Text evaluate(String feature, long weight) {
if(feature == null) {
return null;
}
}
package hivemall.ftvec.conv;
double squaredSum = 0.d;
weights[i] = 1.f;
features[i] = ft[0];
if(norm == 0.f) {
String f = features[i];
}
} else {
String f = features[i];
float v = weights[i] / norm;
}
if(min == max) {
return 0.5f;
}
if(min == max) {
return 0.5f;
}
if(stddev == 0.d) {
return new FloatWritable(0.f);
}
return new FloatWritable(v);
if(stddev == 0.f) {
return new FloatWritable(0.f);
}
float v = (value - mean) / stddev;
return new FloatWritable(v);
partial.map.put(new Text(term), new MutableInt(1));
map.put(new Text(term), new MutableInt(1));
package hivemall.ftvec.trans;
package hivemall.ftvec.trans;
package hivemall.ftvec.trans;
float v = PrimitiveObjectInspectorUtils.getFloat(argument, oi);
if(v != 0.f) {
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
public static boolean isNumberOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if(argOI.getCategory() != Category.PRIMITIVE) {
return false;
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case SHORT:
case INT:
case LONG:
case FLOAT:
case DOUBLE:
case BYTE:
case TIMESTAMP:
return true;
default:
return false;
}
}
public static double[] asDoubleArray(@Nullable Object argObj, @Nonnull ListObjectInspector listOI, @Nonnull PrimitiveObjectInspector elemOI) {
if(argObj == null) {
return null;
}
final int length = listOI.getListLength(argObj);
final double[] ary = new double[length];
Object o = listOI.getListElement(argObj, i);
if(o == null) {
continue;
}
double d = PrimitiveObjectInspectorUtils.getDouble(o, elemOI);
ary[i] = d;
}
return ary;
}
@Nonnull
public static BooleanObjectInspector asBooleanOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentException {
if(!BOOLEAN_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (BooleanObjectInspector) argOI;
}
import java.io.InputStream;
import java.io.OutputStream;
import javax.annotation.Nonnull;
private static final int DEFAULT_BUFFER_SIZE = 1024 * 4;
public static String toString(@Nonnull final InputStream input) throws IOException {
FastMultiByteArrayOutputStream output = new FastMultiByteArrayOutputStream();
copy(input, output);
return output.toString();
}
public static int copy(@Nonnull final InputStream input, @Nonnull final OutputStream output)
throws IOException {
final byte[] buffer = new byte[DEFAULT_BUFFER_SIZE];
int count = 0;
int n = 0;
while(-1 != (n = input.read(buffer))) {
output.write(buffer, 0, n);
}
return count;
}
import javax.annotation.Nonnull;
public static final int INDEX_NOT_FOUND = -1;
public static <T> void shuffle(@Nonnull final T[] array, final int size, @Nonnull final Random rnd) {
public static void shuffle(@Nonnull final int[] array, @Nonnull final Random rnd) {
for(int i = array.length; i > 1; i--) {
int randomPosition = rnd.nextInt(i);
swap(array, i - 1, randomPosition);
}
}
public static void swap(@Nonnull final Object[] arr, final int i, final int j) {
public static void swap(@Nonnull final int[] arr, final int i, final int j) {
int tmp = arr[i];
arr[i] = arr[j];
arr[j] = tmp;
}
public static int indexOf(final int[] array, final int valueToFind, int startIndex, int endIndex) {
if(array == null) {
return INDEX_NOT_FOUND;
}
final int til = Math.min(endIndex, array.length);
if(startIndex < 0 || startIndex > til) {
}
if(valueToFind == array[i]) {
return i;
}
}
return INDEX_NOT_FOUND;
}
import java.util.List;
import javax.annotation.Nonnull;
public static boolean isInt(@Nonnull final String i) {
try {
Integer.parseInt(i);
return true;
} catch (NumberFormatException nfe) {
return false;
}
}
public static boolean isDouble(@Nonnull final String i) {
try {
Double.parseDouble(i);
return true;
} catch (NumberFormatException nfe) {
return false;
}
}
public static void clear(@Nonnull final StringBuilder buf) {
buf.setLength(0);
}
public static String concat(@Nonnull final List<String> list, @Nonnull final String sep) {
final StringBuilder buf = new StringBuilder(128);
for(String s : list) {
if(s == null) {
continue;
}
buf.append(s);
buf.append(sep);
}
return buf.toString();
}
import java.io.PrintWriter;
import java.io.StringWriter;
import org.apache.commons.cli.HelpFormatter;
import org.apache.hadoop.hive.ql.exec.Description;
opts.addOption("help", false, "Show function help");
CommandLine cl = CommandLineUtils.parseOptions(args, opts);
if(cl.hasOption("help")) {
Description funcDesc = getClass().getAnnotation(Description.class);
final String cmdLineSyntax;
if(funcDesc == null) {
cmdLineSyntax = getClass().getSimpleName();
} else {
String funcName = funcDesc.name();
cmdLineSyntax = funcName == null ? getClass().getSimpleName()
: funcDesc.value().replace("_FUNC_", funcDesc.name());
}
StringWriter sw = new StringWriter();
sw.write('\n');
PrintWriter pw = new PrintWriter(sw);
HelpFormatter formatter = new HelpFormatter();
formatter.printHelp(pw, HelpFormatter.DEFAULT_WIDTH, cmdLineSyntax, null, opts, HelpFormatter.DEFAULT_LEFT_PAD, HelpFormatter.DEFAULT_DESC_PAD, null, true);
pw.flush();
String helpMsg = sw.toString();
throw new UDFArgumentException(helpMsg);
}
return cl;
public Text evaluate(int feature, int weight) {
}
public Text evaluate(int feature, long weight) {
}
public Text evaluate(long feature, int weight) {
}
public Text evaluate(long feature, long weight) {
}
public Text evaluate(String feature, int weight) {
if(feature == null) {
return null;
}
}
public Text evaluate(String feature, long weight) {
if(feature == null) {
return null;
}
}
package hivemall.ftvec.conv;
double squaredSum = 0.d;
weights[i] = 1.f;
features[i] = ft[0];
if(norm == 0.f) {
String f = features[i];
}
} else {
String f = features[i];
float v = weights[i] / norm;
}
if(min == max) {
return 0.5f;
}
if(min == max) {
return 0.5f;
}
if(stddev == 0.d) {
return new FloatWritable(0.f);
}
return new FloatWritable(v);
if(stddev == 0.f) {
return new FloatWritable(0.f);
}
float v = (value - mean) / stddev;
return new FloatWritable(v);
partial.map.put(new Text(term), new MutableInt(1));
map.put(new Text(term), new MutableInt(1));
package hivemall.ftvec.trans;
package hivemall.ftvec.trans;
package hivemall.ftvec.trans;
float v = PrimitiveObjectInspectorUtils.getFloat(argument, oi);
if(v != 0.f) {
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
public static boolean isNumberOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if(argOI.getCategory() != Category.PRIMITIVE) {
return false;
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case SHORT:
case INT:
case LONG:
case FLOAT:
case DOUBLE:
case BYTE:
case TIMESTAMP:
return true;
default:
return false;
}
}
public static double[] asDoubleArray(@Nullable Object argObj, @Nonnull ListObjectInspector listOI, @Nonnull PrimitiveObjectInspector elemOI) {
if(argObj == null) {
return null;
}
final int length = listOI.getListLength(argObj);
final double[] ary = new double[length];
Object o = listOI.getListElement(argObj, i);
if(o == null) {
continue;
}
double d = PrimitiveObjectInspectorUtils.getDouble(o, elemOI);
ary[i] = d;
}
return ary;
}
@Nonnull
public static BooleanObjectInspector asBooleanOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentException {
if(!BOOLEAN_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (BooleanObjectInspector) argOI;
}
import java.io.InputStream;
import java.io.OutputStream;
import javax.annotation.Nonnull;
private static final int DEFAULT_BUFFER_SIZE = 1024 * 4;
public static String toString(@Nonnull final InputStream input) throws IOException {
FastMultiByteArrayOutputStream output = new FastMultiByteArrayOutputStream();
copy(input, output);
return output.toString();
}
public static int copy(@Nonnull final InputStream input, @Nonnull final OutputStream output)
throws IOException {
final byte[] buffer = new byte[DEFAULT_BUFFER_SIZE];
int count = 0;
int n = 0;
while(-1 != (n = input.read(buffer))) {
output.write(buffer, 0, n);
}
return count;
}
import javax.annotation.Nonnull;
public static final int INDEX_NOT_FOUND = -1;
public static <T> void shuffle(@Nonnull final T[] array, final int size, @Nonnull final Random rnd) {
public static void shuffle(@Nonnull final int[] array, @Nonnull final Random rnd) {
for(int i = array.length; i > 1; i--) {
int randomPosition = rnd.nextInt(i);
swap(array, i - 1, randomPosition);
}
}
public static void swap(@Nonnull final Object[] arr, final int i, final int j) {
public static void swap(@Nonnull final int[] arr, final int i, final int j) {
int tmp = arr[i];
arr[i] = arr[j];
arr[j] = tmp;
}
public static int indexOf(final int[] array, final int valueToFind, int startIndex, int endIndex) {
if(array == null) {
return INDEX_NOT_FOUND;
}
final int til = Math.min(endIndex, array.length);
if(startIndex < 0 || startIndex > til) {
}
if(valueToFind == array[i]) {
return i;
}
}
return INDEX_NOT_FOUND;
}
import java.util.List;
import javax.annotation.Nonnull;
public static boolean isInt(@Nonnull final String i) {
try {
Integer.parseInt(i);
return true;
} catch (NumberFormatException nfe) {
return false;
}
}
public static boolean isDouble(@Nonnull final String i) {
try {
Double.parseDouble(i);
return true;
} catch (NumberFormatException nfe) {
return false;
}
}
public static void clear(@Nonnull final StringBuilder buf) {
buf.setLength(0);
}
public static String concat(@Nonnull final List<String> list, @Nonnull final String sep) {
final StringBuilder buf = new StringBuilder(128);
for(String s : list) {
if(s == null) {
continue;
}
buf.append(s);
buf.append(sep);
}
return buf.toString();
}
@Deprecated
public final class SigmoidUDF extends UDF {
long numMixed = model.getNumMixed();
private long numMixed;
this.numMixed = 0L;
public final long getNumMixed() {
numMixed;
long getNumMixed();
public long getNumMixed() {
long numMixed = model.getNumMixed();
long numMixed = model.getNumMixed();
private long numMixed;
this.numMixed = 0L;
public final long getNumMixed() {
numMixed;
long getNumMixed();
public long getNumMixed() {
switch (event) {
switch (event) {
final short localClock = requestMsg.getClock();
short diffClock = partial.diffClock(localClock);
partial.add(weight, covar, deltaUpdates, scale);
short globalClock = partial.getClock();
public void add(float localWeight, float covar, int deltaUpdates, float scale) {
incrClock(deltaUpdates);
public void add(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
incrClock(deltaUpdates);
import hivemall.utils.math.MathUtils;
protected short globalClock;
this.globalClock = 0;
public abstract void add(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale);
return globalClock;
protected final void incrClock(int deltaUpdates) {
public final short diffClock(short localClock) {
int dist = globalClock - localClock;
if(dist < 0) {
dist = -dist;
}
final short ret;
if(MathUtils.sign(globalClock) == MathUtils.sign(localClock)) {
ret = (short) dist;
} else {
int diff;
if(globalClock < 0) {
diff = globalClock - Short.MIN_VALUE;
} else {
diff = Short.MAX_VALUE - globalClock;
}
if(localClock < 0) {
int tmp = localClock - Short.MIN_VALUE;
} else {
}
assert (diff >= 0) : diff;
if(dist < diff) {
ret = (short) dist;
} else {
ret = (short) diff;
}
}
return ret;
long numMixed = model.getNumMixed();
public static int sign(final short v) {
return v < 0 ? -1 : 1;
}
long numMixed = model.getNumMixed();
private long numMixed;
this.numMixed = 0L;
public final long getNumMixed() {
numMixed;
long getNumMixed();
public long getNumMixed() {
switch (event) {
switch (event) {
final short localClock = requestMsg.getClock();
short diffClock = partial.diffClock(localClock);
partial.add(weight, covar, deltaUpdates, scale);
short globalClock = partial.getClock();
public void add(float localWeight, float covar, int deltaUpdates, float scale) {
incrClock(deltaUpdates);
public void add(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
incrClock(deltaUpdates);
import hivemall.utils.math.MathUtils;
protected short globalClock;
this.globalClock = 0;
public abstract void add(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale);
return globalClock;
protected final void incrClock(int deltaUpdates) {
public final short diffClock(short localClock) {
int dist = globalClock - localClock;
if(dist < 0) {
dist = -dist;
}
final short ret;
if(MathUtils.sign(globalClock) == MathUtils.sign(localClock)) {
ret = (short) dist;
} else {
int diff;
if(globalClock < 0) {
diff = globalClock - Short.MIN_VALUE;
} else {
diff = Short.MAX_VALUE - globalClock;
}
if(localClock < 0) {
int tmp = localClock - Short.MIN_VALUE;
} else {
}
assert (diff >= 0) : diff;
if(dist < diff) {
ret = (short) dist;
} else {
ret = (short) diff;
}
}
return ret;
long numMixed = model.getNumMixed();
public static int sign(final short v) {
return v < 0 ? -1 : 1;
}
public static final String VERSION = "0.4";
public static final String VERSION = "0.4";
public static final String VERSION = "0.4.0";
public static final String VERSION = "0.4.0";
final int PuSize = Pu.size();
final int QiSize = Qi.size();
if(QiSize != PuSize) {
return null;
}
final int PuSize = Pu.size();
final int QiSize = Qi.size();
if(QiSize != PuSize) {
return null;
}
final int factor = Vjf.size();
return;
}
assert (sumV2X2 != null);
private static void add(@Nullable final List<MutableDouble> src, @Nonnull final List<MutableDouble> dst) {
if(src == null) {
return;
}
public static final String VERSION = "0.4.0-1";
public static final String VERSION = "0.4.0-1";
final int factor = Vjf.size();
return;
}
assert (sumV2X2 != null);
private static void add(@Nullable final List<MutableDouble> src, @Nonnull final List<MutableDouble> dst) {
if(src == null) {
return;
}
final int PuSize = Pu.size();
final int QiSize = Qi.size();
if(QiSize != PuSize) {
return null;
}
final int PuSize = Pu.size();
final int QiSize = Qi.size();
if(QiSize != PuSize) {
return null;
}
this.sumVjXj = Arrays.asList(MutableDouble.initArray(factor, 0.d));
this.sumV2X2 = Arrays.asList(MutableDouble.initArray(factor, 0.d));
continue;
if(s.isEmpty()) {
continue;
}
import java.util.ArrayList;
this.list = null;
if(list == null) {
this.list = new ArrayList<String>(size);
} else {
list.clear();
if(s1.isEmpty()) {
continue;
}
list.add(s2);
return list;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
continue;
if(oi.getPrimitiveCategory() == PrimitiveCategory.STRING) {
String s = argument.toString();
if(s.isEmpty()) {
continue;
}
}
continue;
if(s.isEmpty()) {
continue;
}
public IntWritable evaluate(int label) throws UDFArgumentException {
if(label == 0) {
return new IntWritable(-1);
} else if(label == -1) {
return new IntWritable(0);
return new IntWritable(1);
} else if(label == -1.f) {
return new IntWritable(0);
} else if(label == 1.f) {
}
}
public IntWritable evaluate(double label) throws UDFArgumentException {
if(label == 0.d) {
return new IntWritable(-1);
} else if(label == -1.d) {
return new IntWritable(0);
} else if(label == 1.d) {
return new IntWritable(1);
} else {
public static final String VERSION = "0.4.0-2";
import hivemall.mix.MixEnv;
InetSocketAddress addr = NetUtils.getInetSocketAddress(
endpoints[i], MixEnv.MIXSERV_DEFAULT_PORT);
import hivemall.mix.MixEnv;
this.port = Primitives.parseInt(cl.getOptionValue("port"), MixEnv.MIXSERV_DEFAULT_PORT);
public static final String VERSION = "0.4.0-2";
import java.io.PrintWriter;
import java.io.StringWriter;
import org.apache.commons.cli.HelpFormatter;
import org.apache.hadoop.hive.ql.exec.Description;
opts.addOption("help", false, "Show function help");
CommandLine cl = CommandLineUtils.parseOptions(args, opts);
if(cl.hasOption("help")) {
Description funcDesc = getClass().getAnnotation(Description.class);
final String cmdLineSyntax;
if(funcDesc == null) {
cmdLineSyntax = getClass().getSimpleName();
} else {
String funcName = funcDesc.name();
cmdLineSyntax = funcName == null ? getClass().getSimpleName()
: funcDesc.value().replace("_FUNC_", funcDesc.name());
}
StringWriter sw = new StringWriter();
sw.write('\n');
PrintWriter pw = new PrintWriter(sw);
HelpFormatter formatter = new HelpFormatter();
formatter.printHelp(pw, HelpFormatter.DEFAULT_WIDTH, cmdLineSyntax, null, opts, HelpFormatter.DEFAULT_LEFT_PAD, HelpFormatter.DEFAULT_DESC_PAD, null, true);
pw.flush();
String helpMsg = sw.toString();
throw new UDFArgumentException(helpMsg);
}
return cl;
long numMixed = model.getNumMixed();
switch (type) {
public double loss(double p, double y);
protected static void checkTarget(double y) {
if(!(y == 1.d || y == -1.d)) {
}
}
final float z = p - y;
return z * z * 0.5f;
}
@Override
public double loss(double p, double y) {
final double z = p - y;
return z * z * 0.5d;
final float z = y * p;
public double loss(double p, double y) {
checkTarget(y);
final double z = y * p;
if(z > 18.d) {
return Math.exp(-z);
}
if(z < -18.d) {
return -z;
}
}
@Override
public double loss(double p, double y) {
double loss = hingeLoss(p, y, threshold);
return (loss > 0.d) ? loss : 0.d;
}
@Override
public double loss(double p, double y) {
return squaredHingeLoss(p, y);
}
@Override
public double loss(double p, double y) {
double e = y - p;
if(e > 0.d) {
return tau * e;
} else {
return -(1.d - tau) * e;
}
}
@Override
public double loss(double p, double y) {
double loss = Math.abs(y - p) - epsilon;
return (loss > 0.d) ? loss : 0.d;
}
@Override
public static float logLoss(final float p, final float y) {
BinaryLoss.checkTarget(y);
final float z = y * p;
if(z > 18.f) {
return (float) Math.exp(-z);
}
if(z < -18.f) {
return -z;
}
}
public static double logLoss(final double p, final double y) {
BinaryLoss.checkTarget(y);
final double z = y * p;
if(z > 18.d) {
return Math.exp(-z);
}
if(z < -18.d) {
return -z;
}
}
public static float squaredLoss(float p, float y) {
final float z = p - y;
return z * z * 0.5f;
}
public static double squaredLoss(double p, double y) {
final double z = p - y;
return z * z * 0.5d;
}
public static double hingeLoss(final double p, final double y, final double threshold) {
BinaryLoss.checkTarget(y);
double z = y * p;
return threshold - z;
}
public static double hingeLoss(double p, double y) {
return hingeLoss(p, y, 1.d);
}
public static double squaredHingeLoss(final double p, final double y) {
BinaryLoss.checkTarget(y);
double z = y * p;
double d = 1.d - z;
return (d > 0.d) ? d * d : 0.d;
}
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
public DoubleWritable evaluate(String featureVector) throws UDFArgumentException {
public List<DoubleWritable> evaluate(List<String> featureVectors) throws UDFArgumentException {
final DoubleWritable[] output = new DoubleWritable[size];
private static DoubleWritable extractWeights(String ftvec) throws UDFArgumentException {
double d = Double.parseDouble(splits[1]);
return new DoubleWritable(d);
return new DoubleWritable(1.d);
public Text evaluate(int feature, int weight) {
}
public Text evaluate(int feature, long weight) {
}
public Text evaluate(long feature, int weight) {
}
public Text evaluate(long feature, long weight) {
}
public Text evaluate(String feature, int weight) {
if(feature == null) {
return null;
}
}
public Text evaluate(String feature, long weight) {
if(feature == null) {
return null;
}
}
package hivemall.ftvec.conv;
double squaredSum = 0.d;
weights[i] = 1.f;
features[i] = ft[0];
if(norm == 0.f) {
String f = features[i];
}
} else {
String f = features[i];
float v = weights[i] / norm;
}
if(min == max) {
return 0.5f;
}
if(min == max) {
return 0.5f;
}
if(stddev == 0.d) {
return new FloatWritable(0.f);
}
return new FloatWritable(v);
if(stddev == 0.f) {
return new FloatWritable(0.f);
}
float v = (value - mean) / stddev;
return new FloatWritable(v);
partial.map.put(new Text(term), new MutableInt(1));
map.put(new Text(term), new MutableInt(1));
package hivemall.ftvec.trans;
continue;
String s = PrimitiveObjectInspectorUtils.getString(argument, oi);
if(s.isEmpty()) {
continue;
String featureName = featureNames[i];
result.add(f);
package hivemall.ftvec.trans;
@Description(name = "quantitative_features", value = "_FUNC_(array<string> featureNames, ...) - Returns a feature vector array<string>")
public final class QuantitativeFeaturesUDF extends GenericUDF {
inputOIs[i] = HiveUtils.asDoubleCompatibleOI(oi);
continue;
String s = argument.toString();
if(s.isEmpty()) {
}
}
final double v = PrimitiveObjectInspectorUtils.getDouble(argument, oi);
if(v != 0.d) {
package hivemall.ftvec.trans;
continue;
if(s.isEmpty()) {
continue;
}
private long numMixed;
this.numMixed = 0L;
public final long getNumMixed() {
numMixed;
long getNumMixed();
public long getNumMixed() {
package hivemall.mf;
final int PuSize = Pu.size();
final int QiSize = Qi.size();
if(QiSize != PuSize) {
return null;
}
final int PuSize = Pu.size();
final int QiSize = Qi.size();
if(QiSize != PuSize) {
return null;
}
import hivemall.mf.Rating.RatingWithSquaredGrad;
cvState.incrLoss(lambda * Qi * Qi);
cvState.incrLoss(lambda * Pu * Pu);
cvState.incrLoss(lambda * Bu * Bu);
cvState.incrLoss(lambda * Bi * Bi);
import hivemall.common.ConversionState;
import hivemall.mf.FactorizedModel.RankInitScheme;
protected ConversionState cvState;
protected NioFixedSegment fileIO;
boolean conversionCheck = true;
double convergenceRate = 0.005d;
conversionCheck = !cl.hasOption("disable_cvtest");
convergenceRate = Primitives.parseDouble(cl.getOptionValue("cv_rate"), convergenceRate);
this.cvState = new ConversionState(conversionCheck, convergenceRate);
cvState.incrError(Math.abs(err));
cvState.incrLoss(err * err);
cvState.incrLoss(lambda * Qi * Qi);
cvState.incrLoss(lambda * Pu * Pu);
cvState.incrLoss(lambda * Bu * Bu);
cvState.incrLoss(lambda * Bi * Bi);
protected static void writeBuffer(@Nonnull final ByteBuffer srcBuf, @Nonnull final NioFixedSegment dst, final long lastWritePos)
dst.writeRecords(lastWritePos, srcBuf);
final NioFixedSegment fileIO = this.fileIO;
final long numTrainingExamples = count;
int i = 1;
cvState.multiplyLoss(0.5d);
break;
}
NumberUtils.formatNumber(numTrainingExamples)
" training updates in total) ");
bytesRead = fileIO.read(seekPos, inputBuf);
cvState.multiplyLoss(0.5d);
break;
}
" training examples using a secondary storage (thus "
package hivemall.mf;
package hivemall.mf;
package hivemall.mix;
public final class MixEnv {
public final static int MIXSERV_DEFAULT_PORT = 11212;
import hivemall.mix.MixEnv;
InetSocketAddress addr = NetUtils.getInetSocketAddress(
endpoints[i], MixEnv.MIXSERV_DEFAULT_PORT);
long numMixed = model.getNumMixed();
package hivemall.smile.data;
import smile.data.NominalAttribute;
public final class NominalAttribute2 extends NominalAttribute {
private int size;
public NominalAttribute2(String name) {
super(name);
}
public void setSize(int size) {
this.size = size;
}
@Override
public int size() {
return size;
package hivemall.smile.vm;
public class VMRuntimeException extends Exception {
private static final long serialVersionUID = -7378149197872357802L;
public VMRuntimeException(String message) {
super(message);
}
public VMRuntimeException(String message, Throwable cause) {
super(message, cause);
}
public IntWritable evaluate(int label) throws UDFArgumentException {
if(label == 0) {
return new IntWritable(-1);
} else if(label == -1) {
return new IntWritable(0);
return new IntWritable(1);
} else if(label == -1.f) {
return new IntWritable(0);
} else if(label == 1.f) {
}
}
public IntWritable evaluate(double label) throws UDFArgumentException {
if(label == 0.d) {
return new IntWritable(-1);
} else if(label == -1.d) {
return new IntWritable(0);
} else if(label == 1.d) {
return new IntWritable(1);
} else {
@Deprecated
public final class SigmoidUDF extends UDF {
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
public static boolean isNumberOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if(argOI.getCategory() != Category.PRIMITIVE) {
return false;
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case SHORT:
case INT:
case LONG:
case FLOAT:
case DOUBLE:
case BYTE:
case TIMESTAMP:
return true;
default:
return false;
}
}
public static double[] asDoubleArray(@Nullable Object argObj, @Nonnull ListObjectInspector listOI, @Nonnull PrimitiveObjectInspector elemOI) {
if(argObj == null) {
return null;
}
final int length = listOI.getListLength(argObj);
final double[] ary = new double[length];
Object o = listOI.getListElement(argObj, i);
if(o == null) {
continue;
}
double d = PrimitiveObjectInspectorUtils.getDouble(o, elemOI);
ary[i] = d;
}
return ary;
}
@Nonnull
public static BooleanObjectInspector asBooleanOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentException {
if(!BOOLEAN_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (BooleanObjectInspector) argOI;
}
private static final int INT_BYTES = Integer.SIZE / 8;
private static final int CHAR_BYTES = Character.SIZE / 8;
public static int requiredBytes(@Nonnull final String s) {
int size = s.length();
}
public static void putString(@Nonnull final String s, @Nonnull final ByteBuffer dst) {
final char[] array = s.toCharArray();
final int size = array.length;
dst.putInt(size);
dst.putChar(array[i]);
}
}
public static String getString(@Nonnull final ByteBuffer src) {
final int size = src.getInt();
final char[] array = new char[size];
array[i] = src.getChar();
}
return new String(array);
}
public static int writeFully(@Nonnull final FileChannel dst, @Nonnull final ByteBuffer src, @Nonnegative final long position)
int count = 0;
return count;
}
public static int writeFully(@Nonnull final FileChannel dst, @Nonnull final ByteBuffer src)
throws IOException {
int count = 0;
while(src.remaining() > 0) {
}
return count;
public File getFile();
public int read(long filePos, @Nonnull ByteBuffer buf) throws IOException;
public int write(long filePos, @Nonnull ByteBuffer buf) throws IOException;
import javax.annotation.Nonnull;
public static final int INDEX_NOT_FOUND = -1;
public static <T> void shuffle(@Nonnull final T[] array, final int size, @Nonnull final Random rnd) {
public static void shuffle(@Nonnull final int[] array, @Nonnull final Random rnd) {
for(int i = array.length; i > 1; i--) {
int randomPosition = rnd.nextInt(i);
swap(array, i - 1, randomPosition);
}
}
public static void swap(@Nonnull final Object[] arr, final int i, final int j) {
public static void swap(@Nonnull final int[] arr, final int i, final int j) {
int tmp = arr[i];
arr[i] = arr[j];
arr[j] = tmp;
}
public static int indexOf(final int[] array, final int valueToFind, int startIndex, int endIndex) {
if(array == null) {
return INDEX_NOT_FOUND;
}
final int til = Math.min(endIndex, array.length);
if(startIndex < 0 || startIndex > til) {
}
if(valueToFind == array[i]) {
return i;
}
}
return INDEX_NOT_FOUND;
}
import java.util.List;
import javax.annotation.Nonnull;
public static boolean isInt(@Nonnull final String i) {
try {
Integer.parseInt(i);
return true;
} catch (NumberFormatException nfe) {
return false;
}
}
public static boolean isDouble(@Nonnull final String i) {
try {
Double.parseDouble(i);
return true;
} catch (NumberFormatException nfe) {
return false;
}
}
public static void clear(@Nonnull final StringBuilder buf) {
buf.setLength(0);
}
public static String concat(@Nonnull final List<String> list, @Nonnull final String sep) {
final StringBuilder buf = new StringBuilder(128);
for(String s : list) {
if(s == null) {
continue;
}
buf.append(s);
buf.append(sep);
}
return buf.toString();
}
import javax.annotation.Nonnull;
@Nonnull
public static MutableDouble[] initArray(int size, double defaultValue) {
final MutableDouble[] array = new MutableDouble[size];
array[i] = new MutableDouble(0.d);
}
return array;
}
public static int sign(final short v) {
return v < 0 ? -1 : 1;
}
import hivemall.mix.MixEnv;
import javax.annotation.Nonnegative;
private final int numWorkers;
this.port = Primitives.parseInt(cl.getOptionValue("port"), MixEnv.MIXSERV_DEFAULT_PORT);
int procs = Runtime.getRuntime().availableProcessors();
int workers = Math.max(1, (int) Math.round(procs * 1.5f));
this.numWorkers = Primitives.parseInt(cl.getOptionValue("num_workers"), workers);
opts.addOption("workers", "num_workers", true, "The number of MIX workers [default: max(1, round(procs * 1.5))] ");
acceptConnections(initializer, port, numWorkers);
private void acceptConnections(@Nonnull MixServerInitializer initializer, int port, @Nonnegative int numWorkers)
final EventLoopGroup workerGroup = new NioEventLoopGroup(numWorkers);
switch (event) {
switch (event) {
final short localClock = requestMsg.getClock();
short diffClock = partial.diffClock(localClock);
partial.add(weight, covar, deltaUpdates, scale);
short globalClock = partial.getClock();
public void add(float localWeight, float covar, int deltaUpdates, float scale) {
incrClock(deltaUpdates);
public void add(float localWeight, float covar, @Nonnegative int deltaUpdates, float scale) {
assert (deltaUpdates > 0) : deltaUpdates;
incrClock(deltaUpdates);
InetSocketAddress addr = NetUtils.getInetSocketAddress(endpoints[i], MixEnv.MIXSERV_DEFAULT_PORT);
public static final String VERSION = "0.4.1-alpha.1";
public static final String VERSION = "0.4.1-alpha.1";
import hivemall.mix.MixEnv;
InetSocketAddress addr = NetUtils.getInetSocketAddress(endpoints[i], MixEnv.MIXSERV_DEFAULT_PORT);
import hivemall.mix.MixEnv;
this.port = Primitives.parseInt(cl.getOptionValue("port"), MixEnv.MIXSERV_DEFAULT_PORT);
if(featureNames == null) {
throw new UDFArgumentException("#featureNames should not be null");
}
if(featureNames == null) {
throw new UDFArgumentException("#featureNames should not be null");
}
if(featureNames == null) {
throw new UDFArgumentException("#featureNames should not be null");
}
@Nullable
if(constOI.getCategory() != Category.LIST) {
throw new UDFArgumentException("argument must be an array: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
if(lst == null) {
return null;
}
public static void closeQuietly(final Closeable... channels) {
for(Closeable c : channels) {
if(c != null) {
try {
c.close();
} catch (IOException e) {
;
}
}
}
}
public static final String VERSION = "0.4.1-alpha.2";
int diffClock = partial.diffClock(localClock);
public final int diffClock(short localClock) {
short tempValue1 = globalClock;
tempValue1 -= localClock;
short tempValue2 = localClock;
tempValue2 -= globalClock;
return Math.min(Math.abs(tempValue1), Math.abs(tempValue2));
if(tdJarVersion == null) {
String hivemallNprocs = conf.get("hivemall.smile.nprocs");
threads = Primitives.parseInt(hivemallNprocs, 1);
} else {
return Thread.currentThread().getId() * System.nanoTime();
public static final String VERSION = "0.4.1-alpha.1";
if(featureNames == null) {
throw new UDFArgumentException("#featureNames should not be null");
}
if(featureNames == null) {
throw new UDFArgumentException("#featureNames should not be null");
}
if(featureNames == null) {
throw new UDFArgumentException("#featureNames should not be null");
}
return Thread.currentThread().getId() * System.nanoTime();
if(tdJarVersion == null) {
String hivemallNprocs = conf.get("hivemall.smile.nprocs");
threads = Primitives.parseInt(hivemallNprocs, 1);
} else {
@Nullable
if(constOI.getCategory() != Category.LIST) {
throw new UDFArgumentException("argument must be an array: "
TypeInfoUtils.getTypeInfoFromObjectInspector(oi));
}
if(lst == null) {
return null;
}
public static void closeQuietly(final Closeable... channels) {
for(Closeable c : channels) {
if(c != null) {
try {
c.close();
} catch (IOException e) {
;
}
}
}
}
package hivemall.tools.text;
package hivemall.tools.text;
package hivemall.tools.text;
package hivemall.tools.text;
@Description(name = "train_randomforest_regression", value = "_FUNC_(double[] features, double target [, string options]) - "
" takes 2 or 3 arguments: double[] features, double target [, const string options]: "
"(Q for quantitative variable and C for categorical variable. e.g., [Q,C,Q,C])");
"(Q for quantitative variable and C for categorical variable. e.g., [Q,C,Q,C])");
"(Q for quantitative variable and C for categorical variable. e.g., [Q,C,Q,C])");
" int(num_variables * x[0].length) is considered if num_variable is (0,1]");
" int(num_variables * x[0].length) is considered if num_variable is (0,1]");
" int(num_variables * x[0].length) is considered if num_variable is (0,1]");
opts.addOption("depth", "max_depth", true, "The maximum number of the tree depth [default: 8]");
int trees = 500, maxDepth = 8, maxLeafs = Integer.MAX_VALUE, minSplit = 5;
array[i] = new MutableDouble(defaultValue);
import hivemall.utils.hadoop.WritableUtils;
private List<DoubleWritable> sumVjXj;
private List<DoubleWritable> sumV2X2;
this.sumVjXj = WritableUtils.newDoubleList(factor, 0.d);
this.sumV2X2 = WritableUtils.newDoubleList(factor, 0.d);
DoubleWritable sumVXf = sumVjXj.get(f);
sumVXf.set(v1);
DoubleWritable sumVX2f = sumV2X2.get(f);
sumVX2f.set(v2);
DoubleWritable v1 = sumVjXj.get(f);
double d1 = v1.get();
DoubleWritable v2 = sumV2X2.get(f);
double d2 = v2.get();
private static void add(@Nullable final List<DoubleWritable> src, @Nonnull final List<DoubleWritable> dst) {
DoubleWritable s = src.get(i);
DoubleWritable d = dst.get(i);
d.set(v);
public static List<LongWritable> newLongList(final int size) {
public static List<DoubleWritable> newDoubleList(final int size) {
return newDoubleList(size, 0.d);
}
@Nonnull
public static List<DoubleWritable> newDoubleList(final int size, final double defaultValue) {
array[i] = new DoubleWritable(defaultValue);
import hivemall.utils.hadoop.WritableUtils;
private List<DoubleWritable> sumVjXj;
private List<DoubleWritable> sumV2X2;
this.sumVjXj = WritableUtils.newDoubleList(factor, 0.d);
this.sumV2X2 = WritableUtils.newDoubleList(factor, 0.d);
DoubleWritable sumVXf = sumVjXj.get(f);
sumVXf.set(v1);
DoubleWritable sumVX2f = sumV2X2.get(f);
sumVX2f.set(v2);
DoubleWritable v1 = sumVjXj.get(f);
double d1 = v1.get();
DoubleWritable v2 = sumV2X2.get(f);
double d2 = v2.get();
private static void add(@Nullable final List<DoubleWritable> src, @Nonnull final List<DoubleWritable> dst) {
DoubleWritable s = src.get(i);
DoubleWritable d = dst.get(i);
d.set(v);
public static List<LongWritable> newLongList(final int size) {
public static List<DoubleWritable> newDoubleList(final int size) {
return newDoubleList(size, 0.d);
}
@Nonnull
public static List<DoubleWritable> newDoubleList(final int size, final double defaultValue) {
array[i] = new DoubleWritable(defaultValue);
final int length = array.length;
if(length == 0) {
final Set<String> results = new HashSet<String>(length);
String s = array[i];
if(s != null) {
private transient JapaneseAnalyzer analyzer;
int diffClock = partial.diffClock(localClock);
public final int diffClock(short localClock) {
short tempValue1 = globalClock;
tempValue1 -= localClock;
short tempValue2 = localClock;
tempValue2 -= globalClock;
return Math.min(Math.abs(tempValue1), Math.abs(tempValue2));
public final int diffClock(final short localClock) {
if(deltaUpdates <= 0) {
}
private FMPredictUDAF() {}
public Evaluator() {
init();
}
double predict = ret;
return predict;
return predict;
import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
public static boolean isNumberTypeInfo(@Nonnull TypeInfo typeInfo) {
if(typeInfo.getCategory() != ObjectInspector.Category.PRIMITIVE) {
return false;
}
switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {
case BYTE:
case SHORT:
case INT:
case LONG:
case FLOAT:
case DOUBLE:
return true;
default:
return false;
}
}
private Mode _mode;
private String[] _stopWordsArray;
private Set<String> _stoptags;
private transient JapaneseAnalyzer _analyzer;
this._mode = (arglen >= 2) ? tokenizationMode(arguments[1]) : Mode.NORMAL;
this._stopWordsArray = (arglen >= 3) ? HiveUtils.getConstStringArray(arguments[2]) : null;
this._stoptags = (arglen >= 4) ? stopTags(arguments[3])
this._analyzer = null;
JapaneseAnalyzer analyzer = _analyzer;
if(analyzer == null) {
CharArraySet stopwords = stopWords(_stopWordsArray);
analyzer = new JapaneseAnalyzer(null, _mode, stopwords, _stoptags);
this._analyzer = analyzer;
}
IOUtils.closeQuietly(_analyzer);
private static CharArraySet stopWords(@Nonnull final String[] array)
public static final String VERSION = "0.4.1-alpha.2";
return new FloatWritable((float) mu);
if(PuSize == 0) {
return new FloatWritable((float) mu);
} else if(QiSize == 0) {
return new FloatWritable((float) mu);
}
return new FloatWritable((float) mu);
if(Pu == null) {
if(PuSize == 0) {
if(QiSize == 0) {
return new FloatWritable((float) mu);
} else {
return new FloatWritable(ret);
}
} else if(QiSize == 0) {
return new FloatWritable(ret);
}
int threads = Math.max(1, nprocs - 1);
threads = Primitives.parseInt(hivemallNprocs, threads);
import hivemall.mix.utils.StringUtils;
import java.util.ArrayList;
import java.util.List;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(MixServer.class);
logger.info(this.toString());
@Override
public String toString() {
final List<String> configs = new ArrayList<String>();
return StringUtils.join(",", configs);
}
return "MixServer Configurations:["
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log logger = LogFactory.getLog(MixServer.class);
logger.info(this.toString());
@Override
public String toString() {
}
protected boolean is_mini_batch;
protected float mini_batch_ratio;
opts.addOption("mini_batch", false, "Use mini batch algorithm or not");
opts.addOption("mini_batch_ratio", true, "The mini batch sampling ratio against all dataset");
boolean isMinibatch = false;
float miniBatchRatio = 1.f;
isMinibatch = cl.hasOption("mini_batch");
if (isMinibatch) {
miniBatchRatio = Primitives.parseFloat(cl.getOptionValue("mini_batch_ratio"), 1.f);
}
this.is_mini_batch = isMinibatch;
this.mini_batch_ratio = miniBatchRatio;
public void setValue(float value) {
this.value = value;
}
public class AROWRegressionUDTF extends RegressionBaseUDTF {
public final class AdaDeltaUDTF extends RegressionBaseUDTF {
onlineUpdate(features, gradient);
protected void onlineUpdate(@Nonnull final FeatureValue[] features, float gradient) {
public final class AdaGradUDTF extends RegressionBaseUDTF {
onlineUpdate(features, gradient);
protected void onlineUpdate(@Nonnull final FeatureValue[] features, float gradient) {
import hivemall.io.IWeightValue;
import hivemall.io.WeightValue;
public final class LogressUDTF extends RegressionBaseUDTF {
@Override
protected IWeightValue getNewWeight(IWeightValue old_w, float delta) {
float oldWeight = 0.f;
if (old_w != null) {
oldWeight = old_w.get();
}
}
public class PassiveAggressiveRegressionUDTF extends RegressionBaseUDTF {
onlineUpdate(features, coeff);
import java.util.Random;
public abstract class RegressionBaseUDTF extends LearnerBaseUDTF {
private static final Log logger = LogFactory.getLog(RegressionBaseUDTF.class);
protected FeatureValue[] accDelta;
protected int sampled;
protected Random rnd;
this.accDelta = null;
this.rnd = new Random(42);
this.sampled = 0;
if (accDelta == null) {
accDelta = new FeatureValue[featureVector.length];
}
if (this.is_mini_batch) {
batchUpdate(features, d);
} else {
onlineUpdate(features, d);
}
protected IWeightValue getNewWeight(IWeightValue old_w, float delta) {
throw new IllegalStateException();
}
protected void accumulateDelta(@Nonnull final FeatureValue[] features, float coeff) {
if (features[i] == null) {
continue;
}
final Object x = features[i].getFeature();
final float xi = features[i].getValue();
float delta = xi * coeff;
if (accDelta[i] == null) {
accDelta[i] = new FeatureValue(x, delta);
} else {
}
}
}
protected void batchUpdate(@Nonnull final FeatureValue[] features, float coeff) {
if (rnd.nextFloat() <= this.mini_batch_ratio) {
assert features.length == accDelta.length;
accumulateDelta(features, coeff);
}
}
protected void onlineUpdate(@Nonnull final FeatureValue[] features, float coeff) {
if (this.is_mini_batch) {
final Object x = accDelta[i].getFeature();
final float delta = accDelta[i].getValue();
IWeightValue old_w = model.get(x);
IWeightValue new_w = getNewWeight(old_w, delta);
model.set(x, new_w);
}
}
if (channel != null) {
for (Closeable c : channels) {
if (c != null) {
public static String toString(@Nonnull final InputStream input, final int bufSize)
throws IOException {
FastByteArrayOutputStream output = new FastByteArrayOutputStream(bufSize);
copy(input, output);
return output.toString();
}
while (-1 != (n = input.read(buffer))) {
import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
if (o instanceof Integer) {
if (o instanceof IntWritable) {
if (o instanceof LongWritable) {
if (l > 0x7fffffffL) {
if (o == null) {
if (o instanceof Text) {
if (o instanceof LazyString) {
if (o instanceof String) {
if (o == null) {
if (o == null) {
if (o instanceof Integer) {
if (o instanceof LazyInteger) {
if (o instanceof IntWritable) {
public static List<String> asStringList(@Nonnull final DeferredObject arg,
@Nonnull final ListObjectInspector listOI) throws HiveException {
if (argObj == null) {
if (size == 0) {
if (o != null) {
if (argOI.getCategory() != Category.PRIMITIVE) {
if (typeInfo.getCategory() != ObjectInspector.Category.PRIMITIVE) {
if (!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
if (!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
if (constOI.getCategory() != Category.LIST) {
if (lst == null) {
if (o != null) {
if (!isStringOI(oi)) {
if (!isBooleanOI(oi)) {
if (!isIntOI(oi)) {
if (!isBigIntOI(oi)) {
if (INT_TYPE_NAME.equals(typeName)) {
} else if (BIGINT_TYPE_NAME.equals(typeName)) {
} else if (SMALLINT_TYPE_NAME.equals(typeName)) {
} else if (TINYINT_TYPE_NAME.equals(typeName)) {
if (BIGINT_TYPE_NAME.equals(typeName)) {
} else if (INT_TYPE_NAME.equals(typeName)) {
} else if (SMALLINT_TYPE_NAME.equals(typeName)) {
} else if (TINYINT_TYPE_NAME.equals(typeName)) {
if (DOUBLE_TYPE_NAME.equals(typeName)) {
} else if (FLOAT_TYPE_NAME.equals(typeName)) {
} else if (INT_TYPE_NAME.equals(typeName)) {
} else if (BIGINT_TYPE_NAME.equals(typeName)) {
} else if (SMALLINT_TYPE_NAME.equals(typeName)) {
} else if (TINYINT_TYPE_NAME.equals(typeName)) {
public static double[] asDoubleArray(@Nullable Object argObj,
@Nonnull ListObjectInspector listOI, @Nonnull PrimitiveObjectInspector elemOI) {
if (argObj == null) {
if (o == null) {
public static ConstantObjectInspector asConstantObjectInspector(
@Nonnull final ObjectInspector oi) throws UDFArgumentException {
if (!ObjectInspectorUtils.isConstantObjectInspector(oi)) {
public static PrimitiveObjectInspector asPrimitiveObjectInspector(
@Nonnull final ObjectInspector oi) throws UDFArgumentException {
if (oi.getCategory() != Category.PRIMITIVE) {
public static StringObjectInspector asStringOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentException {
if (!STRING_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (StringObjectInspector) argOI;
}
if (!BOOLEAN_TYPE_NAME.equals(argOI.getTypeName())) {
if (!INT_TYPE_NAME.equals(argOI.getTypeName())) {
if (argOI.getCategory() != Category.PRIMITIVE) {
public static PrimitiveObjectInspector
asDoubleCompatibleOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if (argOI.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0,
" is passed.");
if (category != Category.LIST) {
public static LazySimpleSerDe getKeyValueLineSerde(
@Nonnull final PrimitiveObjectInspector keyOI,
@Nonnull final PrimitiveObjectInspector valueOI) throws SerDeException {
if (OIs.length == 0) {
if (index >= src.length) {
if (index >= src.length) {
for (float f : lst) {
public static <T> void shuffle(@Nonnull final T[] array, final int size,
@Nonnull final Random rnd) {
for (int i = size; i > 1; i--) {
for (int i = array.length; i > 1; i--) {
if (array == null) {
if (startIndexInclusive < 0) {
if (endIndexExclusive > array.length) {
if (newSize <= 0) {
if (array == null) {
if (startIndex < 0 || startIndex > til) {
if (valueToFind == array[i]) {
public static byte[] copyOf(final byte[] original, final int newLength) {
final byte[] copy = new byte[newLength];
System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength));
return copy;
}
import java.util.zip.Deflater;
@Description(
name = "deflate",
value = "_FUNC_(TEXT data [, const int compressionLevel]) - Returns compressed string by using Deflater",
extended = "compression level must be in range [-1,9]")
private int compressionLevel;
if (argOIs.length != 1 && argOIs.length != 2) {
throw new UDFArgumentException("_FUNC_ takes 1 or 2 arguments");
int level = Deflater.DEFAULT_COMPRESSION;
if (argOIs.length == 2) {
level = HiveUtils.getConstInt(argOIs[1]);
if ((level < 0 || level > 9) && level != Deflater.DEFAULT_COMPRESSION) {
}
}
this.compressionLevel = level;
final int len = text.getLength();
compressed = codec.compress(original, 0, len, compressionLevel);
try {
channel.close();
raf.close();
} finally {
if (deleteFile) {
if (file.exists()) {
file.delete();
}
@Description(
name = "normalize_unicode",
value = "_FUNC_(String str [, String form]) - Transforms `str` with the specified normalization form. "
"The `form` takes one of NFC (default), NFD, NFKC, or NFKD")
if (str == null) {
if (form == null) {
} else if ("NFC".equals(form)) {
return Normalizer.normalize(str, Normalizer.Form.NFC);
} else if ("NFD".equals(form)) {
} else if ("NFKC".equals(form)) {
} else if ("NFKD".equals(form)) {
public FactorizationMachineModel(boolean classification, int factor, float lambda0,
double sigma, long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta,
@Nonnull VInitScheme vInit) {
final double dloss(@Nonnull final Feature[] x, final double y) throws HiveException {
if (_classification) {
final double predict(@Nonnull final Feature[] x) throws HiveException {
for (Feature e : x) {
for (Feature e : x) {
if (Double.isNaN(ret)) {
throw new HiveException(
"Detected NaN in predict. We recommend to normalize training examples");
}
final void updateV(final double dloss, @Nonnull final Feature x, final int f,
final double sumViX, final float eta) {
for (Feature e : x) {
for (Feature e : x) {
for (Feature e : x) {
if (opt == null) {
} else if ("gaussian".equalsIgnoreCase(opt)) {
} else if ("random".equalsIgnoreCase(opt)) {
protected static final void uniformFill(final float[] a, final Random rand,
final float maxInitValue) {
protected static final void gaussianFill(final float[] a, final Random[] rand,
final double stddev) {
import hivemall.utils.lang.NumberUtils;
final double dloss(@Nonnull final Feature[] x, final double y) {
final double predict(@Nonnull final Feature[] x) {
float w = getW(e);
double wx = w * xj;
if (!NumberUtils.isFinite(ret)) {
" in predict. We recommend to normalize training examples.\n"
private String varDump(@Nonnull final Feature[] x) {
final StringBuilder buf = new StringBuilder(1024);
Feature e = x[i];
String j = e.getFeature();
double xj = e.getValue();
if (i != 0) {
buf.append(", ");
}
buf.append("x[").append(j).append("] => ").append(xj);
}
buf.append("\n\n");
buf.append("W0 => ").append(getW0()).append('\n');
Feature e = x[i];
String j = e.getFeature();
float wi = getW(e);
if (i != 0) {
buf.append(", ");
}
buf.append("W[").append(j).append("] => ").append(wi);
}
buf.append("\n\n");
Feature e = x[i];
String j = e.getFeature();
float vjf = getV(e, f);
if (i != 0) {
buf.append(", ");
}
buf.append('V').append(f).append('[').append(j).append("] => ").append(vjf);
}
buf.append('\n');
}
return buf.toString();
}
if (!NumberUtils.isFinite(nextW0)) {
}
if (!NumberUtils.isFinite(nextWi)) {
}
if (!NumberUtils.isFinite(nextVif)) {
}
if (!NumberUtils.isFinite(ret)) {
Arrays.toString(x));
}
float v = rand.nextFloat() * (maxInitValue / len);
@Description(
name = "train_fm",
value = "_FUNC_(array<string> x, double y [, const string options]) - Returns a prediction value")
opts.addOption("lambda", "lambda0", true,
"The initial lambda value for regularization [default: 0.01]");
opts.addOption("power_t", true,
"The exponent for inverse scaling learning rate [default 0.1]");
opts.addOption("disable_cv", "disable_cvtest", false,
"Whether to disable convergence check [default: OFF]");
opts.addOption("cv_rate", "convergence_rate", true,
"Threshold to determine convergence [default: 0.005]");
opts.addOption("adareg", "adaptive_regularizaion", false,
"Whether to enable adaptive regularization [default: OFF]");
opts.addOption("va_ratio", "validation_ratio", true,
"Ratio of training data used for validation [default: 0.05f]");
opts.addOption(
"va_threshold",
"validation_threshold",
true,
"Threshold to start validation. At least N training examples are used before validation [default: 1000]");
opts.addOption("init_v", true,
"Initialization strategy of matrix V [random, gaussian] (default: random)");
opts.addOption("maxval", "max_init_value", true,
"The maximum initial value in the matrix V [default: 1.0]");
opts.addOption("min_init_stddev", true,
"The minimum standard deviation of initial matrix V [default: 0.1]");
opts.addOption("int_feature", "feature_as_integer", false,
"Parse a feature as integer [default: OFF, ON if -p option is specified]");
if (argOIs.length >= 3) {
validationRatio = Primitives.parseFloat(cl.getOptionValue("validation_ratio"),
validationRatio);
validationThreshold = Primitives.parseInt(cl.getOptionValue("validation_threshold"),
validationThreshold);
if (p == -1) {
if (adaptiveReglarization) {
if (_validationRatio < 0.f || _validationRatio >= 1.f) {
if (argOIs.length != 2 && argOIs.length != 3) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 or 3 arguments: array<string> x, double y [, CONSTANT STRING options]: "
Arrays.toString(argOIs));
if (!HiveUtils.isStringOI(_xOI.getListElementObjectInspector())) {
if (_parseFeatureAsInt) {
if (_p == -1) {
this._model = new FMIntFeatureMapModel(_classification, _factor, _lambda0, _sigma,
_seed, _min_target, _max_target, _etaEstimator, _vInit);
this._model = new FMArrayModel(_classification, _factor, _lambda0, _sigma, _p,
_seed, _min_target, _max_target, _etaEstimator, _vInit);
this._model = new FMStringFeatureMapModel(_classification, _factor, _lambda0, _sigma,
_seed, _min_target, _max_target, _etaEstimator, _vInit);
if (_parseFeatureAsInt) {
if (x == null) {
if (_classification) {
if (_iterations <= 1) {
if (inputBuf == null) {
if (!file.canWrite()) {
if (remain < requiredBytes) {
for (Feature f : x) {
public void train(@Nonnull final Feature[] x, final double y,
final boolean adaptiveRegularization) throws HiveException {
try {
if (adaptiveRegularization) {
assert (_va_rand != null);
final float rnd = _va_rand.nextFloat();
if (rnd < _validationRatio) {
} else {
trainTheta(x, y);
}
} catch (Exception ex) {
for (Feature e : x) {
if (_t == 0) {
if (_iterations > 1) {
if (P <= 0) {
if (_parseFeatureAsInt) {
if (vi == null) {
while (itor.next() != -1) {
if (inputBuf.position() == 0) {
while (inputBuf.remaining() > 0) {
if (inputBuf.remaining() > 0) {
if (logger.isInfoEnabled()) {
while (true) {
if (remain < INT_BYTES) {
while (remain >= INT_BYTES) {
if (remain < recordBytes) {
public String getFeature() {
return Integer.toString(index);
}
@Override
opts.addOption("va_threshold", "validation_threshold", true,
"Threshold to start validation. "
"At least N training examples are used before validation [default: 1000]");
if (Character.isLetter(last)) {
switch (last) {
if (s == null) {
if (size < 0) {
if (size < 1024) {
if (kb < 1024f) {
if (mb < 1024f) {
public static boolean isFinite(final double v) {
return (v > Double.NEGATIVE_INFINITY) & (v < Double.POSITIVE_INFINITY);
}
public static boolean isFinite(final float v) {
return (v > Float.NEGATIVE_INFINITY) & (v < Float.POSITIVE_INFINITY);
}
this(true, true);
}
public DeflateCodec(boolean compress, boolean decompress) {
this.compressor = compress ? new Deflater(DEFAULT_COMPRESSION, true) : null;
this.decompressor = decompress ? new Inflater(true) : null;
if (compressor != null) {
compressor.end();
}
if (decompressor != null) {
decompressor.end();
}
import hivemall.smile.data.Attribute;
import hivemall.smile.data.Attribute.AttributeType;
import hivemall.smile.data.Attribute.NominalAttribute;
import hivemall.smile.data.Attribute.NumericAttribute;
import smile.sort.QuickSort;
public final class SmileExtUtils {
if (opt == null) {
if ("Q".equals(type)) {
attr[i] = new NumericAttribute(i);
} else if ("C".equals(type)) {
attr[i] = new NominalAttribute(i);
public static Attribute[] attributeTypes(@Nullable Attribute[] attributes,
@Nonnull final double[][] x) {
if (attributes == null) {
int p = x[0].length;
attributes = new Attribute[p];
attributes[i] = new NumericAttribute(i);
}
} else {
int size = attributes.length;
Attribute attr = attributes[j];
if (attr.type == AttributeType.NOMINAL) {
int max_x = 0;
int x_ij = (int) x[i][j];
if (x_ij > max_x) {
max_x = x_ij;
}
}
}
}
}
return attributes;
}
@Nonnull
public static Attribute[] convertAttributeTypes(@Nonnull final smile.data.Attribute[] original) {
final int size = original.length;
final Attribute[] dst = new Attribute[size];
smile.data.Attribute o = original[i];
switch (o.type) {
case NOMINAL:
dst[i] = new NominalAttribute(i);
break;
case NUMERIC:
dst[i] = new NumericAttribute(i);
break;
default:
}
}
return dst;
}
@Nonnull
public static int[][] sort(@Nonnull final Attribute[] attributes, @Nonnull final double[][] x) {
final int n = x.length;
final int p = x[0].length;
final double[] a = new double[n];
final int[][] index = new int[p][];
if (attributes[j].type == AttributeType.NUMERIC) {
a[i] = x[i][j];
}
index[j] = QuickSort.sort(a);
}
}
return index;
}
@Nonnull
final int[] labels = smile.math.Math.unique(y);
if (labels.length < 2) {
if (labels[i] < 0) {
if (i > 0 && labels[i] - labels[i - 1] > 1) {
if ("gini".equalsIgnoreCase(ruleName)) {
} else if ("entropy".equalsIgnoreCase(ruleName)) {
if (numVars <= 0.f) {
} else if (numVars > 0.f && numVars <= 1.f) {
for (int i = x.length; i > 1; i--) {
swap(x, i - 1, j);
if (x.length != y.length) {
if (seed == -1L) {
for (int i = x.length; i > 1; i--) {
swap(y, i - 1, j);
if (x.length != y.length) {
if (seed == -1L) {
for (int i = x.length; i > 1; i--) {
swap(y, i - 1, j);
private static void swap(final int[] x, final int i, final int j) {
int s = x[i];
x[i] = x[j];
x[j] = s;
}
private static void swap(final double[] x, final int i, final int j) {
double s = x[i];
x[i] = x[j];
x[j] = s;
}
private static void swap(final double[][] x, final int i, final int j) {
import hivemall.smile.data.Attribute;
import hivemall.smile.data.Attribute.AttributeType;
import hivemall.utils.io.FastByteArrayInputStream;
import hivemall.utils.io.FastMultiByteArrayOutputStream;
import hivemall.utils.io.IOUtils;
import java.io.Externalizable;
import java.io.IOException;
import java.io.InputStream;
import java.io.ObjectInput;
import java.io.ObjectInputStream;
import java.io.ObjectOutput;
import java.io.ObjectOutputStream;
import java.io.OutputStream;
import java.util.zip.DeflaterOutputStream;
import java.util.zip.InflaterInputStream;
import org.apache.hadoop.hive.ql.metadata.HiveException;
private final int _maxDepth;
static class Node implements Externalizable {
@Nullable
transient Attribute[] attributes;
public Node(@Nonnull Attribute[] attributes) {
this.attributes = attributes;
}
public Node(@Nonnull Attribute[] attributes, int output) {
this.attributes = attributes;
public int predict(final double[] x) {
if (trueChild == null && falseChild == null) {
if (attributes[splitFeature].type == AttributeType.NOMINAL) {
if (x[splitFeature] == splitValue) {
} else if (attributes[splitFeature].type == AttributeType.NUMERIC) {
if (x[splitFeature] <= splitValue) {
attributes[splitFeature].type);
public int predict(final double[] x, final Attribute[] attributes) {
if (trueChild == null && falseChild == null) {
return output;
} else {
if (attributes[splitFeature].type == AttributeType.NOMINAL) {
if (x[splitFeature] == splitValue) {
return trueChild.predict(x);
} else {
return falseChild.predict(x);
}
} else if (attributes[splitFeature].type == AttributeType.NUMERIC) {
if (x[splitFeature] <= splitValue) {
return trueChild.predict(x);
} else {
return falseChild.predict(x);
}
} else {
throw new IllegalStateException("Unsupported attribute type: "
attributes[splitFeature].type);
}
}
}
public void jsCodegen(@Nonnull final StringBuilder builder, final int depth) {
if (trueChild == null && falseChild == null) {
if (attributes[splitFeature].type == AttributeType.NOMINAL) {
builder.append("if(x[")
.append(splitFeature)
.append("] == ")
.append(splitValue)
.append(") {\n");
} else if (attributes[splitFeature].type == AttributeType.NUMERIC) {
builder.append("if(x[")
.append(splitFeature)
.append("] <= ")
.append(splitValue)
.append(") {\n");
attributes[splitFeature].type);
public int opCodegen(final List<String> scripts, int depth) {
if (trueChild == null && falseChild == null) {
if (attributes[splitFeature].type == AttributeType.NOMINAL) {
int trueDepth = trueChild.opCodegen(scripts, depth);
} else if (attributes[splitFeature].type == AttributeType.NUMERIC) {
int trueDepth = trueChild.opCodegen(scripts, depth);
attributes[splitFeature].type);
@Override
public void writeExternal(ObjectOutput out) throws IOException {
out.writeInt(output);
out.writeInt(splitFeature);
out.writeDouble(splitValue);
if (trueChild == null) {
out.writeBoolean(false);
} else {
out.writeBoolean(true);
trueChild.writeExternal(out);
}
if (falseChild == null) {
out.writeBoolean(false);
} else {
out.writeBoolean(true);
falseChild.writeExternal(out);
}
}
@Override
public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {
if(attributes == null) {
throw new IOException("attributes is not set");
}
this.output = in.readInt();
this.splitFeature = in.readInt();
this.splitValue = in.readDouble();
if (in.readBoolean()) {
this.trueChild = new Node(attributes);
trueChild.readExternal(in);
}
if (in.readBoolean()) {
this.falseChild = new Node(attributes);
falseChild.readExternal(in);
}
}
final int depth;
public TrainNode(Node node, double[][] x, int[] y, int[] samples, int depth) {
this.depth = depth;
if (depth >= _maxDepth) {
return false;
}
final int N = x.length;
if (samples[i] > 0) {
if (label == -1) {
} else if (y[i] != label) {
if (pure) {
final int[] count = new int[_k];
if (samples[i] > 0) {
final double impurity = impurity(count, n, _rule);
final int p = _attributes.length;
final int[] variableIndex = new int[p];
variableIndex[i] = i;
}
if (_numVars < p) {
SmileExtUtils.shuffle(variableIndex, _rnd);
final int[] falseCount = new int[_k];
Node split = findBestSplit(n, count, falseCount, impurity, variableIndex[j]);
if (split.splitScore > node.splitScore) {
public Node findBestSplit(final int n, final int[] count, final int[] falseCount,
final double impurity, final int j) {
final int N = x.length;
final Node splitNode = new Node(_attributes);
if (_attributes[j].type == AttributeType.NOMINAL) {
final int m = _attributes[j].getSize();
final int[][] trueCount = new int[m][_k];
if (samples[i] > 0) {
int x_ij = (int) x[i][j];
final int tc = Math.sum(trueCount[l]);
final int fc = n - tc;
if (tc == 0 || fc == 0) {
final double gain = impurity - (double) tc / n
* impurity(trueCount[l], tc, _rule) - (double) fc / n
* impurity(falseCount, fc, _rule);
if (gain > splitNode.splitScore) {
int trueLabel = Math.whichMax(trueCount[l]);
int falseLabel = Math.whichMax(falseCount);
} else if (_attributes[j].type == AttributeType.NUMERIC) {
final int[] trueCount = new int[_k];
for (int i : _order[j]) {
if (samples[i] > 0) {
final double x_ij = x[i][j];
final int y_i = y[i];
if (Double.isNaN(prevx) || x_ij == prevx || y_i == prevy) {
prevx = x_ij;
prevy = y_i;
final int tc = Math.sum(trueCount);
final int fc = n - tc;
if (tc == 0 || fc == 0) {
prevx = x_ij;
prevy = y_i;
final double gain = impurity - (double) tc / n
* impurity(trueCount, tc, _rule) - (double) fc / n
* impurity(falseCount, fc, _rule);
if (gain > splitNode.splitScore) {
int trueLabel = Math.whichMax(trueCount);
int falseLabel = Math.whichMax(falseCount);
prevx = x_ij;
prevy = y_i;
public boolean split(@Nullable final PriorityQueue<TrainNode> nextSplits) {
if (node.splitFeature < 0) {
final int n = x.length;
final int[] trueSamples = new int[n];
final int[] falseSamples = new int[n];
if (_attributes[node.splitFeature].type == AttributeType.NOMINAL) {
if (samples[i] > 0) {
if (x[i][node.splitFeature] == node.splitValue) {
} else if (_attributes[node.splitFeature].type == AttributeType.NUMERIC) {
if (samples[i] > 0) {
if (x[i][node.splitFeature] <= node.splitValue) {
if (tc == 0 || fc == 0) {
node.trueChild = new Node(_attributes, node.trueChildOutput);
node.falseChild = new Node(_attributes, node.falseChildOutput);
if (tc >= _minSplit && trueChild.findBestSplit()) {
if (nextSplits != null) {
final TrainNode falseChild = new TrainNode(node.falseChild, x, y, falseSamples,
if (fc >= _minSplit && falseChild.findBestSplit()) {
if (nextSplits != null) {
private static double impurity(@Nonnull final int[] count, final int n,
@Nonnull final SplitRule rule) {
switch (rule) {
if (count[i] > 0) {
if (count[i] > 0) {
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y,
int J) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, J, 2, null, null, SplitRule.GINI, null);
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y,
int numVars, int maxDepth, int numLeafs, int minSplits, @Nullable int[] samples,
@Nullable int[][] order, @Nonnull SplitRule rule, @Nullable smile.math.Random rand) {
if (x.length != y.length) {
throw new IllegalArgumentException(String.format(
"The sizes of X and Y don't match: %d != %d", x.length, y.length));
if (numVars <= 0 || numVars > x[0].length) {
throw new IllegalArgumentException(
if (numLeafs < 2) {
if (_k < 2) {
if (attributes.length != x[0].length) {
this._maxDepth = maxDepth;
final int n = y.length;
final int[] count = new int[_k];
if (samples == null) {
this._root = new Node(_attributes, Math.whichMax(count));
final TrainNode trainRoot = new TrainNode(_root, x, y, samples, 1);
if (numLeafs == Integer.MAX_VALUE) {
if (trainRoot.findBestSplit()) {
final PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
if (trainRoot.findBestSplit()) {
if (node == null) {
public int predict(final double[] x) {
public String predictJsCodegen() {
_root.jsCodegen(buf, 0);
_root.opCodegen(opslist, 0);
@Nonnull
public byte[] predictSerCodegen(boolean compress) throws HiveException {
final Attribute[] attrs = _attributes;
assert (attrs != null);
FastMultiByteArrayOutputStream bos = new FastMultiByteArrayOutputStream();
OutputStream wrapped = compress ? new DeflaterOutputStream(bos) : bos;
ObjectOutputStream oos = null;
try {
oos = new ObjectOutputStream(wrapped);
oos.writeInt(attrs.length);
attrs[i].writeTo(oos);
}
_root.writeExternal(oos);
oos.flush();
} catch (IOException ioe) {
throw new HiveException("IOException cause while serializing DecisionTree object", ioe);
} catch (Exception e) {
throw new HiveException("Exception cause while serializing DecisionTree object", e);
} finally {
IOUtils.closeQuietly(oos);
}
return bos.toByteArray_clear();
}
public static Node deserializeNode(final byte[] serializedObj, final boolean compressed)
throws HiveException {
FastByteArrayInputStream bis = new FastByteArrayInputStream(serializedObj);
InputStream wrapped = compressed ? new InflaterInputStream(bis) : bis;
final Node root;
ObjectInputStream ois = null;
try {
ois = new ObjectInputStream(wrapped);
final int numAttrs = ois.readInt();
final Attribute[] attrs = new Attribute[numAttrs];
attrs[i] = Attribute.readFrom(ois);
}
root = new Node(attrs);
root.readExternal(ois);
} catch (IOException ioe) {
throw new HiveException("IOException cause while deserializing DecisionTree object",
ioe);
} catch (Exception e) {
throw new HiveException("Exception cause while deserializing DecisionTree object", e);
} finally {
IOUtils.closeQuietly(ois);
}
return root;
}
import hivemall.smile.data.Attribute;
import hivemall.smile.data.Attribute.AttributeType;
import hivemall.utils.io.FastByteArrayInputStream;
import hivemall.utils.io.FastMultiByteArrayOutputStream;
import hivemall.utils.io.IOUtils;
import java.io.Externalizable;
import java.io.IOException;
import java.io.InputStream;
import java.io.ObjectInput;
import java.io.ObjectInputStream;
import java.io.ObjectOutput;
import java.io.ObjectOutputStream;
import java.io.OutputStream;
import java.util.zip.DeflaterOutputStream;
import java.util.zip.InflaterInputStream;
import org.apache.hadoop.hive.ql.metadata.HiveException;
static class Node implements Externalizable {
transient Attribute[] attributes;
public Node(@Nonnull Attribute[] attributes) {
this.attributes = attributes;
}
public Node(@Nonnull Attribute[] attributes, double output) {
this.attributes = attributes;
public double predict(final double[] x) {
if (trueChild == null && falseChild == null) {
if (attributes[splitFeature].type == AttributeType.NOMINAL) {
if (x[splitFeature] == splitValue) {
} else if (attributes[splitFeature].type == AttributeType.NUMERIC) {
if (x[splitFeature] <= splitValue) {
attributes[splitFeature].type);
public double predict(final int[] x) {
if (trueChild == null && falseChild == null) {
} else if (x[splitFeature] == (int) splitValue) {
public void jsCodegen(@Nonnull final StringBuilder builder, final int depth) {
if (trueChild == null && falseChild == null) {
if (attributes[splitFeature].type == AttributeType.NOMINAL) {
builder.append("if(x[")
.append(splitFeature)
.append("] == ")
.append(splitValue)
.append(") {\n");
} else if (attributes[splitFeature].type == AttributeType.NUMERIC) {
builder.append("if(x[")
.append(splitFeature)
.append("] <= ")
.append(splitValue)
.append(") {\n");
attributes[splitFeature].type);
public int opCodegen(final List<String> scripts, int depth) {
if (trueChild == null && falseChild == null) {
if (attributes[splitFeature].type == AttributeType.NOMINAL) {
int trueDepth = trueChild.opCodegen(scripts, depth);
} else if (attributes[splitFeature].type == AttributeType.NUMERIC) {
int trueDepth = trueChild.opCodegen(scripts, depth);
attributes[splitFeature].type);
@Override
public void writeExternal(ObjectOutput out) throws IOException {
out.writeDouble(output);
out.writeInt(splitFeature);
out.writeDouble(splitValue);
if (trueChild == null) {
out.writeBoolean(false);
} else {
out.writeBoolean(true);
trueChild.writeExternal(out);
}
if (falseChild == null) {
out.writeBoolean(false);
} else {
out.writeBoolean(true);
falseChild.writeExternal(out);
}
}
@Override
public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {
if (attributes == null) {
throw new IOException("attributes is not set");
}
this.output = in.readDouble();
this.splitFeature = in.readInt();
this.splitValue = in.readDouble();
if (in.readBoolean()) {
this.trueChild = new Node(attributes);
trueChild.readExternal(in);
}
if (in.readBoolean()) {
this.falseChild = new Node(attributes);
falseChild.readExternal(in);
}
}
public int compareTo(final TrainNode a) {
public void calculateOutput(final NodeOutput output) {
if (node.trueChild == null && node.falseChild == null) {
if (trueChild != null) {
if (falseChild != null) {
if (depth >= _maxDepth) {
for (int s : samples) {
if (n <= _minSplit) {
final double sum = node.output * n;
final int p = _attributes.length;
final int[] variables = new int[p];
if (_numVars < p) {
if (split.splitScore > node.splitScore) {
public Node findBestSplit(final int n, final double sum, final int j) {
final int N = x.length;
final Node split = new Node(_attributes, 0.0);
if (_attributes[j].type == AttributeType.NOMINAL) {
final int m = _attributes[j].getSize();
final double[] trueSum = new double[m];
final int[] trueCount = new int[m];
if (samples[i] > 0) {
final double tc = (double) trueCount[k];
final double fc = n - tc;
if (tc == 0 || fc == 0) {
final double trueMean = trueSum[k] / tc;
final double falseMean = (sum - trueSum[k]) / fc;
if (gain > split.splitScore) {
} else if (_attributes[j].type == AttributeType.NUMERIC) {
for (int i : _order[j]) {
if (samples[i] > 0) {
if (Double.isNaN(prevx) || x[i][j] == prevx) {
final double falseCount = n - trueCount;
if (trueCount == 0 || falseCount == 0) {
final double trueMean = trueSum / trueCount;
final double falseMean = (sum - trueSum) / falseCount;
* falseMean * falseMean)
if (gain > split.splitScore) {
public boolean split(final PriorityQueue<TrainNode> nextSplits) {
if (node.splitFeature < 0) {
final int n = x.length;
final int[] trueSamples = new int[n];
final int[] falseSamples = new int[n];
if (_attributes[node.splitFeature].type == AttributeType.NOMINAL) {
if (samples[i] > 0) {
if (x[i][node.splitFeature] == node.splitValue) {
} else if (_attributes[node.splitFeature].type == AttributeType.NUMERIC) {
if (samples[i] > 0) {
if (x[i][node.splitFeature] <= node.splitValue) {
if (tc == 0 || fc == 0) {
node.trueChild = new Node(_attributes, node.trueChildOutput);
node.falseChild = new Node(_attributes, node.falseChildOutput);
if (tc >= _minSplit && trueChild.findBestSplit()) {
if (nextSplits != null) {
if (fc >= _minSplit && falseChild.findBestSplit()) {
if (nextSplits != null) {
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x,
@Nonnull double[] y, int maxLeafs) {
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x,
@Nonnull double[] y, int numVars, int maxDepth, int maxLeafs, int minSplits,
@Nullable int[][] order, @Nullable int[] samples, @Nullable smile.math.Random rand) {
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x,
@Nonnull double[] y, int numVars, int maxDepth, int maxLeafs, int minSplits,
@Nullable int[][] order, @Nullable int[] samples, @Nullable NodeOutput output,
@Nullable smile.math.Random rand) {
if (x.length != y.length) {
throw new IllegalArgumentException(String.format(
"The sizes of X and Y don't match: %d != %d", x.length, y.length));
if (numVars <= 0 || numVars > x[0].length) {
throw new IllegalArgumentException(
if (minSplits <= 0) {
if (maxLeafs < 2) {
if (attributes.length != x[0].length) {
if (samples == null) {
this._root = new Node(_attributes, sum / n);
if (maxLeafs == Integer.MAX_VALUE) {
if (trainRoot.findBestSplit()) {
if (trainRoot.findBestSplit()) {
if (node == null) {
if (output != null) {
public String predictJsCodegen() {
_root.jsCodegen(buf, 0);
_root.opCodegen(opslist, 0);
@Nonnull
public byte[] predictSerCodegen(boolean compress) throws HiveException {
final Attribute[] attrs = _attributes;
assert (attrs != null);
FastMultiByteArrayOutputStream bos = new FastMultiByteArrayOutputStream();
OutputStream wrapped = compress ? new DeflaterOutputStream(bos) : bos;
ObjectOutputStream oos = null;
try {
oos = new ObjectOutputStream(wrapped);
oos.writeInt(attrs.length);
attrs[i].writeTo(oos);
}
_root.writeExternal(oos);
oos.flush();
} catch (IOException ioe) {
throw new HiveException("IOException cause while serializing DecisionTree object", ioe);
} catch (Exception e) {
throw new HiveException("Exception cause while serializing DecisionTree object", e);
} finally {
IOUtils.closeQuietly(oos);
}
return bos.toByteArray_clear();
}
public static Node deserializeNode(final byte[] serializedObj, final boolean compressed)
throws HiveException {
FastByteArrayInputStream bis = new FastByteArrayInputStream(serializedObj);
InputStream wrapped = compressed ? new InflaterInputStream(bis) : bis;
final Node root;
ObjectInputStream ois = null;
try {
ois = new ObjectInputStream(wrapped);
final int numAttrs = ois.readInt();
final Attribute[] attrs = new Attribute[numAttrs];
attrs[i] = Attribute.readFrom(ois);
}
root = new Node(attrs);
root.readExternal(ois);
} catch (IOException ioe) {
throw new HiveException("IOException cause while deserializing DecisionTree object",
ioe);
} catch (Exception e) {
throw new HiveException("Exception cause while deserializing DecisionTree object", e);
} finally {
IOUtils.closeQuietly(ois);
}
return root;
}
@Deprecated
@Deprecated
import hivemall.smile.ModelType;
import hivemall.smile.data.Attribute;
import hivemall.utils.compress.DeflateCodec;
import hivemall.utils.io.IOUtils;
import java.io.IOException;
@Description(
name = "train_randomforest_classifier",
value = "_FUNC_(double[] features, int label [, string options]) - "
"Returns a relation consists of <string pred_model, double[] var_importance, int oob_errors, int oob_tests>")
private int _maxDepth;
private ModelType _outputType;
opts.addOption("trees", "num_trees", true,
"The number of trees for each task [default: 50]");
opts.addOption("vars", "num_variables", true,
"The number of random selected features [default: round(max(sqrt(x[0].length),x[0].length/3.0))]."
" int(num_variables * x[0].length) is considered if num_variable is (0,1]");
opts.addOption("depth", "max_depth", true,
"The maximum number of the tree depth [default: Integer.MAX_VALUE]");
opts.addOption("leafs", "max_leaf_nodes", true,
"The maximum number of leaf nodes [default: Integer.MAX_VALUE]");
opts.addOption("splits", "min_split", true,
"A node that has greater than or equals to `min_split` examples will split [default: 2]");
opts.addOption("output", "output_type", true,
"The output type (opscode/vm or javascript/js) [default: opscode]");
opts.addOption("disable_compression", false,
"Whether to disable compression of the output script [default: false]");
int trees = 50, maxDepth = Integer.MAX_VALUE, numLeafs = Integer.MAX_VALUE, minSplits = 2;
float numVars = -1.f;
boolean compress = true;
if (argOIs.length >= 3) {
trees = Primitives.parseInt(cl.getOptionValue("num_trees"), trees);
if (trees < 1) {
numVars = Primitives.parseFloat(cl.getOptionValue("num_variables"), numVars);
maxDepth = Primitives.parseInt(cl.getOptionValue("max_depth"), maxDepth);
numLeafs = Primitives.parseInt(cl.getOptionValue("max_leaf_nodes"), numLeafs);
minSplits = Primitives.parseInt(cl.getOptionValue("min_split"), minSplits);
if (cl.hasOption("disable_compression")) {
compress = false;
}
this._numTrees = trees;
this._numVars = numVars;
this._maxLeafNodes = numLeafs;
this._minSamplesSplit = minSplits;
this._outputType = ModelType.resolve(output, compress);
if (argOIs.length != 2 && argOIs.length != 3) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 or 3 arguments: double[] features, int label [, const string options]: "
argOIs.length);
ArrayList<String> fieldNames = new ArrayList<String>(5);
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(5);
fieldNames.add("model_id");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
if (args[0] == null) {
if (numExamples > 0) {
if (_minSamplesSplit <= 0) {
if (x.length != y.length) {
throw new HiveException(String.format("The sizes of X and Y don't match: %d != %d",
x.length, y.length));
if (logger.isInfoEnabled()) {
tasks.add(new TrainingTask(this, i, attributes, x, y, numInputVars, _maxDepth,
_maxLeafNodes, _minSamplesSplit, order, prediction, _splitRule, s, remainingTasks));
synchronized void forward(final int modelId, @Nonnull final Text model,
@Nonnull final double[] importance, final int[] y,
final int[][] prediction, final boolean lastTask)
if (lastTask) {
if (prediction[i][pred] > 0) {
if (pred != y[i]) {
Object[] forwardObjs = new Object[5];
forwardObjs[0] = new IntWritable(modelId);
forwardObjs[1] = model;
forwardObjs[2] = WritableUtils.toWritableList(importance);
forwardObjs[3] = new IntWritable(oobErrors);
forwardObjs[4] = new IntWritable(oobTests);
private final int _maxDepth;
private final int _taskId;
TrainingTask(RandomForestClassifierUDTF udtf, int taskId, Attribute[] attributes,
double[][] x, int[] y, int numVars, int maxDepth, int maxLeafs,
int minSamplesSplit, int[][] order, int[][] prediction, SplitRule splitRule,
long seed, AtomicInteger remainingTasks) {
this._taskId = taskId;
this._maxDepth = maxDepth;
long s = (this._seed == -1L) ? SmileExtUtils.generateSeed() : new smile.math.Random(
_seed).nextLong();
DecisionTree tree = new DecisionTree(_attributes, _x, _y, _numVars, _maxDepth,
_maxLeafs, _minSamplesSplit, samples, _order, _splitRule, rnd2);
if (samples[i] == 0) {
synchronized (_prediction[i]) {
Text model = getModel(tree, _udtf._outputType);
private static Text getModel(@Nonnull final DecisionTree tree,
@Nonnull final ModelType outputType) throws HiveException {
final Text model;
case serialization:
case serialization_compressed: {
byte[] b = tree.predictSerCodegen(outputType.isCompressed());
model = new Text(b);
case opscode:
case opscode_compressed: {
String s = tree.predictOpCodegen(StackMachine.SEP);
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
model = new Text(b);
} else {
model = new Text(s);
}
case javascript:
case javascript_compressed: {
String s = tree.predictJsCodegen();
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
model = new Text(b);
} else {
model = new Text(s);
}
break;
}
default:
import hivemall.smile.ModelType;
import hivemall.smile.data.Attribute;
import hivemall.utils.compress.DeflateCodec;
import hivemall.utils.io.IOUtils;
import java.io.IOException;
@Description(
name = "train_randomforest_regression",
value = "_FUNC_(double[] features, double target [, string options]) - "
"Returns a relation consists of <string pred_model, double[] var_importance, int oob_errors, int oob_tests>")
private ModelType _outputType;
opts.addOption("trees", "num_trees", true,
"The number of trees for each task [default: 50]");
opts.addOption("vars", "num_variables", true,
"The number of random selected features [default: round(max(sqrt(x[0].length),x[0].length/3.0))]."
" int(num_variables * x[0].length) is considered if num_variable is (0,1]");
opts.addOption("depth", "max_depth", true,
"The maximum number of the tree depth [default: Integer.MAX_VALUE]");
opts.addOption("leafs", "max_leaf_nodes", true,
"The maximum number of leaf nodes [default: Integer.MAX_VALUE]");
opts.addOption("split", "min_split", true,
"A node that has greater than or equals to `min_split` examples will split [default: 5]");
opts.addOption("output", "output_type", true,
"The output type (opscode/vm or javascript/js) [default: opscode]");
opts.addOption("disable_compression", false,
"Whether to disable compression of the output script [default: false]");
boolean compress = true;
if (argOIs.length >= 3) {
if (trees < 1) {
if (cl.hasOption("disable_compression")) {
compress = false;
}
this._outputType = ModelType.resolve(output, compress);
if (argOIs.length != 2 && argOIs.length != 3) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 or 3 arguments: double[] features, double target [, const string options]: "
argOIs.length);
ArrayList<String> fieldNames = new ArrayList<String>(5);
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(5);
fieldNames.add("model_id");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
if (args[0] == null) {
if (numExamples > 0) {
if (_minSamplesSplit <= 0) {
if (_maxDepth < 1) {
if (x.length != y.length) {
throw new HiveException(String.format("The sizes of X and Y don't match: %d != %d",
x.length, y.length));
if (logger.isInfoEnabled()) {
int numExamples = x.length;
tasks.add(new TrainingTask(this, i, attributes, x, y, numInputVars, _maxDepth,
_maxLeafNodes, _minSamplesSplit, order, prediction, oob, s, remainingTasks));
synchronized void forward(final int modelId, @Nonnull final Text model,
@Nonnull final double[] importance, final double[] y, final double[] prediction,
final int[] oob, final boolean lastTask) throws HiveException {
if (lastTask) {
if (oob[i] > 0) {
Object[] forwardObjs = new Object[5];
forwardObjs[0] = new IntWritable(modelId);
forwardObjs[1] = model;
forwardObjs[2] = WritableUtils.toWritableList(importance);
forwardObjs[3] = new DoubleWritable(oobErrors);
forwardObjs[4] = new IntWritable(oobTests);
private final int _taskId;
TrainingTask(RandomForestRegressionUDTF udtf, int taskId, Attribute[] attributes,
double[][] x, double[] y, int numVars, int maxDepth, int maxLeafs,
int minSamplesSplit, int[][] order, double[] prediction, int[] oob, long seed,
AtomicInteger remainingTasks) {
this._taskId = taskId;
RegressionTree tree = new RegressionTree(attributes, x, y, numVars, maxDepth, maxLeafs,
minSamplesSplit, order, samples, rnd2);
if (samples[i] == 0) {
synchronized (x[i]) {
Text model = getModel(tree, udtf._outputType);
private static Text getModel(@Nonnull final RegressionTree tree,
@Nonnull final ModelType outputType) throws HiveException {
final Text model;
case serialization:
case serialization_compressed: {
byte[] b = tree.predictSerCodegen(outputType.isCompressed());
model = new Text(b);
case opscode:
case opscode_compressed: {
String s = tree.predictOpCodegen(StackMachine.SEP);
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
model = new Text(b);
} else {
model = new Text(s);
}
case javascript:
case javascript_compressed: {
String s = tree.predictJsCodegen();
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
model = new Text(b);
} else {
model = new Text(s);
}
break;
}
default:
import hivemall.smile.ModelType;
import hivemall.smile.data.Attribute;
import hivemall.utils.compress.DeflateCodec;
import hivemall.utils.io.IOUtils;
import java.io.IOException;
import org.apache.hadoop.io.Text;
@Description(
name = "train_gradient_tree_boosting_classifier",
value = "_FUNC_(double[] features, int label [, string options]) - "
"Returns a relation consists of <string pred_model, double[] var_importance, int oob_errors, int oob_tests>")
private ModelType _outputType;
opts.addOption("trees", "num_trees", true,
"The number of trees for each task [default: 500]");
opts.addOption("eta", "learning_rate", true,
"The learning rate (0, 1]  of procedure [default: 0.05]");
opts.addOption("subsample", "sampling_frac", true,
"The fraction of samples to be used for fitting the individual base learners [default: 0.7]");
opts.addOption("vars", "num_variables", true,
"The number of random selected features [default: round(max(sqrt(x[0].length),x[0].length/3.0))]."
" int(num_variables * x[0].length) is considered if num_variable is (0,1]");
opts.addOption("depth", "max_depth", true,
"The maximum number of the tree depth [default: 8]");
opts.addOption("leafs", "max_leaf_nodes", true,
"The maximum number of leaf nodes [default: Integer.MAX_VALUE]");
opts.addOption("splits", "min_split", true,
"A node that has greater than or equals to `min_split` examples will split [default: 5]");
opts.addOption("output", "output_type", true,
"The output type (opscode/vm or javascript/js) [default: opscode]");
opts.addOption("disable_compression", false,
"Whether to disable compression of the output script [default: false]");
boolean compress = true;
if (argOIs.length >= 3) {
if (trees < 1) {
if (cl.hasOption("disable_compression")) {
compress = false;
}
this._outputType = ModelType.resolve(output, compress);
if (argOIs.length != 2 && argOIs.length != 3) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 or 3 arguments: double[] features, int label [, const string options]: "
argOIs.length);
if (args[0] == null) {
if (_eta <= 0.d || _eta > 1.d) {
if (_subsample <= 0.d || _subsample > 1.d) {
if (_minSamplesSplit <= 0) {
if (_maxDepth < 1) {
if (x.length != y.length) {
throw new HiveException(String.format("The sizes of X and Y don't match: %d != %d",
x.length, y.length));
if (k < 2) {
throw new UDFArgumentException("Only one class or negative class labels.");
if (k == 2) {
if (y[i] == 1) {
if (logger.isInfoEnabled()) {
final int[][] order = SmileExtUtils.sort(_attributes, x);
RegressionTree tree = new RegressionTree(_attributes, x, response, numVars, _maxDepth,
_maxLeafNodes, _minSamplesSplit, order, samples, output, rnd2);
if (samples[i] == 0) {
if (pred != y[i]) {
if (oobTests > 0) {
if (logger.isInfoEnabled()) {
final int[][] order = SmileExtUtils.sort(_attributes, x);
if (max < h_ji) {
if (y[i] == j) {
RegressionTree tree = new RegressionTree(_attributes, x, response[j], numVars,
_maxDepth, _maxLeafNodes, _minSamplesSplit, order, samples, output[j], rnd2);
if (h_ji > max_h) {
if (samples[i] == 0) {
if (prediction[i] != y[i]) {
if (oobTests > 0) {
private void forward(final int m, final double intercept, final double shrinkage,
final float oobErrorRate, @Nonnull final RegressionTree... trees) throws HiveException {
Text[] models = getModel(trees, _outputType);
for (RegressionTree tree : trees) {
forwardObjs[1] = models;
private static Text[] getModel(@Nonnull final RegressionTree[] trees,
@Nonnull final ModelType outputType) throws HiveException {
final Text[] models = new Text[m];
case serialization:
case serialization_compressed: {
byte[] b = trees[i].predictSerCodegen(outputType.isCompressed());
models[i] = new Text(b);
case opscode:
case opscode_compressed: {
String s = trees[i].predictOpCodegen(StackMachine.SEP);
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
models[i] = new Text(b);
} else {
models[i] = new Text(s);
}
case javascript:
case javascript_compressed: {
String s = trees[i].predictJsCodegen();
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
models[i] = new Text(b);
} else {
models[i] = new Text(s);
}
}
break;
}
default:
if (samples[i] > 0) {
if (samples[i] > 0) {
if (de < 1E-10d) {
this.codec = new DeflateCodec(true, false);
this.codec = new DeflateCodec(false, true);
public static class Node implements Externalizable {
AttributeType splitFeatureType = null;
public Node(int output) {
if (splitFeatureType == AttributeType.NOMINAL) {
} else if (splitFeatureType == AttributeType.NUMERIC) {
splitFeatureType);
if (splitFeatureType == AttributeType.NOMINAL) {
} else if (splitFeatureType == AttributeType.NUMERIC) {
splitFeatureType);
if (splitFeatureType == AttributeType.NOMINAL) {
} else if (splitFeatureType == AttributeType.NUMERIC) {
splitFeatureType);
if (splitFeatureType == null) {
out.writeInt(-1);
} else {
out.writeInt(splitFeatureType.getTypeId());
}
int typeId = in.readInt();
if (typeId == -1) {
this.splitFeatureType = null;
} else {
this.splitFeatureType = AttributeType.resolve(typeId);
}
this.trueChild = new Node();
this.falseChild = new Node();
node.splitFeatureType = split.splitFeatureType;
final double impurity, final int j) {
final Node splitNode = new Node();
splitNode.splitFeatureType = AttributeType.NOMINAL;
splitNode.splitFeatureType = AttributeType.NUMERIC;
if (node.splitFeatureType == AttributeType.NOMINAL) {
} else if (node.splitFeatureType == AttributeType.NUMERIC) {
node.splitFeatureType);
node.splitFeatureType = null;
node.trueChild = new Node(node.trueChildOutput);
node.falseChild = new Node(node.falseChildOutput);
@Nonnull final SplitRule rule) {
this._root = new Node(Math.whichMax(count));
public static Node deserializeNode(final byte[] serializedObj, final int length, final boolean compressed)
root = new Node();
public interface NodeOutput {
public static class Node implements Externalizable {
AttributeType splitFeatureType = null;
public Node(double output) {
if (splitFeatureType == AttributeType.NOMINAL) {
} else if (splitFeatureType == AttributeType.NUMERIC) {
splitFeatureType);
if (splitFeatureType == AttributeType.NOMINAL) {
} else if (splitFeatureType == AttributeType.NUMERIC) {
splitFeatureType);
if (splitFeatureType == AttributeType.NOMINAL) {
} else if (splitFeatureType == AttributeType.NUMERIC) {
splitFeatureType);
if (splitFeatureType == null) {
out.writeInt(-1);
} else {
out.writeInt(splitFeatureType.getTypeId());
}
int typeId = in.readInt();
if (typeId == -1) {
this.splitFeatureType = null;
} else {
this.splitFeatureType = AttributeType.resolve(typeId);
}
this.trueChild = new Node();
this.falseChild = new Node();
node.splitFeatureType = split.splitFeatureType;
final Node split = new Node(0.d);
split.splitFeatureType = AttributeType.NOMINAL;
split.splitFeatureType = AttributeType.NUMERIC;
if (node.splitFeatureType == AttributeType.NOMINAL) {
} else if (node.splitFeatureType == AttributeType.NUMERIC) {
node.splitFeatureType);
node.splitFeatureType = null;
node.trueChild = new Node(node.trueChildOutput);
node.falseChild = new Node(node.falseChildOutput);
this._root = new Node(sum / n);
public static Node deserializeNode(final byte[] serializedObj, final int length, final boolean compressed)
FastByteArrayInputStream bis = new FastByteArrayInputStream(serializedObj, length);
root = new Node();
fieldNames.add("model_type");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
Object[] forwardObjs = new Object[7];
forwardObjs[1] = new IntWritable(_outputType.getId());
forwardObjs[2] = models;
forwardObjs[3] = new DoubleWritable(intercept);
forwardObjs[4] = new DoubleWritable(shrinkage);
forwardObjs[5] = WritableUtils.toWritableList(importance);
forwardObjs[6] = new FloatWritable(oobErrorRate);
ArrayList<String> fieldNames = new ArrayList<String>(6);
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(6);
fieldNames.add("model_type");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
Object[] forwardObjs = new Object[6];
forwardObjs[1] = new IntWritable(_outputType.getId());
forwardObjs[2] = model;
forwardObjs[3] = WritableUtils.toWritableList(importance);
forwardObjs[4] = new IntWritable(oobErrors);
forwardObjs[5] = new IntWritable(oobTests);
fieldNames.add("model_type");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
Object[] forwardObjs = new Object[6];
forwardObjs[1] = new IntWritable(_outputType.getId());
forwardObjs[2] = model;
forwardObjs[3] = WritableUtils.toWritableList(importance);
forwardObjs[4] = new DoubleWritable(oobErrors);
forwardObjs[5] = new IntWritable(oobTests);
String output = "serialization";
String output = "serialization";
String output = "serialization";
@Description(name = "train_gradient_tree_boosting_classifier",
"Returns a relation consists of "
"<int iteration, int model_type, array<string> pred_models, double intercept, "
"double shrinkage, array<double> var_importance, float oob_error_rate>")
fieldNames.add("pred_models");
"Returns a relation consists of "
"<int model_id, int model_type, string pred_model, array<double> var_importance, int oob_errors, int oob_tests>")
@Nonnull final double[] importance, final int[] y, final int[][] prediction,
final boolean lastTask) throws HiveException {
@Nonnull final ModelType outputType) throws HiveException {
"Returns a relation consists of "
"<int model_id, int model_type, string pred_model, array<double> var_importance, int oob_errors, int oob_tests>")
public static Node deserializeNode(final byte[] serializedObj, final int length,
final boolean compressed) throws HiveException {
@Override
public String toString() {
return _root == null ? "" : predictJsCodegen();
}
this._maxDepth = maxDepth;
if (_maxDepth < 1) {
}
FastByteArrayInputStream bis = new FastByteArrayInputStream(serializedObj, length);
import javax.annotation.Nullable;
import org.apache.hadoop.io.BytesWritable;
@Description(name = "deflate", value = "_FUNC_(TEXT data [, const int compressionLevel]) - "
"Returns a compressed BINARY obeject by using Deflater.",
extended = "The compression level must be in range [-1,9]")
@Nullable
@Nullable
private transient BytesWritable result;
return PrimitiveObjectInspectorFactory.writableBinaryObjectInspector;
if (result == null) {
this.result = new BytesWritable(compressed);
} else {
result.set(compressed, 0, compressed.length);
}
return result;
this.result = null;
import javax.annotation.Nonnull;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
import org.apache.hadoop.io.BytesWritable;
value = "_FUNC_(BINARY compressedData) - Returns a decompressed STRING by using Inflater")
private BinaryObjectInspector binaryOI;
@Nonnull
@Nonnull
private transient Text result;
this.binaryOI = HiveUtils.asBinaryOI(argOIs[0]);
BytesWritable b = binaryOI.getPrimitiveWritableObject(arg0);
byte[] compressed = b.getBytes();
final int len = b.getLength();
if (result == null) {
result = new Text(decompressed);
} else {
result.set(decompressed, 0, decompressed.length);
}
return result;
this.result = null;
import static hivemall.HivemallConstants.BINARY_TYPE_NAME;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
public static BinaryObjectInspector asBinaryOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentException {
if (!BINARY_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (BinaryObjectInspector) argOI;
}
public static PrimitiveObjectInspector asDoubleCompatibleOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
public BytesWritable evaluate(DeferredObject[] arguments) throws HiveException {
public Text evaluate(DeferredObject[] arguments) throws HiveException {
import javax.annotation.CheckForNull;
private byte[] buf;
this.count = 0;
}
public FastByteArrayOutputStream(@CheckForNull byte[] buf) {
if (buf == null) {
throw new IllegalArgumentException("buf should not be NULL");
}
this.buf = buf;
this.count = 0;
import hivemall.utils.compress.Base91;
"The output type (serialization/ser or opscode/vm or javascript/js) [default: serialization]");
b = Base91.encode(b);
b = Base91.encode(b);
b = Base91.encode(b);
import hivemall.utils.compress.Base91;
"The output type (serialization/ser or opscode/vm or javascript/js) [default: serialization]");
b = Base91.encode(b);
b = Base91.encode(b);
b = Base91.encode(b);
import hivemall.utils.compress.Base91;
"The output type (serialization/ser or opscode/vm or javascript/js) [default: serialization]");
b = Base91.encode(b);
b = Base91.encode(b);
b = Base91.encode(b);
import hivemall.utils.compress.Base91;
b = Base91.decode(b, 0, length);
this.cNode = DecisionTree.deserializeNode(b, b.length, compressed);
b = Base91.decode(b, 0, length);
this.rNode = RegressionTree.deserializeNode(b, b.length, compressed);
b = Base91.decode(b, 0, len);
b = codec.decompress(b);
b = Base91.decode(b, 0, len);
b = codec.decompress(b);
return encode(input, 0, input.length);
}
@Nonnull
public static byte[] encode(@Nonnull final byte[] input, final int offset, final int len) {
int estimatedSize = (int) Math.ceil(len * WORST_ENCODING_RATIO);
encode(input, offset, len, output);
return decode(input, 0, input.length);
}
@Nonnull
public static byte[] decode(@Nonnull final byte[] input, final int offset, final int len) {
int expectedSize = Math.round(len / BEST_ENCODING_RATIO);
decode(input, offset, len, output);
"The number of random selected features [default: ceil(sqrt(x[0].length))]."
"The number of random selected features [default: ceil(sqrt(x[0].length))]."
"The number of random selected features [default: ceil(sqrt(x[0].length))]."
numInputVars = (int) Math.ceil(Math.sqrt(dims));
final int sample = samples[i];
if (sample > 0) {
for (final int i : _order[j]) {
final int sample = samples[i];
if (sample > 0) {
final int sample = samples[i];
if (sample > 0) {
trueSamples[i] = sample;
falseSamples[i] = sample;
final int sample = samples[i];
if (sample > 0) {
trueSamples[i] = sample;
falseSamples[i] = sample;
final int pred = smile.math.Math.whichMax(prediction[i]);
import hivemall.smile.data.Attribute;
import hivemall.smile.data.Attribute.AttributeType;
import hivemall.utils.io.FastByteArrayInputStream;
import hivemall.utils.io.FastMultiByteArrayOutputStream;
import hivemall.utils.io.IOUtils;
import java.io.Externalizable;
import java.io.IOException;
import java.io.InputStream;
import java.io.ObjectInput;
import java.io.ObjectInputStream;
import java.io.ObjectOutput;
import java.io.ObjectOutputStream;
import java.io.OutputStream;
import java.util.zip.DeflaterOutputStream;
import java.util.zip.InflaterInputStream;
import org.apache.hadoop.hive.ql.metadata.HiveException;
private final int _maxDepth;
public static class Node implements Externalizable {
AttributeType splitFeatureType = null;
public int predict(final double[] x) {
if (trueChild == null && falseChild == null) {
if (splitFeatureType == AttributeType.NOMINAL) {
if (x[splitFeature] == splitValue) {
} else if (splitFeatureType == AttributeType.NUMERIC) {
if (x[splitFeature] <= splitValue) {
splitFeatureType);
public void jsCodegen(@Nonnull final StringBuilder builder, final int depth) {
if (trueChild == null && falseChild == null) {
if (splitFeatureType == AttributeType.NOMINAL) {
builder.append("if(x[")
.append(splitFeature)
.append("] == ")
.append(splitValue)
.append(") {\n");
} else if (splitFeatureType == AttributeType.NUMERIC) {
builder.append("if(x[")
.append(splitFeature)
.append("] <= ")
.append(splitValue)
.append(") {\n");
splitFeatureType);
public int opCodegen(final List<String> scripts, int depth) {
if (trueChild == null && falseChild == null) {
if (splitFeatureType == AttributeType.NOMINAL) {
int trueDepth = trueChild.opCodegen(scripts, depth);
} else if (splitFeatureType == AttributeType.NUMERIC) {
int trueDepth = trueChild.opCodegen(scripts, depth);
splitFeatureType);
@Override
public void writeExternal(ObjectOutput out) throws IOException {
out.writeInt(output);
out.writeInt(splitFeature);
if (splitFeatureType == null) {
out.writeInt(-1);
} else {
out.writeInt(splitFeatureType.getTypeId());
}
out.writeDouble(splitValue);
if (trueChild == null) {
out.writeBoolean(false);
} else {
out.writeBoolean(true);
trueChild.writeExternal(out);
}
if (falseChild == null) {
out.writeBoolean(false);
} else {
out.writeBoolean(true);
falseChild.writeExternal(out);
}
}
@Override
public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {
this.output = in.readInt();
this.splitFeature = in.readInt();
int typeId = in.readInt();
if (typeId == -1) {
this.splitFeatureType = null;
} else {
this.splitFeatureType = AttributeType.resolve(typeId);
}
this.splitValue = in.readDouble();
if (in.readBoolean()) {
this.trueChild = new Node();
trueChild.readExternal(in);
}
if (in.readBoolean()) {
this.falseChild = new Node();
falseChild.readExternal(in);
}
}
final int depth;
public TrainNode(Node node, double[][] x, int[] y, int[] samples, int depth) {
this.depth = depth;
if (depth >= _maxDepth) {
return false;
}
final int N = x.length;
if (samples[i] > 0) {
if (label == -1) {
} else if (y[i] != label) {
if (pure) {
final int[] count = new int[_k];
final int sample = samples[i];
if (sample > 0) {
final double impurity = impurity(count, n, _rule);
final int p = _attributes.length;
final int[] variableIndex = new int[p];
variableIndex[i] = i;
}
if (_numVars < p) {
SmileExtUtils.shuffle(variableIndex, _rnd);
final int[] falseCount = new int[_k];
Node split = findBestSplit(n, count, falseCount, impurity, variableIndex[j]);
if (split.splitScore > node.splitScore) {
node.splitFeatureType = split.splitFeatureType;
public Node findBestSplit(final int n, final int[] count, final int[] falseCount,
final double impurity, final int j) {
final int N = x.length;
final Node splitNode = new Node();
if (_attributes[j].type == AttributeType.NOMINAL) {
final int m = _attributes[j].getSize();
final int[][] trueCount = new int[m][_k];
if (samples[i] > 0) {
int x_ij = (int) x[i][j];
final int tc = Math.sum(trueCount[l]);
final int fc = n - tc;
if (tc == 0 || fc == 0) {
final double gain = impurity - (double) tc / n
* impurity(trueCount[l], tc, _rule) - (double) fc / n
* impurity(falseCount, fc, _rule);
if (gain > splitNode.splitScore) {
int trueLabel = Math.whichMax(trueCount[l]);
int falseLabel = Math.whichMax(falseCount);
splitNode.splitFeatureType = AttributeType.NOMINAL;
} else if (_attributes[j].type == AttributeType.NUMERIC) {
final int[] trueCount = new int[_k];
for (final int i : _order[j]) {
final int sample = samples[i];
if (sample > 0) {
final double x_ij = x[i][j];
final int y_i = y[i];
if (Double.isNaN(prevx) || x_ij == prevx || y_i == prevy) {
prevx = x_ij;
prevy = y_i;
final int tc = Math.sum(trueCount);
final int fc = n - tc;
if (tc == 0 || fc == 0) {
prevx = x_ij;
prevy = y_i;
final double gain = impurity - (double) tc / n
* impurity(trueCount, tc, _rule) - (double) fc / n
* impurity(falseCount, fc, _rule);
if (gain > splitNode.splitScore) {
int trueLabel = Math.whichMax(trueCount);
int falseLabel = Math.whichMax(falseCount);
splitNode.splitFeatureType = AttributeType.NUMERIC;
prevx = x_ij;
prevy = y_i;
public boolean split(@Nullable final PriorityQueue<TrainNode> nextSplits) {
if (node.splitFeature < 0) {
final int n = x.length;
final int[] trueSamples = new int[n];
final int[] falseSamples = new int[n];
if (node.splitFeatureType == AttributeType.NOMINAL) {
final int sample = samples[i];
if (sample > 0) {
if (x[i][node.splitFeature] == node.splitValue) {
trueSamples[i] = sample;
falseSamples[i] = sample;
} else if (node.splitFeatureType == AttributeType.NUMERIC) {
final int sample = samples[i];
if (sample > 0) {
if (x[i][node.splitFeature] <= node.splitValue) {
trueSamples[i] = sample;
falseSamples[i] = sample;
node.splitFeatureType);
if (tc == 0 || fc == 0) {
node.splitFeatureType = null;
if (tc >= _minSplit && trueChild.findBestSplit()) {
if (nextSplits != null) {
final TrainNode falseChild = new TrainNode(node.falseChild, x, y, falseSamples,
if (fc >= _minSplit && falseChild.findBestSplit()) {
if (nextSplits != null) {
private static double impurity(@Nonnull final int[] count, final int n,
@Nonnull final SplitRule rule) {
switch (rule) {
if (count[i] > 0) {
if (count[i] > 0) {
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y,
int J) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, J, 2, null, null, SplitRule.GINI, null);
public DecisionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x, @Nonnull int[] y,
int numVars, int maxDepth, int numLeafs, int minSplits, @Nullable int[] samples,
@Nullable int[][] order, @Nonnull SplitRule rule, @Nullable smile.math.Random rand) {
if (x.length != y.length) {
throw new IllegalArgumentException(String.format(
"The sizes of X and Y don't match: %d != %d", x.length, y.length));
if (numVars <= 0 || numVars > x[0].length) {
throw new IllegalArgumentException(
if (numLeafs < 2) {
if (_k < 2) {
if (attributes.length != x[0].length) {
this._maxDepth = maxDepth;
final int n = y.length;
final int[] count = new int[_k];
if (samples == null) {
final TrainNode trainRoot = new TrainNode(_root, x, y, samples, 1);
if (numLeafs == Integer.MAX_VALUE) {
if (trainRoot.findBestSplit()) {
final PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
if (trainRoot.findBestSplit()) {
if (node == null) {
public int predict(final double[] x) {
public String predictJsCodegen() {
_root.jsCodegen(buf, 0);
_root.opCodegen(opslist, 0);
@Nonnull
public byte[] predictSerCodegen(boolean compress) throws HiveException {
FastMultiByteArrayOutputStream bos = new FastMultiByteArrayOutputStream();
OutputStream wrapped = compress ? new DeflaterOutputStream(bos) : bos;
ObjectOutputStream oos = null;
try {
oos = new ObjectOutputStream(wrapped);
_root.writeExternal(oos);
oos.flush();
} catch (IOException ioe) {
throw new HiveException("IOException cause while serializing DecisionTree object", ioe);
} catch (Exception e) {
throw new HiveException("Exception cause while serializing DecisionTree object", e);
} finally {
IOUtils.closeQuietly(oos);
}
return bos.toByteArray_clear();
}
public static Node deserializeNode(final byte[] serializedObj, final int length,
final boolean compressed) throws HiveException {
FastByteArrayInputStream bis = new FastByteArrayInputStream(serializedObj, length);
InputStream wrapped = compressed ? new InflaterInputStream(bis) : bis;
final Node root;
ObjectInputStream ois = null;
try {
ois = new ObjectInputStream(wrapped);
root = new Node();
root.readExternal(ois);
} catch (IOException ioe) {
throw new HiveException("IOException cause while deserializing DecisionTree object",
ioe);
} catch (Exception e) {
throw new HiveException("Exception cause while deserializing DecisionTree object", e);
} finally {
IOUtils.closeQuietly(ois);
}
return root;
}
@Override
public String toString() {
return _root == null ? "" : predictJsCodegen();
}
import hivemall.smile.ModelType;
import hivemall.smile.data.Attribute;
import hivemall.utils.compress.Base91;
import hivemall.utils.compress.DeflateCodec;
import hivemall.utils.io.IOUtils;
import java.io.IOException;
import org.apache.hadoop.io.Text;
@Description(name = "train_gradient_tree_boosting_classifier",
value = "_FUNC_(double[] features, int label [, string options]) - "
"Returns a relation consists of "
"<int iteration, int model_type, array<string> pred_models, double intercept, "
"double shrinkage, array<double> var_importance, float oob_error_rate>")
private ModelType _outputType;
opts.addOption("trees", "num_trees", true,
"The number of trees for each task [default: 500]");
opts.addOption("eta", "learning_rate", true,
"The learning rate (0, 1]  of procedure [default: 0.05]");
opts.addOption("subsample", "sampling_frac", true,
"The fraction of samples to be used for fitting the individual base learners [default: 0.7]");
opts.addOption("vars", "num_variables", true,
"The number of random selected features [default: ceil(sqrt(x[0].length))]."
" int(num_variables * x[0].length) is considered if num_variable is (0,1]");
opts.addOption("depth", "max_depth", true,
"The maximum number of the tree depth [default: 8]");
opts.addOption("leafs", "max_leaf_nodes", true,
"The maximum number of leaf nodes [default: Integer.MAX_VALUE]");
opts.addOption("splits", "min_split", true,
"A node that has greater than or equals to `min_split` examples will split [default: 5]");
opts.addOption("output", "output_type", true,
"The output type (serialization/ser or opscode/vm or javascript/js) [default: serialization]");
opts.addOption("disable_compression", false,
"Whether to disable compression of the output script [default: false]");
String output = "serialization";
boolean compress = true;
if (argOIs.length >= 3) {
if (trees < 1) {
if (cl.hasOption("disable_compression")) {
compress = false;
}
this._outputType = ModelType.resolve(output, compress);
if (argOIs.length != 2 && argOIs.length != 3) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 or 3 arguments: double[] features, int label [, const string options]: "
argOIs.length);
fieldNames.add("model_type");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
fieldNames.add("pred_models");
if (args[0] == null) {
if (_eta <= 0.d || _eta > 1.d) {
if (_subsample <= 0.d || _subsample > 1.d) {
if (_minSamplesSplit <= 0) {
if (_maxDepth < 1) {
if (x.length != y.length) {
throw new HiveException(String.format("The sizes of X and Y don't match: %d != %d",
x.length, y.length));
if (k < 2) {
throw new UDFArgumentException("Only one class or negative class labels.");
if (k == 2) {
if (y[i] == 1) {
if (logger.isInfoEnabled()) {
final int[][] order = SmileExtUtils.sort(_attributes, x);
RegressionTree tree = new RegressionTree(_attributes, x, response, numVars, _maxDepth,
_maxLeafNodes, _minSamplesSplit, order, samples, output, rnd2);
if (samples[i] == 0) {
if (pred != y[i]) {
if (oobTests > 0) {
if (logger.isInfoEnabled()) {
final int[][] order = SmileExtUtils.sort(_attributes, x);
if (max < h_ji) {
if (y[i] == j) {
RegressionTree tree = new RegressionTree(_attributes, x, response[j], numVars,
_maxDepth, _maxLeafNodes, _minSamplesSplit, order, samples, output[j], rnd2);
if (h_ji > max_h) {
if (samples[i] == 0) {
if (prediction[i] != y[i]) {
if (oobTests > 0) {
private void forward(final int m, final double intercept, final double shrinkage,
final float oobErrorRate, @Nonnull final RegressionTree... trees) throws HiveException {
Text[] models = getModel(trees, _outputType);
for (RegressionTree tree : trees) {
Object[] forwardObjs = new Object[7];
forwardObjs[1] = new IntWritable(_outputType.getId());
forwardObjs[2] = models;
forwardObjs[3] = new DoubleWritable(intercept);
forwardObjs[4] = new DoubleWritable(shrinkage);
forwardObjs[5] = WritableUtils.toWritableList(importance);
forwardObjs[6] = new FloatWritable(oobErrorRate);
private static Text[] getModel(@Nonnull final RegressionTree[] trees,
@Nonnull final ModelType outputType) throws HiveException {
final Text[] models = new Text[m];
case serialization:
case serialization_compressed: {
byte[] b = trees[i].predictSerCodegen(outputType.isCompressed());
b = Base91.encode(b);
models[i] = new Text(b);
case opscode:
case opscode_compressed: {
String s = trees[i].predictOpCodegen(StackMachine.SEP);
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
b = Base91.encode(b);
models[i] = new Text(b);
} else {
models[i] = new Text(s);
}
case javascript:
case javascript_compressed: {
String s = trees[i].predictJsCodegen();
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
b = Base91.encode(b);
models[i] = new Text(b);
} else {
models[i] = new Text(s);
}
}
break;
}
default:
if (samples[i] > 0) {
if (samples[i] > 0) {
if (de < 1E-10d) {
import hivemall.smile.ModelType;
import hivemall.smile.data.Attribute;
import hivemall.utils.compress.Base91;
import hivemall.utils.compress.DeflateCodec;
import hivemall.utils.io.IOUtils;
import java.io.IOException;
@Description(
name = "train_randomforest_classifier",
value = "_FUNC_(double[] features, int label [, string options]) - "
"Returns a relation consists of "
"<int model_id, int model_type, string pred_model, array<double> var_importance, int oob_errors, int oob_tests>")
private int _maxDepth;
private ModelType _outputType;
opts.addOption("trees", "num_trees", true,
"The number of trees for each task [default: 50]");
opts.addOption("vars", "num_variables", true,
"The number of random selected features [default: ceil(sqrt(x[0].length))]."
" int(num_variables * x[0].length) is considered if num_variable is (0,1]");
opts.addOption("depth", "max_depth", true,
"The maximum number of the tree depth [default: Integer.MAX_VALUE]");
opts.addOption("leafs", "max_leaf_nodes", true,
"The maximum number of leaf nodes [default: Integer.MAX_VALUE]");
opts.addOption("splits", "min_split", true,
"A node that has greater than or equals to `min_split` examples will split [default: 2]");
opts.addOption("output", "output_type", true,
"The output type (serialization/ser or opscode/vm or javascript/js) [default: serialization]");
opts.addOption("disable_compression", false,
"Whether to disable compression of the output script [default: false]");
int trees = 50, maxDepth = Integer.MAX_VALUE, numLeafs = Integer.MAX_VALUE, minSplits = 2;
float numVars = -1.f;
String output = "serialization";
boolean compress = true;
if (argOIs.length >= 3) {
trees = Primitives.parseInt(cl.getOptionValue("num_trees"), trees);
if (trees < 1) {
numVars = Primitives.parseFloat(cl.getOptionValue("num_variables"), numVars);
maxDepth = Primitives.parseInt(cl.getOptionValue("max_depth"), maxDepth);
numLeafs = Primitives.parseInt(cl.getOptionValue("max_leaf_nodes"), numLeafs);
minSplits = Primitives.parseInt(cl.getOptionValue("min_split"), minSplits);
if (cl.hasOption("disable_compression")) {
compress = false;
}
this._numTrees = trees;
this._numVars = numVars;
this._maxDepth = maxDepth;
this._maxLeafNodes = numLeafs;
this._minSamplesSplit = minSplits;
this._outputType = ModelType.resolve(output, compress);
if (argOIs.length != 2 && argOIs.length != 3) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 or 3 arguments: double[] features, int label [, const string options]: "
argOIs.length);
ArrayList<String> fieldNames = new ArrayList<String>(6);
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(6);
fieldNames.add("model_id");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
fieldNames.add("model_type");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
if (args[0] == null) {
if (numExamples > 0) {
if (_minSamplesSplit <= 0) {
if (_maxDepth < 1) {
}
if (x.length != y.length) {
throw new HiveException(String.format("The sizes of X and Y don't match: %d != %d",
x.length, y.length));
if (logger.isInfoEnabled()) {
tasks.add(new TrainingTask(this, i, attributes, x, y, numInputVars, _maxDepth,
_maxLeafNodes, _minSamplesSplit, order, prediction, _splitRule, s, remainingTasks));
synchronized void forward(final int modelId, @Nonnull final Text model,
@Nonnull final double[] importance, final int[] y, final int[][] prediction,
final boolean lastTask) throws HiveException {
if (lastTask) {
final int pred = smile.math.Math.whichMax(prediction[i]);
if (prediction[i][pred] > 0) {
if (pred != y[i]) {
Object[] forwardObjs = new Object[6];
forwardObjs[0] = new IntWritable(modelId);
forwardObjs[1] = new IntWritable(_outputType.getId());
forwardObjs[2] = model;
forwardObjs[3] = WritableUtils.toWritableList(importance);
forwardObjs[4] = new IntWritable(oobErrors);
forwardObjs[5] = new IntWritable(oobTests);
private final int _maxDepth;
private final int _taskId;
TrainingTask(RandomForestClassifierUDTF udtf, int taskId, Attribute[] attributes,
double[][] x, int[] y, int numVars, int maxDepth, int maxLeafs,
int minSamplesSplit, int[][] order, int[][] prediction, SplitRule splitRule,
long seed, AtomicInteger remainingTasks) {
this._taskId = taskId;
this._maxDepth = maxDepth;
long s = (this._seed == -1L) ? SmileExtUtils.generateSeed() : new smile.math.Random(
_seed).nextLong();
DecisionTree tree = new DecisionTree(_attributes, _x, _y, _numVars, _maxDepth,
_maxLeafs, _minSamplesSplit, samples, _order, _splitRule, rnd2);
if (samples[i] == 0) {
synchronized (_prediction[i]) {
Text model = getModel(tree, _udtf._outputType);
private static Text getModel(@Nonnull final DecisionTree tree,
@Nonnull final ModelType outputType) throws HiveException {
final Text model;
case serialization:
case serialization_compressed: {
byte[] b = tree.predictSerCodegen(outputType.isCompressed());
b = Base91.encode(b);
model = new Text(b);
case opscode:
case opscode_compressed: {
String s = tree.predictOpCodegen(StackMachine.SEP);
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
b = Base91.encode(b);
model = new Text(b);
} else {
model = new Text(s);
}
case javascript:
case javascript_compressed: {
String s = tree.predictJsCodegen();
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
b = Base91.encode(b);
model = new Text(b);
} else {
model = new Text(s);
}
break;
}
default:
import hivemall.smile.ModelType;
import hivemall.smile.data.Attribute;
import hivemall.utils.compress.Base91;
import hivemall.utils.compress.DeflateCodec;
import hivemall.utils.io.IOUtils;
import java.io.IOException;
@Description(
name = "train_randomforest_regression",
value = "_FUNC_(double[] features, double target [, string options]) - "
"Returns a relation consists of "
"<int model_id, int model_type, string pred_model, array<double> var_importance, int oob_errors, int oob_tests>")
private ModelType _outputType;
opts.addOption("trees", "num_trees", true,
"The number of trees for each task [default: 50]");
opts.addOption("vars", "num_variables", true,
"The number of random selected features [default: ceil(sqrt(x[0].length))]."
" int(num_variables * x[0].length) is considered if num_variable is (0,1]");
opts.addOption("depth", "max_depth", true,
"The maximum number of the tree depth [default: Integer.MAX_VALUE]");
opts.addOption("leafs", "max_leaf_nodes", true,
"The maximum number of leaf nodes [default: Integer.MAX_VALUE]");
opts.addOption("split", "min_split", true,
"A node that has greater than or equals to `min_split` examples will split [default: 5]");
opts.addOption("output", "output_type", true,
"The output type (serialization/ser or opscode/vm or javascript/js) [default: serialization]");
opts.addOption("disable_compression", false,
"Whether to disable compression of the output script [default: false]");
String output = "serialization";
boolean compress = true;
if (argOIs.length >= 3) {
if (trees < 1) {
if (cl.hasOption("disable_compression")) {
compress = false;
}
this._outputType = ModelType.resolve(output, compress);
if (argOIs.length != 2 && argOIs.length != 3) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 or 3 arguments: double[] features, double target [, const string options]: "
argOIs.length);
ArrayList<String> fieldNames = new ArrayList<String>(5);
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(5);
fieldNames.add("model_id");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
fieldNames.add("model_type");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
if (args[0] == null) {
if (numExamples > 0) {
if (_minSamplesSplit <= 0) {
if (_maxDepth < 1) {
if (x.length != y.length) {
throw new HiveException(String.format("The sizes of X and Y don't match: %d != %d",
x.length, y.length));
if (logger.isInfoEnabled()) {
int numExamples = x.length;
tasks.add(new TrainingTask(this, i, attributes, x, y, numInputVars, _maxDepth,
_maxLeafNodes, _minSamplesSplit, order, prediction, oob, s, remainingTasks));
synchronized void forward(final int modelId, @Nonnull final Text model,
@Nonnull final double[] importance, final double[] y, final double[] prediction,
final int[] oob, final boolean lastTask) throws HiveException {
if (lastTask) {
if (oob[i] > 0) {
Object[] forwardObjs = new Object[6];
forwardObjs[0] = new IntWritable(modelId);
forwardObjs[1] = new IntWritable(_outputType.getId());
forwardObjs[2] = model;
forwardObjs[3] = WritableUtils.toWritableList(importance);
forwardObjs[4] = new DoubleWritable(oobErrors);
forwardObjs[5] = new IntWritable(oobTests);
private final int _taskId;
TrainingTask(RandomForestRegressionUDTF udtf, int taskId, Attribute[] attributes,
double[][] x, double[] y, int numVars, int maxDepth, int maxLeafs,
int minSamplesSplit, int[][] order, double[] prediction, int[] oob, long seed,
AtomicInteger remainingTasks) {
this._taskId = taskId;
RegressionTree tree = new RegressionTree(attributes, x, y, numVars, maxDepth, maxLeafs,
minSamplesSplit, order, samples, rnd2);
if (samples[i] == 0) {
synchronized (x[i]) {
Text model = getModel(tree, udtf._outputType);
private static Text getModel(@Nonnull final RegressionTree tree,
@Nonnull final ModelType outputType) throws HiveException {
final Text model;
case serialization:
case serialization_compressed: {
byte[] b = tree.predictSerCodegen(outputType.isCompressed());
b = Base91.encode(b);
model = new Text(b);
case opscode:
case opscode_compressed: {
String s = tree.predictOpCodegen(StackMachine.SEP);
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
b = Base91.encode(b);
model = new Text(b);
} else {
model = new Text(s);
}
case javascript:
case javascript_compressed: {
String s = tree.predictJsCodegen();
if (outputType.isCompressed()) {
byte[] b = s.getBytes();
final DeflateCodec codec = new DeflateCodec(true, false);
try {
b = codec.compress(b);
} catch (IOException e) {
throw new HiveException("Failed to compressing a model", e);
} finally {
IOUtils.closeQuietly(codec);
}
b = Base91.encode(b);
model = new Text(b);
} else {
model = new Text(s);
}
break;
}
default:
import hivemall.smile.data.Attribute;
import hivemall.smile.data.Attribute.AttributeType;
import hivemall.utils.io.FastByteArrayInputStream;
import hivemall.utils.io.FastMultiByteArrayOutputStream;
import hivemall.utils.io.IOUtils;
import java.io.Externalizable;
import java.io.IOException;
import java.io.InputStream;
import java.io.ObjectInput;
import java.io.ObjectInputStream;
import java.io.ObjectOutput;
import java.io.ObjectOutputStream;
import java.io.OutputStream;
import java.util.zip.DeflaterOutputStream;
import java.util.zip.InflaterInputStream;
import org.apache.hadoop.hive.ql.metadata.HiveException;
public interface NodeOutput {
public static class Node implements Externalizable {
AttributeType splitFeatureType = null;
public double predict(final double[] x) {
if (trueChild == null && falseChild == null) {
if (splitFeatureType == AttributeType.NOMINAL) {
if (x[splitFeature] == splitValue) {
} else if (splitFeatureType == AttributeType.NUMERIC) {
if (x[splitFeature] <= splitValue) {
splitFeatureType);
public double predict(final int[] x) {
if (trueChild == null && falseChild == null) {
} else if (x[splitFeature] == (int) splitValue) {
public void jsCodegen(@Nonnull final StringBuilder builder, final int depth) {
if (trueChild == null && falseChild == null) {
if (splitFeatureType == AttributeType.NOMINAL) {
builder.append("if(x[")
.append(splitFeature)
.append("] == ")
.append(splitValue)
.append(") {\n");
} else if (splitFeatureType == AttributeType.NUMERIC) {
builder.append("if(x[")
.append(splitFeature)
.append("] <= ")
.append(splitValue)
.append(") {\n");
splitFeatureType);
public int opCodegen(final List<String> scripts, int depth) {
if (trueChild == null && falseChild == null) {
if (splitFeatureType == AttributeType.NOMINAL) {
int trueDepth = trueChild.opCodegen(scripts, depth);
} else if (splitFeatureType == AttributeType.NUMERIC) {
int trueDepth = trueChild.opCodegen(scripts, depth);
splitFeatureType);
@Override
public void writeExternal(ObjectOutput out) throws IOException {
out.writeDouble(output);
out.writeInt(splitFeature);
if (splitFeatureType == null) {
out.writeInt(-1);
} else {
out.writeInt(splitFeatureType.getTypeId());
}
out.writeDouble(splitValue);
if (trueChild == null) {
out.writeBoolean(false);
} else {
out.writeBoolean(true);
trueChild.writeExternal(out);
}
if (falseChild == null) {
out.writeBoolean(false);
} else {
out.writeBoolean(true);
falseChild.writeExternal(out);
}
}
@Override
public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {
this.output = in.readDouble();
this.splitFeature = in.readInt();
int typeId = in.readInt();
if (typeId == -1) {
this.splitFeatureType = null;
} else {
this.splitFeatureType = AttributeType.resolve(typeId);
}
this.splitValue = in.readDouble();
if (in.readBoolean()) {
this.trueChild = new Node();
trueChild.readExternal(in);
}
if (in.readBoolean()) {
this.falseChild = new Node();
falseChild.readExternal(in);
}
}
public int compareTo(final TrainNode a) {
public void calculateOutput(final NodeOutput output) {
if (node.trueChild == null && node.falseChild == null) {
if (trueChild != null) {
if (falseChild != null) {
if (depth >= _maxDepth) {
for (int s : samples) {
if (n <= _minSplit) {
final double sum = node.output * n;
final int p = _attributes.length;
final int[] variables = new int[p];
if (_numVars < p) {
if (split.splitScore > node.splitScore) {
node.splitFeatureType = split.splitFeatureType;
public Node findBestSplit(final int n, final double sum, final int j) {
final int N = x.length;
final Node split = new Node(0.d);
if (_attributes[j].type == AttributeType.NOMINAL) {
final int m = _attributes[j].getSize();
final double[] trueSum = new double[m];
final int[] trueCount = new int[m];
if (samples[i] > 0) {
final double tc = (double) trueCount[k];
final double fc = n - tc;
if (tc == 0 || fc == 0) {
final double trueMean = trueSum[k] / tc;
final double falseMean = (sum - trueSum[k]) / fc;
if (gain > split.splitScore) {
split.splitFeatureType = AttributeType.NOMINAL;
} else if (_attributes[j].type == AttributeType.NUMERIC) {
for (int i : _order[j]) {
if (samples[i] > 0) {
if (Double.isNaN(prevx) || x[i][j] == prevx) {
final double falseCount = n - trueCount;
if (trueCount == 0 || falseCount == 0) {
final double trueMean = trueSum / trueCount;
final double falseMean = (sum - trueSum) / falseCount;
* falseMean * falseMean)
if (gain > split.splitScore) {
split.splitFeatureType = AttributeType.NUMERIC;
public boolean split(final PriorityQueue<TrainNode> nextSplits) {
if (node.splitFeature < 0) {
final int n = x.length;
final int[] trueSamples = new int[n];
final int[] falseSamples = new int[n];
if (node.splitFeatureType == AttributeType.NOMINAL) {
if (samples[i] > 0) {
if (x[i][node.splitFeature] == node.splitValue) {
} else if (node.splitFeatureType == AttributeType.NUMERIC) {
if (samples[i] > 0) {
if (x[i][node.splitFeature] <= node.splitValue) {
node.splitFeatureType);
if (tc == 0 || fc == 0) {
node.splitFeatureType = null;
if (tc >= _minSplit && trueChild.findBestSplit()) {
if (nextSplits != null) {
if (fc >= _minSplit && falseChild.findBestSplit()) {
if (nextSplits != null) {
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x,
@Nonnull double[] y, int maxLeafs) {
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x,
@Nonnull double[] y, int numVars, int maxDepth, int maxLeafs, int minSplits,
@Nullable int[][] order, @Nullable int[] samples, @Nullable smile.math.Random rand) {
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x,
@Nonnull double[] y, int numVars, int maxDepth, int maxLeafs, int minSplits,
@Nullable int[][] order, @Nullable int[] samples, @Nullable NodeOutput output,
@Nullable smile.math.Random rand) {
if (x.length != y.length) {
throw new IllegalArgumentException(String.format(
"The sizes of X and Y don't match: %d != %d", x.length, y.length));
if (numVars <= 0 || numVars > x[0].length) {
throw new IllegalArgumentException(
if (minSplits <= 0) {
if (maxLeafs < 2) {
if (attributes.length != x[0].length) {
if (samples == null) {
if (maxLeafs == Integer.MAX_VALUE) {
if (trainRoot.findBestSplit()) {
if (trainRoot.findBestSplit()) {
if (node == null) {
if (output != null) {
public String predictJsCodegen() {
_root.jsCodegen(buf, 0);
_root.opCodegen(opslist, 0);
@Nonnull
public byte[] predictSerCodegen(boolean compress) throws HiveException {
final Attribute[] attrs = _attributes;
assert (attrs != null);
FastMultiByteArrayOutputStream bos = new FastMultiByteArrayOutputStream();
OutputStream wrapped = compress ? new DeflaterOutputStream(bos) : bos;
ObjectOutputStream oos = null;
try {
oos = new ObjectOutputStream(wrapped);
_root.writeExternal(oos);
oos.flush();
} catch (IOException ioe) {
throw new HiveException("IOException cause while serializing DecisionTree object", ioe);
} catch (Exception e) {
throw new HiveException("Exception cause while serializing DecisionTree object", e);
} finally {
IOUtils.closeQuietly(oos);
}
return bos.toByteArray_clear();
}
public static Node deserializeNode(final byte[] serializedObj, final int length, final boolean compressed)
throws HiveException {
FastByteArrayInputStream bis = new FastByteArrayInputStream(serializedObj, length);
InputStream wrapped = compressed ? new InflaterInputStream(bis) : bis;
final Node root;
ObjectInputStream ois = null;
try {
ois = new ObjectInputStream(wrapped);
root = new Node();
root.readExternal(ois);
} catch (IOException ioe) {
throw new HiveException("IOException cause while deserializing DecisionTree object",
ioe);
} catch (Exception e) {
throw new HiveException("Exception cause while deserializing DecisionTree object", e);
} finally {
IOUtils.closeQuietly(ois);
}
return root;
}
@Deprecated
@Deprecated
import hivemall.smile.data.Attribute;
import hivemall.smile.data.Attribute.AttributeType;
import hivemall.smile.data.Attribute.NominalAttribute;
import hivemall.smile.data.Attribute.NumericAttribute;
import smile.sort.QuickSort;
public final class SmileExtUtils {
if (opt == null) {
if ("Q".equals(type)) {
attr[i] = new NumericAttribute(i);
} else if ("C".equals(type)) {
attr[i] = new NominalAttribute(i);
public static Attribute[] attributeTypes(@Nullable Attribute[] attributes,
@Nonnull final double[][] x) {
if (attributes == null) {
int p = x[0].length;
attributes = new Attribute[p];
attributes[i] = new NumericAttribute(i);
}
} else {
int size = attributes.length;
Attribute attr = attributes[j];
if (attr.type == AttributeType.NOMINAL) {
int max_x = 0;
int x_ij = (int) x[i][j];
if (x_ij > max_x) {
max_x = x_ij;
}
}
}
}
}
return attributes;
}
@Nonnull
public static Attribute[] convertAttributeTypes(@Nonnull final smile.data.Attribute[] original) {
final int size = original.length;
final Attribute[] dst = new Attribute[size];
smile.data.Attribute o = original[i];
switch (o.type) {
case NOMINAL:
dst[i] = new NominalAttribute(i);
break;
case NUMERIC:
dst[i] = new NumericAttribute(i);
break;
default:
}
}
return dst;
}
@Nonnull
public static int[][] sort(@Nonnull final Attribute[] attributes, @Nonnull final double[][] x) {
final int n = x.length;
final int p = x[0].length;
final double[] a = new double[n];
final int[][] index = new int[p][];
if (attributes[j].type == AttributeType.NUMERIC) {
a[i] = x[i][j];
}
index[j] = QuickSort.sort(a);
}
}
return index;
}
@Nonnull
final int[] labels = smile.math.Math.unique(y);
if (labels.length < 2) {
if (labels[i] < 0) {
if (i > 0 && labels[i] - labels[i - 1] > 1) {
if ("gini".equalsIgnoreCase(ruleName)) {
} else if ("entropy".equalsIgnoreCase(ruleName)) {
if (numVars <= 0.f) {
numInputVars = (int) Math.ceil(Math.sqrt(dims));
} else if (numVars > 0.f && numVars <= 1.f) {
for (int i = x.length; i > 1; i--) {
swap(x, i - 1, j);
if (x.length != y.length) {
if (seed == -1L) {
for (int i = x.length; i > 1; i--) {
swap(y, i - 1, j);
if (x.length != y.length) {
if (seed == -1L) {
for (int i = x.length; i > 1; i--) {
swap(y, i - 1, j);
private static void swap(final int[] x, final int i, final int j) {
int s = x[i];
x[i] = x[j];
x[j] = s;
}
private static void swap(final double[] x, final int i, final int j) {
double s = x[i];
x[i] = x[j];
x[j] = s;
}
private static void swap(final double[][] x, final int i, final int j) {
import javax.annotation.Nullable;
import org.apache.hadoop.io.BytesWritable;
@Description(name = "deflate", value = "_FUNC_(TEXT data [, const int compressionLevel]) - "
"Returns a compressed BINARY obeject by using Deflater.",
extended = "The compression level must be in range [-1,9]")
@Nullable
@Nullable
private transient BytesWritable result;
return PrimitiveObjectInspectorFactory.writableBinaryObjectInspector;
public BytesWritable evaluate(DeferredObject[] arguments) throws HiveException {
this.codec = new DeflateCodec(true, false);
if (result == null) {
this.result = new BytesWritable(compressed);
} else {
result.set(compressed, 0, compressed.length);
}
return result;
this.result = null;
import javax.annotation.Nonnull;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
import org.apache.hadoop.io.BytesWritable;
value = "_FUNC_(BINARY compressedData) - Returns a decompressed STRING by using Inflater")
private BinaryObjectInspector binaryOI;
@Nonnull
@Nonnull
private transient Text result;
this.binaryOI = HiveUtils.asBinaryOI(argOIs[0]);
public Text evaluate(DeferredObject[] arguments) throws HiveException {
this.codec = new DeflateCodec(false, true);
BytesWritable b = binaryOI.getPrimitiveWritableObject(arg0);
byte[] compressed = b.getBytes();
final int len = b.getLength();
if (result == null) {
result = new Text(decompressed);
} else {
result.set(decompressed, 0, decompressed.length);
}
return result;
this.result = null;
import static hivemall.HivemallConstants.BINARY_TYPE_NAME;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
public static BinaryObjectInspector asBinaryOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentException {
if (!BINARY_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (BinaryObjectInspector) argOI;
}
public static PrimitiveObjectInspector asDoubleCompatibleOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
import javax.annotation.CheckForNull;
private byte[] buf;
this.count = 0;
}
public FastByteArrayOutputStream(@CheckForNull byte[] buf) {
if (buf == null) {
throw new IllegalArgumentException("buf should not be NULL");
}
this.buf = buf;
this.count = 0;
protected boolean is_mini_batch;
protected float mini_batch_ratio;
opts.addOption("mini_batch", false, "Use mini batch algorithm or not");
opts.addOption("mini_batch_ratio", true, "The mini batch sampling ratio against all dataset");
boolean isMinibatch = false;
float miniBatchRatio = 1.f;
isMinibatch = cl.hasOption("mini_batch");
if (isMinibatch) {
miniBatchRatio = Primitives.parseFloat(cl.getOptionValue("mini_batch_ratio"), 1.f);
}
this.is_mini_batch = isMinibatch;
this.mini_batch_ratio = miniBatchRatio;
public void setValue(float value) {
this.value = value;
}
public class AROWRegressionUDTF extends RegressionBaseUDTF {
public final class AdaDeltaUDTF extends RegressionBaseUDTF {
onlineUpdate(features, gradient);
protected void onlineUpdate(@Nonnull final FeatureValue[] features, float gradient) {
public final class AdaGradUDTF extends RegressionBaseUDTF {
onlineUpdate(features, gradient);
protected void onlineUpdate(@Nonnull final FeatureValue[] features, float gradient) {
import hivemall.io.IWeightValue;
import hivemall.io.WeightValue;
public final class LogressUDTF extends RegressionBaseUDTF {
@Override
protected IWeightValue getNewWeight(IWeightValue old_w, float delta) {
float oldWeight = 0.f;
if (old_w != null) {
oldWeight = old_w.get();
}
}
public class PassiveAggressiveRegressionUDTF extends RegressionBaseUDTF {
onlineUpdate(features, coeff);
import java.util.Random;
public abstract class RegressionBaseUDTF extends LearnerBaseUDTF {
private static final Log logger = LogFactory.getLog(RegressionBaseUDTF.class);
protected FeatureValue[] accDelta;
protected int sampled;
protected Random rnd;
this.accDelta = null;
this.rnd = new Random(42);
this.sampled = 0;
if (accDelta == null) {
accDelta = new FeatureValue[featureVector.length];
}
if (this.is_mini_batch) {
batchUpdate(features, d);
} else {
onlineUpdate(features, d);
}
protected IWeightValue getNewWeight(IWeightValue old_w, float delta) {
throw new IllegalStateException();
}
protected void accumulateDelta(@Nonnull final FeatureValue[] features, float coeff) {
if (features[i] == null) {
continue;
}
final Object x = features[i].getFeature();
final float xi = features[i].getValue();
float delta = xi * coeff;
if (accDelta[i] == null) {
accDelta[i] = new FeatureValue(x, delta);
} else {
}
}
}
protected void batchUpdate(@Nonnull final FeatureValue[] features, float coeff) {
if (rnd.nextFloat() <= this.mini_batch_ratio) {
assert features.length == accDelta.length;
accumulateDelta(features, coeff);
}
}
protected void onlineUpdate(@Nonnull final FeatureValue[] features, float coeff) {
if (this.is_mini_batch) {
final Object x = accDelta[i].getFeature();
final float delta = accDelta[i].getValue();
IWeightValue old_w = model.get(x);
IWeightValue new_w = getNewWeight(old_w, delta);
model.set(x, new_w);
}
}
this.accDelta = new FeatureValue[featureVector.length];
protected void update(@Nonnull final FeatureValue[] features, final float target, final float predicted) {
final float d = computeUpdate(target, predicted);
if (is_mini_batch) {
if (rnd.nextFloat() <= mini_batch_ratio) {
if (is_mini_batch) {
final float v = f.getValueAsFloat();
float xi = f.getValueAsFloat();
float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
float v = probe.getValueAsFloat();
float v = fv.getValueAsFloat();
float v = ftvec.getValueAsFloat();
final float v = probe.getValueAsFloat();
public FeatureValue(Object f, double v) {
this.feature = f;
this.value = v;
}
public double getValue() {
public float getValueAsFloat() {
return (float) value;
}
public void setValue(double value) {
this.value = value;
}
if (o == null) {
if (pos == 0) {
final double weight;
if (pos > 0) {
weight = Double.parseDouble(s2);
weight = 1.d;
if (pos == 0) {
final double weight;
if (pos > 0) {
weight = Double.parseDouble(s2);
weight = 1.d;
public static void parseFeatureAsString(@Nonnull final String s,
@Nonnull final FeatureValue probe) throws IllegalArgumentException {
if (pos == 0) {
if (pos > 0) {
probe.value = Double.parseDouble(s2);
probe.value = 1.d;
float v1 = probe.getValueAsFloat();
float v2f = probe.getValueAsFloat();
float v1 = probe.getValueAsFloat();
float v2f = probe.getValueAsFloat();
float v1 = probe.getValueAsFloat();
float v2f = probe.getValueAsFloat();
float w = fv.getValueAsFloat();
float w = fv.getValueAsFloat();
float w = fv.getValueAsFloat();
float v = probe.getValueAsFloat();
float v2 = probe.getValueAsFloat();
float v = f.getValueAsFloat();
float xi = f.getValueAsFloat();
float xi = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float xi = features[i].getValueAsFloat();
final float xi = f.getValueAsFloat();
final float delta = accDelta[i].getValueAsFloat();
protected boolean is_mini_batch;
protected int mini_batch_size;
opts.addOption("dims", "feature_dimensions", true,
"The dimension of model [default: 16777216 (2^24)]");
opts.addOption("disable_halffloat", false,
"Toggle this option to disable the use of SpaceEfficientDenseModel");
opts.addOption("mini_batch", "mini_batch_size", true,
"Mini batch size [default: 1]. Expecting ");
opts.addOption("mix_session", "mix_session_name", true,
"Mix session name [default: ${mapred.job.id}]");
opts.addOption("mix_threshold", true,
"Threshold to mix local updates in range (0,127] [default: 3]");
int miniBatchSize = 1;
if (argOIs.length >= 3) {
if (denseModel) {
miniBatchSize = Primitives.parseInt(cl.getOptionValue("mini_batch_size"), miniBatchSize);
if (miniBatchSize <= 0) {
throw new UDFArgumentException("mini_batch_size must be greater than 0: "
miniBatchSize);
}
if (mixThreshold > Byte.MAX_VALUE) {
this.is_mini_batch = miniBatchSize > 1;
this.mini_batch_size = miniBatchSize;
if (dense_model) {
if (disable_halffloat == false && model_dims > 16777216) {
if (mixConnectInfo != null) {
if (label != null) {
protected void loadPredictionModel(PredictionModel model, String filename,
PrimitiveObjectInspector keyOI) {
if (useCovariance()) {
lines = loadPredictionModel(model, new File(filename), keyOI,
writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(model, new File(filename), keyOI,
writableFloatObjectInspector);
if (model.size() > 0) {
private static long loadPredictionModel(PredictionModel model, File file,
PrimitiveObjectInspector keyOI, WritableFloatObjectInspector valueOI)
if (!file.exists()) {
if (!file.getName().endsWith(".crc")) {
if (file.isDirectory()) {
for (File f : file.listFiles()) {
while ((line = reader.readLine()) != null) {
if (f0 == null || f1 == null) {
private static long loadPredictionModel(PredictionModel model, File file,
PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI,
WritableFloatObjectInspector covarOI) throws IOException, SerDeException {
if (!file.exists()) {
if (!file.getName().endsWith(".crc")) {
if (file.isDirectory()) {
for (File f : file.listFiles()) {
while ((line = reader.readLine()) != null) {
if (f0 == null || f1 == null) {
if (mixClient != null) {
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"LogressUDTF takes 2 or 3 arguments: List<Text|Int|BitInt> features, float target [, constant string options]");
opts.addOption("power_t", true,
"The exponent for inverse scaling learning rate [default 0.1]");
if (target < 0.f || target > 1.f) {
import hivemall.utils.lang.FloatAccumulator;
import java.util.HashMap;
import java.util.Map;
protected transient Map<Object, FloatAccumulator> accumulated;
if (argOIs.length < 2) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 arguments: List<Int|BigInt|Text> features, float target [, constant string options]");
if (preloadedModelFile != null) {
if (!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
throw new UDFArgumentTypeException(0,
if (useCovariance()) {
if (is_mini_batch && accumulated == null) {
this.accumulated = new HashMap<Object, FloatAccumulator>(1024);
}
if (featureVector == null) {
if (size == 0) {
if (f == null) {
if (parseFeature) {
if (f == null) {
if (old_w != 0f) {
if (f == null) {
if (old_w != 0f) {
if (f == null) {
if (old_w == null) {
protected void update(@Nonnull final FeatureValue[] features, final float target,
final float predicted) {
final float grad = computeUpdate(target, predicted);
accumulateUpdate(features, grad);
if (sampled >= mini_batch_size) {
batchUpdate();
}
onlineUpdate(features, grad);
protected final void accumulateUpdate(@Nonnull final FeatureValue[] features, final float coeff) {
FloatAccumulator acc = accumulated.get(x);
if (acc == null) {
acc = new FloatAccumulator(delta);
accumulated.put(x, acc);
acc.add(delta);
sampled;
protected final void batchUpdate() {
if (accumulated.isEmpty()) {
this.sampled = 0;
return;
for (Map.Entry<Object, FloatAccumulator> e : accumulated.entrySet()) {
Object x = e.getKey();
FloatAccumulator v = e.getValue();
float delta = v.get();
float old_w = model.getWeight(x);
model.set(x, new WeightValue(new_w));
}
accumulated.clear();
this.sampled = 0;
if (f == null) {
if (model != null) {
batchUpdate();
this.accumulated = null;
if (useCovariance()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
"Mini batch size [default: 1]. Expecting in range [1,100] or so.");
"Mini batch size [default: 1]. Expecting the value in range [1,100] or so.");
protected boolean is_mini_batch;
protected int mini_batch_size;
opts.addOption("dims", "feature_dimensions", true,
"The dimension of model [default: 16777216 (2^24)]");
opts.addOption("disable_halffloat", false,
"Toggle this option to disable the use of SpaceEfficientDenseModel");
opts.addOption("mini_batch", "mini_batch_size", true,
"Mini batch size [default: 1]. Expecting the value in range [1,100] or so.");
opts.addOption("mix_session", "mix_session_name", true,
"Mix session name [default: ${mapred.job.id}]");
opts.addOption("mix_threshold", true,
"Threshold to mix local updates in range (0,127] [default: 3]");
int miniBatchSize = 1;
if (argOIs.length >= 3) {
if (denseModel) {
miniBatchSize = Primitives.parseInt(cl.getOptionValue("mini_batch_size"), miniBatchSize);
if (miniBatchSize <= 0) {
throw new UDFArgumentException("mini_batch_size must be greater than 0: "
miniBatchSize);
}
if (mixThreshold > Byte.MAX_VALUE) {
this.is_mini_batch = miniBatchSize > 1;
this.mini_batch_size = miniBatchSize;
if (dense_model) {
if (disable_halffloat == false && model_dims > 16777216) {
if (mixConnectInfo != null) {
if (label != null) {
protected void loadPredictionModel(PredictionModel model, String filename,
PrimitiveObjectInspector keyOI) {
if (useCovariance()) {
lines = loadPredictionModel(model, new File(filename), keyOI,
writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(model, new File(filename), keyOI,
writableFloatObjectInspector);
if (model.size() > 0) {
private static long loadPredictionModel(PredictionModel model, File file,
PrimitiveObjectInspector keyOI, WritableFloatObjectInspector valueOI)
if (!file.exists()) {
if (!file.getName().endsWith(".crc")) {
if (file.isDirectory()) {
for (File f : file.listFiles()) {
while ((line = reader.readLine()) != null) {
if (f0 == null || f1 == null) {
private static long loadPredictionModel(PredictionModel model, File file,
PrimitiveObjectInspector featureOI, WritableFloatObjectInspector weightOI,
WritableFloatObjectInspector covarOI) throws IOException, SerDeException {
if (!file.exists()) {
if (!file.getName().endsWith(".crc")) {
if (file.isDirectory()) {
for (File f : file.listFiles()) {
while ((line = reader.readLine()) != null) {
if (f0 == null || f1 == null) {
if (mixClient != null) {
final float v = f.getValueAsFloat();
float xi = f.getValueAsFloat();
float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
final float v = f.getValueAsFloat();
float v = probe.getValueAsFloat();
float v = fv.getValueAsFloat();
float v = ftvec.getValueAsFloat();
final float v = probe.getValueAsFloat();
public FeatureValue(Object f, double v) {
this.feature = f;
this.value = v;
}
public double getValue() {
public float getValueAsFloat() {
return (float) value;
}
public void setValue(float value) {
this.value = value;
}
public void setValue(double value) {
this.value = value;
}
if (o == null) {
if (pos == 0) {
final double weight;
if (pos > 0) {
weight = Double.parseDouble(s2);
weight = 1.d;
if (pos == 0) {
final double weight;
if (pos > 0) {
weight = Double.parseDouble(s2);
weight = 1.d;
public static void parseFeatureAsString(@Nonnull final String s,
@Nonnull final FeatureValue probe) throws IllegalArgumentException {
if (pos == 0) {
if (pos > 0) {
probe.value = Double.parseDouble(s2);
probe.value = 1.d;
float v1 = probe.getValueAsFloat();
float v2f = probe.getValueAsFloat();
float v1 = probe.getValueAsFloat();
float v2f = probe.getValueAsFloat();
float v1 = probe.getValueAsFloat();
float v2f = probe.getValueAsFloat();
float w = fv.getValueAsFloat();
float w = fv.getValueAsFloat();
float w = fv.getValueAsFloat();
float v = probe.getValueAsFloat();
float v2 = probe.getValueAsFloat();
public class AROWRegressionUDTF extends RegressionBaseUDTF {
float v = f.getValueAsFloat();
public final class AdaDeltaUDTF extends RegressionBaseUDTF {
onlineUpdate(features, gradient);
protected void onlineUpdate(@Nonnull final FeatureValue[] features, float gradient) {
float xi = f.getValueAsFloat();
public final class AdaGradUDTF extends RegressionBaseUDTF {
onlineUpdate(features, gradient);
protected void onlineUpdate(@Nonnull final FeatureValue[] features, float gradient) {
float xi = f.getValueAsFloat();
public final class LogressUDTF extends RegressionBaseUDTF {
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"LogressUDTF takes 2 or 3 arguments: List<Text|Int|BitInt> features, float target [, constant string options]");
opts.addOption("power_t", true,
"The exponent for inverse scaling learning rate [default 0.1]");
if (target < 0.f || target > 1.f) {
public class PassiveAggressiveRegressionUDTF extends RegressionBaseUDTF {
onlineUpdate(features, coeff);
import hivemall.utils.lang.FloatAccumulator;
import java.util.HashMap;
import java.util.Map;
public abstract class RegressionBaseUDTF extends LearnerBaseUDTF {
private static final Log logger = LogFactory.getLog(RegressionBaseUDTF.class);
protected transient Map<Object, FloatAccumulator> accumulated;
protected int sampled;
if (argOIs.length < 2) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 arguments: List<Int|BigInt|Text> features, float target [, constant string options]");
if (preloadedModelFile != null) {
this.sampled = 0;
if (!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
throw new UDFArgumentTypeException(0,
if (useCovariance()) {
if (is_mini_batch && accumulated == null) {
this.accumulated = new HashMap<Object, FloatAccumulator>(1024);
}
if (featureVector == null) {
if (size == 0) {
if (f == null) {
if (parseFeature) {
if (f == null) {
final float v = f.getValueAsFloat();
if (old_w != 0f) {
if (f == null) {
final float v = f.getValueAsFloat();
if (old_w != 0f) {
if (f == null) {
final float v = f.getValueAsFloat();
if (old_w == null) {
protected void update(@Nonnull final FeatureValue[] features, final float target,
final float predicted) {
final float grad = computeUpdate(target, predicted);
if (is_mini_batch) {
accumulateUpdate(features, grad);
if (sampled >= mini_batch_size) {
batchUpdate();
}
} else {
onlineUpdate(features, grad);
}
protected IWeightValue getNewWeight(IWeightValue old_w, float delta) {
throw new IllegalStateException();
}
protected final void accumulateUpdate(@Nonnull final FeatureValue[] features, final float coeff) {
if (features[i] == null) {
continue;
}
final Object x = features[i].getFeature();
final float xi = features[i].getValueAsFloat();
float delta = xi * coeff;
FloatAccumulator acc = accumulated.get(x);
if (acc == null) {
acc = new FloatAccumulator(delta);
accumulated.put(x, acc);
} else {
acc.add(delta);
}
}
sampled;
}
protected final void batchUpdate() {
if (accumulated.isEmpty()) {
this.sampled = 0;
return;
}
for (Map.Entry<Object, FloatAccumulator> e : accumulated.entrySet()) {
Object x = e.getKey();
FloatAccumulator v = e.getValue();
float delta = v.get();
float old_w = model.getWeight(x);
model.set(x, new WeightValue(new_w));
}
accumulated.clear();
this.sampled = 0;
}
protected void onlineUpdate(@Nonnull final FeatureValue[] features, float coeff) {
if (f == null) {
final float xi = f.getValueAsFloat();
if (model != null) {
batchUpdate();
this.accumulated = null;
}
if (useCovariance()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
private final int _minLeafSize;
ENTROPY,
CLASSIFICATION_ERROR
if (n <= _minSplit) {
return false;
}
private Node findBestSplit(final int n, final int[] count, final int[] falseCount,
if (tc < _minSplit || fc < _minSplit) {
splitNode.trueChildOutput = Math.whichMax(trueCount[l]);
splitNode.falseChildOutput = Math.whichMax(falseCount);
if (tc < _minSplit || fc < _minSplit) {
splitNode.trueChildOutput = Math.whichMax(trueCount);
splitNode.falseChildOutput = Math.whichMax(falseCount);
if (tc < _minLeafSize || fc < _minLeafSize) {
case CLASSIFICATION_ERROR: {
impurity = 0.d;
if (count[i] > 0) {
impurity = Math.max(impurity, count[i] / (double) n);
}
}
impurity = Math.abs(1.d - impurity);
break;
}
int numLeafs) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, numLeafs, 2, 1, null, null, SplitRule.GINI, null);
int numVars, int maxDepth, int maxLeafs, int minSplits, int minLeafSize,
@Nullable int[] samples, @Nullable int[][] order, @Nonnull SplitRule rule,
@Nullable smile.math.Random rand) {
checkArgument(x, y, numVars, maxDepth, maxLeafs, minSplits, minLeafSize);
this._minLeafSize = minLeafSize;
if (maxLeafs == Integer.MAX_VALUE) {
private static void checkArgument(@Nonnull double[][] x, @Nonnull int[] y, int numVars,
int maxDepth, int maxLeafs, int minSplits, int minLeafSize) {
if (x.length != y.length) {
throw new IllegalArgumentException(String.format(
"The sizes of X and Y don't match: %d != %d", x.length, y.length));
}
if (numVars <= 0 || numVars > x[0].length) {
throw new IllegalArgumentException(
}
if (maxDepth < 2) {
}
if (maxLeafs < 2) {
}
if (minSplits < 2) {
throw new IllegalArgumentException(
"Invalid minimum number of samples required to split an internal node: "
minSplits);
}
if (minLeafSize < 1) {
}
}
private int _minSamplesLeaf;
opts.addOption("min_samples_leaf", true,
"The minimum number of samples in a leaf node [default: 1]");
int trees = 500, maxDepth = 8;
int maxLeafs = Integer.MAX_VALUE, minSplit = 5, minSamplesLeaf = 1;
minSamplesLeaf = Primitives.parseInt(cl.getOptionValue("min_samples_leaf"),
minSamplesLeaf);
this._minSamplesLeaf = minSamplesLeaf;
_maxLeafNodes, _minSamplesSplit, _minSamplesLeaf, order, samples, output, rnd2);
_maxDepth, _maxLeafNodes, _minSamplesSplit, _minSamplesLeaf, order, samples,
output[j], rnd2);
private int _minSamplesLeaf;
opts.addOption("min_samples_leaf", true,
"The minimum number of samples in a leaf node [default: 1]");
int trees = 50, maxDepth = Integer.MAX_VALUE;
int numLeafs = Integer.MAX_VALUE, minSplits = 2, minSamplesLeaf = 1;
minSamplesLeaf = Primitives.parseInt(cl.getOptionValue("min_samples_leaf"),
minSamplesLeaf);
this._minSamplesLeaf = minSamplesLeaf;
tasks.add(new TrainingTask(this, i, attributes, x, y, numInputVars, order, prediction,
s, remainingTasks));
double[][] x, int[] y, int numVars, int[][] order, int[][] prediction, long seed,
AtomicInteger remainingTasks) {
DecisionTree tree = new DecisionTree(_attributes, _x, _y, _numVars, _udtf._maxDepth,
_udtf._maxLeafNodes, _udtf._minSamplesSplit, _udtf._minSamplesLeaf, samples,
_order, _udtf._splitRule, rnd2);
private int _minSamplesLeaf;
opts.addOption("min_samples_leaf", true,
"The minimum number of samples in a leaf node [default: 1]");
int trees = 50, maxDepth = Integer.MAX_VALUE;
int maxLeafs = Integer.MAX_VALUE, minSplit = 5, minSamplesLeaf = 1;
minSamplesLeaf = Primitives.parseInt(cl.getOptionValue("min_samples_leaf"),
minSamplesLeaf);
this._minSamplesLeaf = minSamplesLeaf;
tasks.add(new TrainingTask(this, i, attributes, x, y, numInputVars, order, prediction,
oob, s, remainingTasks));
private final Attribute[] _attributes;
private final double[][] _x;
private final double[] _y;
private final int[][] _order;
private final int _numVars;
private final double[] _prediction;
private final int[] _oob;
private final RandomForestRegressionUDTF _udtf;
private final long _seed;
private final AtomicInteger _remainingTasks;
double[][] x, double[] y, int numVars, int[][] order, double[] prediction,
int[] oob, long seed, AtomicInteger remainingTasks) {
this._udtf = udtf;
this._attributes = attributes;
this._x = x;
this._y = y;
this._order = order;
this._numVars = numVars;
this._prediction = prediction;
this._oob = oob;
this._seed = seed;
this._remainingTasks = remainingTasks;
long s = (this._seed == -1L) ? SmileExtUtils.generateSeed() : new smile.math.Random(
_seed).nextLong();
final int n = _x.length;
RegressionTree tree = new RegressionTree(_attributes, _x, _y, _numVars,
_udtf._maxDepth, _udtf._maxLeafNodes, _udtf._minSamplesSplit,
_udtf._minSamplesLeaf, _order, samples, rnd2);
double pred = tree.predict(_x[i]);
synchronized (_x[i]) {
Text model = getModel(tree, _udtf._outputType);
int remain = _remainingTasks.decrementAndGet();
private final int _minLeafSize;
private Node findBestSplit(final int n, final double sum, final int j) {
if (tc < _minSplit || fc < _minSplit) {
if (trueCount < _minSplit || falseCount < _minSplit) {
if (tc < _minLeafSize || fc < _minLeafSize) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, maxLeafs, 5, 1, null, null, null);
int minLeafSize, @Nullable int[][] order, @Nullable int[] samples,
@Nullable smile.math.Random rand) {
this(attributes, x, y, numVars, maxDepth, maxLeafs, minSplits, minLeafSize, order, samples, null, rand);
int minLeafSize, @Nullable int[][] order, @Nullable int[] samples,
@Nullable NodeOutput output, @Nullable smile.math.Random rand) {
checkArgument(x, y, numVars, maxDepth, maxLeafs, minSplits, minLeafSize);
this._minLeafSize = minLeafSize;
private static void checkArgument(@Nonnull double[][] x, @Nonnull double[] y, int numVars,
int maxDepth, int maxLeafs, int minSplits, int minLeafSize) {
if (x.length != y.length) {
throw new IllegalArgumentException(String.format(
"The sizes of X and Y don't match: %d != %d", x.length, y.length));
}
if (numVars <= 0 || numVars > x[0].length) {
throw new IllegalArgumentException(
}
if (maxDepth < 2) {
}
if (maxLeafs < 2) {
}
if (minSplits < 2) {
throw new IllegalArgumentException(
"Invalid minimum number of samples required to split an internal node: "
minSplits);
}
if (minLeafSize < 1) {
}
}
public static Node deserializeNode(final byte[] serializedObj, final int length,
final boolean compressed) throws HiveException {
} else if ("classification_error".equalsIgnoreCase(ruleName)) {
return SplitRule.CLASSIFICATION_ERROR;
private final int _minLeafSize;
ENTROPY,
CLASSIFICATION_ERROR
if (n <= _minSplit) {
return false;
}
private Node findBestSplit(final int n, final int[] count, final int[] falseCount,
if (tc < _minSplit || fc < _minSplit) {
splitNode.trueChildOutput = Math.whichMax(trueCount[l]);
splitNode.falseChildOutput = Math.whichMax(falseCount);
if (tc < _minSplit || fc < _minSplit) {
splitNode.trueChildOutput = Math.whichMax(trueCount);
splitNode.falseChildOutput = Math.whichMax(falseCount);
if (tc < _minLeafSize || fc < _minLeafSize) {
case CLASSIFICATION_ERROR: {
impurity = 0.d;
if (count[i] > 0) {
impurity = Math.max(impurity, count[i] / (double) n);
}
}
impurity = Math.abs(1.d - impurity);
break;
}
int numLeafs) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, numLeafs, 2, 1, null, null, SplitRule.GINI, null);
int numVars, int maxDepth, int maxLeafs, int minSplits, int minLeafSize,
@Nullable int[] samples, @Nullable int[][] order, @Nonnull SplitRule rule,
@Nullable smile.math.Random rand) {
checkArgument(x, y, numVars, maxDepth, maxLeafs, minSplits, minLeafSize);
this._minLeafSize = minLeafSize;
if (maxLeafs == Integer.MAX_VALUE) {
private static void checkArgument(@Nonnull double[][] x, @Nonnull int[] y, int numVars,
int maxDepth, int maxLeafs, int minSplits, int minLeafSize) {
if (x.length != y.length) {
throw new IllegalArgumentException(String.format(
"The sizes of X and Y don't match: %d != %d", x.length, y.length));
}
if (numVars <= 0 || numVars > x[0].length) {
throw new IllegalArgumentException(
}
if (maxDepth < 2) {
}
if (maxLeafs < 2) {
}
if (minSplits < 2) {
throw new IllegalArgumentException(
"Invalid minimum number of samples required to split an internal node: "
minSplits);
}
if (minLeafSize < 1) {
}
}
private int _minSamplesLeaf;
opts.addOption("min_samples_leaf", true,
"The minimum number of samples in a leaf node [default: 1]");
int trees = 500, maxDepth = 8;
int maxLeafs = Integer.MAX_VALUE, minSplit = 5, minSamplesLeaf = 1;
minSamplesLeaf = Primitives.parseInt(cl.getOptionValue("min_samples_leaf"),
minSamplesLeaf);
this._minSamplesLeaf = minSamplesLeaf;
_maxLeafNodes, _minSamplesSplit, _minSamplesLeaf, order, samples, output, rnd2);
_maxDepth, _maxLeafNodes, _minSamplesSplit, _minSamplesLeaf, order, samples,
output[j], rnd2);
private int _minSamplesLeaf;
opts.addOption("min_samples_leaf", true,
"The minimum number of samples in a leaf node [default: 1]");
int trees = 50, maxDepth = Integer.MAX_VALUE;
int numLeafs = Integer.MAX_VALUE, minSplits = 2, minSamplesLeaf = 1;
minSamplesLeaf = Primitives.parseInt(cl.getOptionValue("min_samples_leaf"),
minSamplesLeaf);
this._minSamplesLeaf = minSamplesLeaf;
tasks.add(new TrainingTask(this, i, attributes, x, y, numInputVars, order, prediction,
s, remainingTasks));
double[][] x, int[] y, int numVars, int[][] order, int[][] prediction, long seed,
AtomicInteger remainingTasks) {
DecisionTree tree = new DecisionTree(_attributes, _x, _y, _numVars, _udtf._maxDepth,
_udtf._maxLeafNodes, _udtf._minSamplesSplit, _udtf._minSamplesLeaf, samples,
_order, _udtf._splitRule, rnd2);
private int _minSamplesLeaf;
opts.addOption("min_samples_leaf", true,
"The minimum number of samples in a leaf node [default: 1]");
int trees = 50, maxDepth = Integer.MAX_VALUE;
int maxLeafs = Integer.MAX_VALUE, minSplit = 5, minSamplesLeaf = 1;
minSamplesLeaf = Primitives.parseInt(cl.getOptionValue("min_samples_leaf"),
minSamplesLeaf);
this._minSamplesLeaf = minSamplesLeaf;
tasks.add(new TrainingTask(this, i, attributes, x, y, numInputVars, order, prediction,
oob, s, remainingTasks));
private final Attribute[] _attributes;
private final double[][] _x;
private final double[] _y;
private final int[][] _order;
private final int _numVars;
private final double[] _prediction;
private final int[] _oob;
private final RandomForestRegressionUDTF _udtf;
private final long _seed;
private final AtomicInteger _remainingTasks;
double[][] x, double[] y, int numVars, int[][] order, double[] prediction,
int[] oob, long seed, AtomicInteger remainingTasks) {
this._udtf = udtf;
this._attributes = attributes;
this._x = x;
this._y = y;
this._order = order;
this._numVars = numVars;
this._prediction = prediction;
this._oob = oob;
this._seed = seed;
this._remainingTasks = remainingTasks;
long s = (this._seed == -1L) ? SmileExtUtils.generateSeed() : new smile.math.Random(
_seed).nextLong();
final int n = _x.length;
RegressionTree tree = new RegressionTree(_attributes, _x, _y, _numVars,
_udtf._maxDepth, _udtf._maxLeafNodes, _udtf._minSamplesSplit,
_udtf._minSamplesLeaf, _order, samples, rnd2);
double pred = tree.predict(_x[i]);
synchronized (_x[i]) {
Text model = getModel(tree, _udtf._outputType);
int remain = _remainingTasks.decrementAndGet();
private final int _minLeafSize;
private Node findBestSplit(final int n, final double sum, final int j) {
if (tc < _minSplit || fc < _minSplit) {
if (trueCount < _minSplit || falseCount < _minSplit) {
if (tc < _minLeafSize || fc < _minLeafSize) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, maxLeafs, 5, 1, null, null, null);
int minLeafSize, @Nullable int[][] order, @Nullable int[] samples,
@Nullable smile.math.Random rand) {
this(attributes, x, y, numVars, maxDepth, maxLeafs, minSplits, minLeafSize, order, samples, null, rand);
int minLeafSize, @Nullable int[][] order, @Nullable int[] samples,
@Nullable NodeOutput output, @Nullable smile.math.Random rand) {
checkArgument(x, y, numVars, maxDepth, maxLeafs, minSplits, minLeafSize);
this._minLeafSize = minLeafSize;
private static void checkArgument(@Nonnull double[][] x, @Nonnull double[] y, int numVars,
int maxDepth, int maxLeafs, int minSplits, int minLeafSize) {
if (x.length != y.length) {
throw new IllegalArgumentException(String.format(
"The sizes of X and Y don't match: %d != %d", x.length, y.length));
}
if (numVars <= 0 || numVars > x[0].length) {
throw new IllegalArgumentException(
}
if (maxDepth < 2) {
}
if (maxLeafs < 2) {
}
if (minSplits < 2) {
throw new IllegalArgumentException(
"Invalid minimum number of samples required to split an internal node: "
minSplits);
}
if (minLeafSize < 1) {
}
}
public static Node deserializeNode(final byte[] serializedObj, final int length,
final boolean compressed) throws HiveException {
} else if ("classification_error".equalsIgnoreCase(ruleName)) {
return SplitRule.CLASSIFICATION_ERROR;
public static final String VERSION = "0.4.1-alpha.3";
import org.apache.hadoop.hive.ql.exec.MapredContext;
import org.apache.hadoop.mapred.JobConf;
private boolean support_javascript_eval = true;
@Override
public void configure(MapredContext context) {
super.configure(context);
if (context != null) {
JobConf conf = context.getJobConf();
String tdJarVersion = conf.get("td.jar.version");
if (tdJarVersion != null) {
this.support_javascript_eval = false;
}
}
}
this.evaluator = getEvaluator(modelType, support_javascript_eval);
private static Evaluator getEvaluator(@Nonnull ModelType type, boolean supportJavascriptEval)
throws UDFArgumentException {
if (!supportJavascriptEval) {
throw new UDFArgumentException(
"Javascript evaluation is not allowed in Treasure Data env");
}
int[] samples;
int[] trueSamples = new int[n];
int[] falseSamples = new int[n];
int[] samples;
int[] trueSamples = new int[n];
int[] falseSamples = new int[n];
public static final String VERSION = "0.4.1-alpha.4";
public static final String VERSION = "0.4.1-alpha.4";
int[] samples;
int[] trueSamples = new int[n];
int[] falseSamples = new int[n];
int[] samples;
int[] trueSamples = new int[n];
int[] falseSamples = new int[n];
import hivemall.utils.lang.ArrayUtils;
final int depth;
@Nullable
int[] fixedSamples;
public TrainNode(Node node, double[][] x, int[] y, int depth) {
this(node, x, y, depth, null);
}
public TrainNode(Node node, double[][] x, int[] y, int depth, @Nonnull int[] samples) {
this.fixedSamples = samples;
public boolean findBestSplit(final int[] samples) {
Node split = findBestSplit(n, samples, count, falseCount, impurity,
variableIndex[j]);
private Node findBestSplit(final int n, final int[] samples, final int[] count,
final int[] falseCount, final double impurity, final int j) {
public boolean split(@Nullable final PriorityQueue<TrainNode> nextSplits,
final int[] samples, final int[] trueSamples, final int[] falseSamples) {
falseSamples[i] = 0;
trueSamples[i] = 0;
} else {
trueSamples[i] = 0;
falseSamples[i] = 0;
falseSamples[i] = 0;
trueSamples[i] = 0;
} else {
trueSamples[i] = 0;
falseSamples[i] = 0;
if (tc >= _minSplit && trueChild.findBestSplit(trueSamples)) {
trueChild.fixedSamples = Arrays.copyOf(trueSamples, trueSamples.length);
ArrayUtils.copy(trueSamples, samples);
trueChild.split(null, samples, trueSamples, falseSamples);
if (fc >= _minSplit && falseChild.findBestSplit(falseSamples)) {
falseChild.fixedSamples = Arrays.copyOf(falseSamples, falseSamples.length);
ArrayUtils.copy(falseSamples, samples);
falseChild.split(null, samples, trueSamples, falseSamples);
int[] trueSamples = new int[n];
int[] falseSamples = new int[n];
final TrainNode trainRoot = new TrainNode(_root, x, y, 1, samples);
if (trainRoot.findBestSplit(samples)) {
trainRoot.split(null, samples, trueSamples, falseSamples);
if (trainRoot.findBestSplit(samples)) {
if (node.fixedSamples == null) {
throw new IllegalStateException("node.fixedSamples is not set");
}
node.split(nextSplits, node.fixedSamples, trueSamples, falseSamples);
node.fixedSamples = null;
import hivemall.utils.lang.ArrayUtils;
final int depth;
@Nullable
int[] fixedSamples;
public TrainNode(Node node, double[][] x, double[] y, int depth) {
this(node, x, y, null, depth);
}
this.fixedSamples = samples;
assert (fixedSamples != null);
node.output = output.calculate(fixedSamples);
public boolean findBestSplit(final int[] samples) {
Node split = findBestSplit(n, samples, sum, variables[j]);
private Node findBestSplit(final int n, final int[] samples, final double sum, final int j) {
public boolean split(final PriorityQueue<TrainNode> nextSplits, final int[] samples,
final int[] trueSamples, final int[] falseSamples) {
falseSamples[i] = 0;
trueSamples[i] = 0;
} else {
trueSamples[i] = 0;
falseSamples[i] = 0;
falseSamples[i] = 0;
trueSamples[i] = 0;
} else {
trueSamples[i] = 0;
falseSamples[i] = 0;
this.fixedSamples = Arrays.copyOf(samples, samples.length);
if (tc >= _minSplit && trueChild.findBestSplit(trueSamples)) {
trueChild.fixedSamples = Arrays.copyOf(trueSamples, trueSamples.length);
ArrayUtils.copy(trueSamples, samples);
trueChild.split(null, samples, trueSamples, falseSamples);
} else {
trueChild.fixedSamples = Arrays.copyOf(trueSamples, trueSamples.length);
if (fc >= _minSplit && falseChild.findBestSplit(falseSamples)) {
falseChild.fixedSamples = Arrays.copyOf(falseSamples, falseSamples.length);
ArrayUtils.copy(falseSamples, samples);
falseChild.split(null, samples, trueSamples, falseSamples);
} else {
falseChild.fixedSamples = Arrays.copyOf(falseSamples, falseSamples.length);
int[] trueSamples = new int[n];
int[] falseSamples = new int[n];
if (trainRoot.findBestSplit(samples)) {
trainRoot.split(null, samples, trueSamples, falseSamples);
if (trainRoot.findBestSplit(samples)) {
if (node.fixedSamples == null) {
throw new IllegalStateException("node.fixedSamples is not set");
}
node.split(nextSplits, node.fixedSamples, trueSamples, falseSamples);
node.fixedSamples = null;
public static void copy(final int[] src, final int[] dest) {
if (src.length != dest.length) {
}
System.arraycopy(src, 0, dest, 0, src.length);
}
public static final String VERSION = "0.4.1-alpha.5";
public static int[] copyOf(final int[] src) {
int len = src.length;
int[] dest = new int[len];
System.arraycopy(src, 0, dest, 0, len);
return dest;
}
public static int[] append(int[] array, int currentSize, int element) {
int[] newArray = new int[currentSize * 2];
System.arraycopy(array, 0, newArray, 0, currentSize);
array = newArray;
}
array[currentSize] = element;
return array;
}
public static int[] insert(int[] array, int currentSize, int index, int element) {
array[index] = element;
return array;
}
int[] newArray = new int[currentSize * 2];
System.arraycopy(array, 0, newArray, 0, index);
newArray[index] = element;
return newArray;
}
public final class SparseIntArray implements IntArray {
private static final long serialVersionUID = -2814248784231540118L;
mValues = new int[initialCapacity];
public IntArray deepCopy() {
@Override
@Override
@Override
public void increment(int key, int value) {
int i = Arrays.binarySearch(mKeys, 0, mSize, key);
if (i >= 0) {
mValues[i] = value;
} else {
i = ~i;
mKeys = ArrayUtils.insert(mKeys, mSize, i, key);
mValues = ArrayUtils.insert(mValues, mSize, i, value);
mSize;
}
}
@Override
@Override
import hivemall.utils.collections.IntArrayList;
int[] bags;
public TrainNode(Node node, double[][] x, int[] y, int[] bags, int depth) {
this.bags = bags;
public boolean findBestSplit() {
final int numSamples = bags.length;
if (numSamples <= _minSplit) {
return false;
}
final int[] count = new int[_k];
int index = bags[i];
int y_i = y[index];
if (label == -1) {
label = y_i;
} else if (y_i != label) {
pure = false;
final double impurity = impurity(count, numSamples, _rule);
Node split = findBestSplit(numSamples, count, falseCount, impurity,
private Node findBestSplit(final int n, final int[] count, final int[] falseCount,
final double impurity, final int j) {
int index = bags[i];
int x_ij = (int) x[index][j];
final int[] samples = SmileExtUtils.bagsToSamples(bags, x.length);
public boolean split(@Nullable final PriorityQueue<TrainNode> nextSplits) {
int childBagSize = (int) (bags.length * 0.4);
IntArrayList trueBags = new IntArrayList(childBagSize);
IntArrayList falseBags = new IntArrayList(childBagSize);
int tc = splitSamples(trueBags, falseBags);
int fc = bags.length - tc;
if (tc >= _minSplit && trueChild.findBestSplit()) {
trueChild.split(null);
node.falseChild = new Node(node.falseChildOutput);
TrainNode falseChild = new TrainNode(node.falseChild, x, y, falseBags.toArray(),
if (fc >= _minSplit && falseChild.findBestSplit()) {
falseChild.split(null);
private int splitSamples(@Nonnull final IntArrayList trueBags,
@Nonnull final IntArrayList falseBags) {
int tc = 0;
if (node.splitFeatureType == AttributeType.NOMINAL) {
final int splitFeature = node.splitFeature;
final double splitValue = node.splitValue;
final int index = bags[i];
if (x[index][splitFeature] == splitValue) {
trueBags.add(index);
tc;
} else {
falseBags.add(index);
}
}
} else if (node.splitFeatureType == AttributeType.NUMERIC) {
final int splitFeature = node.splitFeature;
final double splitValue = node.splitValue;
final int index = bags[i];
if (x[index][splitFeature] <= splitValue) {
trueBags.add(index);
tc;
} else {
falseBags.add(index);
}
}
} else {
throw new IllegalStateException("Unsupported attribute type: "
node.splitFeatureType);
}
return tc;
}
impurity = Math.max(impurity, (double) count[i] / n);
public DecisionTree(@Nullable Attribute[] attributes, @Nullable double[][] x,
@Nullable int[] y, int numLeafs, @Nullable smile.math.Random rand) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, numLeafs, 2, 1, null, null, SplitRule.GINI, rand);
}
@Nullable int[] bags, @Nullable int[][] order, @Nonnull SplitRule rule,
if (bags == null) {
bags = new int[n];
bags[i] = i;
int index = bags[i];
final TrainNode trainRoot = new TrainNode(_root, x, y, bags, 1);
if (trainRoot.findBestSplit()) {
trainRoot.split(null);
if (trainRoot.findBestSplit()) {
TrainNode parent = nextSplits.poll();
if (parent == null) {
import java.util.BitSet;
final int numInstances = x.length;
final int numSamples = (int) Math.round(numInstances * _subsample);
final BitSet sampled = new BitSet(numInstances);
final int[] bag = new int[numSamples];
final int[] perm = new int[numSamples];
int index = perm[i];
bag[i] = index;
sampled.set(index);
_maxLeafNodes, _minSamplesSplit, _minSamplesLeaf, order, bag, output, rnd2);
oobTests;
final int pred = (h[i] > 0.d) ? 1 : 0;
if (pred != y[i]) {
sampled.clear();
final int numInstances = x.length;
final int numSamples = (int) Math.round(numInstances * _subsample);
final BitSet sampled = new BitSet(numInstances);
final int[] bag = new int[numSamples];
final int[] perm = new int[numSamples];
final int[] prediction = new int[numInstances];
int oobTests = 0, oobErrors = 0;
int index = perm[i];
bag[i] = index;
sampled.set(i);
_maxDepth, _maxLeafNodes, _minSamplesSplit, _minSamplesLeaf, order, bag,
oobTests;
if (prediction[i] != y[i]) {
sampled.clear();
double y_i = y[i];
double abs = Math.abs(y_i);
double y_i = y[i];
double abs = Math.abs(y_i);
return ((k - 1.0d) / k) * (nu / de);
import java.util.BitSet;
final int N = _x.length;
final int[] bags = new int[N];
final BitSet sampled = new BitSet(N);
int index = rnd1.nextInt(N);
bags[i] = index;
sampled.set(index);
_udtf._maxLeafNodes, _udtf._minSamplesSplit, _udtf._minSamplesLeaf, bags, _order,
_udtf._splitRule, rnd2);
final int p = tree.predict(_x[i]);
synchronized (_prediction[i]) {
import java.util.BitSet;
final int N = _x.length;
final int[] bags = new int[N];
final BitSet sampled = new BitSet(N);
int index = rnd1.nextInt(N);
bags[i] = index;
sampled.set(index);
_udtf._minSamplesLeaf, _order, bags, rnd2);
double pred = tree.predict(_x[i]);
synchronized (_x[i]) {
import hivemall.utils.collections.IntArrayList;
private final NodeOutput _nodeOutput;
int[] bags;
public TrainNode(Node node, double[][] x, double[] y, int[] bags, int depth) {
this.bags = bags;
int[] samples = SmileExtUtils.bagsToSamples(bags);
node.output = output.calculate(samples);
public boolean findBestSplit() {
final int numSamples = bags.length;
if (numSamples <= _minSplit) {
final double sum = node.output * numSamples;
Node split = findBestSplit(numSamples, sum, variables[j]);
return node.splitFeature != -1;
private Node findBestSplit(final int n, final double sum, final int j) {
final int[] samples = SmileExtUtils.bagsToSamples(bags, N);
public boolean split(final PriorityQueue<TrainNode> nextSplits) {
int childBagSize = (int) (bags.length * 0.4);
IntArrayList trueBags = new IntArrayList(childBagSize);
IntArrayList falseBags = new IntArrayList(childBagSize);
int tc = splitSamples(trueBags, falseBags);
int fc = bags.length - tc;
if (_nodeOutput == null) {
this.bags = null;
}
node.trueChild = new Node(node.trueChildOutput);
if (tc >= _minSplit && trueChild.findBestSplit()) {
trueChild.split(null);
node.falseChild = new Node(node.falseChildOutput);
if (fc >= _minSplit && falseChild.findBestSplit()) {
falseChild.split(null);
private int splitSamples(@Nonnull final IntArrayList trueBags,
@Nonnull final IntArrayList falseBags) {
int tc = 0;
if (node.splitFeatureType == AttributeType.NOMINAL) {
final int splitFeature = node.splitFeature;
final double splitValue = node.splitValue;
final int index = bags[i];
if (x[index][splitFeature] == splitValue) {
trueBags.add(index);
tc;
} else {
falseBags.add(index);
}
}
} else if (node.splitFeatureType == AttributeType.NUMERIC) {
final int splitFeature = node.splitFeature;
final double splitValue = node.splitValue;
final int index = bags[i];
if (x[index][splitFeature] <= splitValue) {
trueBags.add(index);
tc;
} else {
falseBags.add(index);
}
}
} else {
throw new IllegalStateException("Unsupported attribute type: "
node.splitFeatureType);
}
return tc;
}
@Nonnull double[] y, int maxLeafs, @Nullable smile.math.Random rand) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, maxLeafs, 5, 1, null, null, rand);
}
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x,
int minLeafSize, @Nullable int[][] order, @Nullable int[] bags,
this(attributes, x, y, numVars, maxDepth, maxLeafs, minSplits, minLeafSize, order, bags, null, rand);
int minLeafSize, @Nullable int[][] order, @Nullable int[] bags,
this._nodeOutput = output;
if (bags == null) {
bags = new int[n];
bags[i] = i;
n = bags.length;
int index = bags[i];
TrainNode trainRoot = new TrainNode(_root, x, y, bags, 1);
if (trainRoot.findBestSplit()) {
trainRoot.split(null);
if (trainRoot.findBestSplit()) {
@Nonnull
public static int[] bagsToSamples(@Nonnull final int[] bags) {
int maxIndex = -1;
for (int e : bags) {
if (e > maxIndex) {
maxIndex = e;
}
}
}
@Nonnull
public static int[] bagsToSamples(@Nonnull final int[] bags, final int samplesLength) {
final int[] samples = new int[samplesLength];
}
return samples;
}
public static final String VERSION = "0.4.1-alpha.5";
import hivemall.utils.collections.IntArrayList;
int[] bags;
public TrainNode(Node node, double[][] x, int[] y, int[] bags, int depth) {
this.bags = bags;
final int numSamples = bags.length;
if (numSamples <= _minSplit) {
return false;
}
final int[] count = new int[_k];
int index = bags[i];
int y_i = y[index];
if (label == -1) {
label = y_i;
} else if (y_i != label) {
pure = false;
final double impurity = impurity(count, numSamples, _rule);
Node split = findBestSplit(numSamples, count, falseCount, impurity,
variableIndex[j]);
int index = bags[i];
int x_ij = (int) x[index][j];
final int[] samples = SmileExtUtils.bagsToSamples(bags, x.length);
int childBagSize = (int) (bags.length * 0.4);
IntArrayList trueBags = new IntArrayList(childBagSize);
IntArrayList falseBags = new IntArrayList(childBagSize);
int tc = splitSamples(trueBags, falseBags);
int fc = bags.length - tc;
node.falseChild = new Node(node.falseChildOutput);
TrainNode falseChild = new TrainNode(node.falseChild, x, y, falseBags.toArray(),
private int splitSamples(@Nonnull final IntArrayList trueBags,
@Nonnull final IntArrayList falseBags) {
int tc = 0;
if (node.splitFeatureType == AttributeType.NOMINAL) {
final int splitFeature = node.splitFeature;
final double splitValue = node.splitValue;
final int index = bags[i];
if (x[index][splitFeature] == splitValue) {
trueBags.add(index);
tc;
} else {
falseBags.add(index);
}
}
} else if (node.splitFeatureType == AttributeType.NUMERIC) {
final int splitFeature = node.splitFeature;
final double splitValue = node.splitValue;
final int index = bags[i];
if (x[index][splitFeature] <= splitValue) {
trueBags.add(index);
tc;
} else {
falseBags.add(index);
}
}
} else {
throw new IllegalStateException("Unsupported attribute type: "
node.splitFeatureType);
}
return tc;
}
impurity = Math.max(impurity, (double) count[i] / n);
public DecisionTree(@Nullable Attribute[] attributes, @Nullable double[][] x,
@Nullable int[] y, int numLeafs, @Nullable smile.math.Random rand) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, numLeafs, 2, 1, null, null, SplitRule.GINI, rand);
}
@Nullable int[] bags, @Nullable int[][] order, @Nonnull SplitRule rule,
if (bags == null) {
bags = new int[n];
bags[i] = i;
int index = bags[i];
final TrainNode trainRoot = new TrainNode(_root, x, y, bags, 1);
TrainNode parent = nextSplits.poll();
if (parent == null) {
import java.util.BitSet;
final int numInstances = x.length;
final int numSamples = (int) Math.round(numInstances * _subsample);
final BitSet sampled = new BitSet(numInstances);
final int[] bag = new int[numSamples];
final int[] perm = new int[numSamples];
int index = perm[i];
bag[i] = index;
sampled.set(index);
_maxLeafNodes, _minSamplesSplit, _minSamplesLeaf, order, bag, output, rnd2);
oobTests;
final int pred = (h[i] > 0.d) ? 1 : 0;
if (pred != y[i]) {
sampled.clear();
final int numInstances = x.length;
final int numSamples = (int) Math.round(numInstances * _subsample);
final BitSet sampled = new BitSet(numInstances);
final int[] bag = new int[numSamples];
final int[] perm = new int[numSamples];
final int[] prediction = new int[numInstances];
int oobTests = 0, oobErrors = 0;
int index = perm[i];
bag[i] = index;
sampled.set(i);
_maxDepth, _maxLeafNodes, _minSamplesSplit, _minSamplesLeaf, order, bag,
oobTests;
if (prediction[i] != y[i]) {
sampled.clear();
double y_i = y[i];
double abs = Math.abs(y_i);
double y_i = y[i];
double abs = Math.abs(y_i);
return ((k - 1.0d) / k) * (nu / de);
import java.util.BitSet;
final int N = _x.length;
final int[] bags = new int[N];
final BitSet sampled = new BitSet(N);
int index = rnd1.nextInt(N);
bags[i] = index;
sampled.set(index);
_udtf._maxLeafNodes, _udtf._minSamplesSplit, _udtf._minSamplesLeaf, bags, _order,
_udtf._splitRule, rnd2);
final int p = tree.predict(_x[i]);
synchronized (_prediction[i]) {
import java.util.BitSet;
final int N = _x.length;
final int[] bags = new int[N];
final BitSet sampled = new BitSet(N);
int index = rnd1.nextInt(N);
bags[i] = index;
sampled.set(index);
_udtf._minSamplesLeaf, _order, bags, rnd2);
double pred = tree.predict(_x[i]);
synchronized (_x[i]) {
import hivemall.utils.collections.IntArrayList;
private final NodeOutput _nodeOutput;
int[] bags;
public TrainNode(Node node, double[][] x, double[] y, int[] bags, int depth) {
this.bags = bags;
int[] samples = SmileExtUtils.bagsToSamples(bags);
final int numSamples = bags.length;
if (numSamples <= _minSplit) {
final double sum = node.output * numSamples;
Node split = findBestSplit(numSamples, sum, variables[j]);
return node.splitFeature != -1;
final int[] samples = SmileExtUtils.bagsToSamples(bags, N);
int childBagSize = (int) (bags.length * 0.4);
IntArrayList trueBags = new IntArrayList(childBagSize);
IntArrayList falseBags = new IntArrayList(childBagSize);
int tc = splitSamples(trueBags, falseBags);
int fc = bags.length - tc;
if (_nodeOutput == null) {
this.bags = null;
}
node.trueChild = new Node(node.trueChildOutput);
node.falseChild = new Node(node.falseChildOutput);
private int splitSamples(@Nonnull final IntArrayList trueBags,
@Nonnull final IntArrayList falseBags) {
int tc = 0;
if (node.splitFeatureType == AttributeType.NOMINAL) {
final int splitFeature = node.splitFeature;
final double splitValue = node.splitValue;
final int index = bags[i];
if (x[index][splitFeature] == splitValue) {
trueBags.add(index);
tc;
} else {
falseBags.add(index);
}
}
} else if (node.splitFeatureType == AttributeType.NUMERIC) {
final int splitFeature = node.splitFeature;
final double splitValue = node.splitValue;
final int index = bags[i];
if (x[index][splitFeature] <= splitValue) {
trueBags.add(index);
tc;
} else {
falseBags.add(index);
}
}
} else {
throw new IllegalStateException("Unsupported attribute type: "
node.splitFeatureType);
}
return tc;
}
@Nonnull double[] y, int maxLeafs, @Nullable smile.math.Random rand) {
this(attributes, x, y, x[0].length, Integer.MAX_VALUE, maxLeafs, 5, 1, null, null, rand);
}
public RegressionTree(@Nullable Attribute[] attributes, @Nonnull double[][] x,
int minLeafSize, @Nullable int[][] order, @Nullable int[] bags,
this(attributes, x, y, numVars, maxDepth, maxLeafs, minSplits, minLeafSize, order, bags, null, rand);
int minLeafSize, @Nullable int[][] order, @Nullable int[] bags,
this._nodeOutput = output;
if (bags == null) {
bags = new int[n];
bags[i] = i;
n = bags.length;
int index = bags[i];
TrainNode trainRoot = new TrainNode(_root, x, y, bags, 1);
@Nonnull
public static int[] bagsToSamples(@Nonnull final int[] bags) {
int maxIndex = -1;
for (int e : bags) {
if (e > maxIndex) {
maxIndex = e;
}
}
}
@Nonnull
public static int[] bagsToSamples(@Nonnull final int[] bags, final int samplesLength) {
final int[] samples = new int[samplesLength];
}
return samples;
}
public static int[] copyOf(final int[] src) {
int len = src.length;
int[] dest = new int[len];
System.arraycopy(src, 0, dest, 0, len);
return dest;
}
public static void copy(final int[] src, final int[] dest) {
if (src.length != dest.length) {
}
System.arraycopy(src, 0, dest, 0, src.length);
}
public static int[] append(int[] array, int currentSize, int element) {
int[] newArray = new int[currentSize * 2];
System.arraycopy(array, 0, newArray, 0, currentSize);
array = newArray;
}
array[currentSize] = element;
return array;
}
public static int[] insert(int[] array, int currentSize, int index, int element) {
array[index] = element;
return array;
}
int[] newArray = new int[currentSize * 2];
System.arraycopy(array, 0, newArray, 0, index);
newArray[index] = element;
return newArray;
}
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import org.apache.hadoop.hive.ql.exec.MapredContext;
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
@Nullable
protected MapredContext mapredContext;
@Override
public void configure(MapredContext mapredContext) {
this.mapredContext = mapredContext;
}
@Nullable
protected Reporter getReportter() {
if (mapredContext == null) {
return null;
}
return mapredContext.getReporter();
}
protected static void reportProgress(@Nonnull Reporter reporter) {
if (reporter != null) {
synchronized (reporter) {
reporter.progress();
}
}
}
protected static void setCounterValue(@Nullable Counter counter, long value) {
if (counter != null) {
synchronized (counter) {
counter.setValue(value);
}
}
}
protected static void incrCounter(@Nullable Counter counter, long incr) {
if (counter != null) {
synchronized (counter) {
counter.increment(incr);
}
}
}
if (cl.hasOption("help")) {
if (funcDesc == null) {
formatter.printHelp(pw, HelpFormatter.DEFAULT_WIDTH, cmdLineSyntax, null, opts,
HelpFormatter.DEFAULT_LEFT_PAD, HelpFormatter.DEFAULT_DESC_PAD, null, true);
protected final List<FeatureValue> parseFeatures(final List<?> features,
final ObjectInspector featureInspector, final boolean parseFeature) {
if (numFeatures == 0) {
for (Object f : features) {
if (f == null) {
if (parseFeature) {
Object o = ObjectInspectorUtils.copyToStandardObject(f, featureInspector,
ObjectInspectorCopyOption.WRITABLE);
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
final Reporter reporter = getReportter();
final Counter iterCounter = (reporter == null) ? null : reporter.getCounter(
"hivemall.fm.FactorizationMachines$Counter", "iteration");
reportProgress(reporter);
setCounterValue(iterCounter, i);
setCounterValue(iterCounter, i);
reportProgress(reporter);
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.Counters.Counter;
final Reporter reporter = getReportter();
final Counter iterCounter = (reporter == null) ? null : reporter.getCounter(
"hivemall.mf.MatrixFactorization$Counter", "iteration");
reportProgress(reporter);
setCounterValue(iterCounter, i);
setCounterValue(iterCounter, i);
reportProgress(reporter);
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
@Nullable
private Reporter _progressReporter;
@Nullable
private Counter _iterationCounter;
this._progressReporter = getReportter();
this._iterationCounter = (_progressReporter == null) ? null : _progressReporter.getCounter(
"hivemall.smile.GradientTreeBoostingClassifier$Counter", "iteration");
reportProgress(_progressReporter);
reportProgress(_progressReporter);
reportProgress(_progressReporter);
reportProgress(_progressReporter);
incrCounter(_iterationCounter, 1);
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
@Nullable
private Reporter _progressReporter;
@Nullable
private Counter _treeBuildTaskCounter;
this._progressReporter = getReportter();
this._treeBuildTaskCounter = (_progressReporter == null) ? null : _progressReporter.getCounter(
"hivemall.smile.RandomForestClassifier$Counter", "finishedTreeBuildTasks");
reportProgress(_progressReporter);
int numExamples = featuresList.size();
reportProgress(_progressReporter);
incrCounter(_treeBuildTaskCounter, 1);
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
@Nullable
private Reporter _progressReporter;
@Nullable
private Counter _treeBuildTaskCounter;
this._progressReporter = getReportter();
this._treeBuildTaskCounter = (_progressReporter == null) ? null
: _progressReporter.getCounter("hivemall.smile.RandomForestRegression$Counter",
"finishedTreeBuildTasks");
reportProgress(_progressReporter);
int numExamples = featuresList.size();
reportProgress(_progressReporter);
incrCounter(_treeBuildTaskCounter, 1);
public final void configure(MapredContext mapredContext) {
protected final Reporter getReporter() {
final Reporter reporter = getReporter();
import org.apache.hadoop.hive.ql.exec.MapredContextAccessor;
import org.apache.hadoop.mapred.Reporter;
public abstract class OnlineMatrixFactorizationUDTF extends UDTFWithOptions implements
RatingInitilizer {
opts.addOption("update_mean", "update_mu", false,
"Whether update (and return) the mean rating or not");
opts.addOption("rankinit", true,
"Initialization strategy of rank matrix [random, gaussian] (default: random)");
opts.addOption("maxval", "max_init_value", true,
"The maximum initial value in the rank matrix [default: 1.0]");
opts.addOption("min_init_stddev", true,
"The minimum standard deviation of initial rank matrix [default: 0.1]");
opts.addOption("disable_cv", "disable_cvtest", false,
"Whether to disable convergence check [default: enabled]");
opts.addOption("cv_rate", "convergence_rate", true,
"Threshold to determine convergence [default: 0.005]");
if (argOIs.length >= 4) {
if (iterations < 1) {
throw new UDFArgumentException(
if (noBias && updateMeanRating) {
if (argOIs.length < 3) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 3 arguments: INT user, INT item, FLOAT rating [, CONSTANT STRING options]");
if (useBiasClause) {
if (updateMeanRating) {
if (mapredContext != null && iterations > 1) {
if (!file.canWrite()) {
if (user < 0) {
if (item < 0) {
if (useBiasClause) {
if (updateMeanRating) {
protected void beforeTrain(final long rowNum, final int user, final int item,
final double rating) throws HiveException {
if (inputBuf != null) {
if (remain < RECORD_BYTES) {
protected void onUpdate(final int user, final int item, final Rating[] users,
final Rating[] items, final double err) throws HiveException {}
protected double predict(final int user, final int item, final float[] userProbe,
final float[] itemProbe) {
if (users == null) {
if (items == null) {
if (useBiasClause == false) {
protected void updateItemRating(final Rating rating, final float Pu, final float Qi,
final double err, final float eta) {
protected void updateUserRating(final Rating rating, final float Pu, final float Qi,
final double err, final float eta) {
if (model != null) {
if (count == 0) {
if (iterations > 1) {
if (updateMeanRating) {
forwardObj = new Object[] {idx, Pu, Qi, Bu, Bi, mu};
if (useBiasClause) {
forwardObj = new Object[] {idx, Pu, Qi, Bu, Bi};
forwardObj = new Object[] {idx, Pu, Qi};
if (userRatings == null) {
if (itemRatings == null) {
if (useBiasClause) {
protected static void writeBuffer(@Nonnull final ByteBuffer srcBuf,
@Nonnull final NioFixedSegment dst, final long lastWritePos) throws HiveException {
e);
final Reporter reporter = getReporter();
MapredContextAccessor.get());
if (inputBuf.position() == 0) {
while (inputBuf.remaining() > 0) {
if (inputBuf.position() > 0) {
} else if (lastWritePos == 0) {
if (logger.isInfoEnabled()) {
while (true) {
for (; remain >= RECORD_BYTES; remain -= RECORD_BYTES) {
this._progressReporter = getReporter();
this._progressReporter = getReporter();
this._progressReporter = getReporter();
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import org.apache.hadoop.hive.ql.exec.MapredContext;
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
@Nullable
protected MapredContext mapredContext;
@Override
public final void configure(MapredContext mapredContext) {
this.mapredContext = mapredContext;
}
@Nullable
protected final Reporter getReporter() {
if (mapredContext == null) {
return null;
}
return mapredContext.getReporter();
}
protected static void reportProgress(@Nonnull Reporter reporter) {
if (reporter != null) {
synchronized (reporter) {
reporter.progress();
}
}
}
protected static void setCounterValue(@Nullable Counter counter, long value) {
if (counter != null) {
synchronized (counter) {
counter.setValue(value);
}
}
}
protected static void incrCounter(@Nullable Counter counter, long incr) {
if (counter != null) {
synchronized (counter) {
counter.increment(incr);
}
}
}
if (cl.hasOption("help")) {
if (funcDesc == null) {
formatter.printHelp(pw, HelpFormatter.DEFAULT_WIDTH, cmdLineSyntax, null, opts,
HelpFormatter.DEFAULT_LEFT_PAD, HelpFormatter.DEFAULT_DESC_PAD, null, true);
protected final List<FeatureValue> parseFeatures(final List<?> features,
final ObjectInspector featureInspector, final boolean parseFeature) {
if (numFeatures == 0) {
for (Object f : features) {
if (f == null) {
if (parseFeature) {
Object o = ObjectInspectorUtils.copyToStandardObject(f, featureInspector,
ObjectInspectorCopyOption.WRITABLE);
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
final Reporter reporter = getReporter();
final Counter iterCounter = (reporter == null) ? null : reporter.getCounter(
"hivemall.fm.FactorizationMachines$Counter", "iteration");
reportProgress(reporter);
setCounterValue(iterCounter, i);
setCounterValue(iterCounter, i);
reportProgress(reporter);
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
public abstract class OnlineMatrixFactorizationUDTF extends UDTFWithOptions implements
RatingInitilizer {
opts.addOption("update_mean", "update_mu", false,
"Whether update (and return) the mean rating or not");
opts.addOption("rankinit", true,
"Initialization strategy of rank matrix [random, gaussian] (default: random)");
opts.addOption("maxval", "max_init_value", true,
"The maximum initial value in the rank matrix [default: 1.0]");
opts.addOption("min_init_stddev", true,
"The minimum standard deviation of initial rank matrix [default: 0.1]");
opts.addOption("disable_cv", "disable_cvtest", false,
"Whether to disable convergence check [default: enabled]");
opts.addOption("cv_rate", "convergence_rate", true,
"Threshold to determine convergence [default: 0.005]");
if (argOIs.length >= 4) {
if (iterations < 1) {
throw new UDFArgumentException(
if (noBias && updateMeanRating) {
if (argOIs.length < 3) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 3 arguments: INT user, INT item, FLOAT rating [, CONSTANT STRING options]");
if (useBiasClause) {
if (updateMeanRating) {
if (mapredContext != null && iterations > 1) {
if (!file.canWrite()) {
if (user < 0) {
if (item < 0) {
if (useBiasClause) {
if (updateMeanRating) {
protected void beforeTrain(final long rowNum, final int user, final int item,
final double rating) throws HiveException {
if (inputBuf != null) {
if (remain < RECORD_BYTES) {
protected void onUpdate(final int user, final int item, final Rating[] users,
final Rating[] items, final double err) throws HiveException {}
protected double predict(final int user, final int item, final float[] userProbe,
final float[] itemProbe) {
if (users == null) {
if (items == null) {
if (useBiasClause == false) {
protected void updateItemRating(final Rating rating, final float Pu, final float Qi,
final double err, final float eta) {
protected void updateUserRating(final Rating rating, final float Pu, final float Qi,
final double err, final float eta) {
if (model != null) {
if (count == 0) {
if (iterations > 1) {
if (updateMeanRating) {
forwardObj = new Object[] {idx, Pu, Qi, Bu, Bi, mu};
if (useBiasClause) {
forwardObj = new Object[] {idx, Pu, Qi, Bu, Bi};
forwardObj = new Object[] {idx, Pu, Qi};
if (userRatings == null) {
if (itemRatings == null) {
if (useBiasClause) {
protected static void writeBuffer(@Nonnull final ByteBuffer srcBuf,
@Nonnull final NioFixedSegment dst, final long lastWritePos) throws HiveException {
e);
final Reporter reporter = getReporter();
final Counter iterCounter = (reporter == null) ? null : reporter.getCounter(
"hivemall.mf.MatrixFactorization$Counter", "iteration");
if (inputBuf.position() == 0) {
reportProgress(reporter);
setCounterValue(iterCounter, i);
while (inputBuf.remaining() > 0) {
if (inputBuf.position() > 0) {
} else if (lastWritePos == 0) {
if (logger.isInfoEnabled()) {
setCounterValue(iterCounter, i);
while (true) {
reportProgress(reporter);
for (; remain >= RECORD_BYTES; remain -= RECORD_BYTES) {
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
@Nullable
private Reporter _progressReporter;
@Nullable
private Counter _iterationCounter;
this._progressReporter = getReporter();
this._iterationCounter = (_progressReporter == null) ? null : _progressReporter.getCounter(
"hivemall.smile.GradientTreeBoostingClassifier$Counter", "iteration");
reportProgress(_progressReporter);
reportProgress(_progressReporter);
reportProgress(_progressReporter);
reportProgress(_progressReporter);
incrCounter(_iterationCounter, 1);
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
@Nullable
private Reporter _progressReporter;
@Nullable
private Counter _treeBuildTaskCounter;
this._progressReporter = getReporter();
this._treeBuildTaskCounter = (_progressReporter == null) ? null : _progressReporter.getCounter(
"hivemall.smile.RandomForestClassifier$Counter", "finishedTreeBuildTasks");
reportProgress(_progressReporter);
int numExamples = featuresList.size();
reportProgress(_progressReporter);
incrCounter(_treeBuildTaskCounter, 1);
import javax.annotation.Nullable;
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.Reporter;
@Nullable
private Reporter _progressReporter;
@Nullable
private Counter _treeBuildTaskCounter;
this._progressReporter = getReporter();
this._treeBuildTaskCounter = (_progressReporter == null) ? null
: _progressReporter.getCounter("hivemall.smile.RandomForestRegression$Counter",
"finishedTreeBuildTasks");
reportProgress(_progressReporter);
int numExamples = featuresList.size();
reportProgress(_progressReporter);
incrCounter(_treeBuildTaskCounter, 1);
import hivemall.utils.hadoop.HiveUtils;
@Description(name = "binarize_label", value = "_FUNC_(int/long positive, int/long negative, ...) "
"- Returns positive/negative records that are represented "
"as (..., int label) where label is 0 or 1")
public final class BinarizeLabelUDTF extends GenericUDTF {
public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
throw new UDFArgumentException("binalize_label(int/long positive, "
"int/long negative, *) takes at least three arguments");
this.positiveOI = HiveUtils.asIntCompatibleOI(argOIs[0]);
this.negativeOI = HiveUtils.asIntCompatibleOI(argOIs[1]);
positiveObjs[positiveObjs.length - 1] = 0;
negativeObjs[negativeObjs.length - 1] = 1;
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
final int positive = PrimitiveObjectInspectorUtils.getInt(args[0], positiveOI);
final int negative = PrimitiveObjectInspectorUtils.getInt(args[1], negativeOI);
import hivemall.utils.hadoop.HiveUtils;
@Description(name = "binarize_label", value = "_FUNC_(int/long positive, int/long negative, ...) "
"- Returns positive/negative records that are represented "
"as (..., int label) where label is 0 or 1")
public final class BinarizeLabelUDTF extends GenericUDTF {
public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
throw new UDFArgumentException("binalize_label(int/long positive, "
"int/long negative, *) takes at least three arguments");
this.positiveOI = HiveUtils.asIntCompatibleOI(argOIs[0]);
this.negativeOI = HiveUtils.asIntCompatibleOI(argOIs[1]);
positiveObjs[positiveObjs.length - 1] = 0;
negativeObjs[negativeObjs.length - 1] = 1;
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
final int positive = PrimitiveObjectInspectorUtils.getInt(args[0], positiveOI);
final int negative = PrimitiveObjectInspectorUtils.getInt(args[1], negativeOI);
import hivemall.utils.datetime.StopWatch;
import java.util.concurrent.TimeUnit;
@Nullable
private Counter _treeConstuctionTimeCounter;
@Nullable
private Counter _treeSerializationTimeCounter;
"Number of finished tree construction tasks");
this._treeConstuctionTimeCounter = (_progressReporter == null) ? null
: _progressReporter.getCounter("hivemall.smile.RandomForestRegression$Counter",
"Elapsed time in seconds for tree construction");
this._treeSerializationTimeCounter = (_progressReporter == null) ? null
: _progressReporter.getCounter("hivemall.smile.RandomForestRegression$Counter",
"Elapsed time in seconds for tree serialization");
StopWatch stopwatch = new StopWatch();
incrCounter(_udtf._treeConstuctionTimeCounter, stopwatch.elapsed(TimeUnit.SECONDS));
stopwatch.reset().start();
incrCounter(_udtf._treeSerializationTimeCounter, stopwatch.elapsed(TimeUnit.SECONDS));
import java.util.concurrent.TimeUnit;
import javax.annotation.Nonnull;
public StopWatch reset() {
return this;
if (end != 0) {
public long elapsed(@Nonnull TimeUnit unit) {
return unit.convert(elapsed(), TimeUnit.MILLISECONDS);
}
if (label != null) {
if (showInSec) {
final int[] samples = SmileExtUtils.containsNumericType(_attributes) ? SmileExtUtils.bagsToSamples(
bags, x.length) : null;
variableIndex[j], samples);
final double impurity, final int j, @Nullable final int[] samples) {
assert (samples != null);
final int[] samples = SmileExtUtils.bagsToSamples(bags, x.length);
Node split = findBestSplit(numSamples, sum, variables[j], samples);
private Node findBestSplit(final int n, final double sum, final int j, final int[] samples) {
public static boolean containsNumericType(@Nonnull final Attribute[] attributes) {
for (Attribute attr : attributes) {
if (attr.type == AttributeType.NUMERIC) {
return true;
}
}
return false;
}
private final boolean _hasNumericType;
final int[] samples = _hasNumericType ? SmileExtUtils.bagsToSamples(bags, x.length)
: null;
this._hasNumericType = SmileExtUtils.containsNumericType(_attributes);
@Description(name = "r2",
value = "_FUNC_(predicted, actual) - Return R Squared (coefficient of determination)")
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
this.residual_sum_of_squares = 0.d;
this.sum_actuals = 0.d;
double avg_actuals = this.sum_actuals / this.count;
double total_sum_of_squares = 0.d;
for (Double a : actuals) {
}
if (total_sum_of_squares == 0.d) {
return 1.d;
}
return 1.d - this.residual_sum_of_squares / total_sum_of_squares;
private final boolean _hasNumericType;
final int[] samples = _hasNumericType ? SmileExtUtils.bagsToSamples(bags, x.length)
: null;
variableIndex[j], samples);
final double impurity, final int j, @Nullable final int[] samples) {
assert (samples != null);
this._hasNumericType = SmileExtUtils.containsNumericType(_attributes);
final int[] samples = SmileExtUtils.bagsToSamples(bags, x.length);
Node split = findBestSplit(numSamples, sum, variables[j], samples);
private Node findBestSplit(final int n, final double sum, final int j, final int[] samples) {
public static boolean containsNumericType(@Nonnull final Attribute[] attributes) {
for (Attribute attr : attributes) {
if (attr.type == AttributeType.NUMERIC) {
return true;
}
}
return false;
}
private final boolean _hasNumericType;
final int[] samples = _hasNumericType ? SmileExtUtils.bagsToSamples(bags, x.length)
: null;
private Node findBestSplit(final int n, final double sum, final int j,
@Nullable final int[] samples) {
int i = bags[b];
int index = (int) x[i][j];
trueCount[index];
final int sample = samples[i];
if (sample > 0) {
this._hasNumericType = SmileExtUtils.containsNumericType(_attributes);
@Description(name = "rf_ensemble",
value = "_FUNC_(int y) - Returns emsebled prediction results of Random Forest classifiers")
@Override
if (k == null) {
if (partial == null) {
if (partial == null) {
if (partial.size() == 0) {
if (o == null) {
if (partial == null) {
this.partial = new Counter<Integer>();
partial.addAll(o);
if (partial == null) {
if (partial.size() == 0) {
for (Map.Entry<Integer, Integer> e : counts.entrySet()) {
if (cnt >= maxCnt) {
if (cnt == null) {
this.size = -1;
if (attr.getSize() != -1) {
continue;
}
case NOMINAL: {
}
case NUMERIC: {
}
public final class DecisionTree implements Classifier<double[]> {
public static final class Node implements Externalizable {
private final class TrainNode implements Comparable<TrainNode> {
final boolean pure = sampleCount(count);
private boolean sampleCount(@Nonnull final int[] count) {
int label = -1;
boolean pure = true;
int index = bags[i];
int y_i = y[index];
if (label == -1) {
label = y_i;
} else if (y_i != label) {
pure = false;
}
}
return pure;
}
this._order = (order == null) ? SmileExtUtils.sort(_attributes, x) : order;
this._importance = new double[_attributes.length];
public final class RegressionTree implements Regression<double[]> {
public static final class Node implements Externalizable {
private final class TrainNode implements Comparable<TrainNode> {
if (_attributes.length != x[0].length) {
this._order = (order == null) ? SmileExtUtils.sort(_attributes, x) : order;
this._importance = new double[_attributes.length];
public static final String VERSION = "0.4.1-alpha.6";
ArrayList<String> fieldNames = new ArrayList<String>();
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
fieldNames.add("idx");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
fieldNames.add("Pu");
fieldOIs.add(ObjectInspectorFactory.getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableFloatObjectInspector));
fieldNames.add("Qi");
fieldOIs.add(ObjectInspectorFactory.getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableFloatObjectInspector));
if (useBiasClause) {
fieldNames.add("Bu");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
fieldNames.add("Bi");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
if (updateMeanRating) {
fieldNames.add("mu");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
}
}
public boolean isLossIncreased() {
return currLosses > prevLosses;
}
import javax.annotation.Nonnegative;
if (t > total_steps) {
public static final class AdjustingEtaEstimator extends EtaEstimator {
private final float eta0;
private float eta;
public AdjustingEtaEstimator(float eta) {
this.eta0 = eta;
this.eta = eta;
}
public void update(@Nonnegative float multipler) {
this.eta = Math.max(eta0, eta * multipler);
}
@Override
public float eta(long t) {
return eta;
}
}
if (cl == null) {
if (etaValue != null) {
if (cl.hasOption("t")) {
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor,
@Nonnull RankInitScheme initScheme) {
this(ratingInitializer, factor, 0.f, initScheme, 136861);
}
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor,
float meanRating, @Nonnull RankInitScheme initScheme) {
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor,
float meanRating, @Nonnull RankInitScheme initScheme, int expectedSize) {
if (opt == null) {
} else if ("gaussian".equalsIgnoreCase(opt)) {
} else if ("random".equalsIgnoreCase(opt)) {
if (init && v == null) {
switch (initScheme) {
if (init && v == null) {
switch (initScheme) {
if (b == null) {
if (b == null) {
if (b == null) {
if (b == null) {
if (b == null) {
if (b == null) {
private static void uniformFill(final Rating[] a, final Random rand, final float maxInitValue,
final RatingInitilizer init) {
private static void gaussianFill(final Rating[] a, final Random[] rand, final double stddev,
final RatingInitilizer init) {
import java.util.BitSet;
public static float getAsConstFloat(@Nonnull final ObjectInspector numberOI)
throws UDFArgumentException {
final String typeName = numberOI.getTypeName();
if (FLOAT_TYPE_NAME.equals(typeName)) {
FloatWritable v = getConstValue(numberOI);
return v.get();
} else if (DOUBLE_TYPE_NAME.equals(typeName)) {
DoubleWritable v = getConstValue(numberOI);
return (float) v.get();
} else if (INT_TYPE_NAME.equals(typeName)) {
IntWritable v = getConstValue(numberOI);
return v.get();
} else if (BIGINT_TYPE_NAME.equals(typeName)) {
LongWritable v = getConstValue(numberOI);
return v.get();
} else if (SMALLINT_TYPE_NAME.equals(typeName)) {
ShortWritable v = getConstValue(numberOI);
return v.get();
} else if (TINYINT_TYPE_NAME.equals(typeName)) {
ByteWritable v = getConstValue(numberOI);
return v.get();
}
throw new UDFArgumentException("Unexpected argument type to cast as double: "
TypeInfoUtils.getTypeInfoFromObjectInspector(numberOI));
}
public static long[] asLongArray(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI, @Nonnull PrimitiveObjectInspector elemOI) {
if (argObj == null) {
return null;
}
final int length = listOI.getListLength(argObj);
final long[] ary = new long[length];
Object o = listOI.getListElement(argObj, i);
if (o == null) {
continue;
}
ary[i] = PrimitiveObjectInspectorUtils.getLong(o, elemOI);
}
return ary;
}
@Nonnull
public static double[] asDoubleArray(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI,
@Nonnull final PrimitiveObjectInspector elemOI) {
@Nonnull
public static int setBits(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI,
@Nonnull final PrimitiveObjectInspector elemOI, @Nonnull final BitSet bitset)
throws UDFArgumentException {
if (argObj == null) {
return 0;
}
int count = 0;
final int length = listOI.getListLength(argObj);
Object o = listOI.getListElement(argObj, i);
if (o == null) {
continue;
}
int index = PrimitiveObjectInspectorUtils.getInt(o, elemOI);
if (index < 0) {
}
bitset.set(index);
count;
}
return count;
}
public static PrimitiveObjectInspector asLongCompatibleOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if (argOI.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0, "Only primitive type arguments are accepted but "
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case LONG:
case INT:
case SHORT:
case BYTE:
case BOOLEAN:
case FLOAT:
case DOUBLE:
case STRING:
case TIMESTAMP:
case DECIMAL:
break;
default:
"' is passed.");
}
return oi;
}
if (numberOfBits >= 32) {
if (powerOf == 0) {
while (value != 0) {
public static double lnSigmoid(final double x) {
double ex = Math.exp(-x);
}
if (w < 6.25) {
} else if (w < 16.0) {
} else if (!Double.isInfinite(w)) {
for (float e : elements) {
import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
public static boolean isPrimitiveTypeInfo(@Nonnull TypeInfo typeInfo) {
return typeInfo.getCategory() == ObjectInspector.Category.PRIMITIVE;
}
public static boolean isIntegerTypeInfo(@Nonnull TypeInfo typeInfo) {
if (typeInfo.getCategory() != ObjectInspector.Category.PRIMITIVE) {
return false;
}
switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {
case BYTE:
case SHORT:
case INT:
case LONG:
return true;
default:
return false;
}
}
public static ListTypeInfo asListTypeInfo(@Nonnull TypeInfo typeInfo)
throws UDFArgumentException {
if (!typeInfo.getCategory().equals(Category.LIST)) {
}
return (ListTypeInfo) typeInfo;
}
public static double log(final double n, final int base) {
return Math.log(n) / Math.log(base);
}
public final class BinaryResponsesMeasures {
private BinaryResponsesMeasures() {}
outputOI = PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
@Description(name = "array_concat", value = "_FUNC_(x1, x2, ..) - Returns a concatinated array")
public class ArrayConcatUDF extends GenericUDF {
throw new UDFArgumentLengthException("_FUNC_(array1, array2) needs at least 1 argument.");
@Deprecated
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
@Description(
name = "ndcg",
value = "_FUNC_(array rankItems, array correctItems [, const boolean binaryResponses = true])"
" - Returns nDCG")
if (typeInfo.length != 2 && typeInfo.length != 3) {
throw new UDFArgumentTypeException(typeInfo.length - 1,
"_FUNC_ takes two or three arguments");
boolean binaryResponses = true;
if (typeInfo.length == 3) {
binaryResponses = HiveUtils.isBooleanTypeInfo(typeInfo[2]);
if (binaryResponses == false) {
throw new UDFArgumentException(
"nDCG computation for Graded Responses is not supported yet");
}
}
public static boolean isBooleanTypeInfo(@Nonnull TypeInfo typeInfo) {
if (typeInfo.getCategory() != ObjectInspector.Category.PRIMITIVE) {
return false;
}
switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {
case BOOLEAN:
return true;
default:
return false;
}
}
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
private ListObjectInspector mergeOI;
private PrimitiveObjectInspector mergeListElemOI;
this.mergeOI = HiveUtils.asListOI(argOIs[0]);
this.mergeListElemOI = HiveUtils.asPrimitiveObjectInspector(mergeOI.getListElementObjectInspector());
long[] longs = HiveUtils.asLongArray(other, mergeOI, mergeListElemOI);
import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
public static long[] asLongArray(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI, @Nonnull LongObjectInspector elemOI) {
if (argObj == null) {
return null;
}
final int length = listOI.getListLength(argObj);
final long[] ary = new long[length];
Object o = listOI.getListElement(argObj, i);
if (o == null) {
continue;
}
ary[i] = elemOI.get(o);
}
return ary;
}
@Nonnull
public static LongObjectInspector asLongOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentException {
if (!BIGINT_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (LongObjectInspector) argOI;
}
import hivemall.UDTFWithOptions;
import hivemall.utils.lang.Primitives;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.Options;
@Description(name = "bpr_sampling",
value = "_FUNC_(array<int|long> pos_items, const int max_item_id [, const string options])"
public final class BprSamplingUDTF extends UDTFWithOptions {
private boolean bitsetInput;
private boolean withReplacement;
private BitSet _bitset;
private Random _rand;
protected Options getOptions() {
Options opts = new Options();
opts.addOption("bitset", "bitset_input", true,
"Use Bitset for the input of pos_items [default:false]");
opts.addOption("sampling", "sampling_rate", true,
"Sampling rates of positive items [default: 1.0]");
opts.addOption("with_replacement", false, "Do sampling with-replacement [default: false]");
return opts;
}
@Override
protected CommandLine processOptions(ObjectInspector[] argOIs) throws UDFArgumentException {
CommandLine cl = null;
boolean bitsetInput = false;
float samplingRate = 1.f;
boolean withReplacement = false;
if (argOIs.length == 3) {
String args = HiveUtils.getConstString(argOIs[2]);
cl = parseOptions(args);
bitsetInput = cl.hasOption("bitset_input");
withReplacement = cl.hasOption("with_replacement");
samplingRate = Primitives.parseFloat(cl.getOptionValue("sampling_rate"), samplingRate);
if (withReplacement == false && samplingRate > 1.f) {
throw new UDFArgumentException(
"sampling_rate MUST be in less than or equals to 1 where withReplacement is false: "
samplingRate);
}
}
this.bitsetInput = bitsetInput;
this.samplingRate = samplingRate;
this.withReplacement = withReplacement;
return cl;
}
@Override
if (argOIs.length != 2 && argOIs.length != 3) {
throw new UDFArgumentException(
"bpr_sampling(array<long>, const long max_item_id [, const string options])"
" takes at least two arguments");
final int numPosItems;
final BitSet bs;
if (bitsetInput) {
if (_rand == null) {
this._rand = new Random(43);
}
long[] longs = HiveUtils.asLongArray(args[0], listOI, listElemOI);
bs = BitSet.valueOf(longs);
numPosItems = bs.cardinality();
if (_bitset == null) {
bs = new BitSet();
this._bitset = bs;
this._rand = new Random(43);
} else {
bs = _bitset;
bs.clear();
}
numPosItems = HiveUtils.setBits(args[0], listOI, listElemOI, bs);
if (withReplacement) {
sampleWithReplacement(numPosItems, numNegItems, bs);
sampleWithoutReplacement(numPosItems, numNegItems, bs);
}
}
private void sampleWithReplacement(final int numPosItems, final int numNegItems,
@Nonnull final BitSet bitset) throws HiveException {
final int numSamples = Math.max(1, Math.round(numPosItems * samplingRate));
int nth = _rand.nextInt(numPosItems);
int i = bitset.nextSetBit(0);
if (c == nth) {
break;
}
}
if (i == -1) {
}
nth = _rand.nextInt(numNegItems);
int j = bitset.nextClearBit(0);
if (c == nth) {
break;
}
}
if (j < 0 || j > maxItemId) {
}
posItemId.set(i);
negItemId.set(j);
forward(forwardObjs);
int nth = _rand.nextInt(numPosItems);
nth = _rand.nextInt(numNegItems);
this._bitset = null;
this._rand = null;
package hivemall.ftvec.ranking;
opts.addOption("bitset", "bitset_input", false,
processOptions(argOIs);
final BitSet bits;
bits = BitSet.valueOf(longs);
numPosItems = bits.cardinality();
bits = new BitSet();
this._bitset = bits;
bits = _bitset;
bits.clear();
numPosItems = HiveUtils.setBits(args[0], listOI, listElemOI, bits);
sampleWithReplacement(numPosItems, numNegItems, bits);
sampleWithoutReplacement(numPosItems, numNegItems, bits);
numPosItems);
final BitSet bitsetForNegSampling = BitSet.valueOf(bitset.toLongArray());
numPosItems);
opts.addOption("bitset", "bitset_input", false,
processOptions(argOIs);
Object arg0 = args[0];
if (arg0 == null || listOI.getListLength(arg0) == 0) {
populateAll();
}
long[] longs = HiveUtils.asLongArray(arg0, listOI, listElemOI);
HiveUtils.setBits(arg0, listOI, listElemOI, bits);
private void populateAll() throws HiveException {
populatedItemId.set(i);
forward(forwardObjs);
}
}
return;
import javax.annotation.Nonnull;
@Description(
name = "each_top_k",
value = "_FUNC_(int K, Object group, double cmpKey, *) - Returns top-K values (or tail-K values when k is less than 0)")
private ObjectInspector[] argOIs;
private PrimitiveObjectInspector kOI;
private ObjectInspector prevGroupOI;
private PrimitiveObjectInspector cmpKeyOI;
private boolean constantK;
private int prevK;
if (numArgs < 4) {
throw new UDFArgumentException(
"each_top_k(int K, Object group, double cmpKey, *) takes at least 4 arguments: "
numArgs);
this.constantK = ObjectInspectorUtils.isConstantObjectInspector(argOIs[0]);
if (constantK) {
final int k = HiveUtils.getAsConstInt(argOIs[0]);
if (k == 0) {
throw new UDFArgumentException("k should not be 0");
}
this.queue = getQueue(k);
} else {
this.kOI = HiveUtils.asIntCompatibleOI(argOIs[0]);
this.prevK = 0;
this.prevGroupOI = ObjectInspectorUtils.getStandardObjectInspector(argOIs[1],
ObjectInspectorCopyOption.DEFAULT);
this._tuple = null;
this._previousGroup = null;
final ArrayList<String> fieldNames = new ArrayList<String>(numArgs);
final ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(numArgs);
fieldNames.add("rank");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
fieldNames.add("key");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
ObjectInspector rawOI = argOIs[i];
ObjectInspector retOI = ObjectInspectorUtils.getStandardObjectInspector(rawOI,
ObjectInspectorCopyOption.DEFAULT);
fieldOIs.add(retOI);
}
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
}
@Nonnull
private static BoundedPriorityQueue<TupleWithKey> getQueue(final int k) {
int sizeK = Math.abs(k);
if (k < 0) {
return new BoundedPriorityQueue<TupleWithKey>(sizeK, comparator);
if (constantK == false) {
final int k = PrimitiveObjectInspectorUtils.getInt(args[0], kOI);
if (k == 0) {
return;
}
if (k != prevK) {
this.queue = getQueue(k);
this.prevK = k;
}
}
assert (queue != null);
if (isSameGroup(arg1) == false) {
Object group = ObjectInspectorUtils.copyToStandardObject(arg1, argOIs[1],
if (_tuple == null) {
row[i - 1] = ObjectInspectorUtils.copyToStandardObject(arg, argOI,
ObjectInspectorCopyOption.DEFAULT);
if (queue.offer(tuple)) {
if (arg1 == null && _previousGroup == null) {
} else if (arg1 == null || _previousGroup == null) {
if (queueSize > 0) {
if (tuple == null) {
for (int i = queueSize - 1; i >= 0; i--) {
if (key != lastKey) {
import hivemall.utils.hadoop.HiveUtils;
@Description(name = "to_map",
value = "_FUNC_(key, value) - Convert two aggregated columns into a key-value map")
public GenericUDAFEvaluator getEvaluator(TypeInfo[] typeInfo) throws SemanticException {
if (typeInfo.length != 2) {
throw new UDFArgumentTypeException(typeInfo.length - 1,
if (typeInfo[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0,
"Only primitive type arguments are accepted for the key but "
public ObjectInspector init(Mode mode, ObjectInspector[] argOIs) throws HiveException {
super.init(mode, argOIs);
inputKeyOI = HiveUtils.asPrimitiveObjectInspector(argOIs[0]);
inputValueOI = argOIs[1];
internalMergeOI = (StandardMapObjectInspector) argOIs[0];
inputKeyOI = HiveUtils.asPrimitiveObjectInspector(internalMergeOI.getMapKeyObjectInspector());
inputValueOI = internalMergeOI.getMapValueObjectInspector();
return ObjectInspectorFactory.getStandardMapObjectInspector(
ObjectInspectorUtils.getStandardObjectInspector(inputKeyOI),
ObjectInspectorUtils.getStandardObjectInspector(inputValueOI));
static class MapAggregationBuffer implements AggregationBuffer {
@Override
((MapAggregationBuffer) agg).container = new HashMap<Object, Object>(64);
@Override
MapAggregationBuffer ret = new MapAggregationBuffer();
@Override
if (key != null) {
MapAggregationBuffer myagg = (MapAggregationBuffer) agg;
@Override
public Map<Object, Object> terminatePartial(AggregationBuffer agg) throws HiveException {
MapAggregationBuffer myagg = (MapAggregationBuffer) agg;
@Override
MapAggregationBuffer myagg = (MapAggregationBuffer) agg;
Map<?, ?> partialResult = internalMergeOI.getMap(partial);
for (Map.Entry<?, ?> entry : partialResult.entrySet()) {
@Override
public Map<Object, Object> terminate(AggregationBuffer agg) throws HiveException {
MapAggregationBuffer myagg = (MapAggregationBuffer) agg;
protected void putIntoMap(Object key, Object value, MapAggregationBuffer myagg) {
import hivemall.utils.hadoop.HiveUtils;
import java.util.Collections;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFParameterInfo;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@Description(name = "to_ordered_map",
value = "_FUNC_(key, value [, const boolean reverseOrder=false]) "
"- Convert two aggregated columns into an ordered key-value map")
public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info)
throws SemanticException {
@SuppressWarnings("deprecation")
TypeInfo[] typeInfo = info.getParameters();
if (typeInfo.length != 2 && typeInfo.length != 3) {
throw new UDFArgumentTypeException(typeInfo.length - 1,
}
if (typeInfo[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0,
"Only primitive type arguments are accepted for the key but "
}
boolean reverseOrder = false;
if (typeInfo.length == 3) {
if (HiveUtils.isBooleanTypeInfo(typeInfo[2]) == false) {
throw new UDFArgumentTypeException(2, "The three argument must be boolean type: "
typeInfo[2].getTypeName());
}
ObjectInspector[] argOIs = info.getParameterObjectInspectors();
reverseOrder = HiveUtils.getConstBoolean(argOIs[2]);
}
if (reverseOrder) {
return new ReverseOrdereMapEvaluator();
} else {
return new NaturalOrdereMapEvaluator();
}
public static class NaturalOrdereMapEvaluator extends UDAFToMapEvaluator {
((MapAggregationBuffer) agg).container = new TreeMap<Object, Object>();
}
}
public static class ReverseOrdereMapEvaluator extends UDAFToMapEvaluator {
@Override
public void reset(AggregationBuffer agg) throws HiveException {
((MapAggregationBuffer) agg).container = new TreeMap<Object, Object>(
Collections.reverseOrder());
import hivemall.utils.lang.NumberUtils;
float newEta = eta * multipler;
if (!NumberUtils.isFinite(newEta)) {
return;
}
if (!NumberUtils.isFinite(ret)) {
}
if (!NumberUtils.isFinite(newWeight)) {
}
rating.setWeight(newWeight);
if (!NumberUtils.isFinite(newWeight)) {
}
rating.setWeight(newWeight);
if (!NumberUtils.isFinite(Bi)) {
}
if (!NumberUtils.isFinite(Bj)) {
}
private boolean _constantK;
private int _prevK;
private BoundedPriorityQueue<TupleWithKey> _queue;
this._constantK = ObjectInspectorUtils.isConstantObjectInspector(argOIs[0]);
if (_constantK) {
this._queue = getQueue(k);
this._prevK = 0;
if (_queue != null) {
drainQueue();
}
if (_constantK == false) {
final int k = PrimitiveObjectInspectorUtils.getInt(args[0], kOI);
if (k == 0) {
return;
}
if (k != _prevK) {
this._queue = getQueue(k);
this._prevK = k;
}
}
if (_queue.offer(tuple)) {
final int queueSize = _queue.size();
TupleWithKey tuple = _queue.poll();
_queue.clear();
this._queue = null;
@Description(name = "bits_collect",
public final class BitsCollectUDAF extends AbstractGenericUDAFResolver {
ListObjectInspector arg0ListOI = HiveUtils.asListOI(argOIs[0]);
public final class BitsORUDF extends GenericUDF {
public BitsORUDF() {}
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
return ObjectInspectorFactory.getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
import java.util.ArrayList;
final List<LongWritable> list = new ArrayList<LongWritable>(size);
list.add(new LongWritable(0L));
}
return list;
final List<DoubleWritable> list = new ArrayList<DoubleWritable>(size);
list.add(new DoubleWritable(defaultValue));
}
return list;
final List<LongWritable> list = new ArrayList<LongWritable>(src.length);
list.add(new LongWritable(src[i]));
}
return list;
final List<DoubleWritable> list = new ArrayList<DoubleWritable>(src.length);
list.add(new DoubleWritable(src[i]));
}
return list;
final List<Text> list = new ArrayList<Text>(v.length);
String vi = v[i];
Text ti = (vi == null) ? null : new Text(vi);
list.add(ti);
}
return list;
if (object == null) {
if (object instanceof Writable) {
if (object instanceof String) {
if (object instanceof Long) {
if (object instanceof Integer) {
if (object instanceof Byte) {
if (object instanceof Double) {
if (object instanceof Float) {
if (object instanceof Boolean) {
if (object instanceof byte[]) {
@Description(name = "array_concat", value = "_FUNC_(x1, x2, ..) - Returns a concatenated array")
@Description(name = "array_avg", value = "_FUNC_(array<number>) - Returns an array<double>"
" in which each element is the mean of a set of numbers")
if (typeInfo.length != 1) {
throw new UDFArgumentTypeException(typeInfo.length - 1,
"One argument is expected, taking an array as an argument");
if (!typeInfo[0].getCategory().equals(Category.LIST)) {
throw new UDFArgumentTypeException(typeInfo.length - 1,
"One argument is expected, taking an array as an argument");
if (tuple != null) {
if (myAggr._size == -1) {
if (partial != null) {
if (sum instanceof LazyBinaryArray) {
if (count instanceof LazyBinaryArray) {
if (size == -1) {
void doIterate(@Nonnull final Object tuple, @Nonnull ListObjectInspector listOI,
@Nonnull PrimitiveObjectInspector elemOI) throws HiveException {
if (_size == -1) {
if (o != null) {
void merge(final int o_size, @Nonnull final Object o_sum, @Nonnull final Object o_count,
@Nonnull final StandardListObjectInspector sumOI,
@Nonnull final StandardListObjectInspector countOI) throws HiveException {
if (o_size != _size) {
if (_size == -1) {
@Description(name = "array_concat",
value = "_FUNC_(array<ANY> x1, array<ANY> x2, ..) - Returns a concatenated array")
if (arguments.length < 1) {
throw new UDFArgumentLengthException(
"_FUNC_(array1, array2) needs at least 1 argument.");
if (((ListObjectInspector) (arguments[i])).getListElementObjectInspector()
.getCategory()
.equals(Category.PRIMITIVE)) {
if (arrayObject == null) {
value = "_FUNC_(array<ANY> x1, array<ANY> x2, ..) - Returns an intersect of given arrays")
@Description(name = "array_remove",
value = "_FUNC_(array<int|text> original, int|text|array<int> target)"
while (original.remove(target));
while (original.remove(target));
@Description(name = "array_sum", value = "_FUNC_(array<number>) - Returns an array<double>"
if (tuple == null) {
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
if (v != null) {
if (other._size != _size) {
@Description(name = "sort_and_uniq_array", value = "_FUNC_(array<int>) - Takes array<int> and "
"returns a sorted array with duplicate elements eliminated")
@Description(name = "subarray_endwith", value = "_FUNC_(array<int|text> original, int|text key)"
" - Returns an array that ends with the specified key")
if (original == null) {
if (toIndex == -1) {
if (original == null) {
if (toIndex == -1) {
@Description(name = "subarray_startwith", value = "_FUNC_(array<int|text> original, int|text key)"
" - Returns an array that starts with the specified key")
if (original == null) {
if (fromIndex == -1) {
if (original == null) {
if (fromIndex == -1) {
@Description(name = "subarray", value = "_FUNC_(array<int> orignal, int fromIndex, int toIndex)"
" - Returns a slice of the original array"
" between the inclusive fromIndex and the exclusive toIndex")
if (array == null) {
if (fromIndex < 0) {
if (toIndex > arraylength) {
value = "_FUNC_(int|long x) - Returns a bitset in array<long>")
value = "_FUNC_(array<long> b1, array<long> b2, ..) - Returns a logical OR given bitsets")
value = "_FUNC_(long[] bitset) - Returns an long array of the give bitset representation")
@Description(name = "map_get_sum", value = "_FUNC_(map<int,float> src, array<int> keys)"
" - Returns sum of values that are retrieved by keys")
for (IntWritable k : keys) {
if (v != null) {
@Description(name = "jobid", value = "_FUNC_() - Returns the value of mapred.job.id")
value = "_FUNC_(BINARY bin) - Convert the argument from binary to a BASE91 string")
value = "_FUNC_(string str [, string form]) - Transforms `str` with the specified normalization form. "
@Description(name = "split_words", value = "_FUNC_(string query [, string regex]) - Returns an array<text> containing splitted strings")
@Description(name = "is_stopword", value = "_FUNC_(string word) - Returns whether English stopword or not")
@Description(name = "tokenize", value = "_FUNC_(string englishText [, boolean toLowerCase])"
" - Returns tokenized words in array<string>")
while (tokenizer.hasMoreElements()) {
if (toLowerCase) {
public double getPreviousLoss() {
return prevLosses;
}
if (conversionCheck == false) {
if (currLosses > prevLosses) {
if (logger.isInfoEnabled()) {
if (changeRate < convergenceRate) {
if (readyToFinishIterations) {
if (logger.isDebugEnabled()) {
public void logState(int iter, float eta) {
if (logger.isInfoEnabled()) {
}
}
this.regBias = 0.01f;
opts.addOption("reg_bias", true,
"The regularization factor for bias clause [default: 0.01]");
cvState.logState(iter, eta());
if (isLossIncreased == false && cvState.isConverged(iter, numTrainingExamples)) {
cvState.logState(iter, eta());
if (isLossIncreased == false && cvState.isConverged(iter, numTrainingExamples)) {
import java.io.BufferedReader;
import java.io.InputStreamReader;
@Nonnull
public static BufferedReader bufferedReader(@Nonnull InputStream is) {
InputStreamReader in = new InputStreamReader(is);
return new BufferedReader(in);
}
import java.util.ArrayList;
if (str == null || str.length() == 0) {
if (i == sz) {
if ((chars[i] < '0' || chars[i] > '9') && (chars[i] < 'a' || chars[i] > 'f')
if (chars[i] >= '0' && chars[i] <= '9') {
} else if (chars[i] == '.') {
if (hasDecPoint || hasExp) {
} else if (chars[i] == 'e' || chars[i] == 'E') {
if (hasExp) {
if (!foundDigit) {
if (!allowSigns) {
if (i < chars.length) {
if (chars[i] >= '0' && chars[i] <= '9') {
if (chars[i] == 'e' || chars[i] == 'E') {
if (!allowSigns
if (chars[i] == 'l' || chars[i] == 'L') {
for (String s : list) {
if (s == null) {
public static String[] split(final String str, final char separatorChar) {
return split(str, separatorChar, false);
}
public static String[] split(final String str, final char separatorChar,
final boolean preserveAllTokens) {
if (str == null) {
return null;
}
final int len = str.length();
if (len == 0) {
return new String[0];
}
final List<String> list = new ArrayList<String>();
int i = 0, start = 0;
boolean match = false;
boolean lastMatch = false;
while (i < len) {
if (str.charAt(i) == separatorChar) {
if (match || preserveAllTokens) {
list.add(str.substring(start, i));
match = false;
lastMatch = true;
}
continue;
}
lastMatch = false;
match = true;
i;
}
if (match || preserveAllTokens && lastMatch) {
list.add(str.substring(start, i));
}
return list.toArray(new String[list.size()]);
}
this.prevLosses = currLosses;
this.currLosses = 0.d;
if (cvState.isConverged(iter, numTrainingExamples)) {
if (cvState.isLossIncreased()) {
if (cvState.isConverged(iter, numTrainingExamples)) {
if (cvState.isLossIncreased()) {
protected int curIter;
protected float curEta;
this.curIter = 0;
this.curEta = Float.NaN;
this.curIter = iter;
this.curEta = eta;
}
public int getCurrentIteration() {
return curIter;
}
public float getCurrentEta() {
return curEta;
import hivemall.utils.lang.Primitives;
public void update(@Nonnegative float multipler) {}
@Override
public float eta(long t) {
return eta;
}
@Override
if (cl.hasOption("boldDriver")) {
float eta0 = Primitives.parseFloat(cl.getOptionValue("eta0"), 0.3f);
return new AdjustingEtaEstimator(eta0);
}
import hivemall.common.EtaEstimator;
protected EtaEstimator etaEstimator;
opts.addOption("eta", true, "The initial learning rate [default: 0.001]");
opts.addOption("eta0", true, "The initial learning rate [default 0.3]");
opts.addOption("t", "total_steps", true, "The total number of training examples");
opts.addOption("power_t", true,
"The exponent for inverse scaling learning rate [default 0.1]");
opts.addOption("boldDriver", "bold_driver", false,
"Whether to use Bold Driver for learning rate [default: false]");
this.etaEstimator = EtaEstimator.get(cl);
int iter = 2;
int iter = 2;
@Description(name = "item_pairs_sampling",
public final class ItemPairsSamplingUDTF extends UDTFWithOptions {
public ItemPairsSamplingUDTF() {}
"_FUNC_(array<long>, const long max_item_id [, const string options])"
public void add(final int value) {
if (used >= data.length) {
public void add(final int[] values) {
if (needs >= data.length) {
private void expand(final int max) {
while (data.length < max) {
public int remove(final int index) {
if (index > used) {
} else if (index == used) {
public void set(final int index, final int value) {
if (index > used) {
used);
} else if (index == used) {
public int get(final int index) {
if (index >= used) {
}
public int fastGet(final int index) {
public int indexOf(final int key) {
public boolean contains(final int key) {
return ArrayUtils.indexOf(data, key, 0, used) != -1;
}
if (i != 0) {
public static PrimitiveObjectInspector asIntegerOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if (argOI.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0, "Only primitive type arguments are accepted but "
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case INT:
case SHORT:
case LONG:
case BYTE:
break;
default:
"' is passed.");
}
return oi;
}
import hivemall.utils.lang.BitUtils;
import java.util.BitSet;
private boolean withoutReplacement;
opts.addOption("without_replacement", false,
"Do sampling without-replacement sampling [default: false]");
opts.addOption("maxcol", "max_itemid", true, "Max item id index [default: -1]");
int maxItemId = -1;
boolean withoutReplacement = false;
maxItemId = Primitives.parseInt(cl.getOptionValue("max_itemid"), maxItemId);
withoutReplacement = cl.hasOption("without_replacement");
if (withoutReplacement && samplingRate > 1.f) {
throw new UDFArgumentException("sampling_rate MUST be in less than or equals to 1"
this.feedback = pairSampling ? new PerEventPositiveOnlyFeedback(maxItemId)
: new PositiveOnlyFeedback(maxItemId);
this.withoutReplacement = withoutReplacement;
if (withoutReplacement) {
} else {
uniformPairSamplingWithReplacement(evFeedback, numSamples);
if (withoutReplacement) {
} else {
uniformUserSamplingWithReplacement(feedback, numSamples);
if (maxItemId <= 0) {
}
final int[] users = feedback.getUsers();
assert (users.length == numUsers);
final Random rand = new Random(31L);
int user = users[rand.nextInt(numUsers)];
--i;
continue;
}
int numUsers = feedback.getNumUsers();
if (numUsers == 0) {
return;
}
if (maxItemId <= 0) {
}
final BitSet userBits = new BitSet(numUsers);
feedback.getUsers(userBits);
final Random rand = new Random(31L);
int nthUser = rand.nextInt(numUsers);
int user = BitUtils.indexOfSetBit(userBits, nthUser);
if (user == -1) {
" users");
--i;
continue;
}
userBits.clear(user);
--numUsers;
if (maxItemId <= 0) {
}
final Random rand = new Random(31L);
if (maxItemId <= 0) {
}
final Random rand = new Random(31L);
import hivemall.utils.lang.BitUtils;
int i = BitUtils.indexOfSetBit(bitset, nth);
int j = BitUtils.indexOfClearBit(bitset, nth, maxItemId);
int i = BitUtils.indexOfSetBit(bitsetForPosSampling, nth);
int j = BitUtils.indexOfClearBit(bitsetForNegSampling, nth, maxItemId);
public PerEventPositiveOnlyFeedback(int maxItemId) {
super(maxItemId);
import hivemall.utils.collections.IntOpenHashMap.IMapIterator;
import java.util.BitSet;
import javax.annotation.Nullable;
public int[] getUsers() {
final int size = rows.size();
final int[] keys = new int[size];
final IMapIterator<IntArrayList> itor = rows.entries();
if (itor.next() == -1) {
throw new IllegalStateException();
}
int key = itor.getKey();
keys[i] = key;
}
return keys;
}
public void getUsers(@Nonnull final BitSet bitset) {
final IMapIterator<IntArrayList> itor = rows.entries();
while (itor.next() != -1) {
int key = itor.getKey();
bitset.set(key);
}
}
@Nullable
if (itemIds.isEmpty()) {
import javax.annotation.Nonnull;
if (s.charAt(i) == '1') {
public static int indexOfSetBit(@Nonnull final BitSet bits, final int nth) {
if (nth < 0) {
}
int pos = bits.nextSetBit(0);
if (i == nth) {
break;
}
}
return pos;
}
public static int indexOfClearBit(@Nonnull final BitSet bits, final int nth, final int lastIndex) {
int j = bits.nextClearBit(0);
if (c == nth) {
break;
}
}
return j;
}
int posItemIndex = rand.nextInt(size);
int posItem = posItems.fastGet(posItemIndex);
int posItemIndex = rand.nextInt(size);
int posItem = posItems.fastGet(posItemIndex);
posItems.remove(posItemIndex);
protected int curIter;
protected float curEta;
this.curIter = 0;
this.curEta = Float.NaN;
public double getPreviousLoss() {
return prevLosses;
}
public boolean isLossIncreased() {
return currLosses > prevLosses;
}
if (conversionCheck == false) {
this.prevLosses = currLosses;
this.currLosses = 0.d;
if (currLosses > prevLosses) {
if (logger.isInfoEnabled()) {
if (changeRate < convergenceRate) {
if (readyToFinishIterations) {
if (logger.isDebugEnabled()) {
public void logState(int iter, float eta) {
if (logger.isInfoEnabled()) {
}
this.curIter = iter;
this.curEta = eta;
}
public int getCurrentIteration() {
return curIter;
}
public float getCurrentEta() {
return curEta;
}
import hivemall.utils.lang.NumberUtils;
import hivemall.utils.lang.Primitives;
import javax.annotation.Nonnegative;
public void update(@Nonnegative float multipler) {}
if (t > total_steps) {
public static final class AdjustingEtaEstimator extends EtaEstimator {
private final float eta0;
private float eta;
public AdjustingEtaEstimator(float eta) {
this.eta0 = eta;
this.eta = eta;
}
@Override
public float eta(long t) {
return eta;
}
@Override
public void update(@Nonnegative float multipler) {
float newEta = eta * multipler;
if (!NumberUtils.isFinite(newEta)) {
return;
}
}
}
if (cl == null) {
if (cl.hasOption("boldDriver")) {
float eta0 = Primitives.parseFloat(cl.getOptionValue("eta0"), 0.3f);
return new AdjustingEtaEstimator(eta0);
}
if (etaValue != null) {
if (cl.hasOption("t")) {
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor,
@Nonnull RankInitScheme initScheme) {
this(ratingInitializer, factor, 0.f, initScheme, 136861);
}
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor,
float meanRating, @Nonnull RankInitScheme initScheme) {
public FactorizedModel(@Nonnull RatingInitilizer ratingInitializer, @Nonnegative int factor,
float meanRating, @Nonnull RankInitScheme initScheme, int expectedSize) {
if (opt == null) {
} else if ("gaussian".equalsIgnoreCase(opt)) {
} else if ("random".equalsIgnoreCase(opt)) {
if (init && v == null) {
switch (initScheme) {
if (init && v == null) {
switch (initScheme) {
if (b == null) {
if (b == null) {
if (b == null) {
if (b == null) {
if (b == null) {
if (b == null) {
private static void uniformFill(final Rating[] a, final Random rand, final float maxInitValue,
final RatingInitilizer init) {
private static void gaussianFill(final Rating[] a, final Random[] rand, final double stddev,
final RatingInitilizer init) {
ArrayList<String> fieldNames = new ArrayList<String>();
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
fieldNames.add("idx");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
fieldNames.add("Pu");
fieldOIs.add(ObjectInspectorFactory.getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableFloatObjectInspector));
fieldNames.add("Qi");
fieldOIs.add(ObjectInspectorFactory.getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableFloatObjectInspector));
if (useBiasClause) {
fieldNames.add("Bu");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
fieldNames.add("Bi");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
if (updateMeanRating) {
fieldNames.add("mu");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);
}
}
import javax.annotation.Nonnull;
@Description(
name = "each_top_k",
value = "_FUNC_(int K, Object group, double cmpKey, *) - Returns top-K values (or tail-K values when k is less than 0)")
private ObjectInspector[] argOIs;
private PrimitiveObjectInspector kOI;
private ObjectInspector prevGroupOI;
private PrimitiveObjectInspector cmpKeyOI;
private boolean _constantK;
private int _prevK;
private BoundedPriorityQueue<TupleWithKey> _queue;
if (numArgs < 4) {
throw new UDFArgumentException(
"each_top_k(int K, Object group, double cmpKey, *) takes at least 4 arguments: "
numArgs);
this._constantK = ObjectInspectorUtils.isConstantObjectInspector(argOIs[0]);
if (_constantK) {
final int k = HiveUtils.getAsConstInt(argOIs[0]);
if (k == 0) {
throw new UDFArgumentException("k should not be 0");
}
this._queue = getQueue(k);
} else {
this.kOI = HiveUtils.asIntCompatibleOI(argOIs[0]);
this._prevK = 0;
this.prevGroupOI = ObjectInspectorUtils.getStandardObjectInspector(argOIs[1],
ObjectInspectorCopyOption.DEFAULT);
this._tuple = null;
this._previousGroup = null;
final ArrayList<String> fieldNames = new ArrayList<String>(numArgs);
final ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(numArgs);
fieldNames.add("rank");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableIntObjectInspector);
fieldNames.add("key");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
ObjectInspector rawOI = argOIs[i];
ObjectInspector retOI = ObjectInspectorUtils.getStandardObjectInspector(rawOI,
ObjectInspectorCopyOption.DEFAULT);
fieldOIs.add(retOI);
}
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
}
@Nonnull
private static BoundedPriorityQueue<TupleWithKey> getQueue(final int k) {
int sizeK = Math.abs(k);
if (k < 0) {
return new BoundedPriorityQueue<TupleWithKey>(sizeK, comparator);
if (isSameGroup(arg1) == false) {
Object group = ObjectInspectorUtils.copyToStandardObject(arg1, argOIs[1],
if (_queue != null) {
drainQueue();
}
if (_constantK == false) {
final int k = PrimitiveObjectInspectorUtils.getInt(args[0], kOI);
if (k == 0) {
return;
}
if (k != _prevK) {
this._queue = getQueue(k);
this._prevK = k;
}
}
if (_tuple == null) {
row[i - 1] = ObjectInspectorUtils.copyToStandardObject(arg, argOI,
ObjectInspectorCopyOption.DEFAULT);
if (_queue.offer(tuple)) {
if (arg1 == null && _previousGroup == null) {
} else if (arg1 == null || _previousGroup == null) {
final int queueSize = _queue.size();
if (queueSize > 0) {
TupleWithKey tuple = _queue.poll();
if (tuple == null) {
for (int i = queueSize - 1; i >= 0; i--) {
if (key != lastKey) {
_queue.clear();
this._queue = null;
@Description(name = "array_avg", value = "_FUNC_(array<number>) - Returns an array<double>"
" in which each element is the mean of a set of numbers")
if (typeInfo.length != 1) {
throw new UDFArgumentTypeException(typeInfo.length - 1,
"One argument is expected, taking an array as an argument");
if (!typeInfo[0].getCategory().equals(Category.LIST)) {
throw new UDFArgumentTypeException(typeInfo.length - 1,
"One argument is expected, taking an array as an argument");
if (tuple != null) {
if (myAggr._size == -1) {
if (partial != null) {
if (sum instanceof LazyBinaryArray) {
if (count instanceof LazyBinaryArray) {
if (size == -1) {
void doIterate(@Nonnull final Object tuple, @Nonnull ListObjectInspector listOI,
@Nonnull PrimitiveObjectInspector elemOI) throws HiveException {
if (_size == -1) {
if (o != null) {
void merge(final int o_size, @Nonnull final Object o_sum, @Nonnull final Object o_count,
@Nonnull final StandardListObjectInspector sumOI,
@Nonnull final StandardListObjectInspector countOI) throws HiveException {
if (o_size != _size) {
if (_size == -1) {
@Description(name = "array_concat",
value = "_FUNC_(array<ANY> x1, array<ANY> x2, ..) - Returns a concatenated array")
public class ArrayConcatUDF extends GenericUDF {
if (arguments.length < 1) {
throw new UDFArgumentLengthException(
"_FUNC_(array1, array2) needs at least 1 argument.");
if (((ListObjectInspector) (arguments[i])).getListElementObjectInspector()
.getCategory()
.equals(Category.PRIMITIVE)) {
if (arrayObject == null) {
@Description(name = "array_remove",
value = "_FUNC_(array<int|text> original, int|text|array<int> target)"
while (original.remove(target));
while (original.remove(target));
@Description(name = "array_sum", value = "_FUNC_(array<number>) - Returns an array<double>"
if (tuple == null) {
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
if (v != null) {
if (other._size != _size) {
@Deprecated
@Description(name = "sort_and_uniq_array", value = "_FUNC_(array<int>) - Takes array<int> and "
"returns a sorted array with duplicate elements eliminated")
@Description(name = "subarray_endwith", value = "_FUNC_(array<int|text> original, int|text key)"
" - Returns an array that ends with the specified key")
if (original == null) {
if (toIndex == -1) {
if (original == null) {
if (toIndex == -1) {
@Description(name = "subarray_startwith", value = "_FUNC_(array<int|text> original, int|text key)"
" - Returns an array that starts with the specified key")
if (original == null) {
if (fromIndex == -1) {
if (original == null) {
if (fromIndex == -1) {
@Description(name = "subarray", value = "_FUNC_(array<int> orignal, int fromIndex, int toIndex)"
" - Returns a slice of the original array"
" between the inclusive fromIndex and the exclusive toIndex")
if (array == null) {
if (fromIndex < 0) {
if (toIndex > arraylength) {
@Description(name = "map_get_sum", value = "_FUNC_(map<int,float> src, array<int> keys)"
" - Returns sum of values that are retrieved by keys")
for (IntWritable k : keys) {
if (v != null) {
import hivemall.utils.hadoop.HiveUtils;
@Description(name = "to_map",
value = "_FUNC_(key, value) - Convert two aggregated columns into a key-value map")
public GenericUDAFEvaluator getEvaluator(TypeInfo[] typeInfo) throws SemanticException {
if (typeInfo.length != 2) {
throw new UDFArgumentTypeException(typeInfo.length - 1,
if (typeInfo[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0,
"Only primitive type arguments are accepted for the key but "
public ObjectInspector init(Mode mode, ObjectInspector[] argOIs) throws HiveException {
super.init(mode, argOIs);
inputKeyOI = HiveUtils.asPrimitiveObjectInspector(argOIs[0]);
inputValueOI = argOIs[1];
internalMergeOI = (StandardMapObjectInspector) argOIs[0];
inputKeyOI = HiveUtils.asPrimitiveObjectInspector(internalMergeOI.getMapKeyObjectInspector());
inputValueOI = internalMergeOI.getMapValueObjectInspector();
return ObjectInspectorFactory.getStandardMapObjectInspector(
ObjectInspectorUtils.getStandardObjectInspector(inputKeyOI),
ObjectInspectorUtils.getStandardObjectInspector(inputValueOI));
static class MapAggregationBuffer implements AggregationBuffer {
@Override
((MapAggregationBuffer) agg).container = new HashMap<Object, Object>(64);
@Override
MapAggregationBuffer ret = new MapAggregationBuffer();
@Override
if (key != null) {
MapAggregationBuffer myagg = (MapAggregationBuffer) agg;
@Override
public Map<Object, Object> terminatePartial(AggregationBuffer agg) throws HiveException {
MapAggregationBuffer myagg = (MapAggregationBuffer) agg;
@Override
MapAggregationBuffer myagg = (MapAggregationBuffer) agg;
Map<?, ?> partialResult = internalMergeOI.getMap(partial);
for (Map.Entry<?, ?> entry : partialResult.entrySet()) {
@Override
public Map<Object, Object> terminate(AggregationBuffer agg) throws HiveException {
MapAggregationBuffer myagg = (MapAggregationBuffer) agg;
protected void putIntoMap(Object key, Object value, MapAggregationBuffer myagg) {
import hivemall.utils.hadoop.HiveUtils;
import java.util.Collections;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFParameterInfo;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@Description(name = "to_ordered_map",
value = "_FUNC_(key, value [, const boolean reverseOrder=false]) "
"- Convert two aggregated columns into an ordered key-value map")
public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info)
throws SemanticException {
@SuppressWarnings("deprecation")
TypeInfo[] typeInfo = info.getParameters();
if (typeInfo.length != 2 && typeInfo.length != 3) {
throw new UDFArgumentTypeException(typeInfo.length - 1,
}
if (typeInfo[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0,
"Only primitive type arguments are accepted for the key but "
}
boolean reverseOrder = false;
if (typeInfo.length == 3) {
if (HiveUtils.isBooleanTypeInfo(typeInfo[2]) == false) {
throw new UDFArgumentTypeException(2, "The three argument must be boolean type: "
typeInfo[2].getTypeName());
}
ObjectInspector[] argOIs = info.getParameterObjectInspectors();
reverseOrder = HiveUtils.getConstBoolean(argOIs[2]);
}
if (reverseOrder) {
return new ReverseOrdereMapEvaluator();
} else {
return new NaturalOrdereMapEvaluator();
}
public static class NaturalOrdereMapEvaluator extends UDAFToMapEvaluator {
((MapAggregationBuffer) agg).container = new TreeMap<Object, Object>();
}
}
public static class ReverseOrdereMapEvaluator extends UDAFToMapEvaluator {
@Override
public void reset(AggregationBuffer agg) throws HiveException {
((MapAggregationBuffer) agg).container = new TreeMap<Object, Object>(
Collections.reverseOrder());
@Description(name = "jobid", value = "_FUNC_() - Returns the value of mapred.job.id")
value = "_FUNC_(BINARY bin) - Convert the argument from binary to a BASE91 string")
value = "_FUNC_(string str [, string form]) - Transforms `str` with the specified normalization form. "
@Description(name = "split_words", value = "_FUNC_(string query [, string regex]) - Returns an array<text> containing splitted strings")
@Description(name = "is_stopword", value = "_FUNC_(string word) - Returns whether English stopword or not")
@Description(name = "tokenize", value = "_FUNC_(string englishText [, boolean toLowerCase])"
" - Returns tokenized words in array<string>")
while (tokenizer.hasMoreElements()) {
if (toLowerCase) {
public void add(final int value) {
if (used >= data.length) {
public void add(final int[] values) {
if (needs >= data.length) {
private void expand(final int max) {
while (data.length < max) {
public int remove(final int index) {
if (index > used) {
} else if (index == used) {
public void set(final int index, final int value) {
if (index > used) {
used);
} else if (index == used) {
public int get(final int index) {
if (index >= used) {
}
public int fastGet(final int index) {
public int indexOf(final int key) {
public boolean contains(final int key) {
return ArrayUtils.indexOf(data, key, 0, used) != -1;
}
if (i != 0) {
import java.util.BitSet;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
public static boolean isPrimitiveTypeInfo(@Nonnull TypeInfo typeInfo) {
return typeInfo.getCategory() == ObjectInspector.Category.PRIMITIVE;
}
public static boolean isBooleanTypeInfo(@Nonnull TypeInfo typeInfo) {
if (typeInfo.getCategory() != ObjectInspector.Category.PRIMITIVE) {
return false;
}
switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {
case BOOLEAN:
return true;
default:
return false;
}
}
public static boolean isIntegerTypeInfo(@Nonnull TypeInfo typeInfo) {
if (typeInfo.getCategory() != ObjectInspector.Category.PRIMITIVE) {
return false;
}
switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {
case BYTE:
case SHORT:
case INT:
case LONG:
return true;
default:
return false;
}
}
public static ListTypeInfo asListTypeInfo(@Nonnull TypeInfo typeInfo)
throws UDFArgumentException {
if (!typeInfo.getCategory().equals(Category.LIST)) {
}
return (ListTypeInfo) typeInfo;
}
public static float getAsConstFloat(@Nonnull final ObjectInspector numberOI)
throws UDFArgumentException {
final String typeName = numberOI.getTypeName();
if (FLOAT_TYPE_NAME.equals(typeName)) {
FloatWritable v = getConstValue(numberOI);
return v.get();
} else if (DOUBLE_TYPE_NAME.equals(typeName)) {
DoubleWritable v = getConstValue(numberOI);
return (float) v.get();
} else if (INT_TYPE_NAME.equals(typeName)) {
IntWritable v = getConstValue(numberOI);
return v.get();
} else if (BIGINT_TYPE_NAME.equals(typeName)) {
LongWritable v = getConstValue(numberOI);
return v.get();
} else if (SMALLINT_TYPE_NAME.equals(typeName)) {
ShortWritable v = getConstValue(numberOI);
return v.get();
} else if (TINYINT_TYPE_NAME.equals(typeName)) {
ByteWritable v = getConstValue(numberOI);
return v.get();
}
throw new UDFArgumentException("Unexpected argument type to cast as double: "
TypeInfoUtils.getTypeInfoFromObjectInspector(numberOI));
}
public static long[] asLongArray(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI, @Nonnull PrimitiveObjectInspector elemOI) {
if (argObj == null) {
return null;
}
final int length = listOI.getListLength(argObj);
final long[] ary = new long[length];
Object o = listOI.getListElement(argObj, i);
if (o == null) {
continue;
}
ary[i] = PrimitiveObjectInspectorUtils.getLong(o, elemOI);
}
return ary;
}
@Nonnull
public static long[] asLongArray(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI, @Nonnull LongObjectInspector elemOI) {
if (argObj == null) {
return null;
}
final int length = listOI.getListLength(argObj);
final long[] ary = new long[length];
Object o = listOI.getListElement(argObj, i);
if (o == null) {
continue;
}
ary[i] = elemOI.get(o);
}
return ary;
}
@Nonnull
public static double[] asDoubleArray(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI,
@Nonnull final PrimitiveObjectInspector elemOI) {
@Nonnull
public static int setBits(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI,
@Nonnull final PrimitiveObjectInspector elemOI, @Nonnull final BitSet bitset)
throws UDFArgumentException {
if (argObj == null) {
return 0;
}
int count = 0;
final int length = listOI.getListLength(argObj);
Object o = listOI.getListElement(argObj, i);
if (o == null) {
continue;
}
int index = PrimitiveObjectInspectorUtils.getInt(o, elemOI);
if (index < 0) {
}
bitset.set(index);
count;
}
return count;
}
public static LongObjectInspector asLongOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentException {
if (!BIGINT_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (LongObjectInspector) argOI;
}
public static PrimitiveObjectInspector asLongCompatibleOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if (argOI.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0, "Only primitive type arguments are accepted but "
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case LONG:
case INT:
case SHORT:
case BYTE:
case BOOLEAN:
case FLOAT:
case DOUBLE:
case STRING:
case TIMESTAMP:
case DECIMAL:
break;
default:
"' is passed.");
}
return oi;
}
public static PrimitiveObjectInspector asIntegerOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentTypeException {
if (argOI.getCategory() != Category.PRIMITIVE) {
throw new UDFArgumentTypeException(0, "Only primitive type arguments are accepted but "
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case INT:
case SHORT:
case LONG:
case BYTE:
break;
default:
"' is passed.");
}
return oi;
}
import java.util.ArrayList;
final List<LongWritable> list = new ArrayList<LongWritable>(size);
list.add(new LongWritable(0L));
}
return list;
final List<DoubleWritable> list = new ArrayList<DoubleWritable>(size);
list.add(new DoubleWritable(defaultValue));
}
return list;
final List<LongWritable> list = new ArrayList<LongWritable>(src.length);
list.add(new LongWritable(src[i]));
}
return list;
final List<DoubleWritable> list = new ArrayList<DoubleWritable>(src.length);
list.add(new DoubleWritable(src[i]));
}
return list;
final List<Text> list = new ArrayList<Text>(v.length);
String vi = v[i];
Text ti = (vi == null) ? null : new Text(vi);
list.add(ti);
}
return list;
if (object == null) {
if (object instanceof Writable) {
if (object instanceof String) {
if (object instanceof Long) {
if (object instanceof Integer) {
if (object instanceof Byte) {
if (object instanceof Double) {
if (object instanceof Float) {
if (object instanceof Boolean) {
if (object instanceof byte[]) {
import java.io.BufferedReader;
import java.io.InputStreamReader;
@Nonnull
public static BufferedReader bufferedReader(@Nonnull InputStream is) {
InputStreamReader in = new InputStreamReader(is);
return new BufferedReader(in);
}
import javax.annotation.Nonnull;
if (s.charAt(i) == '1') {
public static int indexOfSetBit(@Nonnull final BitSet bits, final int nth) {
if (nth < 0) {
}
int pos = bits.nextSetBit(0);
if (i == nth) {
break;
}
}
return pos;
}
public static int indexOfClearBit(@Nonnull final BitSet bits, final int nth, final int lastIndex) {
int j = bits.nextClearBit(0);
if (c == nth) {
break;
}
}
return j;
}
import java.util.ArrayList;
if (str == null || str.length() == 0) {
if (i == sz) {
if ((chars[i] < '0' || chars[i] > '9') && (chars[i] < 'a' || chars[i] > 'f')
if (chars[i] >= '0' && chars[i] <= '9') {
} else if (chars[i] == '.') {
if (hasDecPoint || hasExp) {
} else if (chars[i] == 'e' || chars[i] == 'E') {
if (hasExp) {
if (!foundDigit) {
if (!allowSigns) {
if (i < chars.length) {
if (chars[i] >= '0' && chars[i] <= '9') {
if (chars[i] == 'e' || chars[i] == 'E') {
if (!allowSigns
if (chars[i] == 'l' || chars[i] == 'L') {
for (String s : list) {
if (s == null) {
public static String[] split(final String str, final char separatorChar) {
return split(str, separatorChar, false);
}
public static String[] split(final String str, final char separatorChar,
final boolean preserveAllTokens) {
if (str == null) {
return null;
}
final int len = str.length();
if (len == 0) {
return new String[0];
}
final List<String> list = new ArrayList<String>();
int i = 0, start = 0;
boolean match = false;
boolean lastMatch = false;
while (i < len) {
if (str.charAt(i) == separatorChar) {
if (match || preserveAllTokens) {
list.add(str.substring(start, i));
match = false;
lastMatch = true;
}
continue;
}
lastMatch = false;
match = true;
i;
}
if (match || preserveAllTokens && lastMatch) {
list.add(str.substring(start, i));
}
return list.toArray(new String[list.size()]);
}
if (numberOfBits >= 32) {
if (powerOf == 0) {
while (value != 0) {
public static double lnSigmoid(final double x) {
double ex = Math.exp(-x);
}
if (w < 6.25) {
} else if (w < 16.0) {
} else if (!Double.isInfinite(w)) {
for (float e : elements) {
public static double log(final double n, final int base) {
return Math.log(n) / Math.log(base);
}
import hivemall.model.DenseModel;
import hivemall.model.PredictionModel;
import hivemall.model.SpaceEfficientDenseModel;
import hivemall.model.SparseModel;
import hivemall.model.SynchronizedModelWrapper;
import hivemall.model.WeightValue;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.WeightValue.WeightValueParamsF2;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionModel;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.PredictionResult;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.Margin;
import hivemall.model.PredictionModel;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.Margin;
import hivemall.model.PredictionModel;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.Margin;
import hivemall.model.PredictionModel;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.Margin;
import hivemall.model.FeatureValue;
import hivemall.model.PredictionResult;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.Margin;
import hivemall.model.PredictionModel;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.ModelUpdateHandler;
package hivemall.model;
package hivemall.model;
import hivemall.model.WeightValue.WeightValueParamsF1;
import hivemall.model.WeightValue.WeightValueParamsF2;
import hivemall.model.WeightValue.WeightValueWithCovar;
package hivemall.model;
package hivemall.model;
package hivemall.model;
package hivemall.model;
package hivemall.model;
package hivemall.model;
package hivemall.model;
import hivemall.model.WeightValue.WeightValueParamsF1;
import hivemall.model.WeightValue.WeightValueParamsF2;
import hivemall.model.WeightValue.WeightValueWithCovar;
package hivemall.model;
import hivemall.model.WeightValueWithClock.WeightValueParamsF1Clock;
import hivemall.model.WeightValueWithClock.WeightValueParamsF2Clock;
import hivemall.model.WeightValueWithClock.WeightValueWithCovarClock;
package hivemall.model;
package hivemall.model;
package hivemall.model;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.WeightValue.WeightValueParamsF2;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.WeightValue.WeightValueParamsF1;
import hivemall.model.FeatureValue;
import hivemall.model.PredictionResult;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionModel;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
int i = x.getFeatureIndex();
int i = x.getFeatureIndex();
final int i = x.getFeatureIndex();
final int i = x.getFeatureIndex();
if(e != null && e.getFeatureIndex() < 1) {
final int i = x.getFeatureIndex();
final int i = x.getFeatureIndex();
int i = x.getFeatureIndex();
final int i = x.getFeatureIndex();
final int idx = e.getFeatureIndex();
public class FMStringFeatureMapModel extends FactorizationMachineModel {
double predict(@Nonnull final Feature[] x) {
protected String varDump(@Nonnull final Feature[] x) {
void updateV(final double dloss, @Nonnull final Feature x, final int f,
public class FactorizationMachineUDTF extends UDTFWithOptions {
protected boolean _classification;
protected long _seed;
protected int _factor;
protected float _lambda0;
protected double _sigma;
protected double _min_target;
protected double _max_target;
protected LossFunction _lossFunction;
protected EtaEstimator _etaEstimator;
protected VInitScheme _vInit;
protected long _t;
protected ConversionState _cvState;
protected FactorizationMachineModel getModel() {
protected void setModel(FactorizationMachineModel model) {
_model = model;
return;
}
public void setField(String f) {
throw new UnsupportedOperationException();
}
public String getField() {
throw new UnsupportedOperationException();
}
public void setFieldIndex(int i) {
throw new UnsupportedOperationException();
}
public int getFieldIndex() {
throw new UnsupportedOperationException();
}
public void setFeatureIndex(int i) {
public int getFeatureIndex() {
probe.setFeatureIndex(index);
probe.setFeatureIndex(index);
private int featureIndex;
this.featureIndex = index;
return Integer.toString(featureIndex);
public int getFeatureIndex() {
return featureIndex;
public void setFeatureIndex(int i) {
this.featureIndex = i;
dst.putInt(featureIndex);
this.featureIndex = src.getInt();
import javax.annotation.Nullable;
private String field, feature;
int index = feature.indexOf(":");
if(index >= 0) {
this.field = feature.substring(0, index);
}
}
public StringFeature(@Nullable String field, @Nonnull String feature, double value) {
super(value);
this.field = field;
public void setField(String f) {
this.field = f;
}
@Override
public String getField() {
return field;
}
@Override
protected ListObjectInspector _xOI;
protected PrimitiveObjectInspector _yOI;
protected Random _va_rand;
protected int _validationThreshold;
protected Feature[] _probes;
protected boolean _parseFeatureAsInt;
import hivemall.common.LossFunctions;
private ArrayList<Object> fieldList = new ArrayList<Object>();
opts.addOption("const", "constant_term", false, "Whether to include constant bias term [default: OFF]");
opts.addOption("lin", "linear_term", false, "Whether to include linear term [default: OFF]");
this.constantTerm = cl.hasOption("const");
this.linearTerm = cl.hasOption("lin");
public void process(Object[] args) throws HiveException {
Feature[] x = Feature.parseFeatures(args[0], _xOI, _probes, _parseFeatureAsInt);
if (x == null) {
return;
}
this._probes = x;
addNewFieldsFrom(x);
double y = PrimitiveObjectInspectorUtils.getDouble(args[1], _yOI);
if (_classification) {
y = (y > 0.d) ? 1.d : -1.d;
}
_t;
recordTrain(x, y);
train(x, y, adaptiveRegularization);
}
@Override
private void addNewFieldsFrom(Feature[] x) {
double norm = norm(x, m);
double loss = LossFunctions.logLoss(p, y);
private double norm(Feature[] x, FieldAwareFactorizationMachineModel m) {
double ret = 0.d;
ret = m.getW0();
ret *= ret;
return ret;
protected void setV(@Nonnull Feature x, @Nonnull String yField, int f, float nextVif) {
final int pos = s.indexOf(':');
int pos2 = s2.indexOf(':');
if(pos2 == -1) {
if(asIntFeature) {
int index = Integer.parseInt(s1);
if(index < 0) {
}
double value = Double.parseDouble(s2);
return new IntFeature(index, value);
} else {
double value = Double.parseDouble(s2);
return new StringFeature(s1, value);
String s3 = s2.substring(0, pos2);
if(asIntFeature) {
} else {
double value = Double.parseDouble(s4);
return new StringFeature(s3, s1, value);
}
int pos2 = s2.indexOf(':');
if(pos2 == -1) {
if(asIntFeature) {
int index = Integer.parseInt(s1);
if(index < 0) {
}
double value = Double.parseDouble(s2);
probe.setFeatureIndex(index);
probe.value = value;
} else {
probe.setFeature(s1);
probe.value = Double.parseDouble(s2);
String s3 = s2.substring(0, pos2);
if(asIntFeature) {
} else {
probe.setField(s3);
probe.setFeature(s1);
probe.value = Double.parseDouble(s4);
}
protected abstract void setV(@Nonnull Feature x, @Nonnull String yField, int f, float nextVif);
setV(x, field.toString(), f, nextV);
public FFMStringFeatureMapModel(boolean classification, int factor, float lambda0,
double sigma, long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta,
@Nonnull VInitScheme vInit) {
if (entry == null) {
if (entry == null) {
if (entry == null) {
if (entry == null) {
import hivemall.common.EtaEstimator;
import hivemall.utils.lang.NumberUtils;
public FieldAwareFactorizationMachineModel(boolean classification, int factor, float lambda0,
double sigma, long seed, double minTarget, double maxTarget, EtaEstimator eta,
VInitScheme vInit) {
assert (!Double.isNaN(ret));
void updateV(final double dloss, @Nonnull final Feature x, final int f, final double sumViX,
final float eta, Object field) {
private double sumVfX(@Nonnull final Feature[] x, final int i, @Nonnull final Object a,
final int f) {
import hivemall.common.LossFunctions;
opts.addOption("const", "constant_term", false,
"Whether to include constant bias term [default: OFF]");
_seed, _min_target, _max_target, _etaEstimator, _vInit));
trainTheta(x, y);
assert (!Double.isNaN(norm));
m.updateV(lossGrad, x[i], f, sumVfx[i][fieldIndex][f], eta,
fieldList.get(fieldIndex));
if (constantTerm) {
if (linearTerm) {
public float getV(@Nonnull Feature x, @Nonnull Object yField, int f) {
int i = x.getIndex();
int i = x.getIndex();
final int i = x.getIndex();
final int i = x.getIndex();
if(e != null && e.getIndex() < 1) {
final int i = x.getIndex();
final int i = x.getIndex();
int i = x.getIndex();
final int i = x.getIndex();
final int idx = e.getIndex();
if (isAdaptiveRegularizationSupported()) {
opts.addOption("adareg", "adaptive_regularizaion", false,
"Whether to enable adaptive regularization [default: OFF]");
opts.addOption("va_ratio", "validation_ratio", true,
"Ratio of training data used for validation [default: 0.05f]");
opts.addOption("va_threshold", "validation_threshold", true,
"Threshold to start validation. "
"At least N training examples are used before validation [default: 1000]");
}
protected boolean isAdaptiveRegularizationSupported() {
return true;
}
this._model = initModel();
protected FactorizationMachineModel initModel() {
if (_parseFeatureAsInt) {
if (_p == -1) {
return new FMIntFeatureMapModel(_classification, _factor, _lambda0, _sigma, _seed,
_min_target, _max_target, _etaEstimator, _vInit);
} else {
return new FMArrayModel(_classification, _factor, _lambda0, _sigma, _p, _seed,
_min_target, _max_target, _etaEstimator, _vInit);
}
} else {
return new FMStringFeatureMapModel(_classification, _factor, _lambda0, _sigma, _seed,
_min_target, _max_target, _etaEstimator, _vInit);
}
}
@Nonnull
public void setIndex(int i) {
public int getIndex() {
throw new UnsupportedOperationException();
}
public void setField(String f) {
throw new UnsupportedOperationException();
}
@Nullable
public String getField() {
for (Feature f : x) {
public static Feature[] parseFeatures(@Nonnull final Object arg,
@Nonnull final ListObjectInspector listOI, @Nullable final Feature[] probes,
final boolean asIntFeature) throws HiveException {
if (arg == null) {
if (probes != null && probes.length == length) {
if (o == null) {
if (f == null) {
if (j == length) {
if (pos == -1) {
if (asIntFeature) {
if (index < 0) {
if (pos2 == -1) {
if (asIntFeature) {
if (index < 0) {
if (asIntFeature) {
private static void parse(@Nonnull final String s, @Nonnull final Feature probe,
final boolean asIntFeature) throws HiveException {
if (pos == -1) {
if (asIntFeature) {
if (index < 0) {
probe.setIndex(index);
if (pos2 == -1) {
if (asIntFeature) {
if (index < 0) {
probe.setIndex(index);
if (asIntFeature) {
if (asIntFeature) {
import java.util.List;
double[][][] sumVfX(@Nonnull Feature[] x, @Nonnull List<String> fieldList) {
private double sumVfX(@Nonnull final Feature[] x, final int i, @Nonnull final String field,
import java.util.List;
import javax.annotation.Nonnull;
private boolean globalBias;
private boolean linearCoeff;
@Nonnull
private final List<String> fieldList;
public FieldAwareFactorizationMachineUDTF() {
super();
this.fieldList = new ArrayList<String>();
}
opts.addOption("w0", "global_bias", false,
"Whether to include global bias term w0 [default: OFF]");
opts.addOption("w_i", "linear_coeff", false,
"Whether to include linear term [default: OFF]");
protected boolean isAdaptiveRegularizationSupported() {
return false;
}
@Override
if (_parseFeatureAsInt) {
throw new UDFArgumentException("int_feature option is not supported yet");
}
this.globalBias = cl.hasOption("global_bias");
this.linearCoeff = cl.hasOption("linear_coeff");
protected FactorizationMachineModel initModel() {
return new FFMStringFeatureMapModel(_classification, _factor, _lambda0, _sigma, _seed,
_min_target, _max_target, _etaEstimator, _vInit);
final FieldAwareFactorizationMachineModel m = (FieldAwareFactorizationMachineModel) getModel();
double loss = _lossFunction.loss(p, y);
if (globalBias) {
if (linearCoeff) {
List<String> fieldList = getFieldList(x);
final double[][][] sumVfx = m.sumVfX(x, fieldList);
fieldList.clear();
@Nonnull
private List<String> getFieldList(@Nonnull Feature[] x) {
for (Feature e : x) {
String field = e.getField();
fieldList.add(field);
return fieldList;
private int index;
this.index = index;
return Integer.toString(index);
public int getIndex() {
return index;
public void setIndex(int i) {
this.index = i;
dst.putInt(index);
this.index = src.getInt();
@Nullable
private String field;
@Nonnull
private String feature;
public float getV(@Nonnull Feature x, @Nonnull String yField, int f) {
int i = x.getFeatureIndex();
int i = x.getFeatureIndex();
final int i = x.getFeatureIndex();
final int i = x.getFeatureIndex();
if(e != null && e.getFeatureIndex() < 1) {
final int i = x.getFeatureIndex();
final int i = x.getFeatureIndex();
int i = x.getFeatureIndex();
final int i = x.getFeatureIndex();
final int idx = e.getFeatureIndex();
public void setFeatureIndex(int i) {
public int getFeatureIndex() {
probe.setFeatureIndex(index);
probe.setFeatureIndex(index);
public abstract float getV(@Nonnull Feature x, @Nonnull String field, int f);
final float eta, String field) {
setV(x, field, f, nextV);
final int factors = _factor;
final int fieldSize = fieldList.size();
final double[][][] ret = new double[xSize][fieldSize][factors];
ret[i][fieldIndex][f] = sumVfX(x, i, fieldList.get(fieldIndex), f);
final List<String> fieldList = getFieldList(x);
public int getFeatureIndex() {
public void setFeatureIndex(int i) {
@Nonnull VInitScheme vInit, boolean useAdaGrad) {
super(classification, factor, lambda0, sigma, seed, minTarget, maxTarget, eta, vInit, useAdaGrad);
entry = newEntry(V);
@Override
protected Entry getEntry(@Nonnull Feature x, @Nonnull String yField) {
private boolean useAdaGrad;
public double eta_debug;
VInitScheme vInit, boolean useAdaGrad) {
this.useAdaGrad = useAdaGrad;
throw new IllegalStateException(
float eta, final double eps, final double scaling, String field) {
if (useAdaGrad) {
Entry current = getEntry(x, field);
current.addGradient(gradV, scaling);
double adagrad = current.getSumOfSquaredGradients();
eta_debug = eta;
}
throw new IllegalStateException(
protected Entry newEntry(float[] V) {
if (useAdaGrad) {
return new AdaGradEntry(0.f, V);
} else {
return new Entry(0.f, V);
}
}
static class Entry {
float W;
@Nonnull
final float[] Vf;
Entry(float W, @Nonnull float[] Vf) {
this.W = W;
this.Vf = Vf;
}
public double getSumOfSquaredGradients() {
throw new UnsupportedOperationException();
}
public void addGradient(double grad, double scaling) {
throw new UnsupportedOperationException();
}
}
static class AdaGradEntry extends Entry {
double sumOfSqGradients;
AdaGradEntry(float W, float[] Vf) {
super(W, Vf);
sumOfSqGradients = 0.d;
}
@Override
public double getSumOfSquaredGradients() {
return sumOfSqGradients;
}
@Override
public void addGradient(double grad, double scaling) {
}
}
import hivemall.utils.lang.Primitives;
private boolean useAdaGrad;
private double eps;
private double scaling;
opts.addOption("adagrad", false,
"Whether to use AdaGrad for tuning learning rate [default: OFF]");
opts.addOption("eps", true, "A constant used in the denominator of AdaGrad [default 1.0]");
opts.addOption("scale", true,
"Internal scaling/descaling factor for cumulative weights [100]");
this.useAdaGrad = cl.hasOption("adagrad");
Primitives.parseFloat(cl.getOptionValue("eta"), 1.f);
this.eps = Primitives.parseFloat(cl.getOptionValue("eps"), 1.f);
this.scaling = Primitives.parseFloat(cl.getOptionValue("scale"), 100f);
protected FieldAwareFactorizationMachineModel initModel() {
_min_target, _max_target, _etaEstimator, _vInit, useAdaGrad);
final FieldAwareFactorizationMachineModel m =
(FieldAwareFactorizationMachineModel) getModel();
m.updateV(lossGrad, x[i], f, sumVfx[i][fieldIndex][f], eta, eps, scaling,
protected final float eta0;
public EtaEstimator(float eta0) {
this.eta0 = eta0;
}
public float eta0() {
return eta0;
}
super(eta);
return eta0;
public SimpleEtaEstimator(float eta0, long total_steps) {
super(eta0);
public InvscalingEtaEstimator(float eta0, double power_t) {
super(eta0);
super(eta);
float eta0 = Primitives.parseFloat(cl.getOptionValue("eta0"), 0.1f);
double power_t = Primitives.parseDouble(cl.getOptionValue("power_t"), 0.1d);
@Nonnull VInitScheme vInit, boolean useAdaGrad, float eta0_V, float eps, float scaling) {
super(classification, factor, lambda0, sigma, seed, minTarget, maxTarget, eta, vInit, useAdaGrad, eta0_V, eps, scaling);
String j = getFeatureOfField(x, yField);
String j = getFeatureOfField(x, yField);
String j = getFeatureOfField(x, yField);
return _map.get(j);
}
@Nonnull
private static String getFeatureOfField(@Nonnull Feature x, @Nonnull String yField) {
for (Feature xi : x) {
_model.updateWi(lossGrad, xi, eta);
_model.updateV(lossGrad, xi, f, sumVfx[f], eta);
protected final boolean useAdaGrad;
protected final float eta0_V;
protected final float eps;
protected final float scaling;
VInitScheme vInit, boolean useAdaGrad, float eta0_V, float eps, float scaling) {
this.eta0_V = eta0_V;
this.eps = eps;
this.scaling = scaling;
" in predict. We recommend to normalize training examples.\n"
void updateV(final double dloss, @Nonnull final Feature x, @Nonnull final String field,
final int f, final double sumViX, long t) {
float lambdaVf = getLambdaV(f);
float eta = etaV(t, x, field, gradV);
protected final float etaV(final long t, @Nonnull final Feature x, @Nonnull final String field,
final float grad) {
if (useAdaGrad) {
Entry theta = getEntry(x, field);
double gg = theta.getSumOfSquaredGradients(scaling);
theta.addGradient(grad, scaling);
} else {
return _eta.eta(t);
}
}
public double getSumOfSquaredGradients(float scaling) {
public void addGradient(float grad, float scaling) {
public double getSumOfSquaredGradients(float scaling) {
return sumOfSqGradients * scaling;
public void addGradient(float grad, float scaling) {
import hivemall.utils.lang.Primitives;
private float eta0_V;
private float eps;
private float scaling;
private FieldAwareFactorizationMachineModel model;
opts.addOption("eta0_V", true, "The initial learning rate for V [default 1.0]");
this.eta0_V = Primitives.parseFloat(cl.getOptionValue("eta0_V"), 1.f);
FFMStringFeatureMapModel model = new FFMStringFeatureMapModel(_classification, _factor,
_lambda0, _sigma, _seed, _min_target, _max_target, _etaEstimator, _vInit, useAdaGrad,
eta0_V, eps, scaling);
this.model = model;
return model;
final float eta_t = _etaEstimator.eta(_t);
final double p = model.predict(x);
final double lossGrad = model.dloss(p, y);
model.updateW0(lossGrad, eta_t);
model.updateWi(lossGrad, x[i], eta_t);
final double[][][] sumVfx = model.sumVfX(x, fieldList);
String feild = fieldList.get(fieldIndex);
model.updateV(lossGrad, x[i], feild, f, sumVfx[i][fieldIndex][f], _t);
import hivemall.utils.hadoop.HiveUtils;
import java.util.List;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
import org.apache.hadoop.io.Text;
value = "_FUNC_(array[string] X, string model) returns a prediction result from a Field-aware Factorization Machine")
private ListObjectInspector xOI;
private StringObjectInspector modelOI;
if (argOIs.length != 2) {
throw new UDFArgumentException("_FUNC_ takes 2 arguments");
}
xOI = HiveUtils.asListOI(argOIs[0]);
if (!(xOI.getListElementObjectInspector() instanceof FloatObjectInspector)) {
throw new UDFArgumentException("Elements of first argument must be float");
}
modelOI = HiveUtils.asStringOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
@SuppressWarnings("unchecked")
List<Feature> xList = (List<Feature>) xOI.getList(args[0]);
Feature[] x = (Feature[]) xList.toArray();
float w = model.getW1(e);
public FFMPredictionModel(String ser) {
}
public float getW1(@Nonnull Feature e) {
return _map.get(e.getFeature()).W;
public FMArrayModel(boolean classification, int factor, float lambda0, double sigma, int p,
long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta,
@Nonnull VInitScheme vInit) {
if (i < 1 || i > _p) {
if (i < 1 || i > _p) {
if (i < 1 || i > _p) {
for (Feature e : x) {
if (e != null && e.getFeatureIndex() < 1) {
public FMIntFeatureMapModel(boolean classification, int factor, float lambda0, double sigma,
long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta,
@Nonnull VInitScheme vInit) {
if (i == 0) {
if (i == 0) {
if (Vi == null) {
for (Feature e : x) {
if (e == null) {
if (idx < 1) {
if (!_w.containsKey(idx)) {
if (!_V.containsKey(idx)) {
void updateV(final double dloss, @Nonnull final Feature x, final int f, final double sumViX,
final float eta) {
if (index >= 0) {
import org.apache.hadoop.io.Text;
Text serModel = modelOI.getPrimitiveWritableObject(args[1]);
public final class FFMStringFeatureMapModel extends FieldAwareFactorizationMachineModel {
String j = StringFeature.getFeatureOfField(x, yField);
String j = StringFeature.getFeatureOfField(x, yField);
String j = StringFeature.getFeatureOfField(x, yField);
protected int _iterations;
protected float _validationRatio;
protected int _p;
protected FactorizationMachineModel _model;
CommandLine cl = processOptions(argOIs);
this._model = initModel(cl);
protected FactorizationMachineModel initModel(@Nullable CommandLine cl) {
float lambda0 = 0.01f;
double sigma = 0.1d;
double min_target = Double.MIN_VALUE, max_target = Double.MAX_VALUE;
String vInitOpt = null;
float maxInitValue = 1.f;
double initStdDev = 0.1d;
if (cl != null) {
lambda0 = Primitives.parseFloat(cl.getOptionValue("lambda0"), lambda0);
sigma = Primitives.parseDouble(cl.getOptionValue("sigma"), sigma);
min_target = Primitives.parseDouble(cl.getOptionValue("min_target"), min_target);
max_target = Primitives.parseDouble(cl.getOptionValue("max_target"), max_target);
vInitOpt = cl.getOptionValue("init_v");
maxInitValue = Primitives.parseFloat(cl.getOptionValue("max_init_value"), 1.f);
initStdDev = Primitives.parseDouble(cl.getOptionValue("min_init_stddev"), 0.1d);
}
VInitScheme vInit = VInitScheme.resolve(vInitOpt);
vInit.setMaxInitValue(maxInitValue);
initStdDev = Math.max(initStdDev, 1.0d / _factor);
vInit.setInitStdDev(initStdDev);
vInit.initRandom(_factor, _seed);
return new FMIntFeatureMapModel(_classification, _factor, lambda0, sigma, _seed,
min_target, max_target, _etaEstimator, vInit);
return new FMArrayModel(_classification, _factor, lambda0, sigma, _p, _seed,
min_target, max_target, _etaEstimator, vInit);
return new FMStringFeatureMapModel(_classification, _factor, lambda0, sigma, _seed,
min_target, max_target, _etaEstimator, vInit);
import hivemall.fm.StringFeature.StringFeatureWithField;
public void setFeature(@Nonnull String f) {
public void setField(@Nonnull String f) {
final int pos1 = s.indexOf(':');
if (pos1 == -1) {
String feature = s.substring(0, pos1);
int pos2 = rest.indexOf(':');
int index = Integer.parseInt(feature);
double value = Double.parseDouble(rest);
double value = Double.parseDouble(rest);
return new StringFeature(feature, value);
String field = rest.substring(0, pos2);
throw new HiveException("Fields are currently unsupported with IntFeatures: "
s);
double value = Double.parseDouble(valueStr);
return new StringFeatureWithField(feature, field, value);
final int pos1 = s.indexOf(":");
if (pos1 == -1) {
String feature = s.substring(0, pos1);
int pos2 = rest.indexOf(':');
int index = Integer.parseInt(feature);
double value = Double.parseDouble(rest);
probe.setFeature(feature);
probe.value = Double.parseDouble(rest);
String field = rest.substring(0, pos2);
throw new HiveException("Fields are currently unsupported with IntFeatures: "
s);
probe.setField(field);
probe.setFeature(feature);
probe.value = Double.parseDouble(valueStr);
import hivemall.fm.FactorizationMachineModel.VInitScheme;
import javax.annotation.Nullable;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
private List<String> fieldList;
public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
StructObjectInspector oi = super.initialize(argOIs);
this.fieldList = new ArrayList<String>();
return oi;
}
@Override
protected FieldAwareFactorizationMachineModel initModel(@Nullable CommandLine cl) {
float lambda0 = 0.01f;
double sigma = 0.1d;
double min_target = Double.MIN_VALUE, max_target = Double.MAX_VALUE;
String vInitOpt = null;
float maxInitValue = 1.f;
double initStdDev = 0.1d;
if (cl != null) {
lambda0 = Primitives.parseFloat(cl.getOptionValue("lambda0"), lambda0);
sigma = Primitives.parseDouble(cl.getOptionValue("sigma"), sigma);
min_target = Primitives.parseDouble(cl.getOptionValue("min_target"), min_target);
max_target = Primitives.parseDouble(cl.getOptionValue("max_target"), max_target);
vInitOpt = cl.getOptionValue("init_v");
maxInitValue = Primitives.parseFloat(cl.getOptionValue("max_init_value"), 1.f);
initStdDev = Primitives.parseDouble(cl.getOptionValue("min_init_stddev"), 0.1d);
}
VInitScheme vInit = VInitScheme.resolve(vInitOpt);
vInit.setMaxInitValue(maxInitValue);
initStdDev = Math.max(initStdDev, 1.0d / _factor);
vInit.setInitStdDev(initStdDev);
vInit.initRandom(_factor, _seed);
boolean useAdaGrad = cl.hasOption("adagrad");
float eta0_V = Primitives.parseFloat(cl.getOptionValue("eta0_V"), 1.f);
float eps = Primitives.parseFloat(cl.getOptionValue("eps"), 1.f);
float scaling = Primitives.parseFloat(cl.getOptionValue("scale"), 100f);
lambda0, sigma, _seed, min_target, max_target, _etaEstimator, vInit, useAdaGrad,
public class StringFeature extends Feature {
protected String feature;
public void setFeature(@Nonnull String feature) {
this.feature = feature;
}
@Override
public static final class StringFeatureWithField extends StringFeature {
@Nonnull
protected String field;
public StringFeatureWithField(@Nonnull String feature, @Nonnull String field, double value) {
super(feature, value);
this.field = field;
}
@Override
public void setField(@Nonnull String f) {
this.field = f;
}
@Override
public String getField() {
return field;
}
@Override
public int bytes() {
}
@Override
public void writeTo(@Nonnull ByteBuffer dst) {
super.writeTo(dst);
NIOUtils.putString(field, dst);
}
@Override
public void readFrom(@Nonnull ByteBuffer src) {
super.readFrom(src);
this.field = NIOUtils.getString(src);
}
@Override
public String toString() {
}
}
@Nonnull
public static String getFeatureOfField(@Nonnull Feature x, @Nonnull String yField) {
}
package hivemall.utils.codec;
package hivemall.utils.codec;
package hivemall.utils.codec;
import hivemall.utils.codec.Base91;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.Base91;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.Base91;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.Base91;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.Base91;
import hivemall.utils.codec.Base91;
import java.io.InputStream;
public static void encode(@Nonnull final InputStream in, @Nonnull final OutputStream out)
throws IOException {
int ebq = 0;
int en = 0;
int b;
while ((b = in.read()) != -1) {
ebq |= (b & 255) << en;
if (en > 13) {
if (ev > 88) {
ebq >>= 13;
en -= 13;
} else {
ebq >>= 14;
en -= 14;
}
out.write(ENCODING_TABLE[ev % BASE]);
out.write(ENCODING_TABLE[ev / BASE]);
}
}
if (en > 0) {
out.write(ENCODING_TABLE[ebq % BASE]);
if (en > 7 || ebq > 90) {
out.write(ENCODING_TABLE[ebq / BASE]);
}
}
}
} while (dn >= 8);
public static void decode(@Nonnull final InputStream in, @Nonnull final OutputStream out)
throws IOException {
int dbq = 0;
int dn = 0;
int dv = -1;
int b;
while ((b = in.read()) != -1) {
if (DECODING_TABLE[b] == -1) {
continue;
}
if (dv == -1) {
dv = DECODING_TABLE[b];
} else {
dbq |= dv << dn;
do {
out.write((byte) dbq);
dbq >>= 8;
dn -= 8;
} while (dn >= 8);
dv = -1;
}
}
if (dv != -1) {
out.write((byte) (dbq | dv << dn));
}
}
import java.io.Externalizable;
import java.io.IOException;
import java.io.ObjectInput;
import java.io.ObjectOutput;
import javax.annotation.Nonnull;
public final class OpenHashTable<K, V> implements Externalizable {
if (size < 1) {
public OpenHashTable(@Nonnull K[] keys, @Nonnull V[] values, @Nonnull byte[] states, int used) {
this._used = used;
this._threshold = keys.length;
this._keys = keys;
this._values = values;
this._states = states;
}
public K[] getKeys() {
return _keys;
}
public V[] getValues() {
return _values;
}
public byte[] getStates() {
return _states;
}
if (i < 0) {
if (expanded) {
if (states[keyIdx] == FULL) {
if (equals(keys[keyIdx], key)) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && equals(keys[keyIdx], key)) {
if (stat == FREE) {
if (stat == REMOVED && equals(_keys[index], key)) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && equals(keys[keyIdx], key)) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && equals(keys[keyIdx], key)) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && equals(keys[keyIdx], key)) {
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
if (states[keyIdx] == FULL && equals(keys[keyIdx], key)) {
while (i.next() != -1) {
if (i.hasNext()) {
if (newCapacity <= oldCapacity) {
if (_states[i] == FULL) {
while (newStates[keyIdx] != FREE) {
if (keyIdx < 0) {
while (index < _keys.length && _states[index] != FULL) {
if (!hasNext()) {
if (lastEntry == -1) {
if (lastEntry == -1) {
@Override
public void writeExternal(ObjectOutput out) throws IOException {
out.writeFloat(_loadFactor);
out.writeFloat(_growFactor);
out.writeInt(_used);
final int size = _keys.length;
out.writeInt(size);
out.writeObject(_keys[i]);
out.writeObject(_values[i]);
out.writeByte(_states[i]);
}
}
@SuppressWarnings("unchecked")
@Override
public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {
this._loadFactor = in.readFloat();
this._growFactor = in.readFloat();
this._used = in.readInt();
final int size = in.readInt();
final Object[] keys = new Object[size];
final Object[] values = new Object[size];
final byte[] states = new byte[size];
keys[i] = in.readObject();
values[i] = in.readObject();
states[i] = in.readByte();
}
this._threshold = size;
this._keys = (K[]) keys;
this._values = (V[]) values;
this._states = states;
}
}
import hivemall.utils.codec.Base91;
import java.io.IOException;
import javax.annotation.Nullable;
@Description(name = "ffm_predict",
value = "_FUNC_(string modelId, string model, array<string> features)"
" returns a prediction result in double from a Field-aware Factorization Machine")
private StringObjectInspector _modelIdOI;
private StringObjectInspector _modelOI;
private ListObjectInspector _featureListOI;
private DoubleWritable _result;
@Nullable
private String _cachedModeId;
@Nullable
private FFMPredictionModel _cachedModel;
@Nullable
private Feature[] _probes;
if (argOIs.length != 3) {
throw new UDFArgumentException("_FUNC_ takes 3 arguments");
this._modelIdOI = HiveUtils.asStringOI(argOIs[0]);
this._modelOI = HiveUtils.asStringOI(argOIs[1]);
this._featureListOI = HiveUtils.asListOI(argOIs[2]);
this._result = new DoubleWritable();
Feature[] x = Feature.parseFeatures(args[2], _featureListOI, _probes, false);
if (x == null) {
}
this._probes = x;
String modelId = _modelIdOI.getPrimitiveJavaObject(args[0]);
if (modelId == null) {
throw new HiveException("modelId is not set");
}
final FFMPredictionModel model;
if (modelId.equals(_cachedModeId)) {
model = this._cachedModel;
} else {
Text serModel = _modelOI.getPrimitiveWritableObject(args[1]);
if (serModel == null) {
}
byte[] b = serModel.getBytes();
int length = serModel.getLength();
b = Base91.decode(b, 0, length);
try {
model = FFMPredictionModel.deserialize(b);
} catch (ClassNotFoundException e) {
throw new HiveException(e);
} catch (IOException e) {
throw new HiveException(e);
}
this._cachedModeId = modelId;
this._cachedModel = model;
}
double predicted = predict(x, model);
_result.set(predicted);
return _result;
private static double predict(@Nonnull final Feature[] x,
@Nonnull final FFMPredictionModel model) throws HiveException {
double xi = e.getValue();
float wi = model.getW1(e);
double wx = wi * xi;
final int factors = model.getNumFactors();
final Feature ei = x[i];
final double xi = ei.getValue();
final String iField = ei.getField();
final Feature ej = x[j];
final double xj = ej.getValue();
final String jField = ej.getField();
final float[] vij = model.getV(ei, jField);
final float[] vji = model.getV(ej, iField);
if (vij == null || vji == null) {
continue;
}
float vijf = vij[f];
float vjif = vji[f];
@Override
public void close() throws IOException {
super.close();
this._cachedModel = null;
this._probes = null;
}
@Override
public String getDisplayString(String[] args) {
}
import hivemall.utils.io.IOUtils;
import hivemall.utils.lang.ObjectUtils;
import java.io.Externalizable;
import java.io.IOException;
import java.io.ObjectInput;
import java.io.ObjectOutput;
import javax.annotation.Nullable;
public final class FFMPredictionModel implements Externalizable {
private double _w0;
private int _factors;
public FFMPredictionModel(@Nonnull OpenHashTable<String, Entry> map, double w0, int factor) {
this._map = map;
this._w0 = w0;
this._factors = factor;
String xi = e.getFeature();
Entry entry = _map.get(xi);
return entry.W;
@Nullable
public float[] getV(@Nonnull Feature x, @Nonnull String field) {
String j = StringFeature.getFeatureOfField(x, field);
Entry entry = _map.get(j);
if (entry == null) {
return null;
}
return entry.Vf;
@Override
public void writeExternal(@Nonnull ObjectOutput out) throws IOException {
out.writeDouble(_w0);
out.writeInt(_factors);
int used = _map.size();
out.writeInt(used);
final String[] keys = _map.getKeys();
final Entry[] values = _map.getValues();
final byte[] status = _map.getStates();
final int size = keys.length;
out.writeInt(size);
String key = keys[i];
if (key == null) {
continue;
} else {
}
IOUtils.writeString(key, out);
Entry v = values[i];
out.writeFloat(v.W);
IOUtils.writeFloats(v.Vf, out);
out.writeByte(status[i]);
}
}
@Override
public void readExternal(@Nonnull ObjectInput in) throws IOException, ClassNotFoundException {
this._w0 = in.readDouble();
this._factors = in.readInt();
int used = in.readInt();
final int size = in.readInt();
final String[] keys = new String[size];
final Entry[] values = new Entry[size];
final byte[] states = new byte[size];
continue;
}
keys[i] = IOUtils.readString(in);
float W = in.readFloat();
float[] Vf = IOUtils.readFloats(in);
values[i] = new Entry(W, Vf);
states[i] = in.readByte();
}
this._map = new OpenHashTable<String, Entry>(keys, values, states, used);
}
public byte[] serialize() throws IOException {
return ObjectUtils.toCompressedBytes(this);
}
public static FFMPredictionModel deserialize(@Nonnull final byte[] serializedObj)
throws ClassNotFoundException, IOException {
FFMPredictionModel model = new FFMPredictionModel();
ObjectUtils.readCompressedObject(serializedObj, model);
return model;
}
@Nonnull
FFMPredictionModel toPredictionModel() {
return new FFMPredictionModel(_map, _w0, _factor);
}
return getOutputOI();
}
@Nonnull
protected StructObjectInspector getOutputOI() {
forwardModel();
}
protected void forwardModel() throws HiveException {
forwardAsIntFeature(_model, _factor);
FMStringFeatureMapModel strModel = (FMStringFeatureMapModel) _model;
forwardAsStringFeature(strModel, _factor);
private void forwardAsIntFeature(@Nonnull final FactorizationMachineModel model,
final int factors) throws HiveException {
final FloatWritable[] f_Vi = HiveUtils.newFloatArray(factors, 0.f);
f_Wi.set(model.getW0());
final float[] vi = model.getV(i);
final float w = model.getW(i);
private void forwardAsStringFeature(@Nonnull final FMStringFeatureMapModel model,
final int factors) throws HiveException {
final FloatWritable[] f_Vi = HiveUtils.newFloatArray(factors, 0.f);
f_Wi.set(model.getW0());
x[j] = instantiateFeature(inputBuf);
x[j] = instantiateFeature(inputBuf);
@Nonnull
protected Feature instantiateFeature(@Nonnull final ByteBuffer input) {
return Feature.createInstance(input, _parseFeatureAsInt);
}
protected double predict(@Nonnull final Feature[] x) {
double xi = e.getValue();
float wi = getW(e);
double wx = wi * xi;
final Feature ei = x[i];
final double xi = ei.getValue();
final String iField = ei.getField();
final Feature ej = x[j];
final double xj = ej.getValue();
final String jField = ej.getField();
float vijf = getV(ei, jField, f);
float vjif = getV(ej, iField, f);
String field = fieldList.get(fieldIndex);
ret[i][fieldIndex][f] = sumVfX(x, i, field, f);
import hivemall.fm.StringFeature.StringFeatureWithField;
import hivemall.utils.codec.Base91;
import hivemall.utils.hadoop.HadoopUtils;
import java.io.IOException;
import java.nio.ByteBuffer;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.apache.hadoop.io.Text;
private boolean _globalBias;
private boolean _linearCoeff;
private FFMStringFeatureMapModel _model;
private List<String> _fieldList;
this._globalBias = cl.hasOption("global_bias");
this._linearCoeff = cl.hasOption("linear_coeff");
this._fieldList = new ArrayList<String>();
protected StructObjectInspector getOutputOI() {
ArrayList<String> fieldNames = new ArrayList<String>();
ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
fieldNames.add("model_id");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
fieldNames.add("model");
fieldOIs.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
}
@Override
protected FFMStringFeatureMapModel initModel(@Nullable CommandLine cl) {
this._model = model;
public void train(@Nonnull Feature[] x, double y, boolean adaptiveRegularization)
throws HiveException {
_model.check(x);
try {
trainTheta(x, y);
} catch (Exception ex) {
}
}
@Override
final double p = _model.predict(x);
final double lossGrad = _model.dloss(p, y);
if (_globalBias) {
_model.updateW0(lossGrad, eta_t);
if (_linearCoeff) {
_model.updateWi(lossGrad, x[i], eta_t);
final double[][][] sumVfx = _model.sumVfX(x, fieldList);
final String feild = fieldList.get(fieldIndex);
_model.updateV(lossGrad, x[i], feild, f, sumVfx[i][fieldIndex][f], _t);
_fieldList.add(field);
return _fieldList;
@Override
protected Feature instantiateFeature(@Nonnull ByteBuffer input) {
return new StringFeatureWithField(input);
}
@Override
public void close() throws HiveException {
super.close();
this._model = null;
this._fieldList = null;
}
@Override
protected void forwardModel() throws HiveException {
Text modelId = new Text();
Text modelObj = new Text();
Object[] forwardObjs = new Object[] {modelId, modelObj};
String taskId = HadoopUtils.getUniqueTaskIdString();
modelId.set(taskId);
FFMPredictionModel predModel = _model.toPredictionModel();
byte[] serialized;
try {
serialized = predModel.serialize();
} catch (IOException e) {
throw new HiveException(e);
}
serialized = Base91.encode(serialized);
modelObj.set(serialized);
forward(forwardObjs);
}
public StringFeatureWithField(@Nonnull ByteBuffer src) {
super(src);
this.field = NIOUtils.getString(src);
}
import java.util.UUID;
if (codec == null) {
if (decompressor != null) {
if (ctx == null) {
if (conf == null) {
if (jobId == null) {
if (jobId == null) {
if (queryId != null) {
if (taskidStr == null) {
if (ctx == null) {
if (jobconf == null) {
if (taskid == -1) {
if (taskid == -1) {
throw new IllegalStateException(
"Both mapred.task.partition and mapreduce.task.partition are not set: "
toString(jobconf));
public static String getUniqueTaskIdString() {
MapredContext ctx = MapredContextAccessor.get();
if (ctx != null) {
JobConf jobconf = ctx.getJobConf();
if (jobconf != null) {
int taskid = jobconf.getInt("mapred.task.partition", -1);
if (taskid == -1) {
taskid = jobconf.getInt("mapreduce.task.partition", -1);
}
if (taskid != -1) {
return String.valueOf(taskid);
}
}
}
return getUUID();
}
public synchronized static String getUUID() {
return UUID.randomUUID().toString();
}
if (!hasNext) {
if (k == null) {
if (regexKey == null || k.matches(regexKey)) {
if (hasNext) {
} while (hasNext);
import java.io.DataInput;
import java.io.DataInputStream;
import java.io.DataOutput;
import java.io.DataOutputStream;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import javax.annotation.Nullable;
@Nonnull
public static byte[] toByteArray(@Nonnull final InputStream input) throws IOException {
FastByteArrayOutputStream output = new FastByteArrayOutputStream();
copy(input, output);
return output.toByteArray();
}
public static void writeInt(final int v, final OutputStream out) throws IOException {
out.write((v >>> 24) & 0xFF);
out.write((v >>> 16) & 0xFF);
out.write((v >>> 8) & 0xFF);
out.write((v >>> 0) & 0xFF);
}
public static int readInt(final InputStream in) throws IOException {
final int ch1 = in.read();
final int ch2 = in.read();
final int ch3 = in.read();
final int ch4 = in.read();
}
public static void writeChar(final char v, final OutputStream out) throws IOException {
out.write(0xff & (v >> 8));
out.write(0xff & v);
}
public static void writeChar(final char v, final FastByteArrayOutputStream out) {
out.write(0xff & (v >> 8));
out.write(0xff & v);
}
public static char readChar(final InputStream in) throws IOException {
final int a = in.read();
final int b = in.read();
return (char) ((a << 8) | (b & 0xff));
}
public static void writeBytes(@Nonnull final byte[] src, @Nonnull final OutputStream dst)
throws IOException {
if (src != null) {
dst.write(src);
}
}
public static void writeBytes(@Nonnull final byte[] src, final int off, final int len,
@Nonnull final OutputStream dst) throws IOException {
if (src != null) {
dst.write(src, off, len);
}
}
public static void writeString(@Nullable final String s, final ObjectOutputStream out)
throws IOException {
writeString(s, (DataOutput) out);
}
public static void writeString(@Nullable final String s, final DataOutputStream out)
throws IOException {
writeString(s, (DataOutput) out);
}
public static void writeString(@Nullable final String s, final DataOutput out)
throws IOException {
if (s == null) {
out.writeInt(-1);
return;
}
final int len = s.length();
out.writeInt(len);
int v = s.charAt(i);
out.writeChar(v);
}
}
public static void writeString(@Nullable final String s, final OutputStream out)
throws IOException {
if (s == null) {
writeInt(-1, out);
return;
}
final int len = s.length();
writeInt(len, out);
char c = s.charAt(i);
writeChar(c, out);
}
}
@Nullable
public static String readString(@Nonnull final ObjectInputStream in) throws IOException {
return readString((DataInput) in);
}
@Nullable
public static String readString(@Nonnull final DataInputStream in) throws IOException {
return readString((DataInput) in);
}
@Nullable
public static String readString(@Nonnull final DataInput in) throws IOException {
final int len = in.readInt();
if (len == -1) {
return null;
}
final char[] ch = new char[len];
ch[i] = in.readChar();
}
return new String(ch);
}
@Nullable
public static String readString(@Nonnull final InputStream in) throws IOException {
final int len = readInt(in);
if (len == -1) {
return null;
}
final char[] ch = new char[len];
ch[i] = readChar(in);
}
return new String(ch);
}
public static void writeFloats(@Nonnull final float[] floats, @Nonnull final DataOutput out)
throws IOException {
final int size = floats.length;
out.writeInt(size);
out.writeFloat(floats[i]);
}
}
@Nonnull
public static float[] readFloats(@Nonnull final DataInput in) throws IOException {
final int size = in.readInt();
final float[] floats = new float[size];
floats[i] = in.readFloat();
}
return floats;
}
import java.io.Externalizable;
public static byte[] toBytes(@Nonnull final Externalizable obj) throws IOException {
FastByteArrayOutputStream bos = new FastByteArrayOutputStream();
toStream(obj, bos);
return bos.toByteArray();
}
public static byte[] toCompressedBytes(@Nonnull final Object obj) throws IOException {
toStream(obj, dos);
} finally {
IOUtils.closeQuietly(dos);
}
return bos.toByteArray_clear();
}
public static byte[] toCompressedBytes(@Nonnull final Externalizable obj) throws IOException {
FastMultiByteArrayOutputStream bos = new FastMultiByteArrayOutputStream();
final DeflaterOutputStream dos = new DeflaterOutputStream(bos);
try {
toStream(obj, dos);
public static void toStream(@Nonnull final Externalizable obj, @Nonnull final OutputStream out)
throws IOException {
ObjectOutputStream oos = new ObjectOutputStream(out);
obj.writeExternal(oos);
oos.flush();
oos.close();
}
public static <T> T readObject(@Nonnull final byte[] obj) throws IOException,
ClassNotFoundException {
public static void readObject(@Nonnull final byte[] src, @Nonnull final Externalizable dst)
throws IOException, ClassNotFoundException {
readObject(new FastByteArrayInputStream(src), dst);
}
public static <T> T readObject(@Nonnull final InputStream is) throws IOException,
ClassNotFoundException {
public static void readObject(@Nonnull final InputStream is, @Nonnull final Externalizable dst)
throws IOException, ClassNotFoundException {
ObjectInputStream ois = new ObjectInputStream(is);
dst.readExternal(ois);
}
public static <T> T readCompressedObject(@Nonnull final byte[] obj) throws IOException,
public static void readCompressedObject(@Nonnull final byte[] src,
@Nonnull final Externalizable dst) throws IOException, ClassNotFoundException {
FastByteArrayInputStream bis = new FastByteArrayInputStream(src);
final InflaterInputStream iis = new InflaterInputStream(bis);
try {
readObject(iis, dst);
} finally {
IOUtils.closeQuietly(iis);
}
}
public static void readCompressedObject(@Nonnull final byte[] src, final int offset,
final int length, @Nonnull final Externalizable dst) throws IOException,
ClassNotFoundException {
FastByteArrayInputStream bis = new FastByteArrayInputStream(src, offset, length);
final InflaterInputStream iis = new InflaterInputStream(bis);
try {
readObject(iis, dst);
} finally {
IOUtils.closeQuietly(iis);
}
}
public static int toUnsignedInt(final byte x) {
return ((int) x) & 0xff;
}
if (s == null) {
if (s == null) {
if (s == null) {
if (s == null) {
if (s == null) {
if (s == null) {
value = "_FUNC_(array<string> x, double y [, const string options]) - Returns a prediction model")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_ffm",
value = "_FUNC_(array<string> x, double y [, const string options]) - Returns a prediction model")
import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray;
Object arg2 = args[2].get();
if (arg2 instanceof LazyBinaryArray) {
arg2 = ((LazyBinaryArray) arg2).getList();
}
Feature[] x = Feature.parseFeatures(arg2, _featureListOI, _probes, false);
if (x == null || x.length == 0) {
String modelId = _modelIdOI.getPrimitiveJavaObject(args[0].get());
Text serModel = _modelOI.getPrimitiveWritableObject(args[1].get());
final Object[] keys = _map.getKeys();
final Object[] values = _map.getValues();
String key = (String) keys[i];
Entry v = (Entry) values[i];
public Object[] getKeys() {
public Object[] getValues() {
opts.addOption("f", "factor", true, "The number of the latent variables [default: 8]");
int factor = 8;
opts.addOption("diasble_adagrad", false,
"Whether to use AdaGrad for tuning learning rate [default: ON]");
boolean useAdaGrad = !cl.hasOption("diasble_adagrad");
opts.addOption("disable_adagrad", false,
boolean useAdaGrad = !cl.hasOption("disable_adagrad");
final String field = fieldList.get(fieldIndex);
_model.updateV(lossGrad, x[i], field, f, sumVfx[i][fieldIndex][f], _t);
private static Feature parse(@Nonnull final String fv, final boolean asIntFeature)
final int pos1 = fv.indexOf(':');
int index = parseFeatureIndex(fv);
String lead = fv.substring(0, pos1);
int index = parseFeatureIndex(lead);
double value = parseFeatureValue(rest);
double value = parseFeatureValue(rest);
fv);
String index = rest.substring(0, pos2);
double value = parseFeatureValue(valueStr);
private static void parse(@Nonnull final String fv, @Nonnull final Feature probe,
final int pos1 = fv.indexOf(":");
int index = parseFeatureIndex(fv);
probe.setFeature(fv);
String lead = fv.substring(0, pos1);
int index = parseFeatureIndex(lead);
probe.value = parseFeatureValue(rest);;
probe.setFeature(lead);
probe.value = parseFeatureValue(rest);
fv);
String index = rest.substring(0, pos2);
probe.setField(lead);
probe.setFeature(index);
probe.value = parseFeatureValue(valueStr);
private static int parseFeatureIndex(@Nonnull final String indexStr) throws HiveException {
final int index;
try {
index = Integer.parseInt(indexStr);
} catch (NumberFormatException e) {
}
if (index < 0) {
}
return index;
}
private static double parseFeatureValue(@Nonnull final String value) throws HiveException {
try {
return Double.parseDouble(value);
} catch (NumberFormatException e) {
}
}
public void setField(@Nullable String f) {
@Nonnull
static Feature parse(@Nonnull final String fv, final boolean asIntFeature) throws HiveException {
static void parse(@Nonnull final String fv, @Nonnull final Feature probe,
probe.setField(null);
probe.setField(null);
protected void trainTheta(@Nonnull Feature[] x, double y) throws HiveException {
protected StringFeature instantiateFeature(@Nonnull ByteBuffer input) {
return new StringFeature(input);
import javax.annotation.Nullable;
@Nullable
protected String field;
this(feature, null, value);
}
StringFeature(int feature, double value) {
this(String.valueOf(feature), null, value);
}
public StringFeature(@Nonnull String feature, @Nullable String field, double value) {
this.field = field;
public void setField(@Nullable String f) {
this.field = f;
}
@Override
public String getField() {
if (field == null) {
}
return field;
}
@Override
NIOUtils.putString(field, dst);
this.field = NIOUtils.getString(src);
if (field == null) {
} else {
import javax.annotation.Nullable;
public static int requiredBytes(@Nullable final String s) {
if (s == null) {
return INT_BYTES;
}
public static void putString(@Nullable final String s, @Nonnull final ByteBuffer dst) {
if (s == null) {
dst.putInt(-1);
return;
}
@Nullable
if (size == -1) {
return null;
}
public static int read(@Nonnull final FileChannel src, @Nonnull final ByteBuffer dst,
@Nonnegative final long position) throws IOException {
while (dst.remaining() > 0) {
if (n == -1) {
while (dst.remaining() > 0) {
public static int writeFully(@Nonnull final FileChannel dst, @Nonnull final ByteBuffer src,
@Nonnegative final long position) throws IOException {
while (src.remaining() > 0) {
while (src.remaining() > 0) {
positiveSum = 0.d;
negativeSum = 0.d;
public boolean iterate(double w) {
if (partial == null) {
if (w > 0) {
} else if (w < 0) {
if (other == null) {
if (partial == null) {
if (partial == null) {
if (partial.positiveCnt > partial.negativeCnt) {
return new DoubleWritable(partial.positiveSum / partial.positiveCnt);
if (partial.negativeCnt == 0) {
assert (partial.negativeSum == 0.d) : partial.negativeSum;
return new DoubleWritable(0.d);
return new DoubleWritable(partial.negativeSum / partial.negativeCnt);
positiveSum = 0.d;
negativeSum = 0.d;
public boolean iterate(double w) {
if (partial == null) {
if (w > 0) {
} else if (w < 0) {
if (other == null) {
if (partial == null) {
if (partial == null) {
if (partial.positiveSum > (-partial.negativeSum)) {
return new DoubleWritable(partial.positiveSum / partial.positiveCnt);
if (partial.negativeCnt == 0) {
assert (partial.negativeSum == 0.d) : partial.negativeSum;
return new DoubleWritable(0.d);
return new DoubleWritable(partial.negativeSum / partial.negativeCnt);
protected final float eta0;
public EtaEstimator(float eta0) {
this.eta0 = eta0;
}
public float eta0() {
return eta0;
}
super(eta);
return eta0;
public SimpleEtaEstimator(float eta0, long total_steps) {
super(eta0);
public InvscalingEtaEstimator(float eta0, double power_t) {
super(eta0);
super(eta);
float eta0 = Primitives.parseFloat(cl.getOptionValue("eta0"), 0.1f);
double power_t = Primitives.parseDouble(cl.getOptionValue("power_t"), 0.1d);
positiveSum = 0.d;
negativeSum = 0.d;
public boolean iterate(double w) {
if (partial == null) {
if (w > 0) {
} else if (w < 0) {
if (other == null) {
if (partial == null) {
if (partial == null) {
if (partial.positiveCnt > partial.negativeCnt) {
return new DoubleWritable(partial.positiveSum / partial.positiveCnt);
if (partial.negativeCnt == 0) {
assert (partial.negativeSum == 0.d) : partial.negativeSum;
return new DoubleWritable(0.d);
return new DoubleWritable(partial.negativeSum / partial.negativeCnt);
positiveSum = 0.d;
negativeSum = 0.d;
public boolean iterate(double w) {
if (partial == null) {
if (w > 0) {
} else if (w < 0) {
if (other == null) {
if (partial == null) {
if (partial == null) {
if (partial.positiveSum > (-partial.negativeSum)) {
return new DoubleWritable(partial.positiveSum / partial.positiveCnt);
if (partial.negativeCnt == 0) {
assert (partial.negativeSum == 0.d) : partial.negativeSum;
return new DoubleWritable(0.d);
return new DoubleWritable(partial.negativeSum / partial.negativeCnt);
public FMArrayModel(boolean classification, int factor, float lambda0, double sigma, int p,
long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta,
@Nonnull VInitScheme vInit) {
int i = x.getFeatureIndex();
int i = x.getFeatureIndex();
if (i < 1 || i > _p) {
final int i = x.getFeatureIndex();
if (i < 1 || i > _p) {
final int i = x.getFeatureIndex();
if (i < 1 || i > _p) {
for (Feature e : x) {
if (e != null && e.getFeatureIndex() < 1) {
public FMIntFeatureMapModel(boolean classification, int factor, float lambda0, double sigma,
long seed, double minTarget, double maxTarget, @Nonnull EtaEstimator eta,
@Nonnull VInitScheme vInit) {
final int i = x.getFeatureIndex();
if (i == 0) {
final int i = x.getFeatureIndex();
if (i == 0) {
int i = x.getFeatureIndex();
if (Vi == null) {
final int i = x.getFeatureIndex();
for (Feature e : x) {
if (e == null) {
final int idx = e.getFeatureIndex();
if (idx < 1) {
if (!_w.containsKey(idx)) {
if (!_V.containsKey(idx)) {
public class FMStringFeatureMapModel extends FactorizationMachineModel {
double predict(@Nonnull final Feature[] x) {
protected String varDump(@Nonnull final Feature[] x) {
void updateV(final double dloss, @Nonnull final Feature x, final int f, final double sumViX,
final float eta) {
value = "_FUNC_(array<string> x, double y [, const string options]) - Returns a prediction model")
public class FactorizationMachineUDTF extends UDTFWithOptions {
protected ListObjectInspector _xOI;
protected PrimitiveObjectInspector _yOI;
protected boolean _classification;
protected long _seed;
protected int _iterations;
protected int _factor;
protected Random _va_rand;
protected float _validationRatio;
protected int _validationThreshold;
protected LossFunction _lossFunction;
protected EtaEstimator _etaEstimator;
protected int _p;
protected FactorizationMachineModel _model;
protected Feature[] _probes;
protected boolean _parseFeatureAsInt;
protected long _t;
protected ConversionState _cvState;
opts.addOption("f", "factor", true, "The number of the latent variables [default: 8]");
if (isAdaptiveRegularizationSupported()) {
opts.addOption("adareg", "adaptive_regularizaion", false,
"Whether to enable adaptive regularization [default: OFF]");
opts.addOption("va_ratio", "validation_ratio", true,
"Ratio of training data used for validation [default: 0.05f]");
opts.addOption("va_threshold", "validation_threshold", true,
"Threshold to start validation. "
"At least N training examples are used before validation [default: 1000]");
}
protected boolean isAdaptiveRegularizationSupported() {
return true;
}
int factor = 8;
protected FactorizationMachineModel getModel() {
CommandLine cl = processOptions(argOIs);
this._model = initModel(cl);
return getOutputOI();
}
@Nonnull
protected StructObjectInspector getOutputOI() {
protected FactorizationMachineModel initModel(@Nullable CommandLine cl) {
float lambda0 = 0.01f;
double sigma = 0.1d;
double min_target = Double.MIN_VALUE, max_target = Double.MAX_VALUE;
String vInitOpt = null;
float maxInitValue = 1.f;
double initStdDev = 0.1d;
if (cl != null) {
lambda0 = Primitives.parseFloat(cl.getOptionValue("lambda0"), lambda0);
sigma = Primitives.parseDouble(cl.getOptionValue("sigma"), sigma);
min_target = Primitives.parseDouble(cl.getOptionValue("min_target"), min_target);
max_target = Primitives.parseDouble(cl.getOptionValue("max_target"), max_target);
vInitOpt = cl.getOptionValue("init_v");
maxInitValue = Primitives.parseFloat(cl.getOptionValue("max_init_value"), 1.f);
initStdDev = Primitives.parseDouble(cl.getOptionValue("min_init_stddev"), 0.1d);
}
VInitScheme vInit = VInitScheme.resolve(vInitOpt);
vInit.setMaxInitValue(maxInitValue);
initStdDev = Math.max(initStdDev, 1.0d / _factor);
vInit.setInitStdDev(initStdDev);
vInit.initRandom(_factor, _seed);
if (_parseFeatureAsInt) {
if (_p == -1) {
return new FMIntFeatureMapModel(_classification, _factor, lambda0, sigma, _seed,
min_target, max_target, _etaEstimator, vInit);
} else {
return new FMArrayModel(_classification, _factor, lambda0, sigma, _p, _seed,
min_target, max_target, _etaEstimator, vInit);
}
} else {
return new FMStringFeatureMapModel(_classification, _factor, lambda0, sigma, _seed,
min_target, max_target, _etaEstimator, vInit);
}
}
for (Feature xi : x) {
_model.updateWi(lossGrad, xi, eta);
_model.updateV(lossGrad, xi, f, sumVfx[f], eta);
forwardModel();
}
protected void forwardModel() throws HiveException {
forwardAsIntFeature(_model, _factor);
FMStringFeatureMapModel strModel = (FMStringFeatureMapModel) _model;
forwardAsStringFeature(strModel, _factor);
private void forwardAsIntFeature(@Nonnull final FactorizationMachineModel model,
final int factors) throws HiveException {
final FloatWritable[] f_Vi = HiveUtils.newFloatArray(factors, 0.f);
f_Wi.set(model.getW0());
final float[] vi = model.getV(i);
final float w = model.getW(i);
private void forwardAsStringFeature(@Nonnull final FMStringFeatureMapModel model,
final int factors) throws HiveException {
final FloatWritable[] f_Vi = HiveUtils.newFloatArray(factors, 0.f);
f_Wi.set(model.getW0());
x[j] = instantiateFeature(inputBuf);
x[j] = instantiateFeature(inputBuf);
@Nonnull
protected Feature instantiateFeature(@Nonnull final ByteBuffer input) {
return Feature.createInstance(input, _parseFeatureAsInt);
}
public void setFeature(@Nonnull String f) {
@Nonnull
public void setFeatureIndex(int i) {
public int getFeatureIndex() {
throw new UnsupportedOperationException();
}
public void setField(@Nullable String f) {
throw new UnsupportedOperationException();
}
@Nonnull
public String getField() {
for (Feature f : x) {
public static Feature[] parseFeatures(@Nonnull final Object arg,
@Nonnull final ListObjectInspector listOI, @Nullable final Feature[] probes,
final boolean asIntFeature) throws HiveException {
if (arg == null) {
if (probes != null && probes.length == length) {
if (o == null) {
if (f == null) {
if (j == length) {
static Feature parse(@Nonnull final String fv, final boolean asIntFeature) throws HiveException {
final int pos1 = fv.indexOf(':');
if (pos1 == -1) {
if (asIntFeature) {
int index = parseFeatureIndex(fv);
String lead = fv.substring(0, pos1);
int pos2 = rest.indexOf(':');
if (pos2 == -1) {
if (asIntFeature) {
int index = parseFeatureIndex(lead);
double value = parseFeatureValue(rest);
return new IntFeature(index, value);
} else {
double value = parseFeatureValue(rest);
if (asIntFeature) {
throw new HiveException("Fields are currently unsupported with IntFeatures: "
fv);
}
String index = rest.substring(0, pos2);
double value = parseFeatureValue(valueStr);
static void parse(@Nonnull final String fv, @Nonnull final Feature probe,
final boolean asIntFeature) throws HiveException {
final int pos1 = fv.indexOf(":");
if (pos1 == -1) {
if (asIntFeature) {
int index = parseFeatureIndex(fv);
probe.setFeatureIndex(index);
probe.setField(null);
probe.setFeature(fv);
String lead = fv.substring(0, pos1);
int pos2 = rest.indexOf(':');
if (pos2 == -1) {
if (asIntFeature) {
int index = parseFeatureIndex(lead);
probe.setFeatureIndex(index);
probe.value = parseFeatureValue(rest);;
} else {
probe.setField(null);
probe.setFeature(lead);
probe.value = parseFeatureValue(rest);
if (asIntFeature) {
throw new HiveException("Fields are currently unsupported with IntFeatures: "
fv);
}
String index = rest.substring(0, pos2);
probe.setField(lead);
probe.setFeature(index);
probe.value = parseFeatureValue(valueStr);
private static int parseFeatureIndex(@Nonnull final String indexStr) throws HiveException {
final int index;
try {
index = Integer.parseInt(indexStr);
} catch (NumberFormatException e) {
}
if (index < 0) {
}
return index;
}
private static double parseFeatureValue(@Nonnull final String value) throws HiveException {
try {
return Double.parseDouble(value);
} catch (NumberFormatException e) {
}
}
if (asIntFeature) {
public int getFeatureIndex() {
public void setFeatureIndex(int i) {
import javax.annotation.Nullable;
public class StringFeature extends Feature {
@Nonnull
protected String feature;
@Nullable
protected String field;
this(feature, null, value);
}
StringFeature(int feature, double value) {
this(String.valueOf(feature), null, value);
}
public StringFeature(@Nonnull String feature, @Nullable String field, double value) {
this.field = field;
public void setFeature(@Nonnull String feature) {
this.feature = feature;
}
@Override
public void setField(@Nullable String f) {
this.field = f;
}
@Override
public String getField() {
if (field == null) {
}
return field;
}
@Override
NIOUtils.putString(field, dst);
this.field = NIOUtils.getString(src);
if (field == null) {
} else {
}
}
@Nonnull
public static String getFeatureOfField(@Nonnull Feature x, @Nonnull String yField) {
import hivemall.utils.codec.Base91;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.Base91;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.Base91;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.Base91;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.DeflateCodec;
import hivemall.utils.codec.Base91;
import hivemall.utils.codec.Base91;
package hivemall.utils.codec;
import java.io.InputStream;
public static void encode(@Nonnull final InputStream in, @Nonnull final OutputStream out)
throws IOException {
int ebq = 0;
int en = 0;
int b;
while ((b = in.read()) != -1) {
ebq |= (b & 255) << en;
if (en > 13) {
if (ev > 88) {
ebq >>= 13;
en -= 13;
} else {
ebq >>= 14;
en -= 14;
}
out.write(ENCODING_TABLE[ev % BASE]);
out.write(ENCODING_TABLE[ev / BASE]);
}
}
if (en > 0) {
out.write(ENCODING_TABLE[ebq % BASE]);
if (en > 7 || ebq > 90) {
out.write(ENCODING_TABLE[ebq / BASE]);
}
}
}
} while (dn >= 8);
public static void decode(@Nonnull final InputStream in, @Nonnull final OutputStream out)
throws IOException {
int dbq = 0;
int dn = 0;
int dv = -1;
int b;
while ((b = in.read()) != -1) {
if (DECODING_TABLE[b] == -1) {
continue;
}
if (dv == -1) {
dv = DECODING_TABLE[b];
} else {
dbq |= dv << dn;
do {
out.write((byte) dbq);
dbq >>= 8;
dn -= 8;
} while (dn >= 8);
dv = -1;
}
}
if (dv != -1) {
out.write((byte) (dbq | dv << dn));
}
}
package hivemall.utils.codec;
package hivemall.utils.codec;
import java.io.Externalizable;
import java.io.IOException;
import java.io.ObjectInput;
import java.io.ObjectOutput;
import javax.annotation.Nonnull;
public final class OpenHashTable<K, V> implements Externalizable {
if (size < 1) {
public OpenHashTable(@Nonnull K[] keys, @Nonnull V[] values, @Nonnull byte[] states, int used) {
this._used = used;
this._threshold = keys.length;
this._keys = keys;
this._values = values;
this._states = states;
}
public Object[] getKeys() {
return _keys;
}
public Object[] getValues() {
return _values;
}
public byte[] getStates() {
return _states;
}
if (i < 0) {
if (expanded) {
if (states[keyIdx] == FULL) {
if (equals(keys[keyIdx], key)) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && equals(keys[keyIdx], key)) {
if (stat == FREE) {
if (stat == REMOVED && equals(_keys[index], key)) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && equals(keys[keyIdx], key)) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && equals(keys[keyIdx], key)) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && equals(keys[keyIdx], key)) {
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
if (states[keyIdx] == FULL && equals(keys[keyIdx], key)) {
while (i.next() != -1) {
if (i.hasNext()) {
if (newCapacity <= oldCapacity) {
if (_states[i] == FULL) {
while (newStates[keyIdx] != FREE) {
if (keyIdx < 0) {
while (index < _keys.length && _states[index] != FULL) {
if (!hasNext()) {
if (lastEntry == -1) {
if (lastEntry == -1) {
@Override
public void writeExternal(ObjectOutput out) throws IOException {
out.writeFloat(_loadFactor);
out.writeFloat(_growFactor);
out.writeInt(_used);
final int size = _keys.length;
out.writeInt(size);
out.writeObject(_keys[i]);
out.writeObject(_values[i]);
out.writeByte(_states[i]);
}
}
@SuppressWarnings("unchecked")
@Override
public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {
this._loadFactor = in.readFloat();
this._growFactor = in.readFloat();
this._used = in.readInt();
final int size = in.readInt();
final Object[] keys = new Object[size];
final Object[] values = new Object[size];
final byte[] states = new byte[size];
keys[i] = in.readObject();
values[i] = in.readObject();
states[i] = in.readByte();
}
this._threshold = size;
this._keys = (K[]) keys;
this._values = (V[]) values;
this._states = states;
}
}
import java.util.UUID;
if (codec == null) {
if (decompressor != null) {
if (ctx == null) {
if (conf == null) {
if (jobId == null) {
if (jobId == null) {
if (queryId != null) {
if (taskidStr == null) {
if (ctx == null) {
if (jobconf == null) {
if (taskid == -1) {
if (taskid == -1) {
throw new IllegalStateException(
"Both mapred.task.partition and mapreduce.task.partition are not set: "
toString(jobconf));
public static String getUniqueTaskIdString() {
MapredContext ctx = MapredContextAccessor.get();
if (ctx != null) {
JobConf jobconf = ctx.getJobConf();
if (jobconf != null) {
int taskid = jobconf.getInt("mapred.task.partition", -1);
if (taskid == -1) {
taskid = jobconf.getInt("mapreduce.task.partition", -1);
}
if (taskid != -1) {
return String.valueOf(taskid);
}
}
}
return getUUID();
}
public synchronized static String getUUID() {
return UUID.randomUUID().toString();
}
if (!hasNext) {
if (k == null) {
if (regexKey == null || k.matches(regexKey)) {
if (hasNext) {
} while (hasNext);
import java.io.DataInput;
import java.io.DataInputStream;
import java.io.DataOutput;
import java.io.DataOutputStream;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import javax.annotation.Nullable;
@Nonnull
public static byte[] toByteArray(@Nonnull final InputStream input) throws IOException {
FastByteArrayOutputStream output = new FastByteArrayOutputStream();
copy(input, output);
return output.toByteArray();
}
public static void writeInt(final int v, final OutputStream out) throws IOException {
out.write((v >>> 24) & 0xFF);
out.write((v >>> 16) & 0xFF);
out.write((v >>> 8) & 0xFF);
out.write((v >>> 0) & 0xFF);
}
public static int readInt(final InputStream in) throws IOException {
final int ch1 = in.read();
final int ch2 = in.read();
final int ch3 = in.read();
final int ch4 = in.read();
}
public static void writeChar(final char v, final OutputStream out) throws IOException {
out.write(0xff & (v >> 8));
out.write(0xff & v);
}
public static void writeChar(final char v, final FastByteArrayOutputStream out) {
out.write(0xff & (v >> 8));
out.write(0xff & v);
}
public static char readChar(final InputStream in) throws IOException {
final int a = in.read();
final int b = in.read();
return (char) ((a << 8) | (b & 0xff));
}
public static void writeBytes(@Nonnull final byte[] src, @Nonnull final OutputStream dst)
throws IOException {
if (src != null) {
dst.write(src);
}
}
public static void writeBytes(@Nonnull final byte[] src, final int off, final int len,
@Nonnull final OutputStream dst) throws IOException {
if (src != null) {
dst.write(src, off, len);
}
}
public static void writeString(@Nullable final String s, final ObjectOutputStream out)
throws IOException {
writeString(s, (DataOutput) out);
}
public static void writeString(@Nullable final String s, final DataOutputStream out)
throws IOException {
writeString(s, (DataOutput) out);
}
public static void writeString(@Nullable final String s, final DataOutput out)
throws IOException {
if (s == null) {
out.writeInt(-1);
return;
}
final int len = s.length();
out.writeInt(len);
int v = s.charAt(i);
out.writeChar(v);
}
}
public static void writeString(@Nullable final String s, final OutputStream out)
throws IOException {
if (s == null) {
writeInt(-1, out);
return;
}
final int len = s.length();
writeInt(len, out);
char c = s.charAt(i);
writeChar(c, out);
}
}
@Nullable
public static String readString(@Nonnull final ObjectInputStream in) throws IOException {
return readString((DataInput) in);
}
@Nullable
public static String readString(@Nonnull final DataInputStream in) throws IOException {
return readString((DataInput) in);
}
@Nullable
public static String readString(@Nonnull final DataInput in) throws IOException {
final int len = in.readInt();
if (len == -1) {
return null;
}
final char[] ch = new char[len];
ch[i] = in.readChar();
}
return new String(ch);
}
@Nullable
public static String readString(@Nonnull final InputStream in) throws IOException {
final int len = readInt(in);
if (len == -1) {
return null;
}
final char[] ch = new char[len];
ch[i] = readChar(in);
}
return new String(ch);
}
public static void writeFloats(@Nonnull final float[] floats, @Nonnull final DataOutput out)
throws IOException {
final int size = floats.length;
out.writeInt(size);
out.writeFloat(floats[i]);
}
}
@Nonnull
public static float[] readFloats(@Nonnull final DataInput in) throws IOException {
final int size = in.readInt();
final float[] floats = new float[size];
floats[i] = in.readFloat();
}
return floats;
}
import javax.annotation.Nullable;
public static int requiredBytes(@Nullable final String s) {
if (s == null) {
return INT_BYTES;
}
public static void putString(@Nullable final String s, @Nonnull final ByteBuffer dst) {
if (s == null) {
dst.putInt(-1);
return;
}
@Nullable
if (size == -1) {
return null;
}
public static int read(@Nonnull final FileChannel src, @Nonnull final ByteBuffer dst,
@Nonnegative final long position) throws IOException {
while (dst.remaining() > 0) {
if (n == -1) {
while (dst.remaining() > 0) {
public static int writeFully(@Nonnull final FileChannel dst, @Nonnull final ByteBuffer src,
@Nonnegative final long position) throws IOException {
while (src.remaining() > 0) {
while (src.remaining() > 0) {
import java.io.Externalizable;
public static byte[] toBytes(@Nonnull final Externalizable obj) throws IOException {
FastByteArrayOutputStream bos = new FastByteArrayOutputStream();
toStream(obj, bos);
return bos.toByteArray();
}
public static byte[] toCompressedBytes(@Nonnull final Object obj) throws IOException {
toStream(obj, dos);
} finally {
IOUtils.closeQuietly(dos);
}
return bos.toByteArray_clear();
}
public static byte[] toCompressedBytes(@Nonnull final Externalizable obj) throws IOException {
FastMultiByteArrayOutputStream bos = new FastMultiByteArrayOutputStream();
final DeflaterOutputStream dos = new DeflaterOutputStream(bos);
try {
toStream(obj, dos);
public static void toStream(@Nonnull final Externalizable obj, @Nonnull final OutputStream out)
throws IOException {
ObjectOutputStream oos = new ObjectOutputStream(out);
obj.writeExternal(oos);
oos.flush();
oos.close();
}
public static <T> T readObject(@Nonnull final byte[] obj) throws IOException,
ClassNotFoundException {
public static void readObject(@Nonnull final byte[] src, @Nonnull final Externalizable dst)
throws IOException, ClassNotFoundException {
readObject(new FastByteArrayInputStream(src), dst);
}
public static <T> T readObject(@Nonnull final InputStream is) throws IOException,
ClassNotFoundException {
public static void readObject(@Nonnull final InputStream is, @Nonnull final Externalizable dst)
throws IOException, ClassNotFoundException {
ObjectInputStream ois = new ObjectInputStream(is);
dst.readExternal(ois);
}
public static <T> T readCompressedObject(@Nonnull final byte[] obj) throws IOException,
public static void readCompressedObject(@Nonnull final byte[] src,
@Nonnull final Externalizable dst) throws IOException, ClassNotFoundException {
FastByteArrayInputStream bis = new FastByteArrayInputStream(src);
final InflaterInputStream iis = new InflaterInputStream(bis);
try {
readObject(iis, dst);
} finally {
IOUtils.closeQuietly(iis);
}
}
public static void readCompressedObject(@Nonnull final byte[] src, final int offset,
final int length, @Nonnull final Externalizable dst) throws IOException,
ClassNotFoundException {
FastByteArrayInputStream bis = new FastByteArrayInputStream(src, offset, length);
final InflaterInputStream iis = new InflaterInputStream(bis);
try {
readObject(iis, dst);
} finally {
IOUtils.closeQuietly(iis);
}
}
public static int toUnsignedInt(final byte x) {
return ((int) x) & 0xff;
}
if (s == null) {
if (s == null) {
if (s == null) {
if (s == null) {
if (s == null) {
if (s == null) {
import hivemall.model.DenseModel;
import hivemall.model.PredictionModel;
import hivemall.model.SpaceEfficientDenseModel;
import hivemall.model.SparseModel;
import hivemall.model.SynchronizedModelWrapper;
import hivemall.model.WeightValue;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.WeightValue.WeightValueParamsF2;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionModel;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.PredictionResult;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.Margin;
import hivemall.model.PredictionModel;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.Margin;
import hivemall.model.PredictionModel;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.Margin;
import hivemall.model.PredictionModel;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.Margin;
import hivemall.model.FeatureValue;
import hivemall.model.PredictionResult;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.Margin;
import hivemall.model.PredictionModel;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.FeatureValue;
import hivemall.model.ModelUpdateHandler;
package hivemall.model;
package hivemall.model;
import hivemall.model.WeightValue.WeightValueParamsF1;
import hivemall.model.WeightValue.WeightValueParamsF2;
import hivemall.model.WeightValue.WeightValueWithCovar;
package hivemall.model;
package hivemall.model;
package hivemall.model;
package hivemall.model;
package hivemall.model;
package hivemall.model;
package hivemall.model;
import hivemall.model.WeightValue.WeightValueParamsF1;
import hivemall.model.WeightValue.WeightValueParamsF2;
import hivemall.model.WeightValue.WeightValueWithCovar;
package hivemall.model;
import hivemall.model.WeightValueWithClock.WeightValueParamsF1Clock;
import hivemall.model.WeightValueWithClock.WeightValueParamsF2Clock;
import hivemall.model.WeightValueWithClock.WeightValueWithCovarClock;
package hivemall.model;
package hivemall.model;
package hivemall.model;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.WeightValue.WeightValueParamsF2;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.WeightValue.WeightValueParamsF1;
import hivemall.model.FeatureValue;
import hivemall.model.PredictionResult;
import hivemall.model.FeatureValue;
import hivemall.model.IWeightValue;
import hivemall.model.PredictionModel;
import hivemall.model.PredictionResult;
import hivemall.model.WeightValue;
import hivemall.model.WeightValue.WeightValueWithCovar;
import hivemall.model.FeatureValue;
public float getW(@Nonnull final Feature x) {
protected void setW(@Nonnull final Feature x, final float nextWi) {
public float getV(@Nonnull final Feature x, @Nonnull final String yField, final int f) {
protected void setV(@Nonnull final Feature x, @Nonnull final String yField, final int f, final float nextVif) {
protected Entry getEntry(@Nonnull final Feature x, @Nonnull final String yField) {
public final class FMStringFeatureMapModel extends FactorizationMachineModel {
final double[][][] sumVfX(@Nonnull final Feature[] x, @Nonnull final List<String> fieldList) {
final double[][] ret_i = ret[i];
final double[] ret_if = ret_i[fieldIndex];
final String field = fieldList.get(fieldIndex);
ret_if[f] = sumVfX(x, i, field, f);
final Feature xi = x[i];
final String xiFeature = xi.getFeature();
final double xiValue = xi.getValue();
final String xiField = xi.getField();
float Vjf = getV(e, xiField, f);
protected Entry newEntry(final float[] V) {
public void train(@Nonnull final Feature[] x, final double y,
final boolean adaptiveRegularization) throws HiveException {
protected void trainTheta(@Nonnull final Feature[] x, final double y) throws HiveException {
final Feature x_i = x[i];
final double[][] sumVf = sumVfx[i];
final double[] sumV = sumVf[fieldIndex];
_model.updateV(lossGrad, x_i, field, f, sumV[f], _t);
private List<String> getFieldList(@Nonnull final Feature[] x) {
protected StringFeature instantiateFeature(@Nonnull final ByteBuffer input) {
public static String getFeatureOfField(@Nonnull final Feature x, @Nonnull final String yField) {
public boolean containsKey(final K key) {
public V get(final K key) {
public V put(final K key, final V value) {
protected int findKey(final K key) {
public V remove(final K key) {
public abstract float getV(@Nonnull Feature x, @Nonnull String yField, int f);
protected final double predict(@Nonnull final Feature[] x) {
float currentV = getV(x, field, f);
protected final Entry newEntry(final float[] V) {
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "mhash",
value = "_FUNC_(string word) returns a murmurhash3 INT value starting from 1")
@UDFType(deterministic = true, stateful = false)
public final class MurmurHash3UDF extends UDF {
public IntWritable evaluate(final String word) throws UDFArgumentException {
public IntWritable evaluate(final String word, final int numFeatures)
throws UDFArgumentException {
if (word == null) {
if (r < 0) {
public IntWritable evaluate(final List<String> words) throws UDFArgumentException {
public IntWritable evaluate(final List<String> words, final int numFeatures)
throws UDFArgumentException {
if (words == null) {
if (size == 0) {
return new IntWritable(1);
final int len = a.length;
final float basev = maxInitValue / len;
float v = rand.nextFloat() * basev;
int h = mhash(word, numFeatures);
return new IntWritable(h);
public static int mhash(final String word) {
return mhash(word, MurmurHash3.DEFAULT_NUM_FEATURES);
}
public static int mhash(final String word, final int numFeatures) {
int r = MurmurHash3.murmurhash3_x86_32(word, 0, word.length(), 0x9747b28c) % numFeatures;
if (r < 0) {
}
}
return get(cl, 0.1f);
}
@Nonnull
public static EtaEstimator get(@Nullable CommandLine cl, float defaultEta0)
throws UDFArgumentException {
return new InvscalingEtaEstimator(defaultEta0, 0.1d);
float eta = Primitives.parseFloat(cl.getOptionValue("eta"), 0.3f);
return new AdjustingEtaEstimator(eta);
float eta0 = Primitives.parseFloat(cl.getOptionValue("eta0"), defaultEta0);
opts.addOption("eta0", true, "The initial learning rate [default 0.05]");
public static final int INT_BYTES = Integer.SIZE / Byte.SIZE;
public static final int DOUBLE_BYTES = Double.SIZE / Byte.SIZE;
import hivemall.utils.collections.DoubleArray3D;
import javax.annotation.Nullable;
@Nonnull
final DoubleArray3D sumVfX(@Nonnull final Feature[] x, @Nonnull final List<String> fieldList,
@Nullable DoubleArray3D cached) {
final int fieldSize = fieldList.size();
final int factors = _factor;
final DoubleArray3D mdarray;
if (cached == null) {
mdarray = new DoubleArray3D();
mdarray.setSanityCheck(false);
} else {
mdarray = cached;
}
mdarray.configure(xSize, fieldSize, factors);
double val = sumVfX(x, i, field, f);
mdarray.set(i, fieldIndex, f, val);
return mdarray;
import hivemall.utils.collections.DoubleArray3D;
@Nullable
private DoubleArray3D _sumVfX;
final DoubleArray3D sumVfX = _model.sumVfX(x, fieldList, _sumVfX);
double sumViX = sumVfX.get(i, fieldIndex, f);
_model.updateV(lossGrad, x_i, field, f, sumViX, _t);
sumVfX.clear();
this._sumVfX = sumVfX;
import hivemall.utils.collections.IntOpenHashTable;
private IntOpenHashTable<Entry> _map;
public FFMPredictionModel(@Nonnull IntOpenHashTable<Entry> map, double w0, int factor) {
public float getW1(@Nonnull final Feature x) {
int j = StringFeature.toIntFeature(x);
Entry entry = _map.get(j);
public float[] getV(@Nonnull final Feature x, @Nonnull final String field) {
int j = StringFeature.toIntFeature(x, field);
final int[] keys = _map.getKeys();
out.writeInt(keys[i]);
final int[] keys = new int[size];
keys[i] = in.readInt();
this._map = new IntOpenHashTable<Entry>(keys, values, states, used);
import hivemall.utils.collections.IntOpenHashTable;
private final IntOpenHashTable<Entry> _map;
this._map = new IntOpenHashTable<FFMStringFeatureMapModel.Entry>(DEFAULT_MAPSIZE);
int j = StringFeature.toIntFeature(x);
final int j = StringFeature.toIntFeature(x);
final int j = StringFeature.toIntFeature(x, yField);
protected void setV(@Nonnull final Feature x, @Nonnull final String yField, final int f,
final float nextVif) {
final int j = StringFeature.toIntFeature(x, yField);
int j = StringFeature.toIntFeature(x, yField);
import hivemall.utils.hashing.MurmurHash3;
public static int toIntFeature(@Nonnull final Feature x) {
String f = x.getFeature();
return MurmurHash3.murmurhash3(f);
}
public static int toIntFeature(@Nonnull final Feature x, @Nonnull final String yField) {
return MurmurHash3.murmurhash3(f);
final int h = murmurhash3_x86_32(data, 0, data.length(), 0x9747b28c);
if (r < 0) {
if (r < 0) {
public static int murmurhash3_x86_32(final CharSequence data, final int offset, final int len,
final int seed) {
while (pos < end) {
if (code < 0x80) {
} else if (code < 0x800) {
} else if (code < 0xD800 || code > 0xDFFF || pos >= end) {
if (shift >= 32) {
if (shift != 0) {
if (shift > 0) {
Feature[] x = Feature.parseFFMFeatures(arg2, _featureListOI, _probes);
final int iField = ei.getField();
final int jField = ej.getField();
int j = x.getFeatureIndex();
public float[] getV(@Nonnull final Feature x, @Nonnull final int yField) {
int j = Feature.toIntFeature(x, yField);
int j = x.getFeatureIndex();
final int j = x.getFeatureIndex();
public float getV(@Nonnull final Feature x, @Nonnull final int yField, final int f) {
final int j = Feature.toIntFeature(x, yField);
protected void setV(@Nonnull final Feature x, @Nonnull final int yField, final int f,
final int j = Feature.toIntFeature(x, yField);
protected Entry getEntry(@Nonnull final Feature x, @Nonnull final int yField) {
int j = Feature.toIntFeature(x, yField);
protected final String varDump(@Nonnull final Feature[] x) {
final void updateV(final double dloss, @Nonnull final Feature x, final int f, final double sumViX,
Feature[] x = parseFeatures(args[0]);
@Nullable
protected Feature[] parseFeatures(@Nonnull final Object arg) throws HiveException {
return Feature.parseFeatures(arg, _xOI, _probes, _parseFeatureAsInt);
}
import hivemall.utils.hashing.MurmurHash3;
import hivemall.utils.lang.NumberUtils;
public static final int NUM_FIELD = 1024;
public short getField() {
public void setField(short field) {
f = parseFeature(s, asIntFeature);
parseFeature(s, f, asIntFeature);
}
ary[j] = f;
}
if (j == length) {
return ary;
} else {
Feature[] dst = new Feature[j];
System.arraycopy(ary, 0, dst, 0, j);
return dst;
}
}
public static Feature[] parseFFMFeatures(@Nonnull final Object arg,
@Nonnull final ListObjectInspector listOI, @Nullable final Feature[] probes)
throws HiveException {
if (arg == null) {
return null;
}
final int length = listOI.getListLength(arg);
final Feature[] ary;
if (probes != null && probes.length == length) {
ary = probes;
} else {
ary = new Feature[length];
}
int j = 0;
Object o = listOI.getListElement(arg, i);
if (o == null) {
continue;
}
String s = o.toString();
Feature f = ary[j];
if (f == null) {
f = parseFFMFeature(s);
} else {
parseFFMFeature(s, f);
static Feature parseFeature(@Nonnull final String fv, final boolean asIntFeature)
throws HiveException {
final String indexStr = fv.substring(0, pos1);
if (asIntFeature) {
int index = parseFeatureIndex(indexStr);
return new IntFeature(index, value);
} else {
double value = parseFeatureValue(valueStr);
@Nonnull
static IntFeature parseFFMFeature(@Nonnull final String fv) throws HiveException {
final int pos1 = fv.indexOf(':');
if (pos1 == -1) {
}
final String lead = fv.substring(0, pos1);
final int pos2 = rest.indexOf(':');
if (pos2 == -1) {
}
final String indexStr = rest.substring(0, pos2);
final int index;
final short field;
if (NumberUtils.isDigits(indexStr) && NumberUtils.isDigits(lead)) {
index = parseFeatureIndex(indexStr);
if (index >= MurmurHash3.DEFAULT_NUM_FEATURES) {
throw new HiveException("Feature index MUST be less than "
}
field = parseField(lead);
} else {
index = MurmurHash3.murmurhash3(indexStr);
field = (short) MurmurHash3.murmurhash3_x86_32(lead, NUM_FIELD);
}
double value = parseFeatureValue(valueStr);
return new IntFeature(index, field, value);
}
static void parseFeature(@Nonnull final String fv, @Nonnull final Feature probe,
final String indexStr = fv.substring(0, pos1);
if (asIntFeature) {
int index = parseFeatureIndex(indexStr);
probe.setFeatureIndex(index);
probe.value = parseFeatureValue(valueStr);;
probe.setFeature(indexStr);
static void parseFFMFeature(@Nonnull final String fv, @Nonnull final Feature probe)
throws HiveException {
final int pos1 = fv.indexOf(":");
if (pos1 == -1) {
}
final String lead = fv.substring(0, pos1);
final int pos2 = rest.indexOf(':');
if (pos2 == -1) {
}
String indexStr = rest.substring(0, pos2);
final int index;
final short field;
if (NumberUtils.isDigits(indexStr) && NumberUtils.isDigits(lead)) {
index = parseFeatureIndex(indexStr);
if (index >= MurmurHash3.DEFAULT_NUM_FEATURES) {
throw new HiveException("Feature index MUST be less than "
}
field = parseField(lead);
} else {
index = MurmurHash3.murmurhash3(indexStr);
field = (short) MurmurHash3.murmurhash3_x86_32(lead, NUM_FIELD);
}
probe.setField(field);
probe.setFeatureIndex(index);
probe.value = parseFeatureValue(valueStr);
}
private static short parseField(@Nonnull final String fieldStr) throws HiveException {
final short field;
try {
field = Short.parseShort(fieldStr);
} catch (NumberFormatException e) {
}
if (field < 0 || field >= NUM_FIELD) {
}
return field;
}
public static int toIntFeature(@Nonnull final Feature x, final int yField) {
int index = x.getFeatureIndex();
}
import hivemall.utils.collections.IntArrayList;
public abstract float getV(@Nonnull Feature x, @Nonnull int yField, int f);
protected abstract void setV(@Nonnull Feature x, @Nonnull int yField, int f, float nextVif);
final int iField = ei.getField();
final int jField = ej.getField();
void updateV(final double dloss, @Nonnull final Feature x, @Nonnull final int yField,
final double h = Xi * sumViX;
final float gradV = (float) (dloss * h);
final float lambdaVf = getLambdaV(f);
final float currentV = getV(x, yField, f);
final float eta = etaV(t, x, yField, gradV);
setV(x, yField, f, nextV);
protected final float etaV(final long t, @Nonnull final Feature x, @Nonnull final int yField,
Entry theta = getEntry(x, yField);
final DoubleArray3D sumVfX(@Nonnull final Feature[] x, @Nonnull final IntArrayList fieldList,
final int yField = fieldList.get(fieldIndex);
double val = sumVfX(x, i, yField, f);
private double sumVfX(@Nonnull final Feature[] x, final int i, @Nonnull final int yField,
final int xiFeature = xi.getFeatureIndex();
final int xiField = xi.getField();
protected abstract Entry getEntry(@Nonnull Feature x, @Nonnull int yField);
static final class AdaGradEntry extends Entry {
import hivemall.utils.collections.IntArrayList;
private IntArrayList _fieldList;
this._fieldList = new IntArrayList();
protected Feature[] parseFeatures(@Nonnull final Object arg) throws HiveException {
return Feature.parseFFMFeatures(arg, _xOI, _probes);
}
@Override
final IntArrayList fieldList = getFieldList(x);
final int yField = fieldList.get(fieldIndex);
_model.updateV(lossGrad, x_i, yField, f, sumViX, _t);
private IntArrayList getFieldList(@Nonnull final Feature[] x) {
int field = e.getField();
private short field;
this(index, (short) -1, value);
}
public IntFeature(int index, short field, double value) {
this.field = field;
public short getField() {
return field;
}
@Override
public void setField(short field) {
this.field = field;
}
@Override
dst.putShort(field);
this.field = src.getShort();
if (field == -1) {
} else {
}
super(value);
this.feature = feature;
this(String.valueOf(feature), value);
import hivemall.fm.Feature;
import hivemall.utils.hashing.MurmurHash3;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
@Description(
name = "ffm_features",
value = "_FUNC_(const boolean mhash=true, const array<string> featureNames, feature1, feature2, ..)"
" - Takes categroical variables and returns a feature vector array<string>"
" in a libffm format <field>:<index>:<value>")
private boolean mhash;
if (numArgOIs < 3) {
throw new UDFArgumentLengthException(
this.mhash = HiveUtils.getConstBoolean(argOIs[0]);
this.featureNames = HiveUtils.getConstStringArray(argOIs[1]);
int numFeatures = numArgOIs - 2;
final int size = arguments.length - 2;
final String featureName = featureNames[i];
final Text f;
if (mhash) {
int field = MurmurHash3.murmurhash3(featureNames[i], Feature.NUM_FIELD);
int index = MurmurHash3.murmurhash3(feature);
} else {
}
public static boolean isDigits(String str) {
if (str == null || str.length() == 0) {
return false;
}
if (!Character.isDigit(str.charAt(i))) {
return false;
}
}
return true;
}
public static final int FEATURE_INDEX_RIGHT_OPEN_BOUND = MurmurHash3.DEFAULT_NUM_FEATURES
NUM_FIELD;
final int index;
if (NumberUtils.isDigits(lead)) {
index = parseFeatureIndex(lead);
if (index < 0 || index >= NUM_FIELD) {
NUM_FIELD);
}
} else {
index = MurmurHash3.murmurhash3_x86_32(lead, NUM_FIELD);
}
short field = (short) index;
double value = parseFeatureValue(rest);
return new IntFeature(index, field, value);
if (index >= FEATURE_INDEX_RIGHT_OPEN_BOUND) {
final int index;
if (NumberUtils.isDigits(lead)) {
index = parseFeatureIndex(lead);
if (index < 0 || index >= NUM_FIELD) {
NUM_FIELD);
}
} else {
index = MurmurHash3.murmurhash3_x86_32(lead, NUM_FIELD);
}
short field = (short) index;
probe.setField(field);
probe.setFeatureIndex(index);
probe.value = parseFeatureValue(rest);
return;
String indexStr = rest.substring(0, pos2);
if (index >= FEATURE_INDEX_RIGHT_OPEN_BOUND) {
@Description(name = "ffm_features",
value = "_FUNC_(boolean mhash, const array<string> featureNames, feature1, feature2, ..)"
final StringBuilder builder = new StringBuilder(128);
final String fv;
fv = builder.append(field).append(':').append(index).append(":1").toString();
builder.setLength(0);
fv = builder.append(featureName)
.append(':')
.append(feature)
.append(":1")
.toString();
builder.setLength(0);
result.add(new Text(fv));
private static final int DEFAULT_MAPSIZE = 65536;
public static final int NUM_FIELDS = 1024;
public static final int NUM_FEATURES = 1048576;
if (index < 0 || index >= NUM_FIELDS) {
NUM_FIELDS);
index = MurmurHash3.murmurhash3_x86_32(lead, NUM_FIELDS);
field = (short) MurmurHash3.murmurhash3_x86_32(lead, NUM_FIELDS);
if (index < 0 || index >= NUM_FIELDS) {
NUM_FIELDS);
index = MurmurHash3.murmurhash3_x86_32(lead, NUM_FIELDS);
field = (short) MurmurHash3.murmurhash3_x86_32(lead, NUM_FIELDS);
if (field < 0 || field >= NUM_FIELDS) {
int field = MurmurHash3.murmurhash3(featureNames[i], Feature.NUM_FIELDS);
protected IntFeature instantiateFeature(@Nonnull final ByteBuffer input) {
return new IntFeature(input);
if (_parseFeatureAsInt) {
return new IntFeature(input);
} else {
return new StringFeature(input);
}
public static final int NUM_FEATURES = 2097152;
index = MurmurHash3.murmurhash3(lead, NUM_FIELDS);
field = (short) MurmurHash3.murmurhash3(lead, NUM_FIELDS);
index = MurmurHash3.murmurhash3(lead, NUM_FIELDS);
field = (short) MurmurHash3.murmurhash3(lead, NUM_FIELDS);
entry = newEntry(Vf);
entry.W = nextWi;
entry = newEntry(V);
final int j = Feature.toIntFeature(x, yField);
final Entry entry = _map.get(j);
if (entry == null) {
yField);
}
return entry;
import javax.annotation.CheckForNull;
@CheckForNull
import hivemall.utils.lang.NumberUtils;
@Override
void updateWi(final double dloss, @Nonnull final Feature x, final float eta) {
final double Xi = x.getValue();
float gradWi = (float) (dloss * Xi);
final Entry theta = getEntry(x);
float wi = theta.W;
if (!NumberUtils.isFinite(nextWi)) {
}
theta.W = nextWi;
}
protected Entry getEntry(@Nonnull final Feature x) {
final int j = x.getFeatureIndex();
Entry entry = _map.get(j);
if (entry == null) {
float[] V = initV();
entry = newEntry(V);
_map.put(j, entry);
}
return entry;
}
@Override
Entry entry = _map.get(j);
float[] V = initV();
entry = newEntry(V);
_map.put(j, entry);
void updateWi(final double dloss, @Nonnull final Feature x, final float eta) {
final void updateV(final double dloss, @Nonnull final Feature x, final int f,
final double sumViX, final float eta) {
final Entry theta = getEntry(x, yField);
final float currentV = getV(theta, f);
final float eta = etaV(theta, t, gradV);
setV(theta, f, nextV);
private static float getV(@Nonnull final Entry theta, final int f) {
return theta.Vf[f];
}
private static float setV(@Nonnull final Entry theta, final int f, final float value) {
return theta.Vf[f] = value;
}
protected final float etaV(@Nonnull final Entry theta, final long t, final float grad) {
@Nonnull
protected abstract Entry getEntry(@Nonnull Feature x);
@Nonnull
return get(cl, 0.1f);
}
@Nonnull
public static EtaEstimator get(@Nullable CommandLine cl, float defaultEta0)
throws UDFArgumentException {
return new InvscalingEtaEstimator(defaultEta0, 0.1d);
float eta = Primitives.parseFloat(cl.getOptionValue("eta"), 0.3f);
return new AdjustingEtaEstimator(eta);
float eta0 = Primitives.parseFloat(cl.getOptionValue("eta0"), defaultEta0);
Feature[] x = Feature.parseFFMFeatures(arg2, _featureListOI, _probes);
final int iField = ei.getField();
final int jField = ej.getField();
import hivemall.utils.collections.IntOpenHashTable;
private IntOpenHashTable<Entry> _map;
public FFMPredictionModel(@Nonnull IntOpenHashTable<Entry> map, double w0, int factor) {
public float getW1(@Nonnull final Feature x) {
int j = x.getFeatureIndex();
Entry entry = _map.get(j);
public float[] getV(@Nonnull final Feature x, @Nonnull final int yField) {
int j = Feature.toIntFeature(x, yField);
final int[] keys = _map.getKeys();
out.writeInt(keys[i]);
final int[] keys = new int[size];
keys[i] = in.readInt();
this._map = new IntOpenHashTable<Entry>(keys, values, states, used);
import hivemall.utils.collections.IntOpenHashTable;
import hivemall.utils.lang.NumberUtils;
private static final int DEFAULT_MAPSIZE = 65536;
private final IntOpenHashTable<Entry> _map;
this._map = new IntOpenHashTable<FFMStringFeatureMapModel.Entry>(DEFAULT_MAPSIZE);
int j = x.getFeatureIndex();
final int j = x.getFeatureIndex();
entry = newEntry(Vf);
entry.W = nextWi;
@Override
void updateWi(final double dloss, @Nonnull final Feature x, final float eta) {
final double Xi = x.getValue();
float gradWi = (float) (dloss * Xi);
final Entry theta = getEntry(x);
float wi = theta.W;
if (!NumberUtils.isFinite(nextWi)) {
}
theta.W = nextWi;
}
public float getV(@Nonnull final Feature x, @Nonnull final int yField, final int f) {
final int j = Feature.toIntFeature(x, yField);
protected void setV(@Nonnull final Feature x, @Nonnull final int yField, final int f,
final float nextVif) {
final int j = Feature.toIntFeature(x, yField);
entry = newEntry(V);
protected Entry getEntry(@Nonnull final Feature x) {
final int j = x.getFeatureIndex();
Entry entry = _map.get(j);
if (entry == null) {
float[] V = initV();
entry = newEntry(V);
_map.put(j, entry);
}
return entry;
}
@Override
protected Entry getEntry(@Nonnull final Feature x, @Nonnull final int yField) {
final int j = Feature.toIntFeature(x, yField);
Entry entry = _map.get(j);
if (entry == null) {
float[] V = initV();
entry = newEntry(V);
_map.put(j, entry);
}
return entry;
protected final String varDump(@Nonnull final Feature[] x) {
void updateWi(final double dloss, @Nonnull final Feature x, final float eta) {
final void updateV(final double dloss, @Nonnull final Feature x, final int f,
final double sumViX, final float eta) {
final int len = a.length;
final float basev = maxInitValue / len;
float v = rand.nextFloat() * basev;
opts.addOption("eta0", true, "The initial learning rate [default 0.05]");
Feature[] x = parseFeatures(args[0]);
@Nullable
protected Feature[] parseFeatures(@Nonnull final Object arg) throws HiveException {
return Feature.parseFeatures(arg, _xOI, _probes, _parseFeatureAsInt);
}
if (_parseFeatureAsInt) {
return new IntFeature(input);
} else {
return new StringFeature(input);
}
import hivemall.utils.hashing.MurmurHash3;
import hivemall.utils.lang.NumberUtils;
public static final int NUM_FIELDS = 1024;
public static final int NUM_FEATURES = 2097152;
public short getField() {
public void setField(short field) {
f = parseFeature(s, asIntFeature);
parseFeature(s, f, asIntFeature);
}
ary[j] = f;
}
if (j == length) {
return ary;
} else {
Feature[] dst = new Feature[j];
System.arraycopy(ary, 0, dst, 0, j);
return dst;
}
}
public static Feature[] parseFFMFeatures(@Nonnull final Object arg,
@Nonnull final ListObjectInspector listOI, @Nullable final Feature[] probes)
throws HiveException {
if (arg == null) {
return null;
}
final int length = listOI.getListLength(arg);
final Feature[] ary;
if (probes != null && probes.length == length) {
ary = probes;
} else {
ary = new Feature[length];
}
int j = 0;
Object o = listOI.getListElement(arg, i);
if (o == null) {
continue;
}
String s = o.toString();
Feature f = ary[j];
if (f == null) {
f = parseFFMFeature(s);
} else {
parseFFMFeature(s, f);
static Feature parseFeature(@Nonnull final String fv, final boolean asIntFeature)
throws HiveException {
final String indexStr = fv.substring(0, pos1);
if (asIntFeature) {
int index = parseFeatureIndex(indexStr);
return new IntFeature(index, value);
} else {
double value = parseFeatureValue(valueStr);
@Nonnull
static IntFeature parseFFMFeature(@Nonnull final String fv) throws HiveException {
final int pos1 = fv.indexOf(':');
if (pos1 == -1) {
}
final String lead = fv.substring(0, pos1);
final int pos2 = rest.indexOf(':');
final int index;
if (NumberUtils.isDigits(lead)) {
index = parseFeatureIndex(lead);
if (index < 0 || index >= NUM_FIELDS) {
NUM_FIELDS);
}
} else {
index = MurmurHash3.murmurhash3(lead, NUM_FIELDS);
}
short field = (short) index;
double value = parseFeatureValue(rest);
return new IntFeature(index, field, value);
}
final String indexStr = rest.substring(0, pos2);
final int index;
final short field;
if (NumberUtils.isDigits(indexStr) && NumberUtils.isDigits(lead)) {
index = parseFeatureIndex(indexStr);
if (index >= FEATURE_INDEX_RIGHT_OPEN_BOUND) {
throw new HiveException("Feature index MUST be less than "
}
field = parseField(lead);
} else {
field = (short) MurmurHash3.murmurhash3(lead, NUM_FIELDS);
}
double value = parseFeatureValue(valueStr);
return new IntFeature(index, field, value);
}
static void parseFeature(@Nonnull final String fv, @Nonnull final Feature probe,
final String indexStr = fv.substring(0, pos1);
if (asIntFeature) {
int index = parseFeatureIndex(indexStr);
probe.setFeatureIndex(index);
probe.value = parseFeatureValue(valueStr);;
probe.setFeature(indexStr);
static void parseFFMFeature(@Nonnull final String fv, @Nonnull final Feature probe)
throws HiveException {
final int pos1 = fv.indexOf(":");
if (pos1 == -1) {
}
final String lead = fv.substring(0, pos1);
final int pos2 = rest.indexOf(':');
final int index;
if (NumberUtils.isDigits(lead)) {
index = parseFeatureIndex(lead);
if (index < 0 || index >= NUM_FIELDS) {
NUM_FIELDS);
}
} else {
index = MurmurHash3.murmurhash3(lead, NUM_FIELDS);
}
short field = (short) index;
probe.setField(field);
probe.setFeatureIndex(index);
probe.value = parseFeatureValue(rest);
return;
}
String indexStr = rest.substring(0, pos2);
final int index;
final short field;
if (NumberUtils.isDigits(indexStr) && NumberUtils.isDigits(lead)) {
index = parseFeatureIndex(indexStr);
if (index >= FEATURE_INDEX_RIGHT_OPEN_BOUND) {
throw new HiveException("Feature index MUST be less than "
}
field = parseField(lead);
} else {
field = (short) MurmurHash3.murmurhash3(lead, NUM_FIELDS);
}
probe.setField(field);
probe.setFeatureIndex(index);
probe.value = parseFeatureValue(valueStr);
}
private static short parseField(@Nonnull final String fieldStr) throws HiveException {
final short field;
try {
field = Short.parseShort(fieldStr);
} catch (NumberFormatException e) {
if (field < 0 || field >= NUM_FIELDS) {
}
return field;
public static int toIntFeature(@Nonnull final Feature x, final int yField) {
int index = x.getFeatureIndex();
}
import hivemall.utils.collections.DoubleArray3D;
import hivemall.utils.collections.IntArrayList;
import javax.annotation.Nullable;
public abstract float getV(@Nonnull Feature x, @Nonnull int yField, int f);
protected abstract void setV(@Nonnull Feature x, @Nonnull int yField, int f, float nextVif);
protected final double predict(@Nonnull final Feature[] x) {
final int iField = ei.getField();
final int jField = ej.getField();
void updateV(final double dloss, @Nonnull final Feature x, @Nonnull final int yField,
final double h = Xi * sumViX;
final float gradV = (float) (dloss * h);
final float lambdaVf = getLambdaV(f);
final Entry theta = getEntry(x, yField);
final float currentV = getV(theta, f);
final float eta = etaV(theta, t, gradV);
setV(theta, f, nextV);
private static float getV(@Nonnull final Entry theta, final int f) {
return theta.Vf[f];
}
private static float setV(@Nonnull final Entry theta, final int f, final float value) {
return theta.Vf[f] = value;
}
protected final float etaV(@Nonnull final Entry theta, final long t, final float grad) {
@Nonnull
final DoubleArray3D sumVfX(@Nonnull final Feature[] x, @Nonnull final IntArrayList fieldList,
@Nullable DoubleArray3D cached) {
final int fieldSize = fieldList.size();
final int factors = _factor;
final DoubleArray3D mdarray;
if (cached == null) {
mdarray = new DoubleArray3D();
mdarray.setSanityCheck(false);
} else {
mdarray = cached;
}
mdarray.configure(xSize, fieldSize, factors);
final int yField = fieldList.get(fieldIndex);
double val = sumVfX(x, i, yField, f);
mdarray.set(i, fieldIndex, f, val);
return mdarray;
private double sumVfX(@Nonnull final Feature[] x, final int i, @Nonnull final int yField,
final int xiFeature = xi.getFeatureIndex();
final int xiField = xi.getField();
@Nonnull
protected abstract Entry getEntry(@Nonnull Feature x);
@Nonnull
protected abstract Entry getEntry(@Nonnull Feature x, @Nonnull int yField);
protected final Entry newEntry(final float[] V) {
static final class AdaGradEntry extends Entry {
import hivemall.utils.collections.DoubleArray3D;
import hivemall.utils.collections.IntArrayList;
private IntArrayList _fieldList;
@Nullable
private DoubleArray3D _sumVfX;
this._fieldList = new IntArrayList();
protected Feature[] parseFeatures(@Nonnull final Object arg) throws HiveException {
return Feature.parseFFMFeatures(arg, _xOI, _probes);
}
@Override
final IntArrayList fieldList = getFieldList(x);
final DoubleArray3D sumVfX = _model.sumVfX(x, fieldList, _sumVfX);
final int yField = fieldList.get(fieldIndex);
double sumViX = sumVfX.get(i, fieldIndex, f);
_model.updateV(lossGrad, x_i, yField, f, sumViX, _t);
sumVfX.clear();
this._sumVfX = sumVfX;
private IntArrayList getFieldList(@Nonnull final Feature[] x) {
int field = e.getField();
protected IntFeature instantiateFeature(@Nonnull final ByteBuffer input) {
return new IntFeature(input);
private short field;
this(index, (short) -1, value);
}
public IntFeature(int index, short field, double value) {
this.field = field;
public short getField() {
return field;
}
@Override
public void setField(short field) {
this.field = field;
}
@Override
dst.putShort(field);
this.field = src.getShort();
if (field == -1) {
} else {
}
super(value);
this.feature = feature;
this(String.valueOf(feature), value);
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "mhash",
value = "_FUNC_(string word) returns a murmurhash3 INT value starting from 1")
@UDFType(deterministic = true, stateful = false)
public final class MurmurHash3UDF extends UDF {
public IntWritable evaluate(final String word) throws UDFArgumentException {
public IntWritable evaluate(final String word, final int numFeatures)
throws UDFArgumentException {
if (word == null) {
int h = mhash(word, numFeatures);
return new IntWritable(h);
public IntWritable evaluate(final List<String> words) throws UDFArgumentException {
public IntWritable evaluate(final List<String> words, final int numFeatures)
throws UDFArgumentException {
if (words == null) {
if (size == 0) {
return new IntWritable(1);
public static int mhash(final String word) {
return mhash(word, MurmurHash3.DEFAULT_NUM_FEATURES);
}
public static int mhash(final String word, final int numFeatures) {
int r = MurmurHash3.murmurhash3_x86_32(word, 0, word.length(), 0x9747b28c) % numFeatures;
if (r < 0) {
}
}
import hivemall.fm.Feature;
import hivemall.utils.hashing.MurmurHash3;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
@Description(name = "ffm_features",
value = "_FUNC_(boolean mhash, const array<string> featureNames, feature1, feature2, ..)"
" - Takes categroical variables and returns a feature vector array<string>"
" in a libffm format <field>:<index>:<value>")
private boolean mhash;
if (numArgOIs < 3) {
throw new UDFArgumentLengthException(
this.mhash = HiveUtils.getConstBoolean(argOIs[0]);
this.featureNames = HiveUtils.getConstStringArray(argOIs[1]);
int numFeatures = numArgOIs - 2;
final StringBuilder builder = new StringBuilder(128);
final int size = arguments.length - 2;
final String featureName = featureNames[i];
final String fv;
if (mhash) {
int field = MurmurHash3.murmurhash3(featureNames[i], Feature.NUM_FIELDS);
fv = builder.append(field).append(':').append(index).append(":1").toString();
builder.setLength(0);
} else {
fv = builder.append(featureName)
.append(':')
.append(feature)
.append(":1")
.toString();
builder.setLength(0);
}
result.add(new Text(fv));
final int h = murmurhash3_x86_32(data, 0, data.length(), 0x9747b28c);
if (r < 0) {
if (r < 0) {
public static int murmurhash3_x86_32(final CharSequence data, final int offset, final int len,
final int seed) {
while (pos < end) {
if (code < 0x80) {
} else if (code < 0x800) {
} else if (code < 0xD800 || code > 0xDFFF || pos >= end) {
if (shift >= 32) {
if (shift != 0) {
if (shift > 0) {
public static boolean isDigits(String str) {
if (str == null || str.length() == 0) {
return false;
}
if (!Character.isDigit(str.charAt(i))) {
return false;
}
}
return true;
}
public static final int INT_BYTES = Integer.SIZE / Byte.SIZE;
public static final int DOUBLE_BYTES = Double.SIZE / Byte.SIZE;
byte status_i = status[i];
out.writeByte(status_i);
if (status_i != IntOpenHashTable.FULL) {
continue;
}
byte status_i = in.readByte();
states[i] = status_i;
if (status_i != IntOpenHashTable.FULL) {
continue;
}
public static final byte FREE = 0;
public static final byte FULL = 1;
public static final byte REMOVED = 2;
b = null;
predModel = null;
import java.io.IOException;
@Override
public void close() throws IOException {
this.buf = null;
}
public void close() throws IOException {
clear();
}
public void close() throws IOException {
clear();
}
this.buffer = null;
this.buffers = null;
return bos.toByteArray_clear();
return bos.toByteArray_clear();
import hivemall.UDFWithOptions;
import hivemall.utils.lang.Primitives;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.Options;
@Description(
name = "ffm_features",
value = "_FUNC_(const array<string> featureNames, feature1, feature2, .. [, const string options])"
public final class FFMFeaturesUDF extends UDFWithOptions {
private String[] _featureNames;
private PrimitiveObjectInspector[] _inputOIs;
private List<Text> _result;
private boolean _mhash = true;
private int _numFeatures = Feature.DEFAULT_NUM_FEATURES;
private int _numFields = Feature.DEFAULT_NUM_FIELDS;
@Override
protected Options getOptions() {
Options opts = new Options();
opts.addOption("no_hash", "disable_feature_hashing", false,
"Wheather to disable feature hashing [default: false]");
opts.addOption("hash", "feature_hashing", true,
"The number of bits for feature hashing in range [18,31] [default:21]");
opts.addOption("fields", "num_fields", true, "The number of fields [default:1024]");
return opts;
}
@Override
protected CommandLine processOptions(@Nonnull String optionValue) throws UDFArgumentException {
CommandLine cl = parseOptions(optionValue);
int hashbits = Primitives.parseInt(cl.getOptionValue("feature_hashing"),
Feature.DEFAULT_FEATURE_BITS);
if (hashbits < 18 || hashbits > 31) {
}
int numFeatures = 1 << hashbits;
int numFields = Primitives.parseInt(cl.getOptionValue("num_fields"),
Feature.DEFAULT_NUM_FIELDS);
if (numFields <= 1) {
}
this._numFeatures = numFeatures;
this._numFields = numFields;
return cl;
}
if (numArgOIs < 2) {
this._featureNames = HiveUtils.getConstStringArray(argOIs[0]);
if (_featureNames == null) {
int numFeatureNames = _featureNames.length;
if (numFeatureNames < 1) {
throw new UDFArgumentException("#featureNames must be greater than or equals to 1: "
numFeatureNames);
}
for (String featureName : _featureNames) {
final int numFeatures;
final int lastArgIndex = numArgOIs - 1;
if (lastArgIndex > numFeatureNames) {
&& HiveUtils.isConstString(argOIs[lastArgIndex])) {
String optionValue = HiveUtils.getConstString(argOIs[lastArgIndex]);
processOptions(optionValue);
numFeatures = numArgOIs - 2;
} else {
throw new UDFArgumentException(
"Unexpected arguments for _FUNC_"
"(const array<string> featureNames, feature1, feature2, .. [, const string options])");
}
} else {
numFeatures = lastArgIndex;
this._inputOIs = new PrimitiveObjectInspector[numFeatures];
_inputOIs[i] = HiveUtils.asPrimitiveObjectInspector(oi);
this._result = new ArrayList<Text>(numFeatures);
_result.clear();
final int size = _featureNames.length;
PrimitiveObjectInspector oi = _inputOIs[i];
final String featureName = _featureNames[i];
if (_mhash) {
int field = MurmurHash3.murmurhash3(_featureNames[i], _numFields);
_result.add(new Text(fv));
return _result;
public static boolean isConstString(@Nonnull final ObjectInspector oi) {
return ObjectInspectorUtils.isConstantObjectInspector(oi) && isStringOI(oi);
}
int numFeatures = model.getNumFactors();
int numFields = model.getNumFields();
Object arg2 = args[2].get();
if (arg2 instanceof LazyBinaryArray) {
arg2 = ((LazyBinaryArray) arg2).getList();
}
Feature[] x = Feature.parseFFMFeatures(arg2, _featureListOI, _probes, numFeatures, numFields);
if (x == null || x.length == 0) {
}
this._probes = x;
private int _numFeatures;
private int _numFields;
public FFMPredictionModel(@Nonnull IntOpenHashTable<Entry> map, double w0, int factor,
int numFeatures, int numFields) {
this._numFeatures = numFeatures;
this._numFields = numFields;
public int getNumFeatures() {
return _numFeatures;
}
public int getNumFields() {
return _numFields;
}
int j = Feature.toIntFeature(x, yField, _numFields);
out.writeInt(_numFeatures);
out.writeInt(_numFields);
this._numFeatures = in.readInt();
this._numFields = in.readInt();
private final int _numFeatures;
private final int _numFields;
@Nonnull VInitScheme vInit, boolean useAdaGrad, float eta0_V, float eps, float scaling, int numFeatures, int numFields) {
this._numFeatures = numFeatures;
this._numFields = numFields;
return new FFMPredictionModel(_map, _w0, _factor, _numFeatures, _numFields);
final int j = Feature.toIntFeature(x, yField, _numFields);
final int j = Feature.toIntFeature(x, yField, _numFields);
final int j = Feature.toIntFeature(x, yField, _numFields);
protected FactorizationMachineModel initModel(@Nullable CommandLine cl)
throws UDFArgumentException {
public static final int DEFAULT_NUM_FIELDS = 1024;
public static final int DEFAULT_FEATURE_BITS = 21;
@Nonnull final ListObjectInspector listOI, @Nullable final Feature[] probes, final int numFeatures, final int numFields)
f = parseFFMFeature(s, numFeatures, numFields);
parseFFMFeature(s, f, numFeatures, numFields);
return parseFFMFeature(fv, DEFAULT_NUM_FEATURES, DEFAULT_NUM_FIELDS);
}
@Nonnull
static IntFeature parseFFMFeature(@Nonnull final String fv, final int numFeatures, final int numFields) throws HiveException {
if (index < 0 || index >= numFields) {
numFields);
index = MurmurHash3.murmurhash3(lead, numFields);
field = parseField(lead, numFields);
field = (short) MurmurHash3.murmurhash3(lead, numFields);
}
}
static void parseFFMFeature(@Nonnull final String fv, @Nonnull final Feature probe) throws HiveException {
parseFFMFeature(fv, probe, DEFAULT_NUM_FEATURES, DEFAULT_NUM_FIELDS);
static void parseFFMFeature(@Nonnull final String fv, @Nonnull final Feature probe, final int numFeatures, final int numFields)
if (index < 0 || index >= numFields) {
numFields);
index = MurmurHash3.murmurhash3(lead, numFields);
field = parseField(lead, numFields);
field = (short) MurmurHash3.murmurhash3(lead, numFields);
private static short parseField(@Nonnull final String fieldStr, final int numFields) throws HiveException {
if (field < 0 || field >= numFields) {
public static int toIntFeature(@Nonnull final Feature x, final int yField, final int numFields) {
private int _numFeatures;
private int _numFields;
opts.addOption("feature_hashing", true,
"The number of bits for feature hashing in range [18,31] [default:21]");
opts.addOption("num_fields", true, "The number of fields [default:1024]");
int hashbits = Primitives.parseInt(cl.getOptionValue("feature_hashing"),
Feature.DEFAULT_FEATURE_BITS);
if (hashbits < 18 || hashbits > 31) {
}
int numFeatures = 1 << hashbits;
int numFields = Primitives.parseInt(cl.getOptionValue("num_fields"),
Feature.DEFAULT_NUM_FIELDS);
if (numFields <= 1) {
}
this._numFeatures = numFeatures;
this._numFields = numFields;
protected FFMStringFeatureMapModel initModel(@Nullable CommandLine cl)
throws UDFArgumentException {
eta0_V, eps, scaling, _numFeatures, _numFields);
return Feature.parseFFMFeatures(arg, _xOI, _probes, _numFeatures, _numFields);
import hivemall.utils.collections.Int2FloatOpenHashTable;
private final Int2FloatOpenHashTable _w;
this._w = new Int2FloatOpenHashTable(DEFAULT_MAPSIZE);
public class Int2FloatOpenHashTable implements Externalizable {
protected Int2FloatOpenHashTable(int size, float loadFactor, float growFactor, boolean forcePrime) {
public Int2FloatOpenHashTable(int size, float loadFactor, float growFactor) {
public Int2FloatOpenHashTable(int size) {
import hivemall.utils.lang.ObjectUtils;
if (compress) {
return ObjectUtils.toCompressedBytes(_root);
} else {
return ObjectUtils.toBytes(_root);
}
final Node root = new Node();
if (compressed) {
ObjectUtils.readCompressedObject(serializedObj, 0, length, root);
} else {
ObjectUtils.readObject(serializedObj, length, root);
}
import hivemall.utils.lang.ObjectUtils;
if (compress) {
return ObjectUtils.toCompressedBytes(_root);
} else {
return ObjectUtils.toBytes(_root);
}
final Node root = new Node();
if (compressed) {
ObjectUtils.readCompressedObject(serializedObj, 0, length, root);
} else {
ObjectUtils.readObject(serializedObj, length, root);
}
return readObject(obj, obj.length);
}
public static <T> T readObject(@Nonnull final byte[] obj, final int length) throws IOException,
ClassNotFoundException {
return readObject(new FastByteArrayInputStream(obj, length));
readObject(src, src.length, dst);
}
public static void readObject(@Nonnull final byte[] src, final int length,
@Nonnull final Externalizable dst) throws IOException, ClassNotFoundException {
readObject(new FastByteArrayInputStream(src, length), dst);
b = null;
int numFeatures = model.getNumFactors();
int numFields = model.getNumFields();
Object arg2 = args[2].get();
if (arg2 instanceof LazyBinaryArray) {
arg2 = ((LazyBinaryArray) arg2).getList();
}
Feature[] x = Feature.parseFFMFeatures(arg2, _featureListOI, _probes, numFeatures, numFields);
if (x == null || x.length == 0) {
}
this._probes = x;
private int _numFeatures;
private int _numFields;
public FFMPredictionModel(@Nonnull IntOpenHashTable<Entry> map, double w0, int factor,
int numFeatures, int numFields) {
this._numFeatures = numFeatures;
this._numFields = numFields;
public int getNumFeatures() {
return _numFeatures;
}
public int getNumFields() {
return _numFields;
}
int j = Feature.toIntFeature(x, yField, _numFields);
out.writeInt(_numFeatures);
out.writeInt(_numFields);
byte status_i = status[i];
out.writeByte(status_i);
if (status_i != IntOpenHashTable.FULL) {
continue;
}
this._numFeatures = in.readInt();
this._numFields = in.readInt();
byte status_i = in.readByte();
states[i] = status_i;
if (status_i != IntOpenHashTable.FULL) {
continue;
}
private final int _numFeatures;
private final int _numFields;
@Nonnull VInitScheme vInit, boolean useAdaGrad, float eta0_V, float eps, float scaling, int numFeatures, int numFields) {
this._numFeatures = numFeatures;
this._numFields = numFields;
return new FFMPredictionModel(_map, _w0, _factor, _numFeatures, _numFields);
final int j = Feature.toIntFeature(x, yField, _numFields);
final int j = Feature.toIntFeature(x, yField, _numFields);
final int j = Feature.toIntFeature(x, yField, _numFields);
import hivemall.utils.collections.Int2FloatOpenHashTable;
private final Int2FloatOpenHashTable _w;
this._w = new Int2FloatOpenHashTable(DEFAULT_MAPSIZE);
protected FactorizationMachineModel initModel(@Nullable CommandLine cl)
throws UDFArgumentException {
public static final int DEFAULT_NUM_FIELDS = 1024;
public static final int DEFAULT_FEATURE_BITS = 21;
@Nonnull final ListObjectInspector listOI, @Nullable final Feature[] probes, final int numFeatures, final int numFields)
f = parseFFMFeature(s, numFeatures, numFields);
parseFFMFeature(s, f, numFeatures, numFields);
return parseFFMFeature(fv, DEFAULT_NUM_FEATURES, DEFAULT_NUM_FIELDS);
}
@Nonnull
static IntFeature parseFFMFeature(@Nonnull final String fv, final int numFeatures, final int numFields) throws HiveException {
if (index < 0 || index >= numFields) {
numFields);
index = MurmurHash3.murmurhash3(lead, numFields);
field = parseField(lead, numFields);
field = (short) MurmurHash3.murmurhash3(lead, numFields);
}
}
static void parseFFMFeature(@Nonnull final String fv, @Nonnull final Feature probe) throws HiveException {
parseFFMFeature(fv, probe, DEFAULT_NUM_FEATURES, DEFAULT_NUM_FIELDS);
static void parseFFMFeature(@Nonnull final String fv, @Nonnull final Feature probe, final int numFeatures, final int numFields)
if (index < 0 || index >= numFields) {
numFields);
index = MurmurHash3.murmurhash3(lead, numFields);
field = parseField(lead, numFields);
field = (short) MurmurHash3.murmurhash3(lead, numFields);
private static short parseField(@Nonnull final String fieldStr, final int numFields) throws HiveException {
if (field < 0 || field >= numFields) {
public static int toIntFeature(@Nonnull final Feature x, final int yField, final int numFields) {
private int _numFeatures;
private int _numFields;
opts.addOption("feature_hashing", true,
"The number of bits for feature hashing in range [18,31] [default:21]");
opts.addOption("num_fields", true, "The number of fields [default:1024]");
int hashbits = Primitives.parseInt(cl.getOptionValue("feature_hashing"),
Feature.DEFAULT_FEATURE_BITS);
if (hashbits < 18 || hashbits > 31) {
}
int numFeatures = 1 << hashbits;
int numFields = Primitives.parseInt(cl.getOptionValue("num_fields"),
Feature.DEFAULT_NUM_FIELDS);
if (numFields <= 1) {
}
this._numFeatures = numFeatures;
this._numFields = numFields;
protected FFMStringFeatureMapModel initModel(@Nullable CommandLine cl)
throws UDFArgumentException {
eta0_V, eps, scaling, _numFeatures, _numFields);
return Feature.parseFFMFeatures(arg, _xOI, _probes, _numFeatures, _numFields);
predModel = null;
import hivemall.UDFWithOptions;
import hivemall.utils.lang.Primitives;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.Options;
@Description(
name = "ffm_features",
value = "_FUNC_(const array<string> featureNames, feature1, feature2, .. [, const string options])"
public final class FFMFeaturesUDF extends UDFWithOptions {
private String[] _featureNames;
private PrimitiveObjectInspector[] _inputOIs;
private List<Text> _result;
private boolean _mhash = true;
private int _numFeatures = Feature.DEFAULT_NUM_FEATURES;
private int _numFields = Feature.DEFAULT_NUM_FIELDS;
@Override
protected Options getOptions() {
Options opts = new Options();
opts.addOption("no_hash", "disable_feature_hashing", false,
"Wheather to disable feature hashing [default: false]");
opts.addOption("hash", "feature_hashing", true,
"The number of bits for feature hashing in range [18,31] [default:21]");
opts.addOption("fields", "num_fields", true, "The number of fields [default:1024]");
return opts;
}
@Override
protected CommandLine processOptions(@Nonnull String optionValue) throws UDFArgumentException {
CommandLine cl = parseOptions(optionValue);
int hashbits = Primitives.parseInt(cl.getOptionValue("feature_hashing"),
Feature.DEFAULT_FEATURE_BITS);
if (hashbits < 18 || hashbits > 31) {
}
int numFeatures = 1 << hashbits;
int numFields = Primitives.parseInt(cl.getOptionValue("num_fields"),
Feature.DEFAULT_NUM_FIELDS);
if (numFields <= 1) {
}
this._numFeatures = numFeatures;
this._numFields = numFields;
return cl;
}
if (numArgOIs < 2) {
this._featureNames = HiveUtils.getConstStringArray(argOIs[0]);
if (_featureNames == null) {
int numFeatureNames = _featureNames.length;
if (numFeatureNames < 1) {
throw new UDFArgumentException("#featureNames must be greater than or equals to 1: "
numFeatureNames);
}
for (String featureName : _featureNames) {
final int numFeatures;
final int lastArgIndex = numArgOIs - 1;
if (lastArgIndex > numFeatureNames) {
&& HiveUtils.isConstString(argOIs[lastArgIndex])) {
String optionValue = HiveUtils.getConstString(argOIs[lastArgIndex]);
processOptions(optionValue);
numFeatures = numArgOIs - 2;
} else {
throw new UDFArgumentException(
"Unexpected arguments for _FUNC_"
"(const array<string> featureNames, feature1, feature2, .. [, const string options])");
}
} else {
numFeatures = lastArgIndex;
this._inputOIs = new PrimitiveObjectInspector[numFeatures];
_inputOIs[i] = HiveUtils.asPrimitiveObjectInspector(oi);
this._result = new ArrayList<Text>(numFeatures);
_result.clear();
final int size = _featureNames.length;
PrimitiveObjectInspector oi = _inputOIs[i];
final String featureName = _featureNames[i];
if (_mhash) {
int field = MurmurHash3.murmurhash3(_featureNames[i], _numFields);
_result.add(new Text(fv));
return _result;
import hivemall.utils.lang.ObjectUtils;
if (compress) {
return ObjectUtils.toCompressedBytes(_root);
} else {
return ObjectUtils.toBytes(_root);
}
final Node root = new Node();
if (compressed) {
ObjectUtils.readCompressedObject(serializedObj, 0, length, root);
} else {
ObjectUtils.readObject(serializedObj, length, root);
}
import hivemall.utils.lang.ObjectUtils;
if (compress) {
return ObjectUtils.toCompressedBytes(_root);
} else {
return ObjectUtils.toBytes(_root);
}
final Node root = new Node();
if (compressed) {
ObjectUtils.readCompressedObject(serializedObj, 0, length, root);
} else {
ObjectUtils.readObject(serializedObj, length, root);
}
public class Int2FloatOpenHashTable implements Externalizable {
protected Int2FloatOpenHashTable(int size, float loadFactor, float growFactor, boolean forcePrime) {
public Int2FloatOpenHashTable(int size, float loadFactor, float growFactor) {
public Int2FloatOpenHashTable(int size) {
public final class Int2IntOpenHashTable implements Externalizable {
protected int defaultReturnValue = -1;
protected int[] _values;
protected Int2IntOpenHashTable(int size, float loadFactor, float growFactor, boolean forcePrime) {
if (size < 1) {
this._values = new int[actualSize];
public Int2IntOpenHashTable(int size, int loadFactor, int growFactor) {
public Int2IntOpenHashTable(int size) {
public void defaultReturnValue(int v) {
public boolean containsKey(final int key) {
public int get(final int key) {
final int i = findKey(key);
if (i < 0) {
public int put(final int key, final int value) {
final int hash = keyHash(key);
if (expanded) {
final int[] keys = _keys;
final int[] values = _values;
final byte[] states = _states;
if (keys[keyIdx] == key) {
int old = values[keyIdx];
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
int old = values[keyIdx];
protected boolean isFree(final int index, final int key) {
final byte stat = _states[index];
if (stat == FREE) {
if (stat == REMOVED && _keys[index] == key) {
protected boolean preAddEntry(final int index) {
protected int findKey(final int key) {
final int[] keys = _keys;
final byte[] states = _states;
final int keyLength = keys.length;
final int hash = keyHash(key);
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
public int remove(final int key) {
final int[] keys = _keys;
final int[] values = _values;
final byte[] states = _states;
final int keyLength = keys.length;
final int hash = keyHash(key);
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
int old = values[keyIdx];
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
int old = values[keyIdx];
while (i.next() != -1) {
if (i.hasNext()) {
protected void ensureCapacity(final int newCapacity) {
private void rehash(final int newCapacity) {
if (newCapacity <= oldCapacity) {
final int[] newkeys = new int[newCapacity];
final int[] newValues = new int[newCapacity];
final byte[] newStates = new byte[newCapacity];
if (_states[i] == FULL) {
int v = _values[i];
while (newStates[keyIdx] != FREE) {
if (keyIdx < 0) {
private static int keyHash(final int key) {
while (i.next() != -1) {
out.writeInt(i.getValue());
final int keylen = in.readInt();
final int[] keys = new int[keylen];
final int[] values = new int[keylen];
final byte[] states = new byte[keylen];
int v = in.readInt();
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
public int getValue();
while (index < _keys.length && _states[index] != FULL) {
if (!hasNext()) {
if (lastEntry == -1) {
public int getValue() {
if (lastEntry == -1) {
public static final byte FREE = 0;
public static final byte FULL = 1;
public static final byte REMOVED = 2;
public static boolean isConstString(@Nonnull final ObjectInspector oi) {
return ObjectInspectorUtils.isConstantObjectInspector(oi) && isStringOI(oi);
}
import java.io.IOException;
@Override
public void close() throws IOException {
this.buf = null;
}
public void close() throws IOException {
clear();
}
public void close() throws IOException {
clear();
}
this.buffer = null;
this.buffers = null;
return bos.toByteArray_clear();
return bos.toByteArray_clear();
return readObject(obj, obj.length);
}
public static <T> T readObject(@Nonnull final byte[] obj, final int length) throws IOException,
ClassNotFoundException {
return readObject(new FastByteArrayInputStream(obj, length));
readObject(src, src.length, dst);
}
public static void readObject(@Nonnull final byte[] src, final int length,
@Nonnull final Externalizable dst) throws IOException, ClassNotFoundException {
readObject(new FastByteArrayInputStream(src, length), dst);
dos.finish();
dos.finish();
DeflaterOutputStream dos = new DeflaterOutputStream(bos);
dos.flush();
DeflaterOutputStream dos = new DeflaterOutputStream(bos);
dos.flush();
final int length = serModel.getLength();
model = FFMPredictionModel.deserialize(b, length);
Feature[] x = Feature.parseFFMFeatures(arg2, _featureListOI, _probes, numFeatures,
numFields);
return ObjectUtils.toCompressedBytes(this, true);
public static FFMPredictionModel deserialize(@Nonnull final byte[] serializedObj, final int len)
ObjectUtils.readCompressedObject(serializedObj, len, model, true);
final byte[] serialized;
import hivemall.utils.codec.Base64InputStream;
import org.apache.commons.codec.binary.Base64OutputStream;
public static byte[] toCompressedBytes(@Nonnull final Externalizable obj, final boolean base64)
throws IOException {
FastMultiByteArrayOutputStream bos = new FastMultiByteArrayOutputStream();
OutputStream out = null;
DeflaterOutputStream dos = null;
try {
out = base64 ? new Base64OutputStream(bos) : bos;
dos = new DeflaterOutputStream(bos);
toStream(obj, dos);
dos.finish();
dos.flush();
return bos.toByteArray_clear();
} finally {
IOUtils.closeQuietly(dos);
IOUtils.closeQuietly(out);
}
}
public static void readCompressedObject(@Nonnull final byte[] src, final int len,
@Nonnull final Externalizable dst, final boolean base64) throws IOException,
ClassNotFoundException {
FastByteArrayInputStream bis = new FastByteArrayInputStream(src, len);
InputStream in = null;
InflaterInputStream iis = null;
try {
in = base64 ? new Base64InputStream(bis) : bis;
iis = new InflaterInputStream(bis);
readObject(iis, dst);
} finally {
IOUtils.closeQuietly(iis);
IOUtils.closeQuietly(in);
}
}
import hivemall.utils.codec.ASCII85InputStream;
import hivemall.utils.codec.ASCII85OutputStream;
public static byte[] toCompressedBytes(@Nonnull final Externalizable obj, final boolean bin2txt)
out = bin2txt ? new ASCII85OutputStream(bos) : bos;
@Nonnull final Externalizable dst, final boolean bin2txt) throws IOException,
in = bin2txt ? new ASCII85InputStream(bis) : bis;
private static final byte TERMINATOR = '~';
private static final byte OFFSET = '!';
private static final byte NEWLINE = '\n';
private static final byte RETURN = '\r';
private static final byte SPACE = ' ';
private static final byte PADDING_U = 'u';
private static final byte Z = 'z';
ascii[k] = PADDING_U;
ascii[k] = PADDING_U;
private static final long a85p2 = 85L * 85L;
private static final long a85p3 = 85L * 85L * 85L;
private static final long a85p4 = 85L * 85L * 85L * 85L;
private static final byte TERMINATOR = '~';
private static final byte OFFSET = '!';
private static final byte Z = 'z';
outdata[0] = Z;
long x = word / a85p4;
word -= x * a85p4;
x = word / a85p3;
word -= x * a85p3;
x = word / a85p2;
word -= x * a85p2;
outdata[i] = OFFSET;
out.write(TERMINATOR);
private final byte[] ascii;
private final byte[] decoded;
decoded = new byte[4];
if (index < n) {
}
if (eof) {
return -1;
}
index = 0;
int k;
byte z;
do {
int zz = (byte) in.read();
if (zz == -1) {
eof = true;
z = (byte) zz;
} while (z == NEWLINE || z == RETURN || z == SPACE);
if (z == TERMINATOR) {
eof = true;
n = 0;
return -1;
} else if (z == Z) {
decoded[0] = decoded[1] = decoded[2] = decoded[3] = 0;
n = 4;
} else {
do {
int zz = (byte) in.read();
if (zz == -1) {
return -1;
z = (byte) zz;
} while (z == NEWLINE || z == RETURN || z == SPACE);
ascii[k] = z;
if (z == TERMINATOR) {
ascii[k] = PADDING_U;
break;
}
n = k - 1;
if (n == 0) {
eof = true;
return -1;
}
if (k < 5) {
ascii[k] = PADDING_U;
eof = true;
}
long t = 0;
z = (byte) (ascii[k] - OFFSET);
if (z < 0 || z > 93) {
throw new IOException("Invalid data in Ascii85 stream");
}
}
for (k = 3; k >= 0; --k) {
decoded[k] = (byte) (t & 0xFFL);
t >>>= 8;
public int read(final byte[] data, final int offset, final int len) throws IOException {
private final byte[] indata;
private final byte[] encoded;
encoded = new byte[5];
count = 0;
encoded[0] = Z;
encoded[1] = 0;
if (encoded[i] == 0) {
out.write(encoded[i]);
if (encoded[0] == Z) {
out.write(encoded[i]);
flush();
super.close();
final int length = serModel.getLength();
model = FFMPredictionModel.deserialize(b, length);
Feature[] x = Feature.parseFFMFeatures(arg2, _featureListOI, _probes, numFeatures,
numFields);
return ObjectUtils.toCompressedBytes(this, true);
public static FFMPredictionModel deserialize(@Nonnull final byte[] serializedObj, final int len)
ObjectUtils.readCompressedObject(serializedObj, len, model, true);
final byte[] serialized;
import hivemall.utils.codec.ASCII85InputStream;
import hivemall.utils.codec.ASCII85OutputStream;
DeflaterOutputStream dos = new DeflaterOutputStream(bos);
dos.finish();
dos.flush();
DeflaterOutputStream dos = new DeflaterOutputStream(bos);
dos.finish();
dos.flush();
public static byte[] toCompressedBytes(@Nonnull final Externalizable obj, final boolean bin2txt)
throws IOException {
FastMultiByteArrayOutputStream bos = new FastMultiByteArrayOutputStream();
OutputStream out = null;
DeflaterOutputStream dos = null;
try {
out = bin2txt ? new ASCII85OutputStream(bos) : bos;
dos = new DeflaterOutputStream(bos);
toStream(obj, dos);
dos.finish();
dos.flush();
return bos.toByteArray_clear();
} finally {
IOUtils.closeQuietly(dos);
IOUtils.closeQuietly(out);
}
}
public static void readCompressedObject(@Nonnull final byte[] src, final int len,
@Nonnull final Externalizable dst, final boolean bin2txt) throws IOException,
ClassNotFoundException {
FastByteArrayInputStream bis = new FastByteArrayInputStream(src, len);
InputStream in = null;
InflaterInputStream iis = null;
try {
in = bin2txt ? new ASCII85InputStream(bis) : bis;
iis = new InflaterInputStream(bis);
readObject(iis, dst);
} finally {
IOUtils.closeQuietly(iis);
IOUtils.closeQuietly(in);
}
}
public int getActualNumFeatures() {
return _map.size();
}
public long consumedBytes() {
int size = _map.size();
int rest = _map.capacity() - size;
if (rest > 0) {
}
return bytes;
}
import hivemall.utils.lang.NumberUtils;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log LOG = LogFactory.getLog(FieldAwareFactorizationMachineUDTF.class);
this._sumVfX = null;
this._fieldList = null;
this._sumVfX = null;
if (LOG.isInfoEnabled()) {
}
byte[] serialized;
throw new HiveException("Failed to serialize a model", e);
if (LOG.isInfoEnabled()) {
NumberUtils.prettySize(serialized.length));
}
serialized = null;
public int capacity() {
return _keys.length;
}
import hivemall.utils.codec.ZigZagCodec;
public long approxBytesConsumed() {
final int factors = _factors;
ZigZagCodec.writeSignedVInt(keys[i], out);
ZigZagCodec.writeFloat(v.W, out);
IOUtils.writeVFloats(v.Vf, factors, out);
final int factors = in.readInt();
this._factors = factors;
keys[i] = ZigZagCodec.readSignedVInt(in);
float W = ZigZagCodec.readFloat(in);
float[] Vf = IOUtils.readVFloats(in, factors);
import hivemall.utils.codec.ZigZagCodec;
public static void writeFloats(@Nonnull final float[] floats, final int size,
@Nonnull final DataOutput out) throws IOException {
out.writeFloat(floats[i]);
}
}
@Nonnull
public static float[] readFloats(@Nonnull final DataInput in, final int size)
throws IOException {
final float[] floats = new float[size];
floats[i] = in.readFloat();
}
return floats;
}
public static void writeVFloats(@Nonnull final float[] floats, final int size,
@Nonnull final DataOutput out) throws IOException {
int bits = Float.floatToIntBits(floats[i]);
ZigZagCodec.writeSignedVInt(bits, out);
}
}
@Nonnull
public static float[] readVFloats(@Nonnull final DataInput in, final int size)
throws IOException {
final float[] floats = new float[size];
int bits = ZigZagCodec.readSignedVInt(in);
floats[i] = Float.intBitsToFloat(bits);
}
return floats;
}
public static int toIntExact(final long longValue) {
final int casted = (int) longValue;
if (casted != longValue) {
}
return casted;
}
public static int floorDiv(final int x, final int y) {
int r = x / y;
if ((x ^ y) < 0 && (r * y != x)) {
r--;
}
return r;
}
public static long floorDiv(final long x, final long y) {
long r = x / y;
if ((x ^ y) < 0 && (r * y != x)) {
r--;
}
return r;
}
import hivemall.utils.codec.ZigZagLEB128Codec;
ZigZagLEB128Codec.writeSignedVInt(keys[i], out);
ZigZagLEB128Codec.writeFloat(v.W, out);
keys[i] = ZigZagLEB128Codec.readSignedVInt(in);
float W = ZigZagLEB128Codec.readFloat(in);
public final class ZigZagLEB128Codec {
private ZigZagLEB128Codec() {}
import hivemall.utils.codec.ZigZagLEB128Codec;
ZigZagLEB128Codec.writeSignedVInt(bits, out);
int bits = ZigZagLEB128Codec.readSignedVInt(in);
", Estimated uncompressed bytes: "
NumberUtils.prettySize(predModel.approxBytesConsumed()));
opts.addOption("all_terms", false,
"Whether to include all terms (i.e., w0 and w_i) [default: OFF]");
if (cl.hasOption("all_terms")) {
this._globalBias = true;
this._linearCoeff = true;
} else {
this._globalBias = cl.hasOption("global_bias");
this._linearCoeff = cl.hasOption("linear_coeff");
}
import hivemall.utils.codec.ZigZagLEB128Codec;
public int getActualNumFeatures() {
return _map.size();
}
public long approxBytesConsumed() {
int size = _map.size();
int rest = _map.capacity() - size;
if (rest > 0) {
}
return bytes;
}
final int factors = _factors;
ZigZagLEB128Codec.writeSignedVInt(keys[i], out);
ZigZagLEB128Codec.writeFloat(v.W, out);
IOUtils.writeVFloats(v.Vf, factors, out);
final int factors = in.readInt();
this._factors = factors;
keys[i] = ZigZagLEB128Codec.readSignedVInt(in);
float W = ZigZagLEB128Codec.readFloat(in);
float[] Vf = IOUtils.readVFloats(in, factors);
import hivemall.utils.lang.NumberUtils;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log LOG = LogFactory.getLog(FieldAwareFactorizationMachineUDTF.class);
opts.addOption("all_terms", false,
"Whether to include all terms (i.e., w0 and w_i) [default: OFF]");
if (cl.hasOption("all_terms")) {
this._globalBias = true;
this._linearCoeff = true;
} else {
this._globalBias = cl.hasOption("global_bias");
this._linearCoeff = cl.hasOption("linear_coeff");
}
this._sumVfX = null;
this._fieldList = null;
this._sumVfX = null;
if (LOG.isInfoEnabled()) {
", Estimated uncompressed bytes: "
NumberUtils.prettySize(predModel.approxBytesConsumed()));
}
byte[] serialized;
throw new HiveException("Failed to serialize a model", e);
if (LOG.isInfoEnabled()) {
NumberUtils.prettySize(serialized.length));
}
serialized = null;
public int capacity() {
return _keys.length;
}
import hivemall.utils.codec.ZigZagLEB128Codec;
public static void writeFloats(@Nonnull final float[] floats, final int size,
@Nonnull final DataOutput out) throws IOException {
out.writeFloat(floats[i]);
}
}
@Nonnull
public static float[] readFloats(@Nonnull final DataInput in, final int size)
throws IOException {
final float[] floats = new float[size];
floats[i] = in.readFloat();
}
return floats;
}
public static void writeVFloats(@Nonnull final float[] floats, final int size,
@Nonnull final DataOutput out) throws IOException {
int bits = Float.floatToIntBits(floats[i]);
ZigZagLEB128Codec.writeSignedVInt(bits, out);
}
}
@Nonnull
public static float[] readVFloats(@Nonnull final DataInput in, final int size)
throws IOException {
final float[] floats = new float[size];
int bits = ZigZagLEB128Codec.readSignedVInt(in);
floats[i] = Float.intBitsToFloat(bits);
}
return floats;
}
public static int toIntExact(final long longValue) {
final int casted = (int) longValue;
if (casted != longValue) {
}
return casted;
}
public static int floorDiv(final int x, final int y) {
int r = x / y;
if ((x ^ y) < 0 && (r * y != x)) {
r--;
}
return r;
}
public static long floorDiv(final long x, final long y) {
long r = x / y;
if ((x ^ y) < 0 && (r * y != x)) {
r--;
}
return r;
}
import java.util.Arrays;
import org.roaringbitmap.RoaringBitmap;
final Object[] values = _map.getValues();
final RoaringBitmap emptyStatus = writeEmptyStates(_map.getStates(), out);
if (emptyStatus.contains(i)) {
ZigZagLEB128Codec.writeSignedVInt(keys[i], out);
@Nonnull
private static RoaringBitmap writeEmptyStates(@Nonnull final byte[] status,
@Nonnull final ObjectOutput out) throws IOException {
final RoaringBitmap emptyBits = new RoaringBitmap();
final int size = status.length;
if (status[i] != IntOpenHashTable.FULL) {
emptyBits.add(i);
}
}
emptyBits.runOptimize();
emptyBits.serialize(out);
return emptyBits;
}
public void readExternal(@Nonnull final ObjectInput in) throws IOException,
ClassNotFoundException {
readStates(in, states);
if (states[i] != IntOpenHashTable.FULL) {
keys[i] = ZigZagLEB128Codec.readSignedVInt(in);
@Nonnull
private static void readStates(@Nonnull final ObjectInput in, @Nonnull final byte[] status)
throws ClassNotFoundException, IOException {
RoaringBitmap emptyBits = new RoaringBitmap();
emptyBits.deserialize(in);
Arrays.fill(status, IntOpenHashTable.FULL);
for (int i : emptyBits) {
status[i] = IntOpenHashTable.FREE;
}
}
final byte[] states = _map.getStates();
writeEmptyStates(states, out);
if (states[i] != IntOpenHashTable.FULL) {
private static void writeEmptyStates(@Nonnull final byte[] status,
ZigZagLEB128Codec.writeSignedInt(keys[i], out);
keys[i] = ZigZagLEB128Codec.readSignedInt(in);
public static void writeSignedInt(final int value, @Nonnull final DataOutput out)
writeUnsignedInt(encode(value), out);
public static void writeUnsignedInt(int value, @Nonnull final DataOutput out)
public static void writeSignedLong(final long value, @Nonnull final DataOutput out)
writeUnsignedLong(encode(value), out);
private static void writeUnsignedLong(long value, @Nonnull final DataOutput out)
public static int readSignedInt(@Nonnull final DataInput in) throws IOException {
int raw = readUnsignedInt(in);
public static int readUnsignedInt(@Nonnull final DataInput in) throws IOException {
public static long readSignedLong(@Nonnull final DataInput in) throws IOException {
long raw = readUnsignedLong(in);
public static long readUnsignedLong(@Nonnull final DataInput in) throws IOException {
writeSignedInt(bits, out);
int bits = readSignedInt(in);
writeSignedLong(bits, out);
long bits = readSignedLong(in);
ZigZagLEB128Codec.writeSignedInt(bits, out);
int bits = ZigZagLEB128Codec.readSignedInt(in);
import hivemall.utils.hadoop.Text3;
Text modelObj = new Text3(serialized);
Object[] forwardObjs = new Object[] {modelId, modelObj};
import hivemall.utils.io.CompressionStreamFactory.CompressionAlgorithm;
return ObjectUtils.toCompressedBytes(this, CompressionAlgorithm.lzma, true);
ObjectUtils.readCompressedObject(serializedObj, len, model, CompressionAlgorithm.lzma, true);
import hivemall.utils.io.CompressionStreamFactory;
import hivemall.utils.io.CompressionStreamFactory.CompressionAlgorithm;
import org.tukaani.xz.FinishableOutputStream;
public static byte[] toCompressedBytes(@Nonnull final Externalizable obj,
@Nonnull final CompressionAlgorithm algo, final boolean bin2txt) throws IOException {
FinishableOutputStream dos = null;
dos = CompressionStreamFactory.createOutputStream(out, algo);
@Nonnull final Externalizable dst, @Nonnull final CompressionAlgorithm algo,
final boolean bin2txt) throws IOException, ClassNotFoundException {
InputStream compressedStream = null;
compressedStream = CompressionStreamFactory.createInputStream(in, algo);
readObject(compressedStream, dst);
IOUtils.closeQuietly(compressedStream);
int numFeatures = model.getNumFeatures();
public static int encode(@Nonnull final byte[] input, final int offset, final int len,
@Nonnull final OutputStream output, @Nonnull final Base91Buf buf) throws IOException {
int ebq = buf.queue;
int en = buf.bits;
int n = 0;
ebq |= (input[i] & 255) << en;
if (ev > 88) {
ebq >>= 13;
en -= 13;
} else {
ebq >>= 14;
en -= 14;
}
output.write(ENCODING_TABLE[ev % BASE]);
output.write(ENCODING_TABLE[ev / BASE]);
}
}
buf.queue = ebq;
buf.bits = en;
return n;
}
public static void encodeEnd(@Nonnull final OutputStream output, @Nonnull final Base91Buf buf)
throws IOException {
int ebq = buf.queue;
int en = buf.bits;
if (en > 0) {
output.write(ENCODING_TABLE[ebq % BASE]);
if (en > 7 || ebq > 90) {
output.write(ENCODING_TABLE[ebq / BASE]);
}
}
buf.clear();
}
public static int decode(@Nonnull final byte[] input, final int offset, final int len,
@Nonnull final OutputStream output, @Nonnull final Base91Buf buf) throws IOException {
int dbq = buf.queue;
int dn = buf.bits;
int dv = buf.value;
int n = 0;
if (DECODING_TABLE[input[i]] == -1) {
}
if (dv == -1) {
} else {
dbq |= dv << dn;
do {
output.write((byte) dbq);
n;
dbq >>= 8;
dn -= 8;
} while (dn >= 8);
}
}
buf.queue = dbq;
buf.bits = dn;
buf.value = dv;
return n;
}
public static void decodeEnd(@Nonnull final OutputStream output, @Nonnull final Base91Buf buf)
throws IOException {
int dbq = buf.queue;
int dn = buf.bits;
int dv = buf.value;
if (dv != -1) {
output.write((byte) (dbq | dv << dn));
}
buf.clear();
}
public static byte decodeEnd(@Nonnull final Base91Buf buf) throws IOException {
int dv = buf.value;
if (dv == -1) {
throw new IllegalStateException("SHOULD not be called");
}
int dbq = buf.queue;
int dn = buf.bits;
buf.clear();
return (byte) (dbq | dv << dn);
}
public static final class Base91Buf {
private int queue;
private int bits;
private int value;
public Base91Buf() {
clear();
}
public boolean isEmpty() {
return value == -1;
}
public void clear() {
this.queue = 0;
this.bits = 0;
this.value = -1;
}
}
package hivemall.utils.io;
package hivemall.utils.io;
return ObjectUtils.toCompressedBytes(this, CompressionAlgorithm.deflate, true);
ObjectUtils.readCompressedObject(serializedObj, len, model, CompressionAlgorithm.deflate, true);
import hivemall.utils.io.Base91InputStream;
import hivemall.utils.io.Base91OutputStream;
import hivemall.utils.io.FinishableOutputStream;
out = bin2txt ? new Base91OutputStream(bos) : bos;
in = bin2txt ? new Base91InputStream(bis) : bis;
import org.tukaani.xz.XZInputStream;
import org.tukaani.xz.XZOutputStream;
deflate, xz, lzma2;
case xz: {
try {
return new XZInputStream(in);
} catch (IOException e) {
throw new IllegalStateException("Failed to decode by XZ", e);
}
}
case lzma2: {
return new FinishableOutputStreamAdapter(deflate) {
case xz: {
final LZMA2Options options;
options = new LZMA2Options(7);
throw new IllegalStateException("LZMA2Option configuration failed", e);
final XZOutputStream xz;
try {
xz = new XZOutputStream(out, options);
} catch (IOException e) {
throw new IllegalStateException("Failed to encode by XZ", e);
}
return new FinishableOutputStreamAdapter(xz) {
@Override
public void finish() throws IOException {
xz.finish();
out.flush();
}
};
}
case lzma2: {
final LZMA2Options options;
try {
options = new LZMA2Options(7);
} catch (UnsupportedOptionsException e) {
throw new IllegalStateException("LZMA2Option configuration failed", e);
}
FinishableWrapperOutputStream wrapped = new FinishableWrapperOutputStream(out);
final org.tukaani.xz.FinishableOutputStream lzma2 = options.getOutputStream(wrapped);
return new FinishableOutputStreamAdapter(lzma2) {
@Override
public void finish() throws IOException {
lzma2.finish();
out.flush();
}
};
IOUtils.finishStream(out);
IOUtils.finishStream(out);
IOUtils.finishStream(out);
public static void finishStream(@Nonnull final OutputStream out) throws IOException {
if (out instanceof FinishableOutputStream) {
((FinishableOutputStream) out).finish();
} else {
out.flush();
}
}
try {
import hivemall.utils.codec.VariableByteCodec;
import java.io.DataInput;
import java.io.DataOutput;
writeStates(states, out);
out.writeFloat(v.W);
IOUtils.writeFloats(v.Vf, factors, out);
static void writeStates(@Nonnull final byte[] status, @Nonnull final DataOutput out)
throws IOException {
int cardinarity = 0;
cardinarity;
out.writeInt(cardinarity);
if (cardinarity == 0) {
return;
}
int prev = 0;
if (status[i] != IntOpenHashTable.FULL) {
int diff = i - prev;
assert (diff >= 0);
VariableByteCodec.encodeUnsignedInt(diff, out);
prev = i;
}
}
float W = in.readFloat();
float[] Vf = IOUtils.readFloats(in, factors);
static void readStates(@Nonnull final DataInput in, @Nonnull final byte[] status)
throws IOException {
final int cardinarity = in.readInt();
int prev = 0;
prev = i;
return ObjectUtils.toCompressedBytes(this, CompressionAlgorithm.deflate_l7, true);
ObjectUtils.readCompressedObject(serializedObj, len, model, CompressionAlgorithm.deflate,
true);
@Deprecated
@Deprecated
@Deprecated
@Deprecated
import java.util.zip.Deflater;
deflate, deflate_l7, xz, lzma2;
case deflate:
case deflate_l7: {
case deflate_l7: {
final Deflater l7 = new Deflater(7);
final DeflaterOutputStream deflate = new hivemall.utils.io.DeflaterOutputStream(
out, l7);
return new FinishableOutputStreamAdapter(deflate) {
@Override
public void finish() throws IOException {
deflate.finish();
deflate.flush();
IOUtils.finishStream(out);
}
};
}
import java.io.EOFException;
@Deprecated
@Deprecated
public static void readFully(final InputStream in, final byte[] b, int offset, int len)
throws IOException {
do {
final int bytesRead = in.read(b, offset, len);
if (bytesRead < 0) {
throw new EOFException();
}
len -= bytesRead;
} while (len != 0);
}
public static void readFully(final InputStream in, final byte[] b) throws IOException {
readFully(in, b, 0, b.length);
}
return ObjectUtils.toCompressedBytes(this, CompressionAlgorithm.lzma2, true);
ObjectUtils.readCompressedObject(serializedObj, len, model, CompressionAlgorithm.lzma2,
import hivemall.utils.lang.HalfFloat;
writeEntry(v, factors, out);
private static void writeEntry(@Nonnull final Entry v, final int factors,
@Nonnull final DataOutput out) throws IOException {
final float W = v.W;
final float[] Vf = v.Vf;
if (isRepresentableAsHalfFloat(W, Vf)) {
out.writeBoolean(true);
out.writeShort(HalfFloat.floatToHalfFloat(W));
out.writeShort(HalfFloat.floatToHalfFloat(Vf[i]));
}
} else {
out.writeBoolean(false);
out.writeFloat(W);
IOUtils.writeFloats(Vf, factors, out);
}
}
private static boolean isRepresentableAsHalfFloat(final float W, @Nonnull final float[] Vf) {
if (!HalfFloat.isRepresentable(W)) {
return false;
}
for (float V : Vf) {
if (!HalfFloat.isRepresentable(V)) {
return false;
}
}
return true;
}
values[i] = readEntry(in, factors);
private static Entry readEntry(@Nonnull final DataInput in, final int factors)
throws IOException {
final float W;
final float[] Vf;
W = HalfFloat.halfFloatToFloat(in.readShort());
Vf = new float[factors];
Vf[i] = HalfFloat.halfFloatToFloat(in.readShort());
}
} else {
W = in.readFloat();
Vf = IOUtils.readFloats(in, factors);
}
return new Entry(W, Vf);
}
@Nonnull
HalfFloat.checkRange(v);
HalfFloat.checkRange(v);
public static final float MAX_FLOAT_INTEGER = 65520f;
public static final float MAX_FLOAT = 65504f;
public static boolean isRepresentable(final float f) {
return Math.abs(f) <= HalfFloat.MAX_FLOAT_INTEGER;
}
public static boolean isRepresentable(final float f, final boolean strict) {
if (strict) {
return Math.abs(f) <= HalfFloat.MAX_FLOAT;
} else {
return Math.abs(f) <= HalfFloat.MAX_FLOAT_INTEGER;
}
}
public static void checkRange(final float f) {
if (Math.abs(f) > HalfFloat.MAX_FLOAT) {
throw new IllegalArgumentException("Acceptable maximum weight is "
}
}
int numFeatures = model.getNumFeatures();
import hivemall.utils.codec.VariableByteCodec;
import hivemall.utils.io.CompressionStreamFactory.CompressionAlgorithm;
import hivemall.utils.lang.HalfFloat;
import java.io.DataInput;
import java.io.DataOutput;
import java.util.Arrays;
final Object[] values = _map.getValues();
final byte[] states = _map.getStates();
writeStates(states, out);
if (states[i] != IntOpenHashTable.FULL) {
ZigZagLEB128Codec.writeSignedInt(keys[i], out);
writeEntry(v, factors, out);
private static void writeEntry(@Nonnull final Entry v, final int factors,
@Nonnull final DataOutput out) throws IOException {
final float W = v.W;
final float[] Vf = v.Vf;
if (isRepresentableAsHalfFloat(W, Vf)) {
out.writeBoolean(true);
out.writeShort(HalfFloat.floatToHalfFloat(W));
out.writeShort(HalfFloat.floatToHalfFloat(Vf[i]));
}
} else {
out.writeBoolean(false);
out.writeFloat(W);
IOUtils.writeFloats(Vf, factors, out);
}
}
private static boolean isRepresentableAsHalfFloat(final float W, @Nonnull final float[] Vf) {
if (!HalfFloat.isRepresentable(W)) {
return false;
}
for (float V : Vf) {
if (!HalfFloat.isRepresentable(V)) {
return false;
}
}
return true;
}
@Nonnull
static void writeStates(@Nonnull final byte[] status, @Nonnull final DataOutput out)
throws IOException {
final int size = status.length;
int cardinarity = 0;
if (status[i] != IntOpenHashTable.FULL) {
cardinarity;
}
}
out.writeInt(cardinarity);
if (cardinarity == 0) {
return;
}
int prev = 0;
if (status[i] != IntOpenHashTable.FULL) {
int diff = i - prev;
assert (diff >= 0);
VariableByteCodec.encodeUnsignedInt(diff, out);
prev = i;
}
}
}
public void readExternal(@Nonnull final ObjectInput in) throws IOException,
ClassNotFoundException {
readStates(in, states);
if (states[i] != IntOpenHashTable.FULL) {
keys[i] = ZigZagLEB128Codec.readSignedInt(in);
values[i] = readEntry(in, factors);
@Nonnull
private static Entry readEntry(@Nonnull final DataInput in, final int factors)
throws IOException {
final float W;
final float[] Vf;
W = HalfFloat.halfFloatToFloat(in.readShort());
Vf = new float[factors];
Vf[i] = HalfFloat.halfFloatToFloat(in.readShort());
}
} else {
W = in.readFloat();
Vf = IOUtils.readFloats(in, factors);
}
return new Entry(W, Vf);
}
@Nonnull
static void readStates(@Nonnull final DataInput in, @Nonnull final byte[] status)
throws IOException {
final int cardinarity = in.readInt();
Arrays.fill(status, IntOpenHashTable.FULL);
int prev = 0;
status[i] = IntOpenHashTable.FREE;
prev = i;
}
}
return ObjectUtils.toCompressedBytes(this, CompressionAlgorithm.lzma2, true);
ObjectUtils.readCompressedObject(serializedObj, len, model, CompressionAlgorithm.lzma2,
true);
import hivemall.utils.hadoop.Text3;
Text modelObj = new Text3(serialized);
Object[] forwardObjs = new Object[] {modelId, modelObj};
HalfFloat.checkRange(v);
HalfFloat.checkRange(v);
public static int encode(@Nonnull final byte[] input, final int offset, final int len,
@Nonnull final OutputStream output, @Nonnull final Base91Buf buf) throws IOException {
int ebq = buf.queue;
int en = buf.bits;
int n = 0;
ebq |= (input[i] & 255) << en;
if (ev > 88) {
ebq >>= 13;
en -= 13;
} else {
ebq >>= 14;
en -= 14;
}
output.write(ENCODING_TABLE[ev % BASE]);
output.write(ENCODING_TABLE[ev / BASE]);
}
}
buf.queue = ebq;
buf.bits = en;
return n;
}
public static void encodeEnd(@Nonnull final OutputStream output, @Nonnull final Base91Buf buf)
throws IOException {
int ebq = buf.queue;
int en = buf.bits;
if (en > 0) {
output.write(ENCODING_TABLE[ebq % BASE]);
if (en > 7 || ebq > 90) {
output.write(ENCODING_TABLE[ebq / BASE]);
}
}
buf.clear();
}
public static int decode(@Nonnull final byte[] input, final int offset, final int len,
@Nonnull final OutputStream output, @Nonnull final Base91Buf buf) throws IOException {
int dbq = buf.queue;
int dn = buf.bits;
int dv = buf.value;
int n = 0;
if (DECODING_TABLE[input[i]] == -1) {
}
if (dv == -1) {
} else {
dbq |= dv << dn;
do {
output.write((byte) dbq);
n;
dbq >>= 8;
dn -= 8;
} while (dn >= 8);
}
}
buf.queue = dbq;
buf.bits = dn;
buf.value = dv;
return n;
}
public static void decodeEnd(@Nonnull final OutputStream output, @Nonnull final Base91Buf buf)
throws IOException {
int dbq = buf.queue;
int dn = buf.bits;
int dv = buf.value;
if (dv != -1) {
output.write((byte) (dbq | dv << dn));
}
buf.clear();
}
public static byte decodeEnd(@Nonnull final Base91Buf buf) throws IOException {
int dv = buf.value;
if (dv == -1) {
throw new IllegalStateException("SHOULD not be called");
}
int dbq = buf.queue;
int dn = buf.bits;
buf.clear();
return (byte) (dbq | dv << dn);
}
public static final class Base91Buf {
private int queue;
private int bits;
private int value;
public Base91Buf() {
clear();
}
public boolean isEmpty() {
return value == -1;
}
public void clear() {
this.queue = 0;
this.bits = 0;
this.value = -1;
}
}
public static void writeSignedInt(final int value, @Nonnull final DataOutput out)
writeUnsignedInt(encode(value), out);
public static void writeUnsignedInt(int value, @Nonnull final DataOutput out)
public static void writeSignedLong(final long value, @Nonnull final DataOutput out)
writeUnsignedLong(encode(value), out);
private static void writeUnsignedLong(long value, @Nonnull final DataOutput out)
public static int readSignedInt(@Nonnull final DataInput in) throws IOException {
int raw = readUnsignedInt(in);
public static int readUnsignedInt(@Nonnull final DataInput in) throws IOException {
public static long readSignedLong(@Nonnull final DataInput in) throws IOException {
long raw = readUnsignedLong(in);
public static long readUnsignedLong(@Nonnull final DataInput in) throws IOException {
@Deprecated
writeSignedInt(bits, out);
@Deprecated
int bits = readSignedInt(in);
@Deprecated
writeSignedLong(bits, out);
@Deprecated
long bits = readSignedLong(in);
package hivemall.utils.io;
package hivemall.utils.io;
import java.io.EOFException;
@Deprecated
ZigZagLEB128Codec.writeSignedInt(bits, out);
@Deprecated
int bits = ZigZagLEB128Codec.readSignedInt(in);
public static void finishStream(@Nonnull final OutputStream out) throws IOException {
if (out instanceof FinishableOutputStream) {
((FinishableOutputStream) out).finish();
} else {
out.flush();
}
}
public static void readFully(final InputStream in, final byte[] b, int offset, int len)
throws IOException {
do {
final int bytesRead = in.read(b, offset, len);
if (bytesRead < 0) {
throw new EOFException();
}
len -= bytesRead;
} while (len != 0);
}
public static void readFully(final InputStream in, final byte[] b) throws IOException {
readFully(in, b, 0, b.length);
}
public static final float MAX_FLOAT_INTEGER = 65520f;
public static final float MAX_FLOAT = 65504f;
public static boolean isRepresentable(final float f) {
return Math.abs(f) <= HalfFloat.MAX_FLOAT_INTEGER;
}
public static boolean isRepresentable(final float f, final boolean strict) {
if (strict) {
return Math.abs(f) <= HalfFloat.MAX_FLOAT;
} else {
return Math.abs(f) <= HalfFloat.MAX_FLOAT_INTEGER;
}
}
public static void checkRange(final float f) {
if (Math.abs(f) > HalfFloat.MAX_FLOAT) {
throw new IllegalArgumentException("Acceptable maximum weight is "
}
}
import hivemall.utils.io.Base91InputStream;
import hivemall.utils.io.Base91OutputStream;
import hivemall.utils.io.CompressionStreamFactory;
import hivemall.utils.io.CompressionStreamFactory.CompressionAlgorithm;
import hivemall.utils.io.FinishableOutputStream;
public static byte[] toCompressedBytes(@Nonnull final Externalizable obj,
@Nonnull final CompressionAlgorithm algo, final boolean bin2txt) throws IOException {
FinishableOutputStream dos = null;
try {
out = bin2txt ? new Base91OutputStream(bos) : bos;
dos = CompressionStreamFactory.createOutputStream(out, algo);
@Nonnull final Externalizable dst, @Nonnull final CompressionAlgorithm algo,
final boolean bin2txt) throws IOException, ClassNotFoundException {
InputStream compressedStream = null;
in = bin2txt ? new Base91InputStream(bis) : bis;
compressedStream = CompressionStreamFactory.createInputStream(in, algo);
readObject(compressedStream, dst);
IOUtils.closeQuietly(compressedStream);
options = new LZMA2Options(LZMA2Options.PRESET_DEFAULT);
options = new LZMA2Options(LZMA2Options.PRESET_DEFAULT);
return ObjectUtils.toCompressedBytes(this, CompressionAlgorithm.lzma2_l5, true);
ObjectUtils.readCompressedObject(serializedObj, len, model, CompressionAlgorithm.lzma2_l5,
deflate, deflate_l7, xz, lzma2, lzma2_l5;
final LZMA2Options options;
try {
options = new LZMA2Options(5);
} catch (UnsupportedOptionsException e) {
throw new IllegalStateException("LZMA2Option configuration failed", e);
}
int dictSize = options.getDictSize();
return new LZMA2InputStream(in, dictSize);
}
case lzma2_l5: {
final LZMA2Options options;
try {
options = new LZMA2Options(5);
} catch (UnsupportedOptionsException e) {
throw new IllegalStateException("LZMA2Option configuration failed", e);
}
FinishableWrapperOutputStream wrapped = new FinishableWrapperOutputStream(out);
final org.tukaani.xz.FinishableOutputStream lzma2 = options.getOutputStream(wrapped);
return new FinishableOutputStreamAdapter(lzma2) {
@Override
public void finish() throws IOException {
lzma2.finish();
IOUtils.finishStream(out);
}
};
}
@Nonnull
import hivemall.fm.FMHyperParameters.FFMHyperParameters;
public FFMStringFeatureMapModel(@Nonnull FFMHyperParameters params) {
super(params);
this._numFeatures = params.numFeatures;
this._numFields = params.numFields;
public FMArrayModel(@Nonnull FMHyperParameters params) {
super(params);
this._p = params.numFeatures;
this._V = new float[params.numFeatures][params.factor];
public FMIntFeatureMapModel(@Nonnull FMHyperParameters params) {
super(params);
public FMStringFeatureMapModel(@Nonnull FMHyperParameters params) {
super(params);
if (entry == null) {
if (entry == null) {
if (entry == null) {
if (entry == null) {
public FactorizationMachineModel(@Nonnull FMHyperParameters params) {
this._classification = params.classification;
this._factor = params.factor;
this._sigma = params.sigma;
this._eta = params.eta;
this._initScheme = params.vInit;
this._rnd = new Random(params.seed);
this._min_target = params.minTarget;
this._max_target = params.maxTarget;
this._lambdaW0 = params.lambda0;
this._lambdaW = params.lambda0;
this._lambdaV = new float[params.factor];
Arrays.fill(_lambdaV, params.lambda0);
@Nullable
protected Feature[] _probes;
protected FMHyperParameters _params;
protected boolean _parseFeatureAsInt;
protected ConversionState _cvState;
opts.addOption("num_features", true, "The size of feature dimensions");
opts.addOption("f", "factor", true, "The number of the latent variables [default: 5]");
"Parse a feature as integer [default: OFF]");
protected CommandLine processOptions(@Nonnull ObjectInspector[] argOIs)
throws UDFArgumentException {
final FMHyperParameters params = _params;
params.processOptions(cl);
this._classification = params.classification;
this._iterations = params.iters;
this._factor = params.factor;
this._parseFeatureAsInt = params.parseFeatureAsInt;
if (params.adaptiveReglarization) {
this._validationRatio = params.validationRatio;
this._validationThreshold = params.validationThreshold;
this._lossFunction = params.classification ? LossFunctions.getLossFunction(LossType.LogLoss)
this._etaEstimator = params.eta;
this._cvState = new ConversionState(params.conversionCheck, params.convergenceRate);
@Nonnull
this._params = newHyperParameters();
this._model = initModel(cl, _params);
return getOutputOI(_params);
protected FMHyperParameters newHyperParameters() {
return new FMHyperParameters();
}
@Nonnull
protected StructObjectInspector getOutputOI(@Nonnull FMHyperParameters params) {
if (params.parseFeatureAsInt) {
@Nonnull
protected FactorizationMachineModel initModel(@Nullable CommandLine cl,
@Nonnull FMHyperParameters params) throws UDFArgumentException {
if (params.parseFeatureAsInt) {
if (params.numFeatures == -1) {
return new FMIntFeatureMapModel(params);
return new FMArrayModel(params);
return new FMStringFeatureMapModel(params);
this._probes = null;
this._model = null;
this._model = null;
import hivemall.fm.FMHyperParameters.FFMHyperParameters;
public FieldAwareFactorizationMachineModel(@Nonnull FFMHyperParameters params) {
super(params);
this.useAdaGrad = params.useAdaGrad;
this.eta0_V = params.eta0_V;
this.eps = params.eps;
this.scaling = params.scaling;
import hivemall.fm.FMHyperParameters.FFMHyperParameters;
private FFMStringFeatureMapModel _ffmModel;
opts.addOption("l1_v", "L1_V", true, "L1 regularization value for AdaGrad [default: 0.01]");
protected FFMHyperParameters newHyperParameters() {
return new FFMHyperParameters();
}
@Override
FFMHyperParameters params = (FFMHyperParameters) _params;
if (params.parseFeatureAsInt) {
this._globalBias = params.globalBias;
this._linearCoeff = params.linearCoeff;
this._numFeatures = params.numFeatures;
this._numFields = params.numFields;
protected StructObjectInspector getOutputOI(@Nonnull FMHyperParameters params) {
protected FFMStringFeatureMapModel initModel(@Nullable CommandLine cl,
@Nonnull FMHyperParameters params) throws UDFArgumentException {
FFMHyperParameters ffmParams = (FFMHyperParameters) params;
FFMStringFeatureMapModel model = new FFMStringFeatureMapModel(ffmParams);
this._ffmModel = model;
_ffmModel.check(x);
final double p = _ffmModel.predict(x);
final double lossGrad = _ffmModel.dloss(p, y);
_ffmModel.updateW0(lossGrad, eta_t);
_ffmModel.updateWi(lossGrad, x[i], eta_t);
final DoubleArray3D sumVfX = _ffmModel.sumVfX(x, fieldList, _sumVfX);
_ffmModel.updateV(lossGrad, x_i, yField, f, sumViX, _t);
this._ffmModel = null;
this._model = null;
FFMPredictionModel predModel = _ffmModel.toPredictionModel();
return ObjectUtils.toCompressedBytes(this, CompressionAlgorithm.lzma2, true);
ObjectUtils.readCompressedObject(serializedObj, len, model, CompressionAlgorithm.lzma2,
public static final int DEFAULT_COMPRESSION_LEVEL = -1;
deflate, xz, lzma2;
}
public static InputStream createInputStream(@Nonnull final InputStream in,
@Nonnull final CompressionAlgorithm algo) {
return createInputStream(in, algo, DEFAULT_COMPRESSION_LEVEL);
@Nonnull final CompressionAlgorithm algo, final int level) {
case deflate: {
case lzma2: {
final int dictSize;
if (level == DEFAULT_COMPRESSION_LEVEL) {
} else {
final LZMA2Options options;
try {
options = new LZMA2Options(level);
} catch (UnsupportedOptionsException e) {
throw new IllegalStateException("LZMA2Option configuration failed", e);
}
dictSize = options.getDictSize();
return createOutputStream(out, algo, DEFAULT_COMPRESSION_LEVEL);
}
@Nonnull
public static FinishableOutputStream createOutputStream(@Nonnull final OutputStream out,
@Nonnull final CompressionAlgorithm algo, int level) {
final DeflaterOutputStream deflateOut;
if (level == DEFAULT_COMPRESSION_LEVEL) {
deflateOut = new DeflaterOutputStream(out);
} else {
Deflater d = new Deflater(level);
deflateOut = new hivemall.utils.io.DeflaterOutputStream(out, d);
}
return new FinishableOutputStreamAdapter(deflateOut) {
deflateOut.finish();
deflateOut.flush();
if (level == DEFAULT_COMPRESSION_LEVEL) {
}
options = new LZMA2Options(level);
case lzma2: {
if (level == DEFAULT_COMPRESSION_LEVEL) {
options = new LZMA2Options(level);
private static final byte HALF_FLOAT_ENTRY = 1;
private static final byte W_ONLY_HALF_FLOAT_ENTRY = 2;
private static final byte FLOAT_ENTRY = 3;
private static final byte W_ONLY_FLOAT_ENTRY = 4;
if (Vf == null) {
if (HalfFloat.isRepresentable(W)) {
out.writeByte(W_ONLY_HALF_FLOAT_ENTRY);
out.writeShort(HalfFloat.floatToHalfFloat(W));
} else {
out.writeByte(W_ONLY_FLOAT_ENTRY);
out.writeFloat(W);
}
} else if (isRepresentableAsHalfFloat(W, Vf)) {
out.writeByte(HALF_FLOAT_ENTRY);
out.writeByte(FLOAT_ENTRY);
final byte type = in.readByte();
switch (type) {
case HALF_FLOAT_ENTRY: {
W = HalfFloat.halfFloatToFloat(in.readShort());
Vf = new float[factors];
Vf[i] = HalfFloat.halfFloatToFloat(in.readShort());
}
break;
case W_ONLY_HALF_FLOAT_ENTRY: {
W = HalfFloat.halfFloatToFloat(in.readShort());
Vf = null;
break;
}
case FLOAT_ENTRY: {
W = in.readFloat();
Vf = IOUtils.readFloats(in, factors);
break;
}
case W_ONLY_FLOAT_ENTRY: {
W = in.readFloat();
Vf = null;
break;
}
default:
entry = newEntry(nextWi);
if(entry.Vf == null) {
V = initV();
entry.Vf = V;
} else {
V = entry.Vf;
}
if(entry.Vf == null) {
V = initV();
entry.Vf = V;
} else {
V = entry.Vf;
}
@Nonnull
protected final FFMHyperParameters _params;
protected final float _eta0_V;
protected final float _eps;
protected final float _scaling;
this._params = params;
this._eta0_V = params.eta0_V;
this._eps = params.eps;
this._scaling = params.scaling;
@Deprecated
final float currentV = theta.Vf[f];
theta.Vf[f] = nextV;
if (_params.useAdaGrad) {
double gg = theta.getSumOfSquaredGradients(_scaling);
theta.addGradient(grad, _scaling);
protected final Entry newEntry(final float W) {
if (_params.useAdaGrad) {
return new AdaGradEntry(W);
} else {
return new Entry(W);
}
}
if (_params.useAdaGrad) {
@Nullable
float[] Vf;
Entry(float W) {
this(W, null);
}
Entry(float W, @Nullable float[] Vf) {
AdaGradEntry(float W) {
this(W, null);
}
AdaGradEntry(float W, @Nullable float[] Vf) {
this.sumOfSqGradients = 0.d;
_ffmModel.updateWi(lossGrad, x[i], eta_t);
}
import hivemall.utils.math.MathUtils;
private final float _alpha;
private final float _beta;
private final float _lambda1;
private final float _lamdda2;
this._alpha = params.alphaFTRL;
this._beta = params.betaFTRL;
this._lambda1 = params.lambda1;
this._lamdda2 = params.lamdda2;
boolean updateWiFTRL(final double dloss, @Nonnull final Feature x, final float eta) {
final double Xi = x.getValue();
float gradWi = (float) (dloss * Xi);
final Entry theta = getEntry(x);
float wi = theta.W;
final float z = theta.updateZ(gradWi, _alpha);
final double n = theta.updateN(gradWi);
final float nextWi;
if (Math.abs(z) <= _lambda1) {
nextWi = 0.f;
} else {
if (!NumberUtils.isFinite(nextWi)) {
}
}
theta.W = nextWi;
return (nextWi != 0) || (wi != 0);
}
if (entry.Vf == null) {
if (entry.Vf == null) {
entry = newEntry(0.f);
} else {
if (entry.Vf == null) {
entry.Vf = initV();
}
float lambda = 0.01f;
float lambdaW0 = 0.01f;
float lambdaW = 0.01f;
float lambdaV = 0.01f;
@Override
public String toString() {
}
this.lambda = Primitives.parseFloat(cl.getOptionValue("lambda"), lambda);
this.lambdaW0 = Primitives.parseFloat(cl.getOptionValue("lambda_w0"), lambda);
this.lambdaW = Primitives.parseFloat(cl.getOptionValue("lambda_w"), lambda);
this.lambdaV = Primitives.parseFloat(cl.getOptionValue("lambda_v"), lambda);
boolean linearCoeff = true;
boolean useFTRL = true;
float lamdda2 = 0.001f;
this.globalBias = cl.hasOption("global_bias");
this.linearCoeff = !cl.hasOption("no_coeff");
this.useFTRL = !cl.hasOption("disable_ftrl");
this.alphaFTRL = Primitives.parseFloat(cl.getOptionValue("alphaFTRL"), alphaFTRL);
this.betaFTRL = Primitives.parseFloat(cl.getOptionValue("betaFTRL"), betaFTRL);
this.lambda1 = Primitives.parseFloat(cl.getOptionValue("lambda1"), lambda1);
this.lamdda2 = Primitives.parseFloat(cl.getOptionValue("lamdda2"), lamdda2);
@Override
public String toString() {
}
this._lambdaW0 = params.lambdaW0;
this._lambdaW = params.lambdaW;
Arrays.fill(_lambdaV, params.lambdaV);
private static final Log LOG = LogFactory.getLog(FactorizationMachineUDTF.class);
opts.addOption("lambda", true,
opts.addOption("lambdaW0", "lambda_w0", true,
"The initial lambda value for W0 regularization [default: 0.01]");
opts.addOption("lambdaWi", "lambda_wi", true,
"The initial lambda value for Wi regularization [default: 0.01]");
opts.addOption("lambdaV", "lambda_v", true,
"The initial lambda value for V regularization [default: 0.01]");
if (LOG.isInfoEnabled()) {
LOG.info(_params);
}
if (LOG.isInfoEnabled()) {
protected final boolean _useAdaGrad;
protected final boolean _useFTRL;
this._useAdaGrad = params.useAdaGrad;
this._useFTRL = params.useFTRL;
if (_useAdaGrad) {
double gg = theta.getSumOfSquaredGradients();
theta.addGradient(grad);
if (_useFTRL) {
return new FTRLEntry(W);
} else if (_useAdaGrad) {
protected final Entry newEntry(@Nonnull final float[] V) {
if (_useFTRL) {
return new FTRLEntry(0.f, V);
} else if (_useAdaGrad) {
double getSumOfSquaredGradients() {
void addGradient(float grad) {
float updateZ(float gradW, float alpha) {
throw new UnsupportedOperationException();
}
double updateN(float gradW) {
throw new UnsupportedOperationException();
}
static class AdaGradEntry extends Entry {
double n;
this.n = 0.d;
final double getSumOfSquaredGradients() {
return n;
final void addGradient(final float grad) {
static final class FTRLEntry extends AdaGradEntry {
float z;
FTRLEntry(float W) {
this(W, null);
}
FTRLEntry(float W, @Nullable float[] Vf) {
super(W, Vf);
this.z = 0.f;
}
@Override
float updateZ(final float gradW, final float alpha) {
double gg = gradW * gradW;
this.z = newZ;
return newZ;
}
@Override
double updateN(final float gradW) {
this.n = newN;
return newN;
}
}
private boolean _FTRL;
opts.addOption("disable_wi", "no_coeff", false, "Not to include linear term [default: OFF]");
opts.addOption("disable_ftrl", false,
"Whether not to use Follow-The-Regularized-Reader [default: OFF]");
opts.addOption("alpha", "alphaFTRL", true,
"Alpha (learning rate) value of Follow-The-Regularized-Reader [default 0.01]");
opts.addOption("beta", "betaFTRL", true,
"Beta (a learning rate constants) value of Follow-The-Regularized-Reader [default 1]");
opts.addOption("lambda1", true,
"L1 value of Follow-The-Regularized-Reader that controls model Sparseness [default 1]");
opts.addOption("lambda2", true, "L2 value of Follow-The-Regularized-Reader [default 0]");
this._FTRL = params.useFTRL;
if (useV == false) {
continue;
private boolean updateWi(double lossGrad, @Nonnull Feature xi, float eta) {
if (!_linearCoeff) {
return true;
}
if (_FTRL) {
return _ffmModel.updateWiFTRL(lossGrad, xi, eta);
} else {
_ffmModel.updateWi(lossGrad, xi, eta);
return true;
}
}
public static float sign(final float v) {
return v < 0.f ? -1.f : 1.f;
}
removeEntry(x);
return wi != 0;
if (!NumberUtils.isFinite(nextWi)) {
}
protected void removeEntry(@Nonnull final Feature x) {
int j = x.getFeatureIndex();
_map.remove(j);
}
if (!NumberUtils.isFinite(newZ)) {
}
if ("SquaredLoss".equalsIgnoreCase(type)) {
} else if ("LogLoss".equalsIgnoreCase(type)) {
} else if ("HingeLoss".equalsIgnoreCase(type)) {
} else if ("SquaredHingeLoss".equalsIgnoreCase(type)) {
} else if ("QuantileLoss".equalsIgnoreCase(type)) {
} else if ("EpsilonInsensitiveLoss".equalsIgnoreCase(type)) {
if (!(y == 1.f || y == -1.f)) {
if (!(y == 1.d || y == -1.d)) {
if (z > 18.f) {
if (z < -18.f) {
if (z > 18.d) {
if (z < -18.d) {
if (z > 18.f) {
if (z < -18.f) {
if (tau <= 0 || tau >= 1.0) {
if (e > 0.f) {
if (e > 0.d) {
if (e == 0.f) {
if (predicted > -100.d) {
if (z > 18.f) {
if (z < -18.f) {
if (z > 18.d) {
if (z < -18.d) {
if (alphaFTRL == 0.f) {
throw new UDFArgumentException("-alphaFTRL SHOULD NOT be 0");
}
if (!NumberUtils.isFinite(newN)) {
gradW);
}
"Alpha value (learning rate) of Follow-The-Regularized-Reader [default 0.1]");
"Beta value (a learning smoothing parameter) of Follow-The-Regularized-Reader [default 1.0]");
opts.addOption(
"lambda1",
true,
"L1 regularization value of Follow-The-Regularized-Reader that controls model Sparseness [default 0.1]");
opts.addOption("lambda2", true,
"L2 regularization value of Follow-The-Regularized-Reader [default 0.01]");
if (lossGrad == 0.0f) {
return;
}
if (x_i.value == 0.f) {
continue;
}
double x2 = Math.max(Math.min(x, 23.d), -23.d);
public int add(@Nonnull final byte[] value, int valueOffset, final int valueLength) {
public int add(@Nonnull final ByteBuffer src, int valueOffset, final int valueLength) {
int i = length / chunkSize;
int j = length % chunkSize;
int remaining = valueLength;
while (remaining > 0) {
int size = Math.min(remaining, chunkSize - j);
src.get(data[i], j, size);
remaining -= size;
j = 0;
}
int result = length;
return result;
}
double gg = theta.getSumOfSquaredGradientsV();
theta.addGradientV(grad);
double getSumOfSquaredGradientsV() {
void addGradientV(float grad) {
double v_gg;
this.v_gg = 0.d;
final double getSumOfSquaredGradientsV() {
return v_gg;
final void addGradientV(final float gradV) {
double n;
this.n = 0.d;
import javax.annotation.Nullable;
public int getInt(final int index) {
}
public float getFloat(final int index) {
return Float.intBitsToFloat(getInt(index));
}
private static int toInt(final byte b3, final byte b2, final byte b1, final byte b0) {
return (((b3) << 24) | ((b2 & 0xff) << 16) | ((b1 & 0xff) << 8) | ((b0 & 0xff)));
}
public long getLong(final int index) {
}
public double getDouble(final int index) {
return Double.longBitsToDouble(getLong(index));
}
private static long toLong(final byte b7, final byte b6, final byte b5, final byte b4,
final byte b3, final byte b2, final byte b1, final byte b0) {
return ((((long) b7) << 56) | (((long) b6 & 0xff) << 48) | (((long) b5 & 0xff) << 40)
| (((long) b4 & 0xff) << 32) | (((long) b3 & 0xff) << 24)
| (((long) b2 & 0xff) << 16) | (((long) b1 & 0xff) << 8) | (((long) b0 & 0xff)));
}
public void setInt(final int index, final int value) {
set(index, int3(value));
}
public void setLong(final int index, final long value) {
set(index, long7(value));
}
public void setFloat(final int index, final float value) {
setInt(index, Float.floatToIntBits(value));
}
public void setDouble(final int index, final double value) {
setLong(index, Double.doubleToLongBits(value));
}
private static byte int3(final int x) {
return (byte) (x >> 24);
}
private static byte int2(final int x) {
return (byte) (x >> 16);
}
private static byte int1(final int x) {
return (byte) (x >> 8);
}
private static byte int0(final int x) {
return (byte) (x);
}
private static byte long7(final long x) {
return (byte) (x >> 56);
}
private static byte long6(final long x) {
return (byte) (x >> 48);
}
private static byte long5(final long x) {
return (byte) (x >> 40);
}
private static byte long4(final long x) {
return (byte) (x >> 32);
}
private static byte long3(final long x) {
return (byte) (x >> 24);
}
private static byte long2(final long x) {
return (byte) (x >> 16);
}
private static byte long1(final long x) {
return (byte) (x >> 8);
}
private static byte long0(final long x) {
return (byte) (x);
}
private final int _chunkBytes;
private int _numAllocated;
private long _skippedBytes;
this._chunkBytes = SizeOf.INT * chunkSize;
this._numAllocated = 0;
this._skippedBytes = 0L;
Preconditions.checkArgument(bytes <= _chunkBytes,
"Cannot allocate memory greater than %s bytes: %s", _chunkBytes, bytes);
int i = Primitives.castToInt(_position / _chunkBytes);
final int j = Primitives.castToInt(_position % _chunkBytes);
if (bytes > (_chunkBytes - j)) {
_position = ((long) i) * _chunkBytes;
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int i = Primitives.castToInt(ptr / _chunkBytes);
int chunkIdx = Primitives.castToInt(ptr / _chunkBytes);
int chunkIdx = Primitives.castToInt(ptr / _chunkBytes);
if (offset >= _chunkBytes) {
long j = ptr % _chunkBytes;
return _chunkBytes;
public static void checkArgument(boolean expression) {
if (!expression) {
throw new IllegalArgumentException();
}
}
public static void checkArgument(boolean expression, @Nullable Object errorMessage) {
if (!expression) {
throw new IllegalArgumentException(String.valueOf(errorMessage));
}
}
public static void checkArgument(boolean expression, @Nullable String errorMessageTemplate,
@Nullable Object... errorMessageArgs) {
if (!expression) {
throw new IllegalArgumentException(StringUtils.format(errorMessageTemplate,
errorMessageArgs));
}
}
public static int castToInt(final long value) {
final int result = (int) value;
if (result != value) {
}
return result;
}
import javax.annotation.Nullable;
@Nonnull
public static String format(@Nullable String template, @Nullable Object... args) {
template = String.valueOf(template);
int templateStart = 0;
int i = 0;
while (i < args.length) {
int placeholderStart = template.indexOf("%s", templateStart);
if (placeholderStart == -1) {
break;
}
builder.append(template, templateStart, placeholderStart);
}
builder.append(template, templateStart, template.length());
if (i < args.length) {
builder.append(" [");
while (i < args.length) {
builder.append(", ");
}
builder.append(']');
}
return builder.toString();
}
this.factor = Primitives.parseInt(cl.getOptionValue("factors"), factor);
opts.addOption("f", "factors", true, "The number of the latent variables [default: 5]");
import hivemall.utils.codec.VariableByteCodec;
import hivemall.utils.codec.ZigZagLEB128Codec;
import java.io.DataInput;
import java.io.DataOutput;
import javax.annotation.Nonnull;
public int capacity() {
return _keys.length;
}
@Override
final int[] keys = _keys;
final int size = keys.length;
out.writeInt(size);
final byte[] states = _states;
writeStates(states, out);
final long[] values = _values;
if (states[i] != FULL) {
continue;
}
ZigZagLEB128Codec.writeSignedInt(keys[i], out);
ZigZagLEB128Codec.writeSignedLong(values[i], out);
@Nonnull
private static void writeStates(@Nonnull final byte[] status, @Nonnull final DataOutput out)
throws IOException {
final int size = status.length;
int cardinarity = 0;
if (status[i] != FULL) {
cardinarity;
}
}
out.writeInt(cardinarity);
if (cardinarity == 0) {
return;
}
int prev = 0;
if (status[i] != FULL) {
int diff = i - prev;
assert (diff >= 0);
VariableByteCodec.encodeUnsignedInt(diff, out);
prev = i;
}
}
}
@Override
final int size = in.readInt();
final int[] keys = new int[size];
final long[] values = new long[size];
final byte[] states = new byte[size];
readStates(in, states);
if (states[i] != FULL) {
continue;
keys[i] = ZigZagLEB128Codec.readSignedInt(in);
values[i] = ZigZagLEB128Codec.readSignedLong(in);
@Nonnull
private static void readStates(@Nonnull final DataInput in, @Nonnull final byte[] status)
throws IOException {
final int cardinarity = in.readInt();
Arrays.fill(status, IntOpenHashTable.FULL);
int prev = 0;
status[i] = IntOpenHashTable.FREE;
prev = i;
}
}
public HeapBuffer(int chunkSize) {
this(DEFAULT_NUM_CHUNKS, chunkSize);
float wi = model.getW(e);
final float[] vij = new float[factors];
final float[] vji = new float[factors];
if (!model.getV(ei, jField, vij)) {
continue;
}
if (!model.getV(ej, iField, vij)) {
import hivemall.utils.buffer.HeapBuffer;
import hivemall.utils.collections.Int2LongOpenHashTable;
import hivemall.utils.lang.SizeOf;
private Int2LongOpenHashTable _map;
private HeapBuffer _buf;
public FFMPredictionModel(@Nonnull Int2LongOpenHashTable map, @Nonnull HeapBuffer buf,
double w0, int factor, int numFeatures, int numFields) {
this._buf = buf;
@Nullable
private Entry getEntry(final int key) {
final long ptr = _map.get(key);
if (ptr == -1L) {
return null;
}
return new Entry(_buf, _factors, ptr);
}
public float getW(@Nonnull final Feature x) {
Entry entry = getEntry(j);
return entry.getW();
public boolean getV(@Nonnull final Feature x, @Nonnull final int yField, @Nonnull float[] dst) {
Entry entry = getEntry(j);
return false;
entry.getV(dst);
return true;
_map.writeExternal(out);
this._map = null;
_buf.writeExternal(out);
this._buf = null;
this._factors = in.readInt();
this._map = new Int2LongOpenHashTable();
_map.readExternal(in);
this._buf = new HeapBuffer();
_buf.readExternal(in);
import hivemall.fm.Entry.AdaGradEntry;
import hivemall.fm.Entry.FTRLEntry;
import hivemall.utils.buffer.HeapBuffer;
import hivemall.utils.collections.Int2LongOpenHashTable;
import javax.annotation.Nullable;
@Nonnull
private final Int2LongOpenHashTable _map;
private final HeapBuffer _buf;
private final int _entrySize;
this._map = new Int2LongOpenHashTable(DEFAULT_MAPSIZE);
this._buf = new HeapBuffer(HeapBuffer.DEFAULT_CHUNK_SIZE);
this._entrySize = entrySize(_factor, _useFTRL, _useAdaGrad);;
return new FFMPredictionModel(_map, _buf, _w0, _factor, _numFeatures, _numFields);
Entry entry = getEntry(j);
return entry.getW();
Entry entry = getEntry(j);
float[] V = initV();
entry = newEntry(nextWi, V);
long ptr = entry.getOffset();
_map.put(j, ptr);
entry.setW(nextWi);
float wi = theta.getW();
theta.setW(nextWi);
float wi = theta.getW();
theta.setW(nextWi);
Entry entry = getEntry(j);
float[] V = initV();
long ptr = entry.getOffset();
_map.put(j, ptr);
return entry.getV(f);
Entry entry = getEntry(j);
float[] V = initV();
long ptr = entry.getOffset();
_map.put(j, ptr);
entry.setV(f, nextVif);
Entry entry = getEntry(j);
float[] V = initV();
entry = newEntry(V);
long ptr = entry.getOffset();
_map.put(j, ptr);
Entry entry = getEntry(j);
long ptr = entry.getOffset();
_map.put(j, ptr);
@Nonnull
protected final Entry newEntry(final float W, @Nonnull final float[] V) {
Entry entry = newEntry();
entry.setW(W);
entry.setV(V);
return entry;
}
@Nonnull
protected final Entry newEntry(@Nonnull final float[] V) {
Entry entry = newEntry();
entry.setV(V);
return entry;
}
@Nonnull
private Entry newEntry() {
if (_useFTRL) {
long ptr = _buf.allocate(_entrySize);
return new FTRLEntry(_buf, _factor, ptr);
} else if (_useAdaGrad) {
long ptr = _buf.allocate(_entrySize);
return new AdaGradEntry(_buf, _factor, ptr);
} else {
long ptr = _buf.allocate(_entrySize);
return new Entry(_buf, _factor, ptr);
}
}
@Nullable
private Entry getEntry(final int key) {
final long ptr = _map.get(key);
if (ptr == -1L) {
return null;
}
return getEntry(ptr);
}
@Nonnull
private Entry getEntry(long ptr) {
if (_useFTRL) {
return new FTRLEntry(_buf, _factor, ptr);
} else if (_useAdaGrad) {
return new AdaGradEntry(_buf, _factor, ptr);
} else {
return new Entry(_buf, _factor, ptr);
}
}
private static int entrySize(int factors, boolean ftrl, boolean adagrad) {
if (ftrl) {
return FTRLEntry.sizeOf(factors);
} else if (adagrad) {
return AdaGradEntry.sizeOf(factors);
} else {
return Entry.sizeOf(factors);
}
}
this._V = new float[params.numFeatures][params.factors];
int factors = 5;
this.factors = Primitives.parseInt(cl.getOptionValue("factors"), factors);
this.vInit = instantiateVInit(cl, factors, seed);
this._factor = params.factors;
this._lambdaV = new float[params.factors];
protected int _factors;
this._factors = params.factors;
forwardAsIntFeature(_model, _factors);
forwardAsStringFeature(strModel, _factors);
final float currentV = theta.getV(f);
theta.setV(f, nextV);
import java.io.Externalizable;
import java.io.IOException;
import java.io.ObjectInput;
import java.io.ObjectOutput;
public final class HeapBuffer implements Externalizable {
public HeapBuffer() {
this._UNSAFE = UnsafeUtils.getUnsafe();
this(chunkSize, DEFAULT_NUM_CHUNKS);
public HeapBuffer(int chunkSize, int initNumChunks) {
@Nonnull
public static HeapBuffer newInstance() {
return new HeapBuffer(DEFAULT_CHUNK_SIZE);
}
public void init(int chunkSize, long position, @Nonnull int[][] chunks) {
this._chunkSize = chunkSize;
this._chunkBytes = chunkSize * SizeOf.INT;
this._chunks = chunks;
this._initializedChunks = chunks.length;
this._position = position;
}
public long consumedBytes() {
return _chunkBytes * _initializedChunks;
@Override
public void writeExternal(@Nonnull final ObjectOutput out) throws IOException {
out.writeInt(_chunkSize);
out.writeLong(_position);
final int[][] chunks = _chunks;
final int numChunks = _initializedChunks;
out.writeInt(numChunks);
final int[] chunk = chunks[i];
if (chunk.length != _chunkSize) {
}
out.writeInt(chunk[j]);
}
chunks[i] = null;
}
this._chunks = null;
}
@Override
public void readExternal(@Nonnull final ObjectInput in) throws IOException,
ClassNotFoundException {
final int chunkSize = in.readInt();
final long position = in.readLong();
final int numChunks = in.readInt();
final int[][] chunks = new int[numChunks][];
final int[] chunk = new int[chunkSize];
chunk[j] = in.readInt();
}
chunks[i] = chunk;
}
init(chunkSize, position, chunks);
}
public static final int DEFAULT_SIZE = 65536;
public static final float DEFAULT_LOAD_FACTOR = 0.7f;
public static final float DEFAULT_GROW_FACTOR = 2.0f;
@Nonnull
public static Int2LongOpenHashTable newInstance() {
return new Int2LongOpenHashTable(DEFAULT_SIZE);
}
@Nonnull
public int[] getKeys() {
return _keys;
}
@Nonnull
public long[] getValues() {
return _values;
}
@Nonnull
public byte[] getStates() {
return _states;
}
public static boolean equals(@Nonnull final float[] array, final float value) {
if (array[i] != value) {
return false;
}
}
return true;
}
public static boolean almostEquals(@Nonnull final float[] array, final float expected) {
return equals(array, expected, 1E-15f);
}
public static boolean equals(@Nonnull final float[] array, final float expected,
final float delta) {
float actual = array[i];
if (Math.abs(expected - actual) > delta) {
return false;
}
}
return true;
}
import hivemall.utils.codec.VariableByteCodec;
import hivemall.utils.codec.ZigZagLEB128Codec;
import hivemall.utils.collections.IntOpenHashTable;
import hivemall.utils.io.IOUtils;
import hivemall.utils.lang.ArrayUtils;
import hivemall.utils.lang.HalfFloat;
import java.io.DataInput;
import java.io.DataOutput;
import java.util.Arrays;
private static final byte HALF_FLOAT_ENTRY = 1;
private static final byte W_ONLY_HALF_FLOAT_ENTRY = 2;
private static final byte FLOAT_ENTRY = 3;
private static final byte W_ONLY_FLOAT_ENTRY = 4;
int rest = _map.capacity() - size;
if (rest > 0) {
}
if (ArrayUtils.equals(dst, 0.f)) {
}
final int factors = _factors;
out.writeInt(factors);
int used = _map.size();
out.writeInt(used);
final int[] keys = _map.getKeys();
final int size = keys.length;
out.writeInt(size);
final byte[] states = _map.getStates();
writeStates(states, out);
final long[] values = _map.getValues();
final HeapBuffer buf = _buf;
final Entry e = new Entry(buf, factors);
final float[] Vf = new float[factors];
if (states[i] != IntOpenHashTable.FULL) {
continue;
}
ZigZagLEB128Codec.writeSignedInt(keys[i], out);
e.setOffset(values[i]);
writeEntry(e, factors, Vf, out);
}
private static void writeEntry(@Nonnull final Entry e, final int factors,
@Nonnull final float[] Vf, @Nonnull final DataOutput out) throws IOException {
final float W = e.getW();
e.getV(Vf);
if (ArrayUtils.almostEquals(Vf, 0.f)) {
if (HalfFloat.isRepresentable(W)) {
out.writeByte(W_ONLY_HALF_FLOAT_ENTRY);
out.writeShort(HalfFloat.floatToHalfFloat(W));
} else {
out.writeByte(W_ONLY_FLOAT_ENTRY);
out.writeFloat(W);
}
} else if (isRepresentableAsHalfFloat(W, Vf)) {
out.writeByte(HALF_FLOAT_ENTRY);
out.writeShort(HalfFloat.floatToHalfFloat(W));
out.writeShort(HalfFloat.floatToHalfFloat(Vf[i]));
}
} else {
out.writeByte(FLOAT_ENTRY);
out.writeFloat(W);
IOUtils.writeFloats(Vf, factors, out);
}
}
private static boolean isRepresentableAsHalfFloat(final float W, @Nonnull final float[] Vf) {
if (!HalfFloat.isRepresentable(W)) {
return false;
}
for (float V : Vf) {
if (!HalfFloat.isRepresentable(V)) {
return false;
}
}
return true;
}
@Nonnull
static void writeStates(@Nonnull final byte[] status, @Nonnull final DataOutput out)
throws IOException {
final int size = status.length;
int cardinarity = 0;
if (status[i] != IntOpenHashTable.FULL) {
cardinarity;
}
}
out.writeInt(cardinarity);
if (cardinarity == 0) {
return;
}
int prev = 0;
if (status[i] != IntOpenHashTable.FULL) {
int diff = i - prev;
assert (diff >= 0);
VariableByteCodec.encodeUnsignedInt(diff, out);
prev = i;
}
}
}
final int factors = in.readInt();
this._factors = factors;
final int used = in.readInt();
final int size = in.readInt();
final int[] keys = new int[size];
final long[] values = new long[size];
final byte[] states = new byte[size];
readStates(in, states);
final int entrySize = Entry.sizeOf(factors);
final HeapBuffer buf = new HeapBuffer(HeapBuffer.DEFAULT_CHUNK_SIZE, numChunks);
final Entry e = new Entry(buf, factors);
final float[] Vf = new float[factors];
if (states[i] != IntOpenHashTable.FULL) {
continue;
}
keys[i] = ZigZagLEB128Codec.readSignedInt(in);
long ptr = buf.allocate(entrySize);
e.setOffset(ptr);
readEntry(in, factors, Vf, e);
values[i] = ptr;
}
this._map = new Int2LongOpenHashTable(keys, values, states, used);
this._buf = buf;
}
@Nonnull
private static void readEntry(@Nonnull final DataInput in, final int factors,
@Nonnull final float[] Vf, @Nonnull Entry dst) throws IOException {
final byte type = in.readByte();
switch (type) {
case HALF_FLOAT_ENTRY: {
float W = HalfFloat.halfFloatToFloat(in.readShort());
dst.setW(W);
Vf[i] = HalfFloat.halfFloatToFloat(in.readShort());
}
dst.setV(Vf);
break;
}
case W_ONLY_HALF_FLOAT_ENTRY: {
float W = HalfFloat.halfFloatToFloat(in.readShort());
dst.setW(W);
break;
}
case FLOAT_ENTRY: {
float W = in.readFloat();
dst.setW(W);
IOUtils.readFloats(in, Vf);
dst.setV(Vf);
break;
}
case W_ONLY_FLOAT_ENTRY: {
float W = in.readFloat();
dst.setW(W);
break;
}
default:
}
}
@Nonnull
static void readStates(@Nonnull final DataInput in, @Nonnull final byte[] status)
throws IOException {
final int cardinarity = in.readInt();
Arrays.fill(status, IntOpenHashTable.FULL);
int prev = 0;
status[i] = IntOpenHashTable.FREE;
prev = i;
}
public final class HeapBuffer {
public static final int DEFAULT_CHUNK_BYTES = SizeOf.INT * DEFAULT_CHUNK_SIZE;
private final int _chunkSize;
private final int _chunkBytes;
this(DEFAULT_CHUNK_SIZE);
}
public int getChunkSize() {
return _chunkBytes;
}
public int getNumInitializedChunks() {
public int getNumChunks() {
return _chunks.length;
public long position() {
return _position;
public int getNumAllocated() {
return _numAllocated;
public long getAllocatedBytes() {
return _allocatedBytes;
}
public long getSkippedBytes() {
return _skippedBytes;
}
protected int _used;
protected int _threshold;
protected long defaultReturnValue = -1L;
this._loadFactor = DEFAULT_LOAD_FACTOR;
this._growFactor = DEFAULT_GROW_FACTOR;
}
public Int2LongOpenHashTable(int size) {
this(size, DEFAULT_LOAD_FACTOR, DEFAULT_GROW_FACTOR, true);
}
public Int2LongOpenHashTable(int size, float loadFactor, float growFactor) {
this(size, loadFactor, growFactor, true);
}
this._used = 0;
public Int2LongOpenHashTable(@Nonnull int[] keys, @Nonnull long[] values, @Nonnull byte[] states,
int used) {
this._keys = keys;
this._values = values;
this._states = states;
this._used = used;
this._threshold = keys.length;
@Nonnull
public static void readFloats(@Nonnull final DataInput in, @Nonnull final float[] dst)
throws IOException {
dst[i] = in.readFloat();
}
}
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
private static final Log LOG = LogFactory.getLog(FFMPredictionModel.class);
protected String varDump(@Nonnull final Feature[] x) {
@Override
protected final String varDump(@Nonnull final Feature[] x) {
final StringBuilder buf = new StringBuilder(1024);
Feature e = x[i];
String j = e.getFeature();
double xj = e.getValue();
if (i != 0) {
buf.append(", ");
}
buf.append("x[").append(j).append("] => ").append(xj);
}
buf.append("\n\n");
buf.append("W0 => ").append(getW0()).append('\n');
Feature e = x[i];
String j = e.getFeature();
float wi = getW(e);
if (i != 0) {
buf.append(", ");
}
buf.append("W[").append(j).append("] => ").append(wi);
}
buf.append("\n");
return buf.toString();
}
int iter = 2;
setCounterValue(iterCounter, iter);
if (_cvState.isConverged(iter, numTrainingExamples)) {
int iter = 2;
setCounterValue(iterCounter, iter);
if (_cvState.isConverged(iter, numTrainingExamples)) {
int iter = 2;
setCounterValue(iterCounter, iter);
if (cvState.isConverged(iter, numTrainingExamples)) {
int iter = 2;
setCounterValue(iterCounter, iter);
if (cvState.isConverged(iter, numTrainingExamples)) {
float maxInitValue = Primitives.parseFloat(cl.getOptionValue("max_init_value"), 0.5f);
import hivemall.utils.math.MathUtils;
"The maximum initial value in the matrix V [default: 0.5]");
if (MathUtils.closeToZero(lossGrad)) {
return;
}
import hivemall.utils.math.MathUtils;
if (MathUtils.closeToZero(lossGrad)) {
public static boolean equals(@Nonnull final float value, final float expected, final float delta) {
if (Math.abs(expected - value) > delta) {
return false;
}
return true;
}
public static boolean equals(@Nonnull final double value, final double expected,
final double delta) {
if (Math.abs(expected - value) > delta) {
return false;
}
return true;
}
public static boolean almostEquals(@Nonnull final float value, final float expected,
final float delta) {
return equals(value, expected, 1E-15f);
}
public static boolean almostEquals(@Nonnull final double value, final double expected,
final double delta) {
return equals(value, expected, 1E-15d);
}
public static boolean closeToZero(@Nonnull final float value) {
if (Math.abs(value) > 1E-15f) {
return false;
}
return true;
}
public static boolean closeToZero(@Nonnull final double value) {
if (Math.abs(value) > 1E-15d) {
return false;
}
return true;
}
final double dloss(@Nonnull final Feature[] x, final double y) throws HiveException {
protected double predict(@Nonnull final Feature[] x) throws HiveException {
buf.append("x[").append(j).append("] = ").append(xj);
buf.append("\n");
buf.append("W0 = ").append(getW0()).append('\n');
buf.append("W[").append(j).append("] = ").append(wi);
buf.append("\n");
buf.append('V').append(f).append('[').append(j).append("] = ").append(vjf);
import org.apache.hadoop.hive.ql.metadata.HiveException;
protected final double predict(@Nonnull final Feature[] x) throws HiveException {
buf.append("x[").append(j).append("] = ").append(xj);
buf.append("\n");
buf.append("W0 = ").append(getW0()).append('\n');
buf.append("W[").append(j).append("] = ").append(wi);
buf.append('\n');
final Feature ei = x[i];
final int iField = ei.getField();
if (i != 0) {
buf.append('\n');
}
final Feature ej = x[j];
final int jField = ej.getField();
float vijf = getV(ei, jField, f);
float vjif = getV(ej, iField, f);
buf.append(", ");
}
buf.append("V[i")
.append(i)
.append("][j")
.append(j)
.append("][f")
.append(f)
.append("] = ")
.append(vijf);
buf.append(", V[j")
.append(j)
.append("][i")
.append(i)
.append("][f")
.append(f)
.append("] = ")
.append(vjif);
}
}
}
this.vInit = instantiateVInit(cl, factors, seed, classification);
private static VInitScheme instantiateVInit(@Nonnull CommandLine cl, int factor, long seed,
final boolean classification) {
VInitScheme defaultInit = classification ? VInitScheme.gaussian : VInitScheme.random;
VInitScheme vInit = VInitScheme.resolve(vInitOpt, defaultInit);
return resolve(opt, random);
}
@Nonnull
public static VInitScheme resolve(@Nullable String opt, @Nonnull VInitScheme defaultScheme) {
return defaultScheme;
return defaultScheme;
opts.addOption("init_v", true, "Initialization strategy of matrix V [random, gaussian]"
"(default: 'random' for regression / 'gaussian' for classification)");
']');
final StringBuilder buf1 = new StringBuilder(1024);
final StringBuilder buf2 = new StringBuilder(1024);
Feature e = x[i];
buf1.append(", ");
buf1.append("x[").append(j).append("] = ").append(xj);
buf1.append("\n");
double ret = getW0();
buf1.append("predict(x) = w0");
buf2.append("predict(x) = ").append(ret);
for (Feature e : x) {
String i = e.getFeature();
double xi = e.getValue();
double wx = wi * xi;
if (!NumberUtils.isFinite(ret)) {
.append(ret)
.append('\n')
.append(buf2)
.append(ret)
.toString();
final String fi = ei.getFeature();
final double xi = ei.getValue();
final String fj = ej.getFeature();
final double xj = ej.getValue();
.append(fi)
.append('j')
.append(jField)
.append('f')
.append(f)
.append("] * v[j")
.append(fj)
.append('i')
.append(iField)
.append('f')
.append(f)
.append("] * x[")
.append(fi)
.append("] * x[")
.append(fj)
.append("])");
.append(vijf)
.append(" * ")
.append(vjif)
.append(" * ")
.append(xi)
.append(" * ")
.append(xj)
.append(')');
if (!NumberUtils.isFinite(ret)) {
.append(ret)
.append('\n')
.append(buf2)
.append(ret)
.toString();
return buf1.append(" = ")
.append(ret)
.append('\n')
.append(buf2)
.append(" = ")
.append(ret)
.toString();
public static double logit(final double p) {
return Math.log(p / (1.d - p));
}
public static double logit(final double p, final double hi, final double lo) {
return Math.log((p - lo) / (hi - p));
}
import javax.annotation.Nonnull;
@Description(
name = "extract_weight",
value = "_FUNC_(feature_vector in array<string>) - Returns the weights of features in array<string>")
if (featureVectors == null) {
static DoubleWritable extractWeights(String ftvec) throws UDFArgumentException {
if (ftvec == null) {
final int pos = ftvec.lastIndexOf(':');
if (pos > 0) {
double d = parseDouble(s);
private static double parseDouble(@Nonnull final String v) throws UDFArgumentException {
try {
return Double.parseDouble(v);
} catch (NumberFormatException nfe) {
throw new UDFArgumentException(nfe);
}
}
Feature e = x[i];
.append("-j")
.append("-f")
.append("-i")
.append("-f")
opts.addOption("p", "num_features", true, "The size of feature dimensions");
opts.addOption("factor", "factors", true, "The number of the latent variables [default: 5]");
opts.addOption("lambda0", "lambda", true,
this.parseFeatureAsInt = cl.hasOption("int_feature");
if (cl.hasOption("int_feature")) {
throw new UDFArgumentException("int_feature option is not supported yet for FFM");
}
@Nonnull
']');
if ("SquaredLoss".equalsIgnoreCase(type)) {
} else if ("LogLoss".equalsIgnoreCase(type)) {
} else if ("HingeLoss".equalsIgnoreCase(type)) {
} else if ("SquaredHingeLoss".equalsIgnoreCase(type)) {
} else if ("QuantileLoss".equalsIgnoreCase(type)) {
} else if ("EpsilonInsensitiveLoss".equalsIgnoreCase(type)) {
if (!(y == 1.f || y == -1.f)) {
if (!(y == 1.d || y == -1.d)) {
if (z > 18.f) {
if (z < -18.f) {
if (z > 18.d) {
if (z < -18.d) {
if (z > 18.f) {
if (z < -18.f) {
if (tau <= 0 || tau >= 1.0) {
if (e > 0.f) {
if (e > 0.d) {
if (e == 0.f) {
if (predicted > -100.d) {
if (z > 18.f) {
if (z < -18.f) {
if (z > 18.d) {
if (z < -18.d) {
float wi = model.getW(e);
final float[] vij = new float[factors];
final float[] vji = new float[factors];
if (!model.getV(ei, jField, vij)) {
continue;
}
if (!model.getV(ej, iField, vij)) {
import hivemall.utils.buffer.HeapBuffer;
import hivemall.utils.collections.Int2LongOpenHashTable;
import hivemall.utils.lang.ArrayUtils;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
public final class FFMPredictionModel implements Externalizable {
private static final Log LOG = LogFactory.getLog(FFMPredictionModel.class);
private static final byte HALF_FLOAT_ENTRY = 1;
private static final byte W_ONLY_HALF_FLOAT_ENTRY = 2;
private static final byte FLOAT_ENTRY = 3;
private static final byte W_ONLY_FLOAT_ENTRY = 4;
private Int2LongOpenHashTable _map;
private HeapBuffer _buf;
public FFMPredictionModel(@Nonnull Int2LongOpenHashTable map, @Nonnull HeapBuffer buf,
double w0, int factor, int numFeatures, int numFields) {
this._buf = buf;
@Nullable
private Entry getEntry(final int key) {
final long ptr = _map.get(key);
if (ptr == -1L) {
return null;
}
return new Entry(_buf, _factors, ptr);
}
public float getW(@Nonnull final Feature x) {
Entry entry = getEntry(j);
return entry.getW();
public boolean getV(@Nonnull final Feature x, @Nonnull final int yField, @Nonnull float[] dst) {
Entry entry = getEntry(j);
return false;
entry.getV(dst);
if (ArrayUtils.equals(dst, 0.f)) {
}
return true;
final int factors = _factors;
out.writeInt(factors);
final long[] values = _map.getValues();
final HeapBuffer buf = _buf;
final Entry e = new Entry(buf, factors);
final float[] Vf = new float[factors];
e.setOffset(values[i]);
writeEntry(e, factors, Vf, out);
this._map = null;
this._buf = null;
private static void writeEntry(@Nonnull final Entry e, final int factors,
@Nonnull final float[] Vf, @Nonnull final DataOutput out) throws IOException {
final float W = e.getW();
e.getV(Vf);
if (ArrayUtils.almostEquals(Vf, 0.f)) {
if (HalfFloat.isRepresentable(W)) {
out.writeByte(W_ONLY_HALF_FLOAT_ENTRY);
out.writeShort(HalfFloat.floatToHalfFloat(W));
} else {
out.writeByte(W_ONLY_FLOAT_ENTRY);
out.writeFloat(W);
}
} else if (isRepresentableAsHalfFloat(W, Vf)) {
out.writeByte(HALF_FLOAT_ENTRY);
out.writeByte(FLOAT_ENTRY);
final int used = in.readInt();
final long[] values = new long[size];
final int entrySize = Entry.sizeOf(factors);
final HeapBuffer buf = new HeapBuffer(HeapBuffer.DEFAULT_CHUNK_SIZE, numChunks);
final Entry e = new Entry(buf, factors);
final float[] Vf = new float[factors];
long ptr = buf.allocate(entrySize);
e.setOffset(ptr);
readEntry(in, factors, Vf, e);
values[i] = ptr;
this._map = new Int2LongOpenHashTable(keys, values, states, used);
this._buf = buf;
private static void readEntry(@Nonnull final DataInput in, final int factors,
@Nonnull final float[] Vf, @Nonnull Entry dst) throws IOException {
final byte type = in.readByte();
switch (type) {
case HALF_FLOAT_ENTRY: {
float W = HalfFloat.halfFloatToFloat(in.readShort());
dst.setW(W);
Vf[i] = HalfFloat.halfFloatToFloat(in.readShort());
}
dst.setV(Vf);
break;
case W_ONLY_HALF_FLOAT_ENTRY: {
float W = HalfFloat.halfFloatToFloat(in.readShort());
dst.setW(W);
break;
}
case FLOAT_ENTRY: {
float W = in.readFloat();
dst.setW(W);
IOUtils.readFloats(in, Vf);
dst.setV(Vf);
break;
}
case W_ONLY_FLOAT_ENTRY: {
float W = in.readFloat();
dst.setW(W);
break;
}
default:
import hivemall.fm.Entry.AdaGradEntry;
import hivemall.fm.Entry.FTRLEntry;
import hivemall.fm.FMHyperParameters.FFMHyperParameters;
import hivemall.utils.buffer.HeapBuffer;
import hivemall.utils.collections.Int2LongOpenHashTable;
import hivemall.utils.math.MathUtils;
import javax.annotation.Nullable;
@Nonnull
private final Int2LongOpenHashTable _map;
private final HeapBuffer _buf;
private final float _alpha;
private final float _beta;
private final float _lambda1;
private final float _lamdda2;
private final int _entrySize;
public FFMStringFeatureMapModel(@Nonnull FFMHyperParameters params) {
super(params);
this._map = new Int2LongOpenHashTable(DEFAULT_MAPSIZE);
this._buf = new HeapBuffer(HeapBuffer.DEFAULT_CHUNK_SIZE);
this._numFeatures = params.numFeatures;
this._numFields = params.numFields;
this._alpha = params.alphaFTRL;
this._beta = params.betaFTRL;
this._lambda1 = params.lambda1;
this._lamdda2 = params.lamdda2;
this._entrySize = entrySize(_factor, _useFTRL, _useAdaGrad);;
return new FFMPredictionModel(_map, _buf, _w0, _factor, _numFeatures, _numFields);
Entry entry = getEntry(j);
return entry.getW();
Entry entry = getEntry(j);
float[] V = initV();
entry = newEntry(nextWi, V);
long ptr = entry.getOffset();
_map.put(j, ptr);
entry.setW(nextWi);
float wi = theta.getW();
theta.setW(nextWi);
boolean updateWiFTRL(final double dloss, @Nonnull final Feature x, final float eta) {
final double Xi = x.getValue();
float gradWi = (float) (dloss * Xi);
final Entry theta = getEntry(x);
float wi = theta.getW();
final float z = theta.updateZ(gradWi, _alpha);
final double n = theta.updateN(gradWi);
if (Math.abs(z) <= _lambda1) {
removeEntry(x);
return wi != 0;
}
if (!NumberUtils.isFinite(nextWi)) {
}
theta.setW(nextWi);
return (nextWi != 0) || (wi != 0);
}
Entry entry = getEntry(j);
float[] V = initV();
long ptr = entry.getOffset();
_map.put(j, ptr);
return entry.getV(f);
Entry entry = getEntry(j);
float[] V = initV();
long ptr = entry.getOffset();
_map.put(j, ptr);
entry.setV(f, nextVif);
Entry entry = getEntry(j);
long ptr = entry.getOffset();
_map.put(j, ptr);
Entry entry = getEntry(j);
long ptr = entry.getOffset();
_map.put(j, ptr);
protected void removeEntry(@Nonnull final Feature x) {
int j = x.getFeatureIndex();
_map.remove(j);
}
@Nonnull
protected final Entry newEntry(final float W, @Nonnull final float[] V) {
Entry entry = newEntry();
entry.setW(W);
entry.setV(V);
return entry;
}
@Nonnull
protected final Entry newEntry(@Nonnull final float[] V) {
Entry entry = newEntry();
entry.setV(V);
return entry;
}
@Nonnull
private Entry newEntry() {
if (_useFTRL) {
long ptr = _buf.allocate(_entrySize);
return new FTRLEntry(_buf, _factor, ptr);
} else if (_useAdaGrad) {
long ptr = _buf.allocate(_entrySize);
return new AdaGradEntry(_buf, _factor, ptr);
} else {
long ptr = _buf.allocate(_entrySize);
return new Entry(_buf, _factor, ptr);
}
}
@Nullable
private Entry getEntry(final int key) {
final long ptr = _map.get(key);
if (ptr == -1L) {
return null;
}
return getEntry(ptr);
}
@Nonnull
private Entry getEntry(long ptr) {
if (_useFTRL) {
return new FTRLEntry(_buf, _factor, ptr);
} else if (_useAdaGrad) {
return new AdaGradEntry(_buf, _factor, ptr);
} else {
return new Entry(_buf, _factor, ptr);
}
}
private static int entrySize(int factors, boolean ftrl, boolean adagrad) {
if (ftrl) {
return FTRLEntry.sizeOf(factors);
} else if (adagrad) {
return AdaGradEntry.sizeOf(factors);
} else {
return Entry.sizeOf(factors);
}
}
public FMArrayModel(@Nonnull FMHyperParameters params) {
super(params);
this._p = params.numFeatures;
this._V = new float[params.numFeatures][params.factors];
public FMIntFeatureMapModel(@Nonnull FMHyperParameters params) {
super(params);
public FMStringFeatureMapModel(@Nonnull FMHyperParameters params) {
super(params);
if (entry == null) {
if (entry == null) {
if (entry == null) {
if (entry == null) {
public FactorizationMachineModel(@Nonnull FMHyperParameters params) {
this._classification = params.classification;
this._factor = params.factors;
this._sigma = params.sigma;
this._eta = params.eta;
this._initScheme = params.vInit;
this._rnd = new Random(params.seed);
this._min_target = params.minTarget;
this._max_target = params.maxTarget;
this._lambdaW0 = params.lambdaW0;
this._lambdaW = params.lambdaW;
this._lambdaV = new float[params.factors];
Arrays.fill(_lambdaV, params.lambdaV);
final double dloss(@Nonnull final Feature[] x, final double y) throws HiveException {
protected double predict(@Nonnull final Feature[] x) throws HiveException {
protected String varDump(@Nonnull final Feature[] x) {
buf.append("x[").append(j).append("] = ").append(xj);
buf.append("\n");
buf.append("W0 = ").append(getW0()).append('\n');
buf.append("W[").append(j).append("] = ").append(wi);
buf.append("\n");
buf.append('V').append(f).append('[').append(j).append("] = ").append(vjf);
return resolve(opt, random);
}
@Nonnull
public static VInitScheme resolve(@Nullable String opt, @Nonnull VInitScheme defaultScheme) {
return defaultScheme;
return defaultScheme;
import hivemall.utils.math.MathUtils;
private static final Log LOG = LogFactory.getLog(FactorizationMachineUDTF.class);
@Nullable
protected Feature[] _probes;
protected FMHyperParameters _params;
protected int _factors;
protected boolean _parseFeatureAsInt;
protected ConversionState _cvState;
opts.addOption("p", "num_features", true, "The size of feature dimensions");
opts.addOption("factor", "factors", true, "The number of the latent variables [default: 5]");
opts.addOption("lambda0", "lambda", true,
opts.addOption("lambdaW0", "lambda_w0", true,
"The initial lambda value for W0 regularization [default: 0.01]");
opts.addOption("lambdaWi", "lambda_wi", true,
"The initial lambda value for Wi regularization [default: 0.01]");
opts.addOption("lambdaV", "lambda_v", true,
"The initial lambda value for V regularization [default: 0.01]");
opts.addOption("init_v", true, "Initialization strategy of matrix V [random, gaussian]"
"(default: 'random' for regression / 'gaussian' for classification)");
"The maximum initial value in the matrix V [default: 0.5]");
"Parse a feature as integer [default: OFF]");
protected CommandLine processOptions(@Nonnull ObjectInspector[] argOIs)
throws UDFArgumentException {
final FMHyperParameters params = _params;
params.processOptions(cl);
this._classification = params.classification;
this._iterations = params.iters;
this._factors = params.factors;
this._parseFeatureAsInt = params.parseFeatureAsInt;
if (params.adaptiveReglarization) {
this._validationRatio = params.validationRatio;
this._validationThreshold = params.validationThreshold;
this._lossFunction = params.classification ? LossFunctions.getLossFunction(LossType.LogLoss)
this._etaEstimator = params.eta;
this._cvState = new ConversionState(params.conversionCheck, params.convergenceRate);
@Nonnull
this._params = newHyperParameters();
this._model = initModel(cl, _params);
if (LOG.isInfoEnabled()) {
LOG.info(_params);
}
return getOutputOI(_params);
protected FMHyperParameters newHyperParameters() {
return new FMHyperParameters();
}
@Nonnull
protected StructObjectInspector getOutputOI(@Nonnull FMHyperParameters params) {
if (params.parseFeatureAsInt) {
@Nonnull
protected FactorizationMachineModel initModel(@Nullable CommandLine cl,
@Nonnull FMHyperParameters params) throws UDFArgumentException {
if (params.parseFeatureAsInt) {
if (params.numFeatures == -1) {
return new FMIntFeatureMapModel(params);
return new FMArrayModel(params);
return new FMStringFeatureMapModel(params);
if (MathUtils.closeToZero(lossGrad)) {
return;
}
this._probes = null;
this._model = null;
this._model = null;
forwardAsIntFeature(_model, _factors);
forwardAsStringFeature(strModel, _factors);
int iter = 2;
setCounterValue(iterCounter, iter);
if (_cvState.isConverged(iter, numTrainingExamples)) {
if (LOG.isInfoEnabled()) {
int iter = 2;
setCounterValue(iterCounter, iter);
if (_cvState.isConverged(iter, numTrainingExamples)) {
import hivemall.fm.FMHyperParameters.FFMHyperParameters;
import org.apache.hadoop.hive.ql.metadata.HiveException;
@Nonnull
protected final FFMHyperParameters _params;
protected final float _eta0_V;
protected final float _eps;
protected final boolean _useAdaGrad;
protected final boolean _useFTRL;
public FieldAwareFactorizationMachineModel(@Nonnull FFMHyperParameters params) {
super(params);
this._params = params;
this._eta0_V = params.eta0_V;
this._eps = params.eps;
this._useAdaGrad = params.useAdaGrad;
this._useFTRL = params.useFTRL;
@Deprecated
protected final double predict(@Nonnull final Feature[] x) throws HiveException {
final float currentV = theta.getV(f);
theta.setV(f, nextV);
if (_useAdaGrad) {
double gg = theta.getSumOfSquaredGradientsV();
theta.addGradientV(grad);
@Override
protected final String varDump(@Nonnull final Feature[] x) {
final StringBuilder buf1 = new StringBuilder(1024);
final StringBuilder buf2 = new StringBuilder(1024);
Feature e = x[i];
String j = e.getFeature();
double xj = e.getValue();
if (i != 0) {
buf1.append(", ");
}
buf1.append("x[").append(j).append("] = ").append(xj);
buf1.append("\n");
double ret = getW0();
buf1.append("predict(x) = w0");
buf2.append("predict(x) = ").append(ret);
for (Feature e : x) {
String i = e.getFeature();
double xi = e.getValue();
float wi = getW(e);
double wx = wi * xi;
if (!NumberUtils.isFinite(ret)) {
.append(ret)
.append('\n')
.append(buf2)
.append(ret)
.toString();
}
final Feature ei = x[i];
final String fi = ei.getFeature();
final double xi = ei.getValue();
final int iField = ei.getField();
final Feature ej = x[j];
final String fj = ej.getFeature();
final double xj = ej.getValue();
final int jField = ej.getField();
float vijf = getV(ei, jField, f);
float vjif = getV(ej, iField, f);
.append(fi)
.append("-j")
.append(jField)
.append("-f")
.append(f)
.append("] * v[j")
.append(fj)
.append("-i")
.append(iField)
.append("-f")
.append(f)
.append("] * x[")
.append(fi)
.append("] * x[")
.append(fj)
.append("])");
.append(vijf)
.append(" * ")
.append(vjif)
.append(" * ")
.append(xi)
.append(" * ")
.append(xj)
.append(')');
if (!NumberUtils.isFinite(ret)) {
.append(ret)
.append('\n')
.append(buf2)
.append(ret)
.toString();
}
}
}
return buf1.append(" = ")
.append(ret)
.append('\n')
.append(buf2)
.append(" = ")
.append(ret)
.toString();
import hivemall.fm.FMHyperParameters.FFMHyperParameters;
import hivemall.utils.math.MathUtils;
private boolean _FTRL;
private FFMStringFeatureMapModel _ffmModel;
opts.addOption("disable_wi", "no_coeff", false, "Not to include linear term [default: OFF]");
opts.addOption("disable_ftrl", false,
"Whether not to use Follow-The-Regularized-Reader [default: OFF]");
opts.addOption("alpha", "alphaFTRL", true,
"Alpha value (learning rate) of Follow-The-Regularized-Reader [default 0.1]");
opts.addOption("beta", "betaFTRL", true,
"Beta value (a learning smoothing parameter) of Follow-The-Regularized-Reader [default 1.0]");
opts.addOption(
"lambda1",
true,
"L1 regularization value of Follow-The-Regularized-Reader that controls model Sparseness [default 0.1]");
opts.addOption("lambda2", true,
"L2 regularization value of Follow-The-Regularized-Reader [default 0.01]");
protected FFMHyperParameters newHyperParameters() {
return new FFMHyperParameters();
}
@Override
FFMHyperParameters params = (FFMHyperParameters) _params;
this._FTRL = params.useFTRL;
this._globalBias = params.globalBias;
this._linearCoeff = params.linearCoeff;
this._numFeatures = params.numFeatures;
this._numFields = params.numFields;
protected StructObjectInspector getOutputOI(@Nonnull FMHyperParameters params) {
protected FFMStringFeatureMapModel initModel(@Nullable CommandLine cl,
@Nonnull FMHyperParameters params) throws UDFArgumentException {
FFMHyperParameters ffmParams = (FFMHyperParameters) params;
FFMStringFeatureMapModel model = new FFMStringFeatureMapModel(ffmParams);
this._ffmModel = model;
_ffmModel.check(x);
final double p = _ffmModel.predict(x);
final double lossGrad = _ffmModel.dloss(p, y);
if (MathUtils.closeToZero(lossGrad)) {
return;
if (_globalBias) {
_ffmModel.updateW0(lossGrad, eta_t);
final DoubleArray3D sumVfX = _ffmModel.sumVfX(x, fieldList, _sumVfX);
if (x_i.value == 0.f) {
continue;
}
if (useV == false) {
continue;
}
_ffmModel.updateV(lossGrad, x_i, yField, f, sumViX, _t);
private boolean updateWi(double lossGrad, @Nonnull Feature xi, float eta) {
if (!_linearCoeff) {
return true;
}
if (_FTRL) {
return _ffmModel.updateWiFTRL(lossGrad, xi, eta);
} else {
_ffmModel.updateWi(lossGrad, xi, eta);
return true;
}
}
this._ffmModel = null;
this._model = null;
FFMPredictionModel predModel = _ffmModel.toPredictionModel();
import javax.annotation.Nonnull;
@Description(
name = "extract_weight",
value = "_FUNC_(feature_vector in array<string>) - Returns the weights of features in array<string>")
if (featureVectors == null) {
static DoubleWritable extractWeights(String ftvec) throws UDFArgumentException {
if (ftvec == null) {
final int pos = ftvec.lastIndexOf(':');
if (pos > 0) {
double d = parseDouble(s);
private static double parseDouble(@Nonnull final String v) throws UDFArgumentException {
try {
return Double.parseDouble(v);
} catch (NumberFormatException nfe) {
throw new UDFArgumentException(nfe);
}
}
int iter = 2;
setCounterValue(iterCounter, iter);
if (cvState.isConverged(iter, numTrainingExamples)) {
int iter = 2;
setCounterValue(iterCounter, iter);
if (cvState.isConverged(iter, numTrainingExamples)) {
public static final int DEFAULT_COMPRESSION_LEVEL = -1;
deflate, xz, lzma2;
}
public static InputStream createInputStream(@Nonnull final InputStream in,
@Nonnull final CompressionAlgorithm algo) {
return createInputStream(in, algo, DEFAULT_COMPRESSION_LEVEL);
@Nonnull final CompressionAlgorithm algo, final int level) {
case deflate: {
final int dictSize;
if (level == DEFAULT_COMPRESSION_LEVEL) {
} else {
final LZMA2Options options;
try {
options = new LZMA2Options(level);
} catch (UnsupportedOptionsException e) {
throw new IllegalStateException("LZMA2Option configuration failed", e);
}
dictSize = options.getDictSize();
}
return new LZMA2InputStream(in, dictSize);
return createOutputStream(out, algo, DEFAULT_COMPRESSION_LEVEL);
}
@Nonnull
public static FinishableOutputStream createOutputStream(@Nonnull final OutputStream out,
@Nonnull final CompressionAlgorithm algo, int level) {
final DeflaterOutputStream deflateOut;
if (level == DEFAULT_COMPRESSION_LEVEL) {
deflateOut = new DeflaterOutputStream(out);
} else {
Deflater d = new Deflater(level);
deflateOut = new hivemall.utils.io.DeflaterOutputStream(out, d);
}
return new FinishableOutputStreamAdapter(deflateOut) {
deflateOut.finish();
deflateOut.flush();
if (level == DEFAULT_COMPRESSION_LEVEL) {
}
options = new LZMA2Options(level);
if (level == DEFAULT_COMPRESSION_LEVEL) {
}
options = new LZMA2Options(level);
@Nonnull
public static void readFloats(@Nonnull final DataInput in, @Nonnull final float[] dst)
throws IOException {
dst[i] = in.readFloat();
}
}
public static boolean equals(@Nonnull final float[] array, final float value) {
if (array[i] != value) {
return false;
}
}
return true;
}
public static boolean almostEquals(@Nonnull final float[] array, final float expected) {
return equals(array, expected, 1E-15f);
}
public static boolean equals(@Nonnull final float[] array, final float expected,
final float delta) {
float actual = array[i];
if (Math.abs(expected - actual) > delta) {
return false;
}
}
return true;
}
public static void checkArgument(boolean expression) {
if (!expression) {
throw new IllegalArgumentException();
}
}
public static void checkArgument(boolean expression, @Nullable Object errorMessage) {
if (!expression) {
throw new IllegalArgumentException(String.valueOf(errorMessage));
}
}
public static void checkArgument(boolean expression, @Nullable String errorMessageTemplate,
@Nullable Object... errorMessageArgs) {
if (!expression) {
throw new IllegalArgumentException(StringUtils.format(errorMessageTemplate,
errorMessageArgs));
}
}
public static int castToInt(final long value) {
final int result = (int) value;
if (result != value) {
}
return result;
}
import javax.annotation.Nullable;
@Nonnull
public static String format(@Nullable String template, @Nullable Object... args) {
template = String.valueOf(template);
int templateStart = 0;
int i = 0;
while (i < args.length) {
int placeholderStart = template.indexOf("%s", templateStart);
if (placeholderStart == -1) {
break;
}
builder.append(template, templateStart, placeholderStart);
}
builder.append(template, templateStart, template.length());
if (i < args.length) {
builder.append(" [");
while (i < args.length) {
builder.append(", ");
}
builder.append(']');
}
return builder.toString();
}
double x2 = Math.max(Math.min(x, 23.d), -23.d);
public static double logit(final double p) {
return Math.log(p / (1.d - p));
}
public static double logit(final double p, final double hi, final double lo) {
return Math.log((p - lo) / (hi - p));
}
public static float sign(final float v) {
return v < 0.f ? -1.f : 1.f;
}
public static boolean equals(@Nonnull final float value, final float expected, final float delta) {
if (Math.abs(expected - value) > delta) {
return false;
}
return true;
}
public static boolean equals(@Nonnull final double value, final double expected,
final double delta) {
if (Math.abs(expected - value) > delta) {
return false;
}
return true;
}
public static boolean almostEquals(@Nonnull final float value, final float expected,
final float delta) {
return equals(value, expected, 1E-15f);
}
public static boolean almostEquals(@Nonnull final double value, final double expected,
final double delta) {
return equals(value, expected, 1E-15d);
}
public static boolean closeToZero(@Nonnull final float value) {
if (Math.abs(value) > 1E-15f) {
return false;
}
return true;
}
public static boolean closeToZero(@Nonnull final double value) {
if (Math.abs(value) > 1E-15d) {
return false;
}
return true;
}
import hivemall.utils.lang.RandomUtils;
fieldOIs.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
this._treeBuildTaskCounter = (_progressReporter == null) ? null
: _progressReporter.getCounter("hivemall.smile.RandomForestClassifier$Counter",
"finishedTreeBuildTasks");
synchronized void forward(final int taskId, @Nonnull final Text model,
String modelId = RandomUtils.getUUID();
final Object[] forwardObjs = new Object[6];
forwardObjs[0] = new Text(modelId);
import hivemall.utils.lang.RandomUtils;
fieldOIs.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
synchronized void forward(final int taskId, @Nonnull final Text model,
String modelId = RandomUtils.getUUID();
final Object[] forwardObjs = new Object[6];
forwardObjs[0] = new Text(modelId);
value = "_FUNC_(string modelId, int modelType, string script, array<double> features [, const boolean classification])"
if (arg0 == null) {
throw new HiveException("ModelId was null");
}
String modelId = arg0.toString();
Writable evaluate(@Nonnull String modelId, boolean compressed, @Nonnull final Text script,
@Nullable
private String prevModelId = null;
public Writable evaluate(@Nonnull String modelId, boolean compressed, @Nonnull Text script,
private IntWritable evaluateClassification(@Nonnull String modelId, boolean compressed,
if (!modelId.equals(prevModelId)) {
private DoubleWritable evaluteRegression(@Nonnull String modelId, boolean compressed,
if (!modelId.equals(prevModelId)) {
private String prevModelId = null;
public Writable evaluate(@Nonnull String modelId, boolean compressed, @Nonnull Text script,
if (modelId.equals(prevModelId)) {
private String prevModelId = null;
public Writable evaluate(@Nonnull String modelId, boolean compressed, @Nonnull Text script,
if (modelId.equals(prevModelId)) {
import hivemall.utils.lang.RandomUtils;
return RandomUtils.getUUID();
import hivemall.utils.lang.RandomUtils;
fieldOIs.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
this._treeBuildTaskCounter = (_progressReporter == null) ? null
: _progressReporter.getCounter("hivemall.smile.RandomForestClassifier$Counter",
"finishedTreeBuildTasks");
synchronized void forward(final int taskId, @Nonnull final Text model,
String modelId = RandomUtils.getUUID();
final Object[] forwardObjs = new Object[6];
forwardObjs[0] = new Text(modelId);
import hivemall.utils.lang.RandomUtils;
fieldOIs.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
synchronized void forward(final int taskId, @Nonnull final Text model,
String modelId = RandomUtils.getUUID();
final Object[] forwardObjs = new Object[6];
forwardObjs[0] = new Text(modelId);
value = "_FUNC_(string modelId, int modelType, string script, array<double> features [, const boolean classification])"
if (arg0 == null) {
throw new HiveException("ModelId was null");
}
String modelId = arg0.toString();
Writable evaluate(@Nonnull String modelId, boolean compressed, @Nonnull final Text script,
@Nullable
private String prevModelId = null;
public Writable evaluate(@Nonnull String modelId, boolean compressed, @Nonnull Text script,
private IntWritable evaluateClassification(@Nonnull String modelId, boolean compressed,
if (!modelId.equals(prevModelId)) {
private DoubleWritable evaluteRegression(@Nonnull String modelId, boolean compressed,
if (!modelId.equals(prevModelId)) {
private String prevModelId = null;
public Writable evaluate(@Nonnull String modelId, boolean compressed, @Nonnull Text script,
if (modelId.equals(prevModelId)) {
private String prevModelId = null;
public Writable evaluate(@Nonnull String modelId, boolean compressed, @Nonnull Text script,
if (modelId.equals(prevModelId)) {
import hivemall.utils.lang.RandomUtils;
return RandomUtils.getUUID();
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(
name = "array_hash_values",
value = "_FUNC_(array<string> values, [string prefix [, int numFeatures], boolean useIndexAsPrefix])"
" returns hash values in array<int>")
@UDFType(deterministic = true, stateful = false)
public final class ArrayHashValuesUDF extends UDF {
public List<IntWritable> evaluate(List<String> values, String prefix, int numFeatures,
boolean useIndexAsPrefix) {
static List<IntWritable> hashValues(List<String> values, String prefix, int numFeatures,
boolean useIndexAsPrefix) {
if (values == null) {
if (values.isEmpty()) {
if (v == null) {
if (useIndexAsPrefix) {
}
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "prefixed_hash_values",
value = "_FUNC_(array<string> values, string prefix [, boolean useIndexAsPrefix])"
" returns array<string> that each element has the specified prefix")
@UDFType(deterministic = true, stateful = false)
public final class ArrayPrefixedHashValuesUDF extends UDF {
if (values == null) {
if (prefix == null) {
List<IntWritable> hashValues = ArrayHashValuesUDF.hashValues(values, null,
MurmurHash3.DEFAULT_NUM_FEATURES, useIndexAsPrefix);
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "sha1", value = "_FUNC_(string word [, int numFeatures]) returns a SHA-1 value")
@UDFType(deterministic = true, stateful = false)
public final class Sha1UDF extends UDF {
if (rawValue) {
int sha1 = sha1(word);
return new IntWritable(sha1);
if (r < 0) {
if (wlength == 0) {
return new IntWritable(1);
@Nonnull
public static boolean isListOI(@Nonnull final ObjectInspector oi) {
Category category = oi.getCategory();
return category == Category.LIST;
}
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(
name = "array_hash_values",
value = "_FUNC_(array<string> values, [string prefix [, int numFeatures], boolean useIndexAsPrefix])"
" returns hash values in array<int>")
@UDFType(deterministic = true, stateful = false)
public final class ArrayHashValuesUDF extends UDF {
public List<IntWritable> evaluate(List<String> values, String prefix, int numFeatures,
boolean useIndexAsPrefix) {
static List<IntWritable> hashValues(List<String> values, String prefix, int numFeatures,
boolean useIndexAsPrefix) {
if (values == null) {
if (values.isEmpty()) {
if (v == null) {
if (useIndexAsPrefix) {
}
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "prefixed_hash_values",
value = "_FUNC_(array<string> values, string prefix [, boolean useIndexAsPrefix])"
" returns array<string> that each element has the specified prefix")
@UDFType(deterministic = true, stateful = false)
public final class ArrayPrefixedHashValuesUDF extends UDF {
if (values == null) {
if (prefix == null) {
List<IntWritable> hashValues = ArrayHashValuesUDF.hashValues(values, null,
MurmurHash3.DEFAULT_NUM_FEATURES, useIndexAsPrefix);
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.udf.UDFType;
@Description(name = "sha1", value = "_FUNC_(string word [, int numFeatures]) returns a SHA-1 value")
@UDFType(deterministic = true, stateful = false)
public final class Sha1UDF extends UDF {
if (rawValue) {
int sha1 = sha1(word);
return new IntWritable(sha1);
if (r < 0) {
if (wlength == 0) {
return new IntWritable(1);
@Nonnull
public static boolean isListOI(@Nonnull final ObjectInspector oi) {
Category category = oi.getCategory();
return category == Category.LIST;
}
@Nonnull
public static boolean isListOI(@Nonnull final ObjectInspector oi) {
Category category = oi.getCategory();
return category == Category.LIST;
}
public static final String VERSION = "0.4.2-rc.1";
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
this.log_sum = 0.d;
double epsilon = 1E-15d;
predicted = Math.min(1.d - epsilon, predicted);
double getLogLoss() {
if (count == 0) {
return 0.d;
}
return -1.d * log_sum / count;
}
@Description(name = "hivemall_version", value = "_FUNC_() - Returns the version of Hivemall",
extended = "Usage: SELECT hivemall_version();")
import hivemall.mix.MixMessage.MixEventName;
import hivemall.mix.client.MixClient;
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_arow",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by Adaptive Regularization of Weight Vectors (AROW) binary classifier")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<String|Int|BitInt> features, Int label [, constant String options]");
opts.addOption("r", "regularization", true,
"Regularization parameter for some r > 0 [default 0.1]");
if (cl != null) {
if (r_str != null) {
if (!(r > 0)) {
throw new UDFArgumentException(
if (m < 1.f) {
protected void update(@Nonnull final FeatureValue[] features, final float y, final float alpha,
final float beta) {
for (FeatureValue f : features) {
if (f == null) {
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float y,
final float alpha, final float beta) {
if (old == null) {
@Description(
name = "train_arow_h",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by AROW binary classifier using hinge loss")
if (cl != null) {
if (c_str != null) {
if (!(c > 0.f)) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "train_adagrad_rda",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight>",
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<Text|Int|BitInt> features, int label [, constant string options]");
opts.addOption("scale", true,
"Internal scaling/descaling factor for cumulative weights [default: 100]");
if (cl == null) {
if (f == null) {
protected void updateWeight(@Nonnull final Object x, final float xi, final float y,
final float t) {
if (old != null) {
if (meansOfGradients < 0.f) {
public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if (argOIs.length < 2) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 arguments: List<Int|BigInt|Text> features, int label [, constant string options]");
PrimitiveObjectInspector featureOutputOI = dense_model ? PrimitiveObjectInspectorFactory.javaIntObjectInspector
if (preloadedModelFile != null) {
if (!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
throw new UDFArgumentTypeException(0,
if (useCovariance()) {
if (featureVector == null) {
if (size == 0) {
if (f == null) {
if (parseFeature) {
if (f == null) {
if (old_w != 0.f) {
if (f == null) {
if (old_w != 0f) {
if (f == null) {
if (old_w == null) {
if (f == null) {
if (model != null) {
if (useCovariance()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_cw",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by Confidence-Weighted (CW) binary classifier")
public final class ConfidenceWeightedUDTF extends BinaryOnlineClassifierUDTF {
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"ConfidenceWeightedUDTF takes 2 or 3 arguments: List<String|Int|BitInt> features, Int label [, constant String options]");
opts.addOption("eta", "hyper_c", true,
"Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
if (cl != null) {
if (phi_str == null) {
if (eta_str != null) {
if (eta <= 0.5 || eta > 1) {
throw new UDFArgumentException(
protected void update(@Nonnull final FeatureValue[] features, final float coeff,
final float alpha) {
for (FeatureValue f : features) {
if (f == null) {
private static IWeightValue getNewWeight(final IWeightValue old, final float x,
final float coeff, final float alpha, final float phi) {
if (old == null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "train_pa",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive (PA) binary classifier")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<Text|Int|BitInt> features, int label [, constant string options]");
@Description(
name = "train_pa1",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive 1 (PA-1) binary classifier")
if (cl != null) {
if (c_str != null) {
if (!(c > 0.f)) {
@Description(
name = "train_pa2",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive 2 (PA-2) binary classifier")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "train_perceptron",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight>",
extended = "Build a prediction model by Perceptron binary classifier")
public final class PerceptronUDTF extends BinaryOnlineClassifierUDTF {
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<Text|Int|BitInt> features, int label [, constant string options]");
import org.apache.hadoop.hive.ql.exec.Description;
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"SoftConfideceWeightedUDTF takes 2 or 3 arguments: List<String|Int|BitInt> features, Int label [, constant String options]");
opts.addOption("eta", "hyper_c", true,
"Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
if (cl != null) {
if (phi_str == null) {
if (eta_str != null) {
if (eta <= 0.5 || eta > 1) {
throw new UDFArgumentException(
if (c_str != null) {
if (!(c > 0.f)) {
if (loss > 0.f) {
if (alpha == 0.f) {
if (beta == 0.f) {
@Description(
name = "train_scw",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by Soft Confidence-Weighted (SCW-1) binary classifier")
if (alpha_denom == 0.f) {
if (alpha <= 0.f) {
if (alpha == 0.f) {
if (beta_den == 0.f) {
@Description(
name = "train_scw2",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by Soft Confidence-Weighted 2 (SCW-2) binary classifier")
public static final class SCW2 extends SCW1 {
if (alpha_numer <= 0.f) {
if (alpha_denom == 0.f) {
protected void update(@Nonnull final FeatureValue[] features, final float y, final float alpha,
final float beta) {
for (FeatureValue f : features) {
if (f == null) {
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float y,
final float alpha, final float beta) {
if (old == null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_multiclass_arow",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight, float covar>",
extended = "Build a prediction model by Adaptive Regularization of Weight Vectors (AROW) multiclass classifier")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<String|Int|BitInt> features, {Int|String} label [, constant String options]");
opts.addOption("r", "regularization", true,
"Regularization parameter for some r > 0 [default 0.1]");
if (cl != null) {
if (r_str != null) {
if (!(r > 0)) {
throw new UDFArgumentException(
if (m >= 1.f) {
protected void update(@Nonnull final FeatureValue[] features, final Object actual_label,
final Object missed_label, final float alpha, final float beta) {
if (actual_label.equals(missed_label)) {
if (model2add == null) {
if (missed_label != null) {
if (model2sub == null) {
if (f == null) {
if (model2sub != null) {
IWeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, beta,
false);
private static IWeightValue getNewWeight(final IWeightValue old, final float v,
final float alpha, final float beta, final boolean positive) {
if (old == null) {
@Description(
name = "train_multiclass_arow_h",
value = "_FUNC_(list<string|int|bigint> features, int|string label [, const string options])"
" - Returns a relation consists of <int|string label, string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by Adaptive Regularization of Weight Vectors (AROW) multiclass classifier using hinge loss")
public static final class AROWh extends MulticlassAROWClassifierUDTF {
if (cl != null) {
if (c_str != null) {
if (!(c > 0.f)) {
if (loss > 0.f) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_multiclass_cw",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight, float covar>",
extended = "Build a prediction model by Confidence-Weighted (CW) multiclass classifier")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<String|Int|BitInt> features, {Int|String} label [, constant String options]");
opts.addOption("eta", "hyper_c", true,
"Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
if (cl != null) {
if (phi_str == null) {
if (eta_str != null) {
if (eta <= 0.5 || eta > 1) {
throw new UDFArgumentException(
protected void update(@Nonnull final FeatureValue[] features, float alpha, Object actual_label,
Object missed_label) {
if (actual_label.equals(missed_label)) {
if (model2add == null) {
if (missed_label != null) {
if (model2sub == null) {
if (f == null) {
if (model2sub != null) {
private static IWeightValue getNewWeight(final IWeightValue old, final float x,
final float alpha, final float phi, final boolean positive) {
if (old == null) {
if (argOIs.length < 2) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 arguments: List<Int|BigInt|Text> features, {Int|BitInt|Text} label [, constant text options]");
if (!STRING_TYPE_NAME.equals(labelTypeName) && !INT_TYPE_NAME.equals(labelTypeName)
PrimitiveObjectInspector featureOutputOI = dense_model ? PrimitiveObjectInspectorFactory.javaIntObjectInspector
: featureInputOI;
if (preloadedModelFile != null) {
if (!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
throw new UDFArgumentTypeException(0,
protected StructObjectInspector getReturnOI(ObjectInspector labelRawOI,
ObjectInspector featureRawOI) {
if (useCovariance()) {
if (featureVector == null) {
if (label == null) {
if (size == 0) {
if (f == null) {
if (parseFeature) {
protected abstract void train(@Nonnull final FeatureValue[] features,
@Nonnull final Object actual_label);
if (maxScoredLabel == null || score > maxScore) {
if (label.equals(actual_label)) {
if (maxAnotherLabel == null || score > maxAnotherScore) {
protected Margin getMarginAndVariance(@Nonnull final FeatureValue[] features,
final Object actual_label) {
protected Margin getMarginAndVariance(@Nonnull final FeatureValue[] features,
final Object actual_label, boolean nonZeroVariance) {
if (label.equals(actual_label)) {
if (maxAnotherLabel == null || score > maxAnotherScore) {
if (f == null) {
protected final float calcScore(@Nonnull final PredictionModel model,
@Nonnull final FeatureValue[] features) {
if (f == null) {
if (old_w != 0f) {
if (f == null) {
protected final PredictionResult calcScoreAndVariance(@Nonnull final PredictionModel model,
@Nonnull final FeatureValue[] features) {
if (f == null) {
if (old_w == null) {
protected void update(@Nonnull final FeatureValue[] features, float coeff, Object actual_label,
Object missed_label) {
if (actual_label.equals(missed_label)) {
if (model2add == null) {
if (missed_label != null) {
if (model2sub == null) {
if (f == null) {
if (model2sub != null) {
if (label2model != null) {
if (useCovariance()) {
for (Map.Entry<Object, PredictionModel> entry : label2model.entrySet()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
for (Map.Entry<Object, PredictionModel> entry : label2model.entrySet()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
protected void loadPredictionModel(Map<Object, PredictionModel> label2model, String filename,
PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI) {
if (useCovariance()) {
lines = loadPredictionModel(label2model, new File(filename), labelOI, featureOI,
writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(label2model, new File(filename), labelOI, featureOI,
writableFloatObjectInspector);
if (!label2model.isEmpty()) {
for (Map.Entry<Object, PredictionModel> e : label2model.entrySet()) {
statsBuf.append('\n')
.append("Label: ")
.append(label)
.append(", Number of Features: ")
.append(numFeatures);
private long loadPredictionModel(Map<Object, PredictionModel> label2model, File file,
PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI,
WritableFloatObjectInspector weightOI) throws IOException, SerDeException {
if (!file.exists()) {
if (!file.getName().endsWith(".crc")) {
if (file.isDirectory()) {
for (File f : file.listFiles()) {
while ((line = reader.readLine()) != null) {
if (f0 == null || f1 == null || f2 == null) {
if (model == null) {
private long loadPredictionModel(Map<Object, PredictionModel> label2model, File file,
PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI,
WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
if (!file.exists()) {
if (!file.getName().endsWith(".crc")) {
if (file.isDirectory()) {
for (File f : file.listFiles()) {
covarOI);
LazySimpleSerDe serde = HiveUtils.getLineSerde(labelOI, featureOI, weightOI,
covarOI);
while ((line = reader.readLine()) != null) {
if (f0 == null || f1 == null || f2 == null) {
if (model == null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_multiclass_pa",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive (PA) multiclass classifier")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"MulticlassPassiveAggressiveUDTF takes 2 or 3 arguments: List<Text|Int|BitInt> features, {Int|Text} label [, constant text options]");
@Description(
name = "train_multiclass_pa1",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive 1 (PA-1) multiclass classifier")
if (cl != null) {
if (c_str != null) {
if (!(c > 0.f)) {
@Description(
name = "train_multiclass_pa2",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive 2 (PA-2) multiclass classifier")
public static final class PA2 extends PA1 {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_multiclass_perceptron",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight>",
extended = "Build a prediction model by Perceptron multiclass classifier")
public final class MulticlassPerceptronUDTF extends MulticlassOnlineClassifierUDTF {
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"MulticlassPerceptronUDTF takes 2 or 3 arguments: List<Text|Int|BitInt> features, {Int|Text} label [, constant text options]");
if (!actual_label.equals(predicted_label)) {
import org.apache.hadoop.hive.ql.exec.Description;
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"MulticlassSoftConfidenceWeightedUDTF takes 2 or 3 arguments: List<String|Int|BitInt> features, {Int|String} label [, constant String options]");
opts.addOption("eta", "hyper_c", true,
"Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
if (cl != null) {
if (phi_str == null) {
if (eta_str != null) {
if (eta <= 0.5 || eta > 1) {
throw new UDFArgumentException(
if (c_str != null) {
if (!(c > 0.f)) {
if (loss > 0.f) {
if (alpha == 0.f) {
if (beta == 0.f) {
@Description(
name = "train_multiclass_scw",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight, float covar>",
extended = "Build a prediction model by Soft Confidence-Weighted (SCW-1) multiclass classifier")
if (alpha_denom == 0.f) {
if (alpha <= 0.f) {
if (alpha == 0.f) {
if (beta_den == 0.f) {
@Description(
name = "train_multiclass_scw2",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight, float covar>",
extended = "Build a prediction model by Soft Confidence-Weighted 2 (SCW-2) multiclass classifier")
public static final class SCW2 extends SCW1 {
if (alpha_numer <= 0.f) {
if (alpha_denom == 0.f) {
protected void update(@Nonnull final FeatureValue[] features, final Object actual_label,
final Object missed_label, final float alpha, final float beta) {
if (actual_label.equals(missed_label)) {
if (model2add == null) {
if (missed_label != null) {
if (model2sub == null) {
if (f == null) {
if (model2sub != null) {
IWeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, beta,
false);
private static IWeightValue getNewWeight(final IWeightValue old, final float v,
final float alpha, final float beta, final boolean positive) {
if (old == null) {
if (n == 0L) {
if (n == 1L) {
if (numBuffers < 1) {
if (xtimes < 1) {
if (numBuffers < 1) {
if (xtimes < 1) {
if (position < numBuffers) {
if (position == numBuffers) {
if (sweepedObj != null) {
if (droppped == null) {
if (listener != null) {
if (sampleSize <= 0) {
if (item == null) {
if (replaceIndex < numSamples) {
}
@Description(
name = "lr_datagen",
value = "_FUNC_(options string) - Generates a logistic regression dataset",
extended = "WITH dual AS (SELECT 1) SELECT lr_datagen('-n_examples 1k -n_features 10') FROM dual;")
private long r_seed;
opts.addOption("ne", "n_examples", true,
"Number of training examples created for each task [DEFAULT: 1000]");
opts.addOption("nf", "n_features", true,
"Number of features contained for each example [DEFAULT: 10]");
opts.addOption("eps", true,
"eps Epsilon factor by which positive examples are scaled [DEFAULT: 3.0]");
opts.addOption("p1", "prob_one", true,
" Probability in [0, 1.0) that a label is 1 [DEFAULT: 0.6]");
opts.addOption("seed", true, "The seed value for random number generator [DEFAULT: 43L]");
opts.addOption(
"dense",
false,
"Make a dense dataset or not. If not specified, a sparse dataset is generated.\n"
"For sparse, n_dims should be much larger than n_features. When disabled, n_features must be equals to n_dims ");
opts.addOption("cl", "classification", false,
"Toggle this option on to generate a classification dataset");
if (argOIs.length != 1) {
this.r_seed = Primitives.parseLong(cl.getOptionValue("seed"), 43L);
if (n_features > n_dimensions) {
if (dense) {
if (n_features != n_dimensions) {
if (dense) {
if (dense) {
if (rnd1 == null) {
final int taskid = HadoopUtils.getTaskId(-1);
final long seed;
if (taskid == -1) {
} else {
}
if (dense) {
if (position == N_BUFFERS) {
if (used.get(f)) {
if (retry < 3) {
if (sort) {
if (dense) {
if (position > 0) {
@Description(
name = "argmin_kld",
value = "_FUNC_(float mean, float covar) - Returns mean or covar that minimize a KL-distance among distributions",
extended = "The returned value is (1.0 / (sum(1.0 / covar))) * (sum(mean / covar)")
if (mean == null || covar == null) {
if (partial == null) {
if (o == null) {
if (partial == null) {
if (partial == null) {
@Description(
name = "maxrow",
value = "_FUNC_(ANY compare, ...) - Returns a row that has maximum value in the 1st argument")
if (!ObjectInspectorUtils.compareSupported(oi)) {
throw new UDFArgumentTypeException(0,
"Cannot support comparison of map<> type or complex type containing map<>.");
if (parameters.length == 1 && parameters[0] instanceof StructObjectInspector) {
if (partial == null) {
if (partial instanceof Object[]) {
} else if (partial instanceof LazyBinaryStruct) {
} else if (inputStructOI != null) {
if (maxagg.objects == null) {
int cmp = ObjectInspectorUtils.compare(maxagg.objects[0], outputOIs[0],
otherObjects.get(0), inputOIs[0]);
if (cmp < 0) {
if (isMax) {
maxagg.objects[i] = ObjectInspectorUtils.copyToStandardObject(
otherObjects.get(i), inputOIs[i]);
}
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "max_label",
value = "_FUNC_(double value, string label) - Returns a label that has the maximum value")
if (partial == null) {
if (v >= partial.maxValue) {
if (other == null) {
if (partial == null) {
if (other.maxValue < partial.maxValue) {
if (partial == null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "voted_avg",
value = "_FUNC_(double value) - Returns an averaged value by bagging for classification")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "weight_voted_avg",
value = "_FUNC_(expr) - Returns an averaged value by considering sum of positive/negative weights")
@Description(name = "f1score",
value = "_FUNC_(array[int], array[int]) - Return a F-measure/F1 score")
if (actual.contains(p)) {
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
if (divisor > 0) {
@Description(name = "logloss",
value = "_FUNC_(double predicted, double actual) - Return a Logrithmic Loss")
@Description(name = "mae",
value = "_FUNC_(double predicted, double actual) - Return a Mean Absolute Error")
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
@Description(name = "mse",
value = "_FUNC_(double predicted, double actual) - Return a Mean Squared Error")
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
@Description(
name = "r2",
value = "_FUNC_(double predicted, double actual) - Return R Squared (coefficient of determination)")
@Description(name = "rmse",
value = "_FUNC_(double predicted, double actual) - Return a Root Mean Squared Error")
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
@Description(
name = "fm_predict",
value = "_FUNC_(Float Wj, array<float> Vjf, float Xj) - Returns a prediction value in Double")
if (typeInfo.length != 3) {
throw new UDFArgumentLengthException(
if (!HiveUtils.isNumberTypeInfo(typeInfo[0])) {
throw new UDFArgumentTypeException(0,
if (typeInfo[1].getCategory() != Category.LIST) {
throw new UDFArgumentTypeException(1,
if (!HiveUtils.isNumberTypeInfo(typeInfo1.getListElementTypeInfo())) {
throw new UDFArgumentTypeException(1,
"Number type is expected for the element type of list Vjf: "
typeInfo1.getTypeName());
if (!HiveUtils.isNumberTypeInfo(typeInfo[2])) {
throw new UDFArgumentTypeException(2,
if (parameters[0] == null) {
if (parameters[2] == null) {
if (buf.sumVjXj != null) {
if (partial == null) {
if (sumVjXj instanceof LazyBinaryArray) {
if (sumV2X2 instanceof LazyBinaryArray) {
void iterate(final double Wj, final double Xj, @Nonnull final Object Vif,
@Nonnull final ListObjectInspector vOI,
@Nonnull final PrimitiveObjectInspector vElemOI) throws HiveException {
if (factors < 1) {
if (sumVjXj == null) {
} else if (sumVjXj.length != factors) {
if (o == null) {
void merge(final double o_ret, @Nullable final Object o_sumVjXj,
@Nullable final Object o_sumV2X2,
@Nonnull final StandardListObjectInspector sumVjXjOI,
@Nonnull final StandardListObjectInspector sumV2X2OI) throws HiveException {
if (o_sumVjXj == null) {
if (sumVjXj == null) {
if (sumVjXj != null) {
@Nonnull final ListObjectInspector listOI, @Nullable final Feature[] probes,
final int numFeatures, final int numFields) throws HiveException {
static IntFeature parseFFMFeature(@Nonnull final String fv, final int numFeatures,
final int numFields) throws HiveException {
}
static void parseFFMFeature(@Nonnull final String fv, @Nonnull final Feature probe)
throws HiveException {
static void parseFFMFeature(@Nonnull final String fv, @Nonnull final Feature probe,
final int numFeatures, final int numFields) throws HiveException {
private static short parseField(@Nonnull final String fieldStr, final int numFields)
throws HiveException {
@Description(
name = "add_bias",
value = "_FUNC_(feature_vector in array<string>) - Returns features with a bias in array<string>")
@Description(
name = "add_feature_index",
value = "_FUNC_(ARRAY[DOUBLE]: dense feature vector) - Returns a feature vector with feature indicies")
if (ftvec == null) {
if (size == 0) {
@Description(name = "extract_feature",
value = "_FUNC_(feature_vector in array<string>) - Returns features in array<string>")
if (featureVector == null) {
if (featureVectors == null) {
if (fv != null) {
if (pos > 0) {
@Description(
name = "feature_index",
value = "_FUNC_(feature_vector in array<string>) - Returns feature indicies in array<index>")
if (feature == null) {
if (featureVector == null) {
for (String f : featureVector) {
if (f != null) {
if (pos > 0) {
@Description(name = "feature",
value = "_FUNC_(string feature, double weight) - Returns a feature string")
if (feature == null) {
if (feature == null) {
if (feature == null) {
if (feature == null) {
@Description(name = "sort_by_feature",
value = "_FUNC_(map in map<int,float>) - Returns a sorted map")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "amplify",
value = "_FUNC_(const int xtimes, *) - amplify the input records x-times")
public final class AmplifierUDTF extends GenericUDTF {
if (!(argOIs.length >= 2)) {
throw new UDFArgumentException("_FUNC_(int xtimes, *) takes at least two arguments");
if (!(xtimes >= 1)) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "rand_amplify", value = "_FUNC_(const int xtimes, const int num_buffers, *)"
" - amplify the input records x-times in map-side")
public final class RandomAmplifierUDTF extends GenericUDTF implements DropoutListener<Object[]> {
if (useSeed) {
if (numArgs < 3) {
throw new UDFArgumentException(
"_FUNC_(int xtimes, int num_buffers, *) takes at least three arguments");
if (!(xtimes >= 1)) {
if (numBuffers < 2) {
if (useSeed) {
ObjectInspector retOI = ObjectInspectorUtils.getStandardObjectInspector(rawOI,
ObjectInspectorCopyOption.DEFAULT);
row[i - 2] = ObjectInspectorUtils.copyToStandardObject(arg, argOI,
ObjectInspectorCopyOption.DEFAULT);
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "conv2dense",
value = "_FUNC_(int feature, float weight, int nDims) - Return a dense model in array<float>")
if (partial == null) {
if (other == null) {
if (partial == null) {
if (x != null) {
if (partial == null) {
@Description(name = "quantify",
value = "_FUNC_(boolean outout, col1, col2, ...) - Returns an identified features")
if (size < 2) {
if (HiveUtils.isNumberOI(argOI)) {
if (outputRow) {
if (identifier == null) {
if (arg == null) {
if (identifier != null) {
if (arg != null) {
@Description(name = "to_dense_features",
value = "_FUNC_(array<string> feature_vector, int dimensions)"
" - Returns a dense feature in array<float>")
if (features == null) {
public List<Float> evaluate(@Nullable final List<String> features,
@Nonnegative final int dimensions) throws HiveException {
if (features == null) {
for (String ft : features) {
if (i > dimensions) {
public List<String> evaluate(@Nullable final List<FloatWritable> features,
@Nullable String biasName) {
if (features == null) {
if (size == 0) {
if (o != null) {
if (biasName != null) {
@Description(name = "polynomial_features",
value = "_FUNC_(feature_vector in array<string>) - Returns a feature vector"
"having polynominal feature space")
public List<Text> evaluate(final List<Text> ftvec, final int degree,
final boolean interactionOnly) throws HiveException {
public List<Text> evaluate(final List<Text> ftvec, final int degree,
final boolean interactionOnly, final boolean truncate) throws HiveException {
if (ftvec == null) {
if (degree < 2) {
if (origSize == 0) {
if (t == null) {
if (truncate == false || (v != 0.f && v != 1.f)) {
private static void addPolynomialFeature(final String baseF, final float baseV,
final int currentDegree, final int degree, final List<FeatureValue> srcVec,
final int currentSrcPos, final List<Text> dstVec, final boolean interactionOnly,
final boolean truncate) {
if (interactionOnly && i == currentSrcPos) {
if (truncate && (v == 0.f || v == 1.f)) {
if (currentDegree < degree && i <= lastSrcIndex) {
interactionOnly, truncate);
@Description(name = "powered_features",
value = "_FUNC_(feature_vector in array<string>, int degree [, boolean truncate])"
" - Returns a feature vector having a powered feature space")
if (ftvec == null) {
if (degree < 2) {
if (origSize == 0) {
if (t == null) {
if (truncate && (v == 0.f || v == 1.f)) {
@Description(name = "l2_normalize", value = "_FUNC_(ftvec string) - Returned a L2 normalized value")
if (ftvecs == null) {
if (ftvec == null) {
if (ftlen == 1) {
} else if (ftlen == 2) {
if (norm == 0.f) {
@Description(name = "rescale",
value = "_FUNC_(value, min, max) - Returns rescaled value by min-max normalization")
if (fv.length != 2) {
if (fv.length != 2) {
if (min == max) {
private static float min_max_normalization(final double value, final double min,
final double max) {
if (min == max) {
@Description(name = "zscore",
value = "_FUNC_(value, mean, stddev) - Returns a standard score (zscore)")
if (stddev == 0.d) {
if (stddev == 0.f) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "tf",
value = "_FUNC_(string text) - Return a term frequency in <string, float>")
if (term == null) {
if (partial == null) {
if (count == null) {
if (other == null) {
if (partial == null) {
for (Map.Entry<Text, MutableInt> e : other_map.entrySet()) {
if (this_count == null) {
if (partial == null) {
for (Map.Entry<Text, MutableInt> e : partial.map.entrySet()) {
@Description(name = "categorical_features",
value = "_FUNC_(array<string> featureNames, ...) - Returns a feature vector array<string>")
if (numArgOIs < 2) {
if (featureNames == null) {
if (numFeatureNames < 1) {
if (numFeatureNames != numFeatures) {
if (argument == null) {
if (s.isEmpty()) {
@Description(
name = "indexed_features",
value = "_FUNC_(double v1, double v2, ...) - Returns a list of features as array<string>: [1:v1, 2:v2, ..]")
if (numArgs < 1) {
throw new UDFArgumentLengthException(
if (list == null) {
if (o == null) {
if (s1.isEmpty()) {
@Description(
name = "quantified_features",
value = "_FUNC_(boolean output, col1, col2, ...) - Returns an identified features in a dence array<double>")
if (size < 2) {
if (HiveUtils.isNumberOI(argOI)) {
if (fowardObjs == null) {
this.fowardObjs = new Object[] {Arrays.asList(columnValues)};
if (outputRow) {
if (identifier == null) {
if (arg == null) {
if (identifier != null) {
if (arg != null) {
@Description(name = "quantitative_features",
value = "_FUNC_(array<string> featureNames, ...) - Returns a feature vector array<string>")
if (numArgOIs < 2) {
if (featureNames == null) {
if (numFeatureNames < 1) {
if (numFeatureNames != numFeatures) {
if (argument == null) {
if (oi.getPrimitiveCategory() == PrimitiveCategory.STRING) {
if (s.isEmpty()) {
}
}
if (v != 0.d) {
@Description(name = "vectorize_features",
value = "_FUNC_(array<string> featureNames, ...) - Returns a feature vector array<string>")
if (numArgOIs < 2) {
if (featureNames == null) {
if (numFeatureNames < 1) {
if (numFeatureNames != numFeatures) {
if (argument == null) {
if (oi.getPrimitiveCategory() == PrimitiveCategory.STRING) {
if (s.isEmpty()) {
}
if (v != 0.f) {
@Description(name = "angular_distance",
value = "_FUNC_(ftvec1, ftvec2) - Returns an angular distance of the given two vectors")
if (argOIs.length != 2) {
@Description(name = "cosine_distance",
value = "_FUNC_(ftvec1, ftvec2) - Returns a cosine distance of the given two vectors")
if (argOIs.length != 2) {
@Description(
name = "euclid_distance",
value = "_FUNC_(ftvec1, ftvec2) - Returns the square root of the sum of the squared differences"
": sqrt(sum((x - y)^2))")
if (argOIs.length != 2) {
for (String ft : ftvec1) {
if (ft == null) {
for (String ft : ftvec2) {
if (ft == null) {
if (v1 == null) {
for (Map.Entry<String, Float> e : map.entrySet()) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "hamming_distance",
value = "_FUNC_(A, B [,int k]) - Returns Hamming distance between A and B")
if (alen < blen) {
@Description(name = "jaccard_distance",
value = "_FUNC_(A, B [,int k]) - Returns Jaccard distance between A and B")
if (a == null && b == null) {
} else if (a == null || b == null) {
if (asize == 0 && bsize == 0) {
} else if (asize == 0 || bsize == 0) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "kld", value = "_FUNC_(double m1, double sigma1, double mu2, double sigma 2)"
" - Returns KL divergence between two distributions")
public static double kld(final double mu1, final double sigma1, final double mu2,
final double sigma2) {
if (argOIs.length != 2) {
for (String ft : ftvec1) {
if (ft == null) {
for (String ft : ftvec2) {
if (ft == null) {
if (v1 == null) {
for (Map.Entry<String, Float> e : map.entrySet()) {
@Description(name = "minkowski_distance",
value = "_FUNC_(list x, list y, double p) - Returns sum(|x - y|^p)^(1/p)")
if (argOIs.length != 3) {
public static double minkowskiDistance(final List<String> ftvec1, final List<String> ftvec2,
final double orderP) {
for (String ft : ftvec1) {
if (ft == null) {
for (String ft : ftvec2) {
if (ft == null) {
if (v1 == null) {
for (Map.Entry<String, Float> e : map.entrySet()) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "popcnt", value = "_FUNC_(a [, b]) - Returns a popcount value")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "minhash",
value = "_FUNC_(ANY item, array<int|bigint|string> features [, constant string options])"
" - Returns n differnce k-depth signatures (i.e., clusteid) for each item <clusteid, item>")
if (argOIs.length < 2) {
throw new UDFArgumentException(
"_FUNC_ takes more than 2 arguments: ANY item, Array<Int|BigInt|Text> features [, constant String options]");
if (!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
throw new UDFArgumentTypeException(0,
opts.addOption("n", "hashes", true,
"Generate N sets of minhash values for each row (DEFAULT: 5)");
if (argOIs.length >= 3) {
if (numHashes != null) {
if (numKeygroups != null) {
for (FeatureValue fv : features) {
if (hashValue < weightedMinHashValues) {
if (w < 0.f) {
if (w == 0.f) {
if (numCandidates == 0) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "minhashes",
value = "_FUNC_(array<> features [, int numHashes, int keyGroup [, boolean noWeight]])"
" - Returns minhash values")
if (seeds == null || seeds.length != numHashes) {
public List<IntWritable> evaluate(List<String> features, int numHashes, int keyGroups,
boolean noWeight) throws HiveException {
for (Integer f : features) {
if (f != null) {
private static List<FeatureValue> parseFeatures(final List<String> features,
final boolean noWeight) {
for (String f : features) {
if (f == null) {
if (noWeight) {
private static List<IntWritable> computeSignatures(final List<FeatureValue> features,
final int numHashes, final int keyGroups, final int[] seeds) throws HiveException {
for (FeatureValue fv : features) {
if (hashValue < weightedMinHashValues) {
if (w < 0.f) {
if (w == 0.f) {
if (numCandidates == 0) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "bbit_minhash", value = "_FUNC_(array<> features [, int numHashes])"
" - Returns a b-bits minhash value")
if (seeds == null || seeds.length != numHashes) {
for (Integer f : features) {
if (f != null) {
private static List<FeatureValue> parseFeatures(final List<String> features,
final boolean noWeight) {
for (String f : features) {
if (f == null) {
if (noWeight) {
private static String computeSignatures(final List<FeatureValue> features, final int numHashes,
final int[] seeds) throws HiveException {
if (numHashes <= 0 || numHashes > 512) {
for (FeatureValue fv : features) {
if (hashValue < weightedMinHashValues) {
if ((hashes[i] & 1) == 1) {
if (w < 0.f) {
if (w == 0.f) {
@Description(name = "angular_similarity",
value = "_FUNC_(ftvec1, ftvec2) - Returns an angular similarity of the given two vectors")
if (argOIs.length != 2) {
@Description(name = "cosine_similarity",
value = "_FUNC_(ftvec1, ftvec2) - Returns a cosine similarity of the given two vectors")
if (argOIs.length != 2) {
if (ftvec1 == null || ftvec2 == null) {
for (String ft : ftvec1) {
for (String ft : ftvec2) {
if (v1 != null) {
if (denom <= 0.f) {
if (argOIs.length != 1) {
@Description(name = "euclid_similarity",
value = "_FUNC_(ftvec1, ftvec2) - Returns a euclid distance based similarity"
if (argOIs.length != 2) {
@Description(name = "jaccard_similarity",
value = "_FUNC_(A, B [,int k]) - Returns Jaccard similarity coefficient of A and B")
if (a == null && b == null) {
} else if (a == null || b == null) {
if (asize == 0 && bsize == 0) {
} else if (asize == 0 || bsize == 0) {
@Description(
name = "mf_predict",
value = "_FUNC_(List<Float> Pu, List<Float> Qi[, double Bu, double Bi[, double mu]]) - Returns the prediction value")
if (Pu == null || Qi == null) {
if (PuSize == 0) {
} else if (QiSize == 0) {
if (QiSize != PuSize) {
if (Pu == null && Qi == null) {
if (Pu == null) {
} else if (Qi == null) {
if (PuSize == 0) {
if (QiSize == 0) {
} else if (QiSize == 0) {
if (QiSize != PuSize) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_mf_adagrad",
value = "_FUNC_(INT user, INT item, FLOAT rating [, CONSTANT STRING options])"
" - Returns a relation consists of <int idx, array<float> Pu, array<float> Qi [, float Bu, float Bi [, float mu]]>")
opts.addOption("scale", true,
"Internal scaling/descaling factor for cumulative weights [100]");
if (cl == null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_mf_sgd",
value = "_FUNC_(INT user, INT item, FLOAT rating [, CONSTANT STRING options])"
" - Returns a relation consists of <int idx, array<float> Pu, array<float> Qi [, float Bu, float Bi [, float mu]]>")
public final class MatrixFactorizationSGDUDTF extends OnlineMatrixFactorizationUDTF {
opts.addOption("power_t", true,
"The exponent for inverse scaling learning rate [default 0.1]");
"_FUNC_ takes 3 arguments: INT user, INT item, FLOAT rating [, CONSTANT STRING options]");
public MixMessage(MixEventName event, Object feature, float weight, short clock,
int deltaUpdates) {
public MixMessage(MixEventName event, Object feature, float weight, float covariance,
short clock, int deltaUpdates) {
public MixMessage(MixEventName event, Object feature, float weight, float covariance,
int deltaUpdates, boolean cancelRequest) {
MixMessage(MixEventName event, Object feature, float weight, float covariance, short clock,
int deltaUpdates, boolean cancelRequest) {
if (feature == null) {
if (deltaUpdates < 0 || deltaUpdates > Byte.MAX_VALUE) {
switch (b) {
if (groupID == null) {
if (hasGroupID) {
if (frame == null) {
MixMessage msg = new MixMessage(event, feature, weight, covariance, clock, deltaUpdates,
cancelRequest);
switch (type) {
if (length == -1) {
}
if (obj instanceof Integer) {
} else if (obj instanceof Text) {
} else if (obj instanceof String) {
} else if (obj instanceof IntWritable) {
} else if (obj instanceof LongWritable) {
if (s == null) {
if (addr == null) {
if (obj == this) {
if (obj instanceof NodeInfo) {
public MixClient(@Nonnull MixEventName event, @CheckForNull String groupID,
@Nonnull String connectURIs, boolean ssl, int mixThreshold, @Nonnull MixedModel model) {
if (groupID == null) {
if (mixThreshold < 1 || mixThreshold > Byte.MAX_VALUE) {
for (NodeInfo node : serverNodes) {
if (ssl) {
if (deltaUpdates < mixThreshold) {
if (!initialized) {
if (groupID.startsWith(DUMMY_JOB_ID)) {
if (workers != null) {
for (Channel ch : channelMap.values()) {
if (model == null) {
if (msgHandler == null) {
if (sslCtx != null) {
if (connectInfo == null) {
if (numEndpoints < 1) {
InetSocketAddress addr = NetUtils.getInetSocketAddress(endpoints[i],
MixEnv.MIXSERV_DEFAULT_PORT);
if (cancelMixRequest) {
if (isDenseModel()) {
protected final void onUpdate(final int feature, final float weight, final float covar,
final short clock, final int deltaUpdates, final boolean hasCovar) {
if (handler != null) {
if (deltaUpdates < 1) {
if (requestSent) {
if (cancelMixRequest) {
if (hasCovar) {
if (prevMixed == null) {
if (prevMixed == null) {
if (handler != null) {
if (!value.isTouched()) {
if (value.hasCovariance()) {
if (requestSent) {
if (cancelMixRequest) {
if (prevMixed == null) {
if (requestSent) {
if (cancelMixRequest) {
if (prevMixed == null) {
if (hasCovariance()) {
if (withCovar) {
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x,
boolean sum_of_gradients) {
if (sum_of_squared_gradients) {
if (sum_of_squared_delta_x) {
if (sum_of_gradients) {
if (clocks == null) {
if (index >= size) {
if (covars != null) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (clocks != null) {
if (i >= size) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
return (T) new WeightValueParamsF2(weights[i], sum_of_squared_gradients[i],
sum_of_squared_delta_x[i]);
} else if (sum_of_gradients != null) {
return (T) new WeightValueParamsF2(weights[i], sum_of_squared_gradients[i],
sum_of_gradients[i]);
} else if (covars != null) {
if (hasCovar) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (clocks != null && value.isTouched()) {
if (i >= size) {
if (covars != null) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (i >= size) {
if (i >= size) {
if (i >= size) {
if (!hasNext()) {
if (covars == null) {
if (covars != null) {
}
boolean onUpdate(@Nonnull Object feature, float weight, float covar, short clock,
int deltaUpdates) throws Exception;
void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x,
boolean sum_of_gradients);
}
if (withCovar) {
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x,
boolean sum_of_gradients) {
if (sum_of_squared_gradients) {
if (sum_of_squared_delta_x) {
if (sum_of_gradients) {
if (clocks == null) {
if (index >= size) {
if (covars != null) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (clocks != null) {
if (i >= size) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
return (T) new WeightValueParamsF2(getWeight(i), sum_of_squared_gradients[i],
sum_of_squared_delta_x[i]);
} else if (sum_of_gradients != null) {
return (T) new WeightValueParamsF2(getWeight(i), sum_of_squared_gradients[i],
sum_of_gradients[i]);
} else if (covars != null) {
if (hasCovar) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (clocks != null && value.isTouched()) {
if (i >= size) {
if (covars != null) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (i >= size) {
if (i >= size) {
if (i >= size) {
if (!hasNext()) {
if (covars == null) {
if (covars != null) {
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x,
boolean sum_of_gradients) {}
if (clockEnabled && value.isTouched()) {
if (old != null) {
if (clockEnabled) {
switch (value.getType()) {
if (w == null) {
protected void _set(final Object feature, final float weight, final float covar,
final short clock) {
if (w == null) {
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x,
boolean sum_of_gradients) {
if (i == 1) {
if (i == 1) {
} else if (i == 2) {
if (src.isTouched()) {
throw new UnsupportedOperationException(
"WeightValueWithClock#setTouched should not be called");
if (deltaUpdates < 0) {
if (i == 1) {
if (i == 1) {
} else if (i == 2) {
}
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_arow_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight, float covar>")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes arguments: List<Int|BigInt|Text> features, float target [, constant string options]");
opts.addOption("r", "regularization", true,
"Regularization parameter for some r > 0 [default 0.1]");
if (cl != null) {
if (r_str != null) {
if (!(r > 0)) {
throw new UDFArgumentException(
protected void update(@Nonnull final FeatureValue[] features, final float coeff,
final float beta) {
for (FeatureValue f : features) {
if (f == null) {
private static IWeightValue getNewWeight(final IWeightValue old, final float x,
final float coeff, final float beta) {
if (old == null) {
@Description(
name = "train_arowe_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight, float covar>")
if (cl != null) {
if (opt_epsilon != null) {
if (loss > 0.f) {
@Description(
name = "train_arowe2_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight, float covar>")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_adadelta_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"AdaDeltaUDTF takes 2 or 3 arguments: List<Text|Int|BitInt> features, float target [, constant string options]");
opts.addOption("scale", true,
"Internal scaling/descaling factor for cumulative weights [100]");
if (cl == null) {
if (target < 0.f || target > 1.f) {
if (f == null) {
protected IWeightValue getNewWeight(@Nullable final IWeightValue old, final float xi,
final float gradient, final float g_g) {
if (old != null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_adagrad_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<Text|Int|BitInt> features, float target [, constant string options]");
opts.addOption("scale", true,
"Internal scaling/descaling factor for cumulative weights [100]");
if (cl == null) {
if (target < 0.f || target > 1.f) {
if (f == null) {
protected IWeightValue getNewWeight(@Nullable final IWeightValue old, final float xi,
final float gradient, final float g_g) {
if (old != null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "logress",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_pa1_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes arguments: List<Int|BigInt|Text> features, float target [, constant string options]");
opts.addOption("c", "aggressiveness", true,
"Aggressiveness paramete [default Float.MAX_VALUE]");
if (cl != null) {
if (opt_c != null) {
if (!(c > 0.f)) {
if (opt_epsilon != null) {
if (loss > 0.f) {
if (!Float.isInfinite(coeff)) {
@Description(
name = "train_pa1a_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
public static final class PA1a extends PassiveAggressiveRegressionUDTF {
@Description(
name = "train_pa2_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
@Description(
name = "train_pa2a_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
public static final class PA2a extends PA2 {
"_FUNC_ takes 2 arguments: List<Int|BigInt|Text> features, float target [, constant string options]");
if (HiveUtils.isNumberOI(argOIs[i])) {
if (i != last) {
return ObjectInspectorUtils.getConstantObjectInspector(
PrimitiveObjectInspectorFactory.javaStringObjectInspector, value);
if (mapredContext != null) {
if (conf != null) {
if (tdJarVersion == null) {
if (threads > 1) {
if (exec == null) {
for (Callable<T> task : tasks) {
for (Future<T> future : futures) {
if (exec != null) {
for (String line : opslist) {
if (ops.length == 2) {
while (IP < code.size()) {
if (done[IP]) {
if (!executeOperation(currentOperation)) {
if (IP < 0) {
if (StringUtils.isInt(currentOperation.operand)) {
if (candidateIP < 0) {
if (a == b) {
if (StringUtils.isInt(currentOperation.operand)) {
if (smile.math.Math.equals(a, b)) {
if (StringUtils.isInt(currentOperation.operand)) {
if (upper >= lower) {
if (StringUtils.isInt(currentOperation.operand)) {
if (upper > lower) {
if (StringUtils.isInt(currentOperation.operand)) {
if (upper <= lower) {
if (StringUtils.isInt(currentOperation.operand)) {
if (upper < lower) {
if (StringUtils.isInt(currentOperation.operand)) {
if (StringUtils.isDouble(currentOperation.operand)) {
if (v == null) {
if (name.equals("end")) {
@Description(
name = "convert_label",
value = "_FUNC_(const int|const float) - Convert from -1|1 to 0.0f|1.0f, or from 0.0f|1.0f to -1|1")
if (label == 0) {
} else if (label == -1) {
} else if (label == 1) {
if (label == 0.f) {
} else if (label == -1.f) {
} else if (label == 1.f) {
if (label == 0.d) {
} else if (label == -1.d) {
} else if (label == 1.d) {
@Description(name = "generate_series",
value = "_FUNC_(const int|bigint start, const int|bigint end) - "
"Generate a series of values, from start to end")
if (argOIs.length != 2) {
if (useBigInt) {
if (start > end) {
if (useBigInt) {
if (start == end) {
if (starti == endi) {
@Description(name = "x_rank",
value = "_FUNC_(KEY) - Generates a pseudo sequence number starting from 1 for each key")
if (key == null) {
if (lastNull) {
if (key.equals(lastKey)) {
@Description(name = "float_array",
value = "_FUNC_(nDims) - Returns an array<float> of nDims elements")
@Description(name = "collect_all",
value = "_FUNC_(x) - Retrurns a set of objects with duplicate elements eliminated")
if (tis.length != 1) {
if (m == Mode.PARTIAL1) {
if (!(parameters[0] instanceof StandardListObjectInspector)) {
if (p != null) {
for (Object i : partial) {
@Description(name = "bits_collect", value = "_FUNC_(int|long x) - Returns a bitset in array<long>")
if (arguments.length != 2) {
throw new UDFArgumentLengthException(
"map_tail_n only takes 2 arguments: map<object, object>, int");
if (!(arguments[0] instanceof MapObjectInspector)) {
if (!(arguments[1] instanceof IntObjectInspector)) {
for (Map.Entry<?, ?> e : m.entrySet()) {
if (tail.size() <= n) {
@Description(
name = "distcache_gets",
value = "_FUNC_(filepath, key, default_value [, parseKey]) - Returns map<key_type, value_type>|value_type")
if (argOIs.length != 3 && argOIs.length != 4) {
throw new UDFArgumentException(
"Invalid number of arguments for distcache_gets(FILEPATH, KEYS, DEFAULT_VAL, PARSE_KEY): "
if (!ObjectInspectorUtils.isConstantObjectInspector(argOIs[2])) {
throw new UDFArgumentException(
"Third argument DEFAULT_VALUE must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(argOIs[2]));
if (argOIs.length == 4) {
if (multipleDefaultValues) {
ObjectInspector valueOutputOI = ObjectInspectorUtils.getStandardObjectInspector(
valueInputOI, ObjectInspectorCopyOption.WRITABLE);
outputOI = ObjectInspectorFactory.getStandardMapObjectInspector(keyInputOI,
valueOutputOI);
if (parseKey && !HiveUtils.isStringOI(keyInputOI)) {
throw new UDFArgumentException(
"parseKey=true is only available for string typed key(s)");
private static void loadValues(OpenHashMap<Object, Object> map, File file,
PrimitiveObjectInspector keyOI, PrimitiveObjectInspector valueOI) throws IOException,
SerDeException {
if (!file.exists()) {
if (!file.getName().endsWith(".crc")) {
if (file.isDirectory()) {
for (File f : file.listFiles()) {
while ((line = reader.readLine()) != null) {
if (multipleKeyLookup) {
if (multipleDefaultValues) {
for (Object k : keys) {
if (k == null) {
if (v == null) {
if (numKeys != defaultValues.size()) {
if (k == null) {
if (v == null) {
if (v != null) {
if (parseKey) {
if (ctx == null) {
if (jobconf == null) {
@Description(name = "rowid",
value = "_FUNC_() - Returns a generated row id of a form {TASK_ID}-{SEQUENCE_NUMBER}")
if (taskId == -1) {
if (argOIs.length != 1) {
if (obj0 == null) {
} else if ("NFC".equals(form)) {
@Description(
name = "split_words",
value = "_FUNC_(string query [, string regex]) - Returns an array<text> containing splitted strings")
if (query == null) {
@Description(name = "is_stopword",
value = "_FUNC_(string word) - Returns whether English stopword or not")
stopwords = new String[] {"i", "me", "my", "myself", "we", "our", "ours", "ourselves",
"s", "t", "can", "will", "just", "don", "should", "now"};
if (size == 0) {
if (t == null) {
if (size < 1) {
if (comparator == null) {
if (e == null) {
if (numElem >= maxSize) {
if (cmp < 0) {
if (used >= data.length) {
if (needs >= data.length) {
while (data.length < max) {
if (index > used) {
} else if (index == used) {
if (index > used) {
} else if (index == used) {
if (index >= used)
if (i != 0) {
}
if (actualSize < expectedSize) {
if (v == null) {
if (_map.containsKey(e)) {
if (size > 0) {
protected Int2FloatOpenHashTable(int size, float loadFactor, float growFactor,
boolean forcePrime) {
if (size < 1) {
if (i < 0) {
if (expanded) {
if (keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
if (stat == FREE) {
if (stat == REMOVED && _keys[index] == key) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
while (i.next() != -1) {
if (i.hasNext()) {
if (newCapacity <= oldCapacity) {
if (_states[i] == FULL) {
while (newStates[keyIdx] != FREE) {
if (keyIdx < 0) {
while (i.next() != -1) {
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
while (index < _keys.length && _states[index] != FULL) {
if (!hasNext()) {
if (lastEntry == -1) {
if (lastEntry == -1) {
public Int2LongOpenHashTable(@Nonnull int[] keys, @Nonnull long[] values,
@Nonnull byte[] states, int used) {
if (size < 1) {
if (i < 0) {
if (expanded) {
if (keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
if (expanded) {
if (keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
if (stat == FREE) {
if (stat == REMOVED && _keys[index] == key) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
while (i.next() != -1) {
if (i.hasNext()) {
if (newCapacity <= oldCapacity) {
if (oldStates[i] == FULL) {
while (newStates[keyIdx] != FREE) {
if (keyIdx < 0) {
while (i.next() != -1) {
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
while (index < _keys.length && _states[index] != FULL) {
if (!hasNext()) {
if (lastEntry == -1) {
if (lastEntry == -1) {
if (key == null) {
for (;;) {
if (searchKey == null) {
} else if (compare(searchKey, key)) {
if (keys[off] != null && compare(keys[off], key)) {
if (keys[off] != null && compare(keys[off], key)) {
for (K key : m.keySet()) {
for (V v : values) {
if (v != null && compare(v, value)) {
for (K key : keys) {
if (key != null) {
for (V value : values) {
if (value != null) {
for (K key : keys) {
if (key != null) {
if (keys[x] != null) {
if (bitSize != bits) {
if (existingKeys != null) {
if (existingKeys[x] != null) {
while (index < keys.length && keys[index] == null) {
if (!hasNext()) {
if (index >= 0) {
}
public static ThreadPoolExecutor newFixedThreadPool(int nThreads, String threadName,
boolean daemon) {
return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS,
new LinkedBlockingQueue<Runnable>(), new NamedThreadFactory(threadName, daemon));
if (threadName == null) {
if (threadName == null) {
if (threadGroup == null) {
if (t.isDaemon() != daemon) {
if (t.getPriority() != threadPriority) {
if (t == 0L) {
if (hour > 0) {
if (min > 0) {
if (sec > 0) {
if (t > 0) {
if (timeInMills == 0d) {
if (hour > 0) {
if (min > 0) {
if (sec > 0) {
if (t > 0 || diff > 0f) {
public static int getTaskId(final int defaultValue) {
MapredContext ctx = MapredContextAccessor.get();
if (ctx == null) {
return defaultValue;
}
JobConf jobconf = ctx.getJobConf();
if (jobconf == null) {
return defaultValue;
}
int taskid = jobconf.getInt("mapred.task.partition", -1);
if (taskid == -1) {
taskid = jobconf.getInt("mapreduce.task.partition", -1);
if (taskid == -1) {
return defaultValue;
}
}
return taskid;
}
package hivemall.utils.io;
import java.io.IOException;
import java.io.InputStream;
public final class FastByteArrayInputStream extends InputStream {
protected int pos = 0;
public FastByteArrayInputStream() {}
public FastByteArrayInputStream(byte[] buf) {
this(buf, buf.length);
}
public FastByteArrayInputStream(byte[] buf, int count) {
this.buf = buf;
this.count = count;
}
public FastByteArrayInputStream(byte buf[], int offset, int length) {
this.buf = buf;
this.pos = offset;
}
public void init(byte[] buf) {
init(buf, buf.length);
}
public void init(byte[] buf, int count) {
this.buf = buf;
this.count = count;
this.pos = 0;
}
@Override
public final int available() {
return count - pos;
}
public final int read() {
}
@Override
public final int read(byte[] b, int off, int len) {
if (pos >= count) {
return -1;
}
len = (count - pos);
}
if (len <= 0) {
return 0;
}
System.arraycopy(buf, pos, b, off, len);
return len;
}
@Override
public final long skip(long n) {
n = count - pos;
}
if (n < 0) {
return 0;
}
return n;
}
@Override
public boolean markSupported() {
return false;
}
@Override
public void mark(int readlimit) {
throw new UnsupportedOperationException();
}
@Override
public void close() throws IOException {
this.buf = null;
}
}
package hivemall.utils.io;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.io.UnsupportedEncodingException;
import java.util.LinkedList;
public final class FastMultiByteArrayOutputStream extends OutputStream {
private static final int DEFAULT_BLOCK_SIZE = 8192;
private final int blockSize;
private byte[] buffer;
private int size;
private LinkedList<byte[]> buffers;
private int index;
public FastMultiByteArrayOutputStream() {
this(DEFAULT_BLOCK_SIZE);
}
public FastMultiByteArrayOutputStream(int aSize) {
this.blockSize = aSize;
this.buffer = new byte[aSize];
}
public int size() {
}
public byte[][] toMultiByteArray() {
if (buffers == null) {
final byte[][] mb = new byte[1][];
mb[0] = buffer;
return mb;
}
final int listsize = buffers.size();
final byte[][] mb = new byte[size][];
buffers.toArray(mb);
mb[listsize] = buffer;
return mb;
}
public byte[] toByteArray() {
final byte[] data = new byte[size()];
int pos = 0;
if (buffers != null) {
for (byte[] bytes : buffers) {
System.arraycopy(bytes, 0, data, pos, blockSize);
}
}
System.arraycopy(buffer, 0, data, pos, index);
return data;
}
public byte[] toByteArray_clear() {
final int size = size();
final byte[] data = new byte[size];
int pos = 0;
if (buffers != null) {
while (!buffers.isEmpty()) {
byte[] bytes = buffers.removeFirst();
System.arraycopy(bytes, 0, data, pos, blockSize);
}
}
System.arraycopy(buffer, 0, data, pos, index);
return data;
}
@Override
public String toString() {
return new String(toByteArray());
}
public String toString(String enc) throws UnsupportedEncodingException {
return new String(toByteArray(), enc);
}
@Override
public void write(int datum) {
if (index >= blockSize) {
addBuffer();
}
}
@Override
public void write(byte[] data, int offset, int length) {
if (data == null) {
throw new NullPointerException();
throw new IndexOutOfBoundsException();
} else {
int copyLength;
do {
if (index == blockSize) {
addBuffer();
}
copyLength = blockSize - index;
if (length < copyLength) {
copyLength = length;
}
System.arraycopy(data, offset, buffer, index, copyLength);
length -= copyLength;
} while (length > 0);
System.arraycopy(data, offset, buffer, index, length);
}
}
}
public void writeInt(int v) throws IOException {
addBuffer();
}
}
public void writeLong(long v) throws IOException {
addBuffer();
}
}
public void writeInts(int[] v, int off, int len) throws IOException {
writeInt(v[i]);
}
}
public void writeLongs(long[] v, int off, int len) throws IOException {
writeLong(v[i]);
}
}
public void reset() {
buffer = new byte[blockSize];
buffers = null;
size = 0;
index = 0;
}
public void writeTo(OutputStream out) throws IOException {
if (buffers != null) {
for (byte[] bytes : buffers) {
out.write(bytes, 0, blockSize);
}
}
out.write(buffer, 0, index);
}
public InputStream getInputStream() {
return new FastByteArrayInputStream(toByteArray());
}
private void addBuffer() {
if (buffers == null) {
buffers = new LinkedList<byte[]>();
}
buffers.addLast(buffer);
buffer = new byte[blockSize];
index = 0;
}
@Override
public void close() throws IOException {
clear();
}
public void clear() {
this.buffer = null;
this.buffers = null;
}
}
if (!file.exists()) {
if (file.isDirectory()) {
if (files != null && files.length > 0) {
for (File f : files) {
}
if (count == null) {
if (count == null) {
if (counter == null) {
for (Map.Entry<E, Integer> e : counter.entrySet()) {
if (counter == null) {
for (Map.Entry<E, Integer> e : counter.entrySet()) {
for (Map.Entry<E, Integer> e : counts.entrySet()) {
if (v >= maxValue) {
for (Map.Entry<E, Integer> e : counts.entrySet()) {
if (v <= minValue) {
if (count == null) {
try {
public final class MutableDouble extends Number implements Copyable<MutableDouble>,
Comparable<MutableDouble>, Serializable {
public final class MutableFloat extends Number implements Copyable<MutableFloat>,
Comparable<MutableFloat>, Serializable {
public final class MutableInt extends Number implements Copyable<MutableInt>,
Comparable<MutableInt>, Serializable {
if (obj instanceof MutableInt) {
public final class MutableLong extends Number implements Copyable<MutableLong>,
Comparable<MutableLong>, Serializable {
if (obj instanceof MutableLong) {
while (true) {
if (state.get()) {
private static final int[] PRIMES = {3, 5, 7, 13, 19, 31, 43, 61, 73, 89, 103, 109, 139, 151,
Integer.MAX_VALUE};
if (idx < 0) {
if (idx >= PRIMES.length) {
if (p < 0 || p > 1) {
if (range <= 0) {
if (p == 0) {
if (p == 1) {
if (v < 0) {
if (pos == -1) {
if (basePort == 0) {
if (basePort < 0 || basePort > 65535) {
if (isPortAvailable(i)) {
if (s != null) {
final ObjectName name = makeMBeanName("hivemall",
final ObjectName name = makeMBeanName("hivemall",
private static ObjectName makeMBeanName(@Nonnull final String domain,
@Nonnull final String type, @Nonnull final String channelName) {
private static String makeMBeanNameString(@Nonnull final String domain,
@Nonnull final String type, @Nonnull final String channelName) {
public ThroughputCounter(@Nonnull ScheduledExecutorService executor, long checkInterval,
@Nonnull MixServerMetrics metrics) {
if (interval == 0) {
if (logger.isInfoEnabled()) {
if (lastReads > 0 || lastWrites > 0) {
opts.addOption("workers", "num_workers", true,
"The number of MIX workers [default: max(1, round(procs * 1.5))] ");
opts.addOption("scale", "scalemodel", true,
"Scale values of prediction models to avoid overflow [default: 1.0 (no-scale)]");
opts.addOption("sync", "sync_threshold", true,
"Synchronization threshold using clock difference [default: 30]");
opts.addOption("ttl", "session_ttl", true,
"The TTL in sec that an idle session lives [default: 120 sec]");
opts.addOption("sweep", "session_sweep_interval", true,
"The interval in sec that the session expiry thread runs [default: 60 sec]");
opts.addOption("jmx", "metrics", false,
"Toggle this option to enable monitoring metrics using JMX [default: false]");
if (ssl) {
MixServerInitializer initializer = new MixServerInitializer(msgHandler, throughputCounter,
sslCtx);
sweepIntervalInSec, TimeUnit.SECONDS);
if (jmx) {
private void acceptConnections(@Nonnull MixServerInitializer initializer, int port,
@Nonnegative int numWorkers) throws InterruptedException {
public MixServerHandler(@Nonnull SessionStore sessionStore, @Nonnegative int syncThreshold,
@Nonnegative float scale) {
if (groupId == null) {
if (groupID == null) {
if (partial == null) {
if (existing != null) {
private void mix(final ChannelHandlerContext ctx, final MixMessage requestMsg,
final PartialResult partial, final SessionObject session) {
if (deltaUpdates <= 0) {
if (cancelRequest) {
responseMsg = new MixMessage(event, feature, averagedWeight, meanCovar,
if (responseMsg != null) {
public MixServerInitializer(@Nonnull MixServerHandler msgHandler,
@Nullable ThroughputCounter throughputCounter, @Nullable SslContext sslCtx) {
if (sslCtx != null) {
if (throughputCounter != null) {
public abstract void add(float localWeight, float covar, @Nonnegative int deltaUpdates,
float scale);
public abstract void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates,
float scale);
if (obj == null) {
if (sessionObj == null) {
ConcurrentMap<Object, PartialResult> map = new ConcurrentHashMap<Object, PartialResult>(
EXPECTED_MODEL_SIZE);
if (existing != null) {
if (removedSession != null) {
while (itor.hasNext()) {
if (elapsedTime > ttl) {
if (logger.isInfoEnabled()) {
@Description(
name = "tokenize_ja",
value = "_FUNC_(String line [, const string mode = \"normal\", const list<string> stopWords, const list<string> stopTags])"
" - returns tokenized strings in array<string>")
if (arglen < 1 || arglen > 4) {
if (analyzer == null) {
if (arg0 == null) {
if (stream != null) {
if (arg == null) {
if ("NORMAL".equalsIgnoreCase(arg)) {
} else if ("SEARCH".equalsIgnoreCase(arg)) {
} else if ("EXTENDED".equalsIgnoreCase(arg)) {
} else if ("DEFAULT".equalsIgnoreCase(arg)) {
throw new UDFArgumentException(
if (array == null) {
if (array.length == 0) {
if (array == null) {
if (length == 0) {
if (s != null) {
while (stream.incrementToken()) {
@Description(name = "lr_datagen",
value = "_FUNC_(options string) - Generates a logistic regression dataset")
protected CommandLine processOptions(ObjectInspector[] objectInspectors)
throws UDFArgumentException {
@Description(name = "add_bias",
value = "_FUNC_(features in array<string>) - Returns features with a bias as array<string>")
if (arguments.length != 1) {
switch (arguments[0].getCategory()) {
if (elmOI.getCategory().equals(Category.PRIMITIVE)) {
if (((PrimitiveObjectInspector) elmOI).getPrimitiveCategory() == PrimitiveCategory.STRING) {
return ObjectInspectorFactory.getStandardListObjectInspector(argumentOI.getListElementObjectInspector());
assert (arguments.length == 1);
name = "add_feature_index",
value = "_FUNC_(dense features in array<double>) - Returns a feature vector with feature indicies")
if (arguments.length != 1) {
switch (arguments[0].getCategory()) {
if (elmOI.getCategory().equals(Category.PRIMITIVE)) {
if (((PrimitiveObjectInspector) elmOI).getPrimitiveCategory() == PrimitiveCategory.DOUBLE) {
return ObjectInspectorFactory.getStandardListObjectInspector(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
assert (arguments.length == 1);
}
@Description(name = "extract_feature",
value = "_FUNC_(feature in string) - Returns a parsed feature as string")
if (arguments.length != 1) {
assert (arguments.length == 1);
@Description(name = "extract_weight",
value = "_FUNC_(feature in string) - Returns the weight of a feature as string")
if (arguments.length != 1) {
return PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(PrimitiveCategory.DOUBLE);
assert (arguments.length == 1);
@Description(name = "sort_by_feature",
value = "_FUNC_(map in map<int,float>) - Returns a sorted map")
if (arguments.length != 1) {
switch (arguments[0].getCategory()) {
if (keyOI.getCategory().equals(Category.PRIMITIVE)
argumentOI.getMapKeyObjectInspector(), argumentOI.getMapValueObjectInspector());
assert (arguments.length == 1);
final Map<IntWritable, FloatWritable> input = (Map<IntWritable, FloatWritable>) argumentOI.getMap(arguments[0].get());
if (arguments.length != 1) {
throw new UDFArgumentLengthException("normalize() has an only single argument.");
switch (arguments[0].getCategory()) {
if (elmOI.getCategory().equals(Category.PRIMITIVE)) {
if (((PrimitiveObjectInspector) elmOI).getPrimitiveCategory() == PrimitiveCategory.STRING) {
ObjectInspector outputElemOI = ObjectInspectorFactory.getReflectionObjectInspector(
Text.class, ObjectInspectorOptions.JAVA);
ObjectInspector outputOI = ObjectInspectorFactory.getStandardListObjectInspector(outputElemOI);
assert (arguments.length == 1);
name = "minhashes",
value = "_FUNC_(features in array<string>, noWeight in boolean) - Returns hashed features as array<int>")
if (arguments.length != 2) {
switch (arguments[0].getCategory()) {
if (elmOI.getCategory().equals(Category.PRIMITIVE)) {
if (((PrimitiveObjectInspector) elmOI).getPrimitiveCategory() == PrimitiveCategory.STRING) {
return ObjectInspectorFactory.getStandardListObjectInspector(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(PrimitiveCategory.INT));
assert (arguments.length == 2);
final Boolean noWeight = PrimitiveObjectInspectorUtils.getBoolean(arguments[1].get(),
noWeightOI);
name = "rowid",
value = "_FUNC_() - Returns a generated row id of a form {TASK_ID}-{UUID}-{SEQUENCE_NUMBER}")
if (arguments.length != 0) {
assert (arguments.length == 0);
name = "train_arowh",
name = "train_multiclass_arowh",
@Description(name = "hivemall_version", value = "_FUNC_() - Returns the version of Hivemall",
extended = "Usage: SELECT hivemall_version();")
import hivemall.mix.MixMessage.MixEventName;
import hivemall.mix.client.MixClient;
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_arow",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by Adaptive Regularization of Weight Vectors (AROW) binary classifier")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<String|Int|BitInt> features, Int label [, constant String options]");
opts.addOption("r", "regularization", true,
"Regularization parameter for some r > 0 [default 0.1]");
if (cl != null) {
if (r_str != null) {
if (!(r > 0)) {
throw new UDFArgumentException(
if (m < 1.f) {
protected void update(@Nonnull final FeatureValue[] features, final float y, final float alpha,
final float beta) {
for (FeatureValue f : features) {
if (f == null) {
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float y,
final float alpha, final float beta) {
if (old == null) {
@Description(
name = "train_arowh",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by AROW binary classifier using hinge loss")
if (cl != null) {
if (c_str != null) {
if (!(c > 0.f)) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "train_adagrad_rda",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight>",
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<Text|Int|BitInt> features, int label [, constant string options]");
opts.addOption("scale", true,
"Internal scaling/descaling factor for cumulative weights [default: 100]");
if (cl == null) {
if (f == null) {
protected void updateWeight(@Nonnull final Object x, final float xi, final float y,
final float t) {
if (old != null) {
if (meansOfGradients < 0.f) {
public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if (argOIs.length < 2) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 arguments: List<Int|BigInt|Text> features, int label [, constant string options]");
PrimitiveObjectInspector featureOutputOI = dense_model ? PrimitiveObjectInspectorFactory.javaIntObjectInspector
if (preloadedModelFile != null) {
if (!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
throw new UDFArgumentTypeException(0,
if (useCovariance()) {
if (featureVector == null) {
if (size == 0) {
if (f == null) {
if (parseFeature) {
if (f == null) {
if (old_w != 0.f) {
if (f == null) {
if (old_w != 0f) {
if (f == null) {
if (old_w == null) {
if (f == null) {
if (model != null) {
if (useCovariance()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_cw",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by Confidence-Weighted (CW) binary classifier")
public final class ConfidenceWeightedUDTF extends BinaryOnlineClassifierUDTF {
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"ConfidenceWeightedUDTF takes 2 or 3 arguments: List<String|Int|BitInt> features, Int label [, constant String options]");
opts.addOption("eta", "hyper_c", true,
"Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
if (cl != null) {
if (phi_str == null) {
if (eta_str != null) {
if (eta <= 0.5 || eta > 1) {
throw new UDFArgumentException(
protected void update(@Nonnull final FeatureValue[] features, final float coeff,
final float alpha) {
for (FeatureValue f : features) {
if (f == null) {
private static IWeightValue getNewWeight(final IWeightValue old, final float x,
final float coeff, final float alpha, final float phi) {
if (old == null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "train_pa",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive (PA) binary classifier")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<Text|Int|BitInt> features, int label [, constant string options]");
@Description(
name = "train_pa1",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive 1 (PA-1) binary classifier")
if (cl != null) {
if (c_str != null) {
if (!(c > 0.f)) {
@Description(
name = "train_pa2",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive 2 (PA-2) binary classifier")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "train_perceptron",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight>",
extended = "Build a prediction model by Perceptron binary classifier")
public final class PerceptronUDTF extends BinaryOnlineClassifierUDTF {
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<Text|Int|BitInt> features, int label [, constant string options]");
import org.apache.hadoop.hive.ql.exec.Description;
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"SoftConfideceWeightedUDTF takes 2 or 3 arguments: List<String|Int|BitInt> features, Int label [, constant String options]");
opts.addOption("eta", "hyper_c", true,
"Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
if (cl != null) {
if (phi_str == null) {
if (eta_str != null) {
if (eta <= 0.5 || eta > 1) {
throw new UDFArgumentException(
if (c_str != null) {
if (!(c > 0.f)) {
if (loss > 0.f) {
if (alpha == 0.f) {
if (beta == 0.f) {
@Description(
name = "train_scw",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by Soft Confidence-Weighted (SCW-1) binary classifier")
if (alpha_denom == 0.f) {
if (alpha <= 0.f) {
if (alpha == 0.f) {
if (beta_den == 0.f) {
@Description(
name = "train_scw2",
value = "_FUNC_(list<string|int|bigint> features, int label [, const string options])"
" - Returns a relation consists of <string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by Soft Confidence-Weighted 2 (SCW-2) binary classifier")
public static final class SCW2 extends SCW1 {
if (alpha_numer <= 0.f) {
if (alpha_denom == 0.f) {
protected void update(@Nonnull final FeatureValue[] features, final float y, final float alpha,
final float beta) {
for (FeatureValue f : features) {
if (f == null) {
private static IWeightValue getNewWeight(final IWeightValue old, final float x, final float y,
final float alpha, final float beta) {
if (old == null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_multiclass_arow",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight, float covar>",
extended = "Build a prediction model by Adaptive Regularization of Weight Vectors (AROW) multiclass classifier")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<String|Int|BitInt> features, {Int|String} label [, constant String options]");
opts.addOption("r", "regularization", true,
"Regularization parameter for some r > 0 [default 0.1]");
if (cl != null) {
if (r_str != null) {
if (!(r > 0)) {
throw new UDFArgumentException(
if (m >= 1.f) {
protected void update(@Nonnull final FeatureValue[] features, final Object actual_label,
final Object missed_label, final float alpha, final float beta) {
if (actual_label.equals(missed_label)) {
if (model2add == null) {
if (missed_label != null) {
if (model2sub == null) {
if (f == null) {
if (model2sub != null) {
IWeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, beta,
false);
private static IWeightValue getNewWeight(final IWeightValue old, final float v,
final float alpha, final float beta, final boolean positive) {
if (old == null) {
@Description(
name = "train_multiclass_arowh",
value = "_FUNC_(list<string|int|bigint> features, int|string label [, const string options])"
" - Returns a relation consists of <int|string label, string|int|bigint feature, float weight, float covar>",
extended = "Build a prediction model by Adaptive Regularization of Weight Vectors (AROW) multiclass classifier using hinge loss")
public static final class AROWh extends MulticlassAROWClassifierUDTF {
if (cl != null) {
if (c_str != null) {
if (!(c > 0.f)) {
if (loss > 0.f) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_multiclass_cw",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight, float covar>",
extended = "Build a prediction model by Confidence-Weighted (CW) multiclass classifier")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<String|Int|BitInt> features, {Int|String} label [, constant String options]");
opts.addOption("eta", "hyper_c", true,
"Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
if (cl != null) {
if (phi_str == null) {
if (eta_str != null) {
if (eta <= 0.5 || eta > 1) {
throw new UDFArgumentException(
protected void update(@Nonnull final FeatureValue[] features, float alpha, Object actual_label,
Object missed_label) {
if (actual_label.equals(missed_label)) {
if (model2add == null) {
if (missed_label != null) {
if (model2sub == null) {
if (f == null) {
if (model2sub != null) {
private static IWeightValue getNewWeight(final IWeightValue old, final float x,
final float alpha, final float phi, final boolean positive) {
if (old == null) {
if (argOIs.length < 2) {
throw new UDFArgumentException(
getClass().getSimpleName()
" takes 2 arguments: List<Int|BigInt|Text> features, {Int|BitInt|Text} label [, constant text options]");
if (!STRING_TYPE_NAME.equals(labelTypeName) && !INT_TYPE_NAME.equals(labelTypeName)
PrimitiveObjectInspector featureOutputOI = dense_model ? PrimitiveObjectInspectorFactory.javaIntObjectInspector
: featureInputOI;
if (preloadedModelFile != null) {
if (!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
throw new UDFArgumentTypeException(0,
protected StructObjectInspector getReturnOI(ObjectInspector labelRawOI,
ObjectInspector featureRawOI) {
if (useCovariance()) {
if (featureVector == null) {
if (label == null) {
if (size == 0) {
if (f == null) {
if (parseFeature) {
protected abstract void train(@Nonnull final FeatureValue[] features,
@Nonnull final Object actual_label);
if (maxScoredLabel == null || score > maxScore) {
if (label.equals(actual_label)) {
if (maxAnotherLabel == null || score > maxAnotherScore) {
protected Margin getMarginAndVariance(@Nonnull final FeatureValue[] features,
final Object actual_label) {
protected Margin getMarginAndVariance(@Nonnull final FeatureValue[] features,
final Object actual_label, boolean nonZeroVariance) {
if (label.equals(actual_label)) {
if (maxAnotherLabel == null || score > maxAnotherScore) {
if (f == null) {
protected final float calcScore(@Nonnull final PredictionModel model,
@Nonnull final FeatureValue[] features) {
if (f == null) {
if (old_w != 0f) {
if (f == null) {
protected final PredictionResult calcScoreAndVariance(@Nonnull final PredictionModel model,
@Nonnull final FeatureValue[] features) {
if (f == null) {
if (old_w == null) {
protected void update(@Nonnull final FeatureValue[] features, float coeff, Object actual_label,
Object missed_label) {
if (actual_label.equals(missed_label)) {
if (model2add == null) {
if (missed_label != null) {
if (model2sub == null) {
if (f == null) {
if (model2sub != null) {
if (label2model != null) {
if (useCovariance()) {
for (Map.Entry<Object, PredictionModel> entry : label2model.entrySet()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
for (Map.Entry<Object, PredictionModel> entry : label2model.entrySet()) {
while (itor.next() != -1) {
if (!probe.isTouched()) {
protected void loadPredictionModel(Map<Object, PredictionModel> label2model, String filename,
PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI) {
if (useCovariance()) {
lines = loadPredictionModel(label2model, new File(filename), labelOI, featureOI,
writableFloatObjectInspector, writableFloatObjectInspector);
lines = loadPredictionModel(label2model, new File(filename), labelOI, featureOI,
writableFloatObjectInspector);
if (!label2model.isEmpty()) {
for (Map.Entry<Object, PredictionModel> e : label2model.entrySet()) {
statsBuf.append('\n')
.append("Label: ")
.append(label)
.append(", Number of Features: ")
.append(numFeatures);
private long loadPredictionModel(Map<Object, PredictionModel> label2model, File file,
PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI,
WritableFloatObjectInspector weightOI) throws IOException, SerDeException {
if (!file.exists()) {
if (!file.getName().endsWith(".crc")) {
if (file.isDirectory()) {
for (File f : file.listFiles()) {
while ((line = reader.readLine()) != null) {
if (f0 == null || f1 == null || f2 == null) {
if (model == null) {
private long loadPredictionModel(Map<Object, PredictionModel> label2model, File file,
PrimitiveObjectInspector labelOI, PrimitiveObjectInspector featureOI,
WritableFloatObjectInspector weightOI, WritableFloatObjectInspector covarOI)
if (!file.exists()) {
if (!file.getName().endsWith(".crc")) {
if (file.isDirectory()) {
for (File f : file.listFiles()) {
covarOI);
LazySimpleSerDe serde = HiveUtils.getLineSerde(labelOI, featureOI, weightOI,
covarOI);
while ((line = reader.readLine()) != null) {
if (f0 == null || f1 == null || f2 == null) {
if (model == null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_multiclass_pa",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive (PA) multiclass classifier")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"MulticlassPassiveAggressiveUDTF takes 2 or 3 arguments: List<Text|Int|BitInt> features, {Int|Text} label [, constant text options]");
@Description(
name = "train_multiclass_pa1",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive 1 (PA-1) multiclass classifier")
if (cl != null) {
if (c_str != null) {
if (!(c > 0.f)) {
@Description(
name = "train_multiclass_pa2",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight>",
extended = "Build a prediction model by Passive-Aggressive 2 (PA-2) multiclass classifier")
public static final class PA2 extends PA1 {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_multiclass_perceptron",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight>",
extended = "Build a prediction model by Perceptron multiclass classifier")
public final class MulticlassPerceptronUDTF extends MulticlassOnlineClassifierUDTF {
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"MulticlassPerceptronUDTF takes 2 or 3 arguments: List<Text|Int|BitInt> features, {Int|Text} label [, constant text options]");
if (!actual_label.equals(predicted_label)) {
import org.apache.hadoop.hive.ql.exec.Description;
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"MulticlassSoftConfidenceWeightedUDTF takes 2 or 3 arguments: List<String|Int|BitInt> features, {Int|String} label [, constant String options]");
opts.addOption("eta", "hyper_c", true,
"Confidence hyperparameter eta in range (0.5, 1] [default 0.85]");
if (cl != null) {
if (phi_str == null) {
if (eta_str != null) {
if (eta <= 0.5 || eta > 1) {
throw new UDFArgumentException(
if (c_str != null) {
if (!(c > 0.f)) {
if (loss > 0.f) {
if (alpha == 0.f) {
if (beta == 0.f) {
@Description(
name = "train_multiclass_scw",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight, float covar>",
extended = "Build a prediction model by Soft Confidence-Weighted (SCW-1) multiclass classifier")
if (alpha_denom == 0.f) {
if (alpha <= 0.f) {
if (alpha == 0.f) {
if (beta_den == 0.f) {
@Description(
name = "train_multiclass_scw2",
value = "_FUNC_(list<string|int|bigint> features, {int|string} label [, const string options])"
" - Returns a relation consists of <{int|string} label, {string|int|bigint} feature, float weight, float covar>",
extended = "Build a prediction model by Soft Confidence-Weighted 2 (SCW-2) multiclass classifier")
public static final class SCW2 extends SCW1 {
if (alpha_numer <= 0.f) {
if (alpha_denom == 0.f) {
protected void update(@Nonnull final FeatureValue[] features, final Object actual_label,
final Object missed_label, final float alpha, final float beta) {
if (actual_label.equals(missed_label)) {
if (model2add == null) {
if (missed_label != null) {
if (model2sub == null) {
if (f == null) {
if (model2sub != null) {
IWeightValue new_wrongclass_w = getNewWeight(old_wrongclass_w, v, alpha, beta,
false);
private static IWeightValue getNewWeight(final IWeightValue old, final float v,
final float alpha, final float beta, final boolean positive) {
if (old == null) {
if (n == 0L) {
if (n == 1L) {
if (numBuffers < 1) {
if (xtimes < 1) {
if (numBuffers < 1) {
if (xtimes < 1) {
if (position < numBuffers) {
if (position == numBuffers) {
if (sweepedObj != null) {
if (droppped == null) {
if (listener != null) {
if (sampleSize <= 0) {
if (item == null) {
if (replaceIndex < numSamples) {
}
@Description(
name = "lr_datagen",
value = "_FUNC_(options string) - Generates a logistic regression dataset",
extended = "WITH dual AS (SELECT 1) SELECT lr_datagen('-n_examples 1k -n_features 10') FROM dual;")
private long r_seed;
opts.addOption("ne", "n_examples", true,
"Number of training examples created for each task [DEFAULT: 1000]");
opts.addOption("nf", "n_features", true,
"Number of features contained for each example [DEFAULT: 10]");
opts.addOption("eps", true,
"eps Epsilon factor by which positive examples are scaled [DEFAULT: 3.0]");
opts.addOption("p1", "prob_one", true,
" Probability in [0, 1.0) that a label is 1 [DEFAULT: 0.6]");
opts.addOption("seed", true, "The seed value for random number generator [DEFAULT: 43L]");
opts.addOption(
"dense",
false,
"Make a dense dataset or not. If not specified, a sparse dataset is generated.\n"
"For sparse, n_dims should be much larger than n_features. When disabled, n_features must be equals to n_dims ");
opts.addOption("cl", "classification", false,
"Toggle this option on to generate a classification dataset");
if (argOIs.length != 1) {
this.r_seed = Primitives.parseLong(cl.getOptionValue("seed"), 43L);
if (n_features > n_dimensions) {
if (dense) {
if (n_features != n_dimensions) {
if (dense) {
if (dense) {
if (rnd1 == null) {
final int taskid = HadoopUtils.getTaskId(-1);
final long seed;
if (taskid == -1) {
} else {
}
if (dense) {
if (position == N_BUFFERS) {
if (used.get(f)) {
if (retry < 3) {
if (sort) {
if (dense) {
if (position > 0) {
@Description(
name = "argmin_kld",
value = "_FUNC_(float mean, float covar) - Returns mean or covar that minimize a KL-distance among distributions",
extended = "The returned value is (1.0 / (sum(1.0 / covar))) * (sum(mean / covar)")
if (mean == null || covar == null) {
if (partial == null) {
if (o == null) {
if (partial == null) {
if (partial == null) {
@Description(
name = "maxrow",
value = "_FUNC_(ANY compare, ...) - Returns a row that has maximum value in the 1st argument")
if (!ObjectInspectorUtils.compareSupported(oi)) {
throw new UDFArgumentTypeException(0,
"Cannot support comparison of map<> type or complex type containing map<>.");
if (parameters.length == 1 && parameters[0] instanceof StructObjectInspector) {
if (partial == null) {
if (partial instanceof Object[]) {
} else if (partial instanceof LazyBinaryStruct) {
} else if (inputStructOI != null) {
if (maxagg.objects == null) {
int cmp = ObjectInspectorUtils.compare(maxagg.objects[0], outputOIs[0],
otherObjects.get(0), inputOIs[0]);
if (cmp < 0) {
if (isMax) {
maxagg.objects[i] = ObjectInspectorUtils.copyToStandardObject(
otherObjects.get(i), inputOIs[i]);
}
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "max_label",
value = "_FUNC_(double value, string label) - Returns a label that has the maximum value")
if (partial == null) {
if (v >= partial.maxValue) {
if (other == null) {
if (partial == null) {
if (other.maxValue < partial.maxValue) {
if (partial == null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "voted_avg",
value = "_FUNC_(double value) - Returns an averaged value by bagging for classification")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "weight_voted_avg",
value = "_FUNC_(expr) - Returns an averaged value by considering sum of positive/negative weights")
@Description(name = "f1score",
value = "_FUNC_(array[int], array[int]) - Return a F-measure/F1 score")
if (actual.contains(p)) {
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
if (divisor > 0) {
@Description(name = "logloss",
value = "_FUNC_(double predicted, double actual) - Return a Logrithmic Loss")
@Description(name = "mae",
value = "_FUNC_(double predicted, double actual) - Return a Mean Absolute Error")
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
@Description(name = "mse",
value = "_FUNC_(double predicted, double actual) - Return a Mean Squared Error")
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
@Description(
name = "r2",
value = "_FUNC_(double predicted, double actual) - Return R Squared (coefficient of determination)")
@Description(name = "rmse",
value = "_FUNC_(double predicted, double actual) - Return a Root Mean Squared Error")
if (partial == null) {
if (other == null) {
if (partial == null) {
if (partial == null) {
@Description(
name = "fm_predict",
value = "_FUNC_(Float Wj, array<float> Vjf, float Xj) - Returns a prediction value in Double")
if (typeInfo.length != 3) {
throw new UDFArgumentLengthException(
if (!HiveUtils.isNumberTypeInfo(typeInfo[0])) {
throw new UDFArgumentTypeException(0,
if (typeInfo[1].getCategory() != Category.LIST) {
throw new UDFArgumentTypeException(1,
if (!HiveUtils.isNumberTypeInfo(typeInfo1.getListElementTypeInfo())) {
throw new UDFArgumentTypeException(1,
"Number type is expected for the element type of list Vjf: "
typeInfo1.getTypeName());
if (!HiveUtils.isNumberTypeInfo(typeInfo[2])) {
throw new UDFArgumentTypeException(2,
if (parameters[0] == null) {
if (parameters[2] == null) {
if (buf.sumVjXj != null) {
if (partial == null) {
if (sumVjXj instanceof LazyBinaryArray) {
if (sumV2X2 instanceof LazyBinaryArray) {
void iterate(final double Wj, final double Xj, @Nonnull final Object Vif,
@Nonnull final ListObjectInspector vOI,
@Nonnull final PrimitiveObjectInspector vElemOI) throws HiveException {
if (factors < 1) {
if (sumVjXj == null) {
} else if (sumVjXj.length != factors) {
if (o == null) {
void merge(final double o_ret, @Nullable final Object o_sumVjXj,
@Nullable final Object o_sumV2X2,
@Nonnull final StandardListObjectInspector sumVjXjOI,
@Nonnull final StandardListObjectInspector sumV2X2OI) throws HiveException {
if (o_sumVjXj == null) {
if (sumVjXj == null) {
if (sumVjXj != null) {
@Nonnull final ListObjectInspector listOI, @Nullable final Feature[] probes,
final int numFeatures, final int numFields) throws HiveException {
static IntFeature parseFFMFeature(@Nonnull final String fv, final int numFeatures,
final int numFields) throws HiveException {
}
static void parseFFMFeature(@Nonnull final String fv, @Nonnull final Feature probe)
throws HiveException {
static void parseFFMFeature(@Nonnull final String fv, @Nonnull final Feature probe,
final int numFeatures, final int numFields) throws HiveException {
private static short parseField(@Nonnull final String fieldStr, final int numFields)
throws HiveException {
@Description(
name = "add_bias",
value = "_FUNC_(feature_vector in array<string>) - Returns features with a bias in array<string>")
@Description(
name = "add_feature_index",
value = "_FUNC_(ARRAY[DOUBLE]: dense feature vector) - Returns a feature vector with feature indicies")
if (ftvec == null) {
if (size == 0) {
@Description(name = "extract_feature",
value = "_FUNC_(feature_vector in array<string>) - Returns features in array<string>")
if (featureVector == null) {
if (featureVectors == null) {
if (fv != null) {
if (pos > 0) {
@Description(
name = "feature_index",
value = "_FUNC_(feature_vector in array<string>) - Returns feature indicies in array<index>")
if (feature == null) {
if (featureVector == null) {
for (String f : featureVector) {
if (f != null) {
if (pos > 0) {
@Description(name = "feature",
value = "_FUNC_(string feature, double weight) - Returns a feature string")
if (feature == null) {
if (feature == null) {
if (feature == null) {
if (feature == null) {
@Description(name = "sort_by_feature",
value = "_FUNC_(map in map<int,float>) - Returns a sorted map")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "amplify",
value = "_FUNC_(const int xtimes, *) - amplify the input records x-times")
public final class AmplifierUDTF extends GenericUDTF {
if (!(argOIs.length >= 2)) {
throw new UDFArgumentException("_FUNC_(int xtimes, *) takes at least two arguments");
if (!(xtimes >= 1)) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "rand_amplify", value = "_FUNC_(const int xtimes, const int num_buffers, *)"
" - amplify the input records x-times in map-side")
public final class RandomAmplifierUDTF extends GenericUDTF implements DropoutListener<Object[]> {
if (useSeed) {
if (numArgs < 3) {
throw new UDFArgumentException(
"_FUNC_(int xtimes, int num_buffers, *) takes at least three arguments");
if (!(xtimes >= 1)) {
if (numBuffers < 2) {
if (useSeed) {
ObjectInspector retOI = ObjectInspectorUtils.getStandardObjectInspector(rawOI,
ObjectInspectorCopyOption.DEFAULT);
row[i - 2] = ObjectInspectorUtils.copyToStandardObject(arg, argOI,
ObjectInspectorCopyOption.DEFAULT);
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "conv2dense",
value = "_FUNC_(int feature, float weight, int nDims) - Return a dense model in array<float>")
if (partial == null) {
if (other == null) {
if (partial == null) {
if (x != null) {
if (partial == null) {
@Description(name = "quantify",
value = "_FUNC_(boolean outout, col1, col2, ...) - Returns an identified features")
if (size < 2) {
if (HiveUtils.isNumberOI(argOI)) {
if (outputRow) {
if (identifier == null) {
if (arg == null) {
if (identifier != null) {
if (arg != null) {
@Description(name = "to_dense_features",
value = "_FUNC_(array<string> feature_vector, int dimensions)"
" - Returns a dense feature in array<float>")
if (features == null) {
public List<Float> evaluate(@Nullable final List<String> features,
@Nonnegative final int dimensions) throws HiveException {
if (features == null) {
for (String ft : features) {
if (i > dimensions) {
public List<String> evaluate(@Nullable final List<FloatWritable> features,
@Nullable String biasName) {
if (features == null) {
if (size == 0) {
if (o != null) {
if (biasName != null) {
@Description(name = "polynomial_features",
value = "_FUNC_(feature_vector in array<string>) - Returns a feature vector"
"having polynominal feature space")
public List<Text> evaluate(final List<Text> ftvec, final int degree,
final boolean interactionOnly) throws HiveException {
public List<Text> evaluate(final List<Text> ftvec, final int degree,
final boolean interactionOnly, final boolean truncate) throws HiveException {
if (ftvec == null) {
if (degree < 2) {
if (origSize == 0) {
if (t == null) {
if (truncate == false || (v != 0.f && v != 1.f)) {
private static void addPolynomialFeature(final String baseF, final float baseV,
final int currentDegree, final int degree, final List<FeatureValue> srcVec,
final int currentSrcPos, final List<Text> dstVec, final boolean interactionOnly,
final boolean truncate) {
if (interactionOnly && i == currentSrcPos) {
if (truncate && (v == 0.f || v == 1.f)) {
if (currentDegree < degree && i <= lastSrcIndex) {
interactionOnly, truncate);
@Description(name = "powered_features",
value = "_FUNC_(feature_vector in array<string>, int degree [, boolean truncate])"
" - Returns a feature vector having a powered feature space")
if (ftvec == null) {
if (degree < 2) {
if (origSize == 0) {
if (t == null) {
if (truncate && (v == 0.f || v == 1.f)) {
@Description(name = "l2_normalize", value = "_FUNC_(ftvec string) - Returned a L2 normalized value")
if (ftvecs == null) {
if (ftvec == null) {
if (ftlen == 1) {
} else if (ftlen == 2) {
if (norm == 0.f) {
@Description(name = "rescale",
value = "_FUNC_(value, min, max) - Returns rescaled value by min-max normalization")
if (fv.length != 2) {
if (fv.length != 2) {
if (min == max) {
private static float min_max_normalization(final double value, final double min,
final double max) {
if (min == max) {
@Description(name = "zscore",
value = "_FUNC_(value, mean, stddev) - Returns a standard score (zscore)")
if (stddev == 0.d) {
if (stddev == 0.f) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "tf",
value = "_FUNC_(string text) - Return a term frequency in <string, float>")
if (term == null) {
if (partial == null) {
if (count == null) {
if (other == null) {
if (partial == null) {
for (Map.Entry<Text, MutableInt> e : other_map.entrySet()) {
if (this_count == null) {
if (partial == null) {
for (Map.Entry<Text, MutableInt> e : partial.map.entrySet()) {
@Description(name = "categorical_features",
value = "_FUNC_(array<string> featureNames, ...) - Returns a feature vector array<string>")
if (numArgOIs < 2) {
if (featureNames == null) {
if (numFeatureNames < 1) {
if (numFeatureNames != numFeatures) {
if (argument == null) {
if (s.isEmpty()) {
@Description(
name = "indexed_features",
value = "_FUNC_(double v1, double v2, ...) - Returns a list of features as array<string>: [1:v1, 2:v2, ..]")
if (numArgs < 1) {
throw new UDFArgumentLengthException(
if (list == null) {
if (o == null) {
if (s1.isEmpty()) {
@Description(
name = "quantified_features",
value = "_FUNC_(boolean output, col1, col2, ...) - Returns an identified features in a dence array<double>")
if (size < 2) {
if (HiveUtils.isNumberOI(argOI)) {
if (fowardObjs == null) {
this.fowardObjs = new Object[] {Arrays.asList(columnValues)};
if (outputRow) {
if (identifier == null) {
if (arg == null) {
if (identifier != null) {
if (arg != null) {
@Description(name = "quantitative_features",
value = "_FUNC_(array<string> featureNames, ...) - Returns a feature vector array<string>")
if (numArgOIs < 2) {
if (featureNames == null) {
if (numFeatureNames < 1) {
if (numFeatureNames != numFeatures) {
if (argument == null) {
if (oi.getPrimitiveCategory() == PrimitiveCategory.STRING) {
if (s.isEmpty()) {
}
}
if (v != 0.d) {
@Description(name = "vectorize_features",
value = "_FUNC_(array<string> featureNames, ...) - Returns a feature vector array<string>")
if (numArgOIs < 2) {
if (featureNames == null) {
if (numFeatureNames < 1) {
if (numFeatureNames != numFeatures) {
if (argument == null) {
if (oi.getPrimitiveCategory() == PrimitiveCategory.STRING) {
if (s.isEmpty()) {
}
if (v != 0.f) {
@Description(name = "angular_distance",
value = "_FUNC_(ftvec1, ftvec2) - Returns an angular distance of the given two vectors")
if (argOIs.length != 2) {
@Description(name = "cosine_distance",
value = "_FUNC_(ftvec1, ftvec2) - Returns a cosine distance of the given two vectors")
if (argOIs.length != 2) {
@Description(
name = "euclid_distance",
value = "_FUNC_(ftvec1, ftvec2) - Returns the square root of the sum of the squared differences"
": sqrt(sum((x - y)^2))")
if (argOIs.length != 2) {
for (String ft : ftvec1) {
if (ft == null) {
for (String ft : ftvec2) {
if (ft == null) {
if (v1 == null) {
for (Map.Entry<String, Float> e : map.entrySet()) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "hamming_distance",
value = "_FUNC_(A, B [,int k]) - Returns Hamming distance between A and B")
if (alen < blen) {
@Description(name = "jaccard_distance",
value = "_FUNC_(A, B [,int k]) - Returns Jaccard distance between A and B")
if (a == null && b == null) {
} else if (a == null || b == null) {
if (asize == 0 && bsize == 0) {
} else if (asize == 0 || bsize == 0) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "kld", value = "_FUNC_(double m1, double sigma1, double mu2, double sigma 2)"
" - Returns KL divergence between two distributions")
public static double kld(final double mu1, final double sigma1, final double mu2,
final double sigma2) {
if (argOIs.length != 2) {
for (String ft : ftvec1) {
if (ft == null) {
for (String ft : ftvec2) {
if (ft == null) {
if (v1 == null) {
for (Map.Entry<String, Float> e : map.entrySet()) {
@Description(name = "minkowski_distance",
value = "_FUNC_(list x, list y, double p) - Returns sum(|x - y|^p)^(1/p)")
if (argOIs.length != 3) {
public static double minkowskiDistance(final List<String> ftvec1, final List<String> ftvec2,
final double orderP) {
for (String ft : ftvec1) {
if (ft == null) {
for (String ft : ftvec2) {
if (ft == null) {
if (v1 == null) {
for (Map.Entry<String, Float> e : map.entrySet()) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "popcnt", value = "_FUNC_(a [, b]) - Returns a popcount value")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "minhash",
value = "_FUNC_(ANY item, array<int|bigint|string> features [, constant string options])"
" - Returns n differnce k-depth signatures (i.e., clusteid) for each item <clusteid, item>")
if (argOIs.length < 2) {
throw new UDFArgumentException(
"_FUNC_ takes more than 2 arguments: ANY item, Array<Int|BigInt|Text> features [, constant String options]");
if (!STRING_TYPE_NAME.equals(keyTypeName) && !INT_TYPE_NAME.equals(keyTypeName)
throw new UDFArgumentTypeException(0,
opts.addOption("n", "hashes", true,
"Generate N sets of minhash values for each row (DEFAULT: 5)");
if (argOIs.length >= 3) {
if (numHashes != null) {
if (numKeygroups != null) {
for (FeatureValue fv : features) {
if (hashValue < weightedMinHashValues) {
if (w < 0.f) {
if (w == 0.f) {
if (numCandidates == 0) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "minhashes",
value = "_FUNC_(array<> features [, int numHashes, int keyGroup [, boolean noWeight]])"
" - Returns minhash values")
if (seeds == null || seeds.length != numHashes) {
public List<IntWritable> evaluate(List<String> features, int numHashes, int keyGroups,
boolean noWeight) throws HiveException {
for (Integer f : features) {
if (f != null) {
private static List<FeatureValue> parseFeatures(final List<String> features,
final boolean noWeight) {
for (String f : features) {
if (f == null) {
if (noWeight) {
private static List<IntWritable> computeSignatures(final List<FeatureValue> features,
final int numHashes, final int keyGroups, final int[] seeds) throws HiveException {
for (FeatureValue fv : features) {
if (hashValue < weightedMinHashValues) {
if (w < 0.f) {
if (w == 0.f) {
if (numCandidates == 0) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(name = "bbit_minhash", value = "_FUNC_(array<> features [, int numHashes])"
" - Returns a b-bits minhash value")
if (seeds == null || seeds.length != numHashes) {
for (Integer f : features) {
if (f != null) {
private static List<FeatureValue> parseFeatures(final List<String> features,
final boolean noWeight) {
for (String f : features) {
if (f == null) {
if (noWeight) {
private static String computeSignatures(final List<FeatureValue> features, final int numHashes,
final int[] seeds) throws HiveException {
if (numHashes <= 0 || numHashes > 512) {
for (FeatureValue fv : features) {
if (hashValue < weightedMinHashValues) {
if ((hashes[i] & 1) == 1) {
if (w < 0.f) {
if (w == 0.f) {
@Description(name = "angular_similarity",
value = "_FUNC_(ftvec1, ftvec2) - Returns an angular similarity of the given two vectors")
if (argOIs.length != 2) {
@Description(name = "cosine_similarity",
value = "_FUNC_(ftvec1, ftvec2) - Returns a cosine similarity of the given two vectors")
if (argOIs.length != 2) {
if (ftvec1 == null || ftvec2 == null) {
for (String ft : ftvec1) {
for (String ft : ftvec2) {
if (v1 != null) {
if (denom <= 0.f) {
if (argOIs.length != 1) {
@Description(name = "euclid_similarity",
value = "_FUNC_(ftvec1, ftvec2) - Returns a euclid distance based similarity"
if (argOIs.length != 2) {
@Description(name = "jaccard_similarity",
value = "_FUNC_(A, B [,int k]) - Returns Jaccard similarity coefficient of A and B")
if (a == null && b == null) {
} else if (a == null || b == null) {
if (asize == 0 && bsize == 0) {
} else if (asize == 0 || bsize == 0) {
@Description(
name = "mf_predict",
value = "_FUNC_(List<Float> Pu, List<Float> Qi[, double Bu, double Bi[, double mu]]) - Returns the prediction value")
if (Pu == null || Qi == null) {
if (PuSize == 0) {
} else if (QiSize == 0) {
if (QiSize != PuSize) {
if (Pu == null && Qi == null) {
if (Pu == null) {
} else if (Qi == null) {
if (PuSize == 0) {
if (QiSize == 0) {
} else if (QiSize == 0) {
if (QiSize != PuSize) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_mf_adagrad",
value = "_FUNC_(INT user, INT item, FLOAT rating [, CONSTANT STRING options])"
" - Returns a relation consists of <int idx, array<float> Pu, array<float> Qi [, float Bu, float Bi [, float mu]]>")
opts.addOption("scale", true,
"Internal scaling/descaling factor for cumulative weights [100]");
if (cl == null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_mf_sgd",
value = "_FUNC_(INT user, INT item, FLOAT rating [, CONSTANT STRING options])"
" - Returns a relation consists of <int idx, array<float> Pu, array<float> Qi [, float Bu, float Bi [, float mu]]>")
public final class MatrixFactorizationSGDUDTF extends OnlineMatrixFactorizationUDTF {
opts.addOption("power_t", true,
"The exponent for inverse scaling learning rate [default 0.1]");
"_FUNC_ takes 3 arguments: INT user, INT item, FLOAT rating [, CONSTANT STRING options]");
public MixMessage(MixEventName event, Object feature, float weight, short clock,
int deltaUpdates) {
public MixMessage(MixEventName event, Object feature, float weight, float covariance,
short clock, int deltaUpdates) {
public MixMessage(MixEventName event, Object feature, float weight, float covariance,
int deltaUpdates, boolean cancelRequest) {
MixMessage(MixEventName event, Object feature, float weight, float covariance, short clock,
int deltaUpdates, boolean cancelRequest) {
if (feature == null) {
if (deltaUpdates < 0 || deltaUpdates > Byte.MAX_VALUE) {
switch (b) {
if (groupID == null) {
if (hasGroupID) {
if (frame == null) {
MixMessage msg = new MixMessage(event, feature, weight, covariance, clock, deltaUpdates,
cancelRequest);
switch (type) {
if (length == -1) {
}
if (obj instanceof Integer) {
} else if (obj instanceof Text) {
} else if (obj instanceof String) {
} else if (obj instanceof IntWritable) {
} else if (obj instanceof LongWritable) {
if (s == null) {
if (addr == null) {
if (obj == this) {
if (obj instanceof NodeInfo) {
public MixClient(@Nonnull MixEventName event, @CheckForNull String groupID,
@Nonnull String connectURIs, boolean ssl, int mixThreshold, @Nonnull MixedModel model) {
if (groupID == null) {
if (mixThreshold < 1 || mixThreshold > Byte.MAX_VALUE) {
for (NodeInfo node : serverNodes) {
if (ssl) {
if (deltaUpdates < mixThreshold) {
if (!initialized) {
if (groupID.startsWith(DUMMY_JOB_ID)) {
if (workers != null) {
for (Channel ch : channelMap.values()) {
if (model == null) {
if (msgHandler == null) {
if (sslCtx != null) {
if (connectInfo == null) {
if (numEndpoints < 1) {
InetSocketAddress addr = NetUtils.getInetSocketAddress(endpoints[i],
MixEnv.MIXSERV_DEFAULT_PORT);
if (cancelMixRequest) {
if (isDenseModel()) {
protected final void onUpdate(final int feature, final float weight, final float covar,
final short clock, final int deltaUpdates, final boolean hasCovar) {
if (handler != null) {
if (deltaUpdates < 1) {
if (requestSent) {
if (cancelMixRequest) {
if (hasCovar) {
if (prevMixed == null) {
if (prevMixed == null) {
if (handler != null) {
if (!value.isTouched()) {
if (value.hasCovariance()) {
if (requestSent) {
if (cancelMixRequest) {
if (prevMixed == null) {
if (requestSent) {
if (cancelMixRequest) {
if (prevMixed == null) {
if (hasCovariance()) {
if (withCovar) {
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x,
boolean sum_of_gradients) {
if (sum_of_squared_gradients) {
if (sum_of_squared_delta_x) {
if (sum_of_gradients) {
if (clocks == null) {
if (index >= size) {
if (covars != null) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (clocks != null) {
if (i >= size) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
return (T) new WeightValueParamsF2(weights[i], sum_of_squared_gradients[i],
sum_of_squared_delta_x[i]);
} else if (sum_of_gradients != null) {
return (T) new WeightValueParamsF2(weights[i], sum_of_squared_gradients[i],
sum_of_gradients[i]);
} else if (covars != null) {
if (hasCovar) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (clocks != null && value.isTouched()) {
if (i >= size) {
if (covars != null) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (i >= size) {
if (i >= size) {
if (i >= size) {
if (!hasNext()) {
if (covars == null) {
if (covars != null) {
}
boolean onUpdate(@Nonnull Object feature, float weight, float covar, short clock,
int deltaUpdates) throws Exception;
void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x,
boolean sum_of_gradients);
}
if (withCovar) {
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x,
boolean sum_of_gradients) {
if (sum_of_squared_gradients) {
if (sum_of_squared_delta_x) {
if (sum_of_gradients) {
if (clocks == null) {
if (index >= size) {
if (covars != null) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (clocks != null) {
if (i >= size) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
return (T) new WeightValueParamsF2(getWeight(i), sum_of_squared_gradients[i],
sum_of_squared_delta_x[i]);
} else if (sum_of_gradients != null) {
return (T) new WeightValueParamsF2(getWeight(i), sum_of_squared_gradients[i],
sum_of_gradients[i]);
} else if (covars != null) {
if (hasCovar) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (clocks != null && value.isTouched()) {
if (i >= size) {
if (covars != null) {
if (sum_of_squared_gradients != null) {
if (sum_of_squared_delta_x != null) {
if (sum_of_gradients != null) {
if (i >= size) {
if (i >= size) {
if (i >= size) {
if (!hasNext()) {
if (covars == null) {
if (covars != null) {
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x,
boolean sum_of_gradients) {}
if (clockEnabled && value.isTouched()) {
if (old != null) {
if (clockEnabled) {
switch (value.getType()) {
if (w == null) {
protected void _set(final Object feature, final float weight, final float covar,
final short clock) {
if (w == null) {
public void configureParams(boolean sum_of_squared_gradients, boolean sum_of_squared_delta_x,
boolean sum_of_gradients) {
if (i == 1) {
if (i == 1) {
} else if (i == 2) {
if (src.isTouched()) {
throw new UnsupportedOperationException(
"WeightValueWithClock#setTouched should not be called");
if (deltaUpdates < 0) {
if (i == 1) {
if (i == 1) {
} else if (i == 2) {
}
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_arow_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight, float covar>")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes arguments: List<Int|BigInt|Text> features, float target [, constant string options]");
opts.addOption("r", "regularization", true,
"Regularization parameter for some r > 0 [default 0.1]");
if (cl != null) {
if (r_str != null) {
if (!(r > 0)) {
throw new UDFArgumentException(
protected void update(@Nonnull final FeatureValue[] features, final float coeff,
final float beta) {
for (FeatureValue f : features) {
if (f == null) {
private static IWeightValue getNewWeight(final IWeightValue old, final float x,
final float coeff, final float beta) {
if (old == null) {
@Description(
name = "train_arowe_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight, float covar>")
if (cl != null) {
if (opt_epsilon != null) {
if (loss > 0.f) {
@Description(
name = "train_arowe2_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight, float covar>")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_adadelta_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"AdaDeltaUDTF takes 2 or 3 arguments: List<Text|Int|BitInt> features, float target [, constant string options]");
opts.addOption("scale", true,
"Internal scaling/descaling factor for cumulative weights [100]");
if (cl == null) {
if (target < 0.f || target > 1.f) {
if (f == null) {
protected IWeightValue getNewWeight(@Nullable final IWeightValue old, final float xi,
final float gradient, final float g_g) {
if (old != null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_adagrad_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes 2 or 3 arguments: List<Text|Int|BitInt> features, float target [, constant string options]");
opts.addOption("scale", true,
"Internal scaling/descaling factor for cumulative weights [100]");
if (cl == null) {
if (target < 0.f || target > 1.f) {
if (f == null) {
protected IWeightValue getNewWeight(@Nullable final IWeightValue old, final float xi,
final float gradient, final float g_g) {
if (old != null) {
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "logress",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
import org.apache.hadoop.hive.ql.exec.Description;
@Description(
name = "train_pa1_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
if (numArgs != 2 && numArgs != 3) {
throw new UDFArgumentException(
"_FUNC_ takes arguments: List<Int|BigInt|Text> features, float target [, constant string options]");
opts.addOption("c", "aggressiveness", true,
"Aggressiveness paramete [default Float.MAX_VALUE]");
if (cl != null) {
if (opt_c != null) {
if (!(c > 0.f)) {
if (opt_epsilon != null) {
if (loss > 0.f) {
if (!Float.isInfinite(coeff)) {
@Description(
name = "train_pa1a_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
public static final class PA1a extends PassiveAggressiveRegressionUDTF {
@Description(
name = "train_pa2_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
@Description(
name = "train_pa2a_regr",
value = "_FUNC_(array<int|bigint|string> features, float target [, constant string options])"
" - Returns a relation consists of <{int|bigint|string} feature, float weight>")
public static final class PA2a extends PA2 {
"_FUNC_ takes 2 arguments: List<Int|BigInt|Text> features, float target [, constant string options]");
if (HiveUtils.isNumberOI(argOIs[i])) {
if (i != last) {
return ObjectInspectorUtils.getConstantObjectInspector(
PrimitiveObjectInspectorFactory.javaStringObjectInspector, value);
if (mapredContext != null) {
if (conf != null) {
if (tdJarVersion == null) {
if (threads > 1) {
if (exec == null) {
for (Callable<T> task : tasks) {
for (Future<T> future : futures) {
if (exec != null) {
for (String line : opslist) {
if (ops.length == 2) {
while (IP < code.size()) {
if (done[IP]) {
if (!executeOperation(currentOperation)) {
if (IP < 0) {
if (StringUtils.isInt(currentOperation.operand)) {
if (candidateIP < 0) {
if (a == b) {
if (StringUtils.isInt(currentOperation.operand)) {
if (smile.math.Math.equals(a, b)) {
if (StringUtils.isInt(currentOperation.operand)) {
if (upper >= lower) {
if (StringUtils.isInt(currentOperation.operand)) {
if (upper > lower) {
if (StringUtils.isInt(currentOperation.operand)) {
if (upper <= lower) {
if (StringUtils.isInt(currentOperation.operand)) {
if (upper < lower) {
if (StringUtils.isInt(currentOperation.operand)) {
if (StringUtils.isDouble(currentOperation.operand)) {
if (v == null) {
if (name.equals("end")) {
@Description(
name = "convert_label",
value = "_FUNC_(const int|const float) - Convert from -1|1 to 0.0f|1.0f, or from 0.0f|1.0f to -1|1")
if (label == 0) {
} else if (label == -1) {
} else if (label == 1) {
if (label == 0.f) {
} else if (label == -1.f) {
} else if (label == 1.f) {
if (label == 0.d) {
} else if (label == -1.d) {
} else if (label == 1.d) {
@Description(name = "generate_series",
value = "_FUNC_(const int|bigint start, const int|bigint end) - "
"Generate a series of values, from start to end")
if (argOIs.length != 2) {
if (useBigInt) {
if (start > end) {
if (useBigInt) {
if (start == end) {
if (starti == endi) {
@Description(name = "x_rank",
value = "_FUNC_(KEY) - Generates a pseudo sequence number starting from 1 for each key")
if (key == null) {
if (lastNull) {
if (key.equals(lastKey)) {
@Description(name = "float_array",
value = "_FUNC_(nDims) - Returns an array<float> of nDims elements")
@Description(name = "collect_all",
value = "_FUNC_(x) - Retrurns a set of objects with duplicate elements eliminated")
if (tis.length != 1) {
if (m == Mode.PARTIAL1) {
if (!(parameters[0] instanceof StandardListObjectInspector)) {
if (p != null) {
for (Object i : partial) {
@Description(name = "bits_collect", value = "_FUNC_(int|long x) - Returns a bitset in array<long>")
if (arguments.length != 2) {
throw new UDFArgumentLengthException(
"map_tail_n only takes 2 arguments: map<object, object>, int");
if (!(arguments[0] instanceof MapObjectInspector)) {
if (!(arguments[1] instanceof IntObjectInspector)) {
for (Map.Entry<?, ?> e : m.entrySet()) {
if (tail.size() <= n) {
@Description(
name = "distcache_gets",
value = "_FUNC_(filepath, key, default_value [, parseKey]) - Returns map<key_type, value_type>|value_type")
if (argOIs.length != 3 && argOIs.length != 4) {
throw new UDFArgumentException(
"Invalid number of arguments for distcache_gets(FILEPATH, KEYS, DEFAULT_VAL, PARSE_KEY): "
if (!ObjectInspectorUtils.isConstantObjectInspector(argOIs[2])) {
throw new UDFArgumentException(
"Third argument DEFAULT_VALUE must be a constant value: "
TypeInfoUtils.getTypeInfoFromObjectInspector(argOIs[2]));
if (argOIs.length == 4) {
if (multipleDefaultValues) {
ObjectInspector valueOutputOI = ObjectInspectorUtils.getStandardObjectInspector(
valueInputOI, ObjectInspectorCopyOption.WRITABLE);
outputOI = ObjectInspectorFactory.getStandardMapObjectInspector(keyInputOI,
valueOutputOI);
if (parseKey && !HiveUtils.isStringOI(keyInputOI)) {
throw new UDFArgumentException(
"parseKey=true is only available for string typed key(s)");
private static void loadValues(OpenHashMap<Object, Object> map, File file,
PrimitiveObjectInspector keyOI, PrimitiveObjectInspector valueOI) throws IOException,
SerDeException {
if (!file.exists()) {
if (!file.getName().endsWith(".crc")) {
if (file.isDirectory()) {
for (File f : file.listFiles()) {
while ((line = reader.readLine()) != null) {
if (multipleKeyLookup) {
if (multipleDefaultValues) {
for (Object k : keys) {
if (k == null) {
if (v == null) {
if (numKeys != defaultValues.size()) {
if (k == null) {
if (v == null) {
if (v != null) {
if (parseKey) {
if (ctx == null) {
if (jobconf == null) {
@Description(name = "rowid",
value = "_FUNC_() - Returns a generated row id of a form {TASK_ID}-{SEQUENCE_NUMBER}")
if (taskId == -1) {
if (argOIs.length != 1) {
if (obj0 == null) {
} else if ("NFC".equals(form)) {
@Description(
name = "split_words",
value = "_FUNC_(string query [, string regex]) - Returns an array<text> containing splitted strings")
if (query == null) {
@Description(name = "is_stopword",
value = "_FUNC_(string word) - Returns whether English stopword or not")
stopwords = new String[] {"i", "me", "my", "myself", "we", "our", "ours", "ourselves",
"s", "t", "can", "will", "just", "don", "should", "now"};
if (size == 0) {
if (t == null) {
if (size < 1) {
if (comparator == null) {
if (e == null) {
if (numElem >= maxSize) {
if (cmp < 0) {
if (used >= data.length) {
if (needs >= data.length) {
while (data.length < max) {
if (index > used) {
} else if (index == used) {
if (index > used) {
} else if (index == used) {
if (index >= used)
if (i != 0) {
}
if (actualSize < expectedSize) {
if (v == null) {
if (_map.containsKey(e)) {
if (size > 0) {
protected Int2FloatOpenHashTable(int size, float loadFactor, float growFactor,
boolean forcePrime) {
if (size < 1) {
if (i < 0) {
if (expanded) {
if (keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
if (stat == FREE) {
if (stat == REMOVED && _keys[index] == key) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
while (i.next() != -1) {
if (i.hasNext()) {
if (newCapacity <= oldCapacity) {
if (_states[i] == FULL) {
while (newStates[keyIdx] != FREE) {
if (keyIdx < 0) {
while (i.next() != -1) {
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
while (index < _keys.length && _states[index] != FULL) {
if (!hasNext()) {
if (lastEntry == -1) {
if (lastEntry == -1) {
public Int2LongOpenHashTable(@Nonnull int[] keys, @Nonnull long[] values,
@Nonnull byte[] states, int used) {
if (size < 1) {
if (i < 0) {
if (expanded) {
if (keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
if (expanded) {
if (keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
if (stat == FREE) {
if (stat == REMOVED && _keys[index] == key) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (isFree(keyIdx, key)) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
if (states[keyIdx] != FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
if (states[keyIdx] == FULL && keys[keyIdx] == key) {
while (i.next() != -1) {
if (i.hasNext()) {
if (newCapacity <= oldCapacity) {
if (oldStates[i] == FULL) {
while (newStates[keyIdx] != FREE) {
if (keyIdx < 0) {
while (i.next() != -1) {
for (;;) {
if (keyIdx < 0) {
if (states[keyIdx] == FREE) {
while (index < _keys.length && _states[index] != FULL) {
if (!hasNext()) {
if (lastEntry == -1) {
if (lastEntry == -1) {
if (key == null) {
for (;;) {
if (searchKey == null) {
} else if (compare(searchKey, key)) {
if (keys[off] != null && compare(keys[off], key)) {
if (keys[off] != null && compare(keys[off], key)) {
for (K key : m.keySet()) {
for (V v : values) {
if (v != null && compare(v, value)) {
for (K key : keys) {
if (key != null) {
for (V value : values) {
if (value != null) {
for (K key : keys) {
if (key != null) {
if (keys[x] != null) {
if (bitSize != bits) {
if (existingKeys != null) {
if (existingKeys[x] != null) {
while (index < keys.length && keys[index] == null) {
if (!hasNext()) {
if (index >= 0) {
}
public static ThreadPoolExecutor newFixedThreadPool(int nThreads, String threadName,
boolean daemon) {
return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS,
new LinkedBlockingQueue<Runnable>(), new NamedThreadFactory(threadName, daemon));
if (threadName == null) {
if (threadName == null) {
if (threadGroup == null) {
if (t.isDaemon() != daemon) {
if (t.getPriority() != threadPriority) {
if (t == 0L) {
if (hour > 0) {
if (min > 0) {
if (sec > 0) {
if (t > 0) {
if (timeInMills == 0d) {
if (hour > 0) {
if (min > 0) {
if (sec > 0) {
if (t > 0 || diff > 0f) {
public static int getTaskId(final int defaultValue) {
MapredContext ctx = MapredContextAccessor.get();
if (ctx == null) {
return defaultValue;
}
JobConf jobconf = ctx.getJobConf();
if (jobconf == null) {
return defaultValue;
}
int taskid = jobconf.getInt("mapred.task.partition", -1);
if (taskid == -1) {
taskid = jobconf.getInt("mapreduce.task.partition", -1);
if (taskid == -1) {
return defaultValue;
}
}
return taskid;
}
package hivemall.utils.io;
import java.io.IOException;
import java.io.InputStream;
public final class FastByteArrayInputStream extends InputStream {
protected int pos = 0;
public FastByteArrayInputStream() {}
public FastByteArrayInputStream(byte[] buf) {
this(buf, buf.length);
}
public FastByteArrayInputStream(byte[] buf, int count) {
this.buf = buf;
this.count = count;
}
public FastByteArrayInputStream(byte buf[], int offset, int length) {
this.buf = buf;
this.pos = offset;
}
public void init(byte[] buf) {
init(buf, buf.length);
}
public void init(byte[] buf, int count) {
this.buf = buf;
this.count = count;
this.pos = 0;
}
@Override
public final int available() {
return count - pos;
}
public final int read() {
}
@Override
public final int read(byte[] b, int off, int len) {
if (pos >= count) {
return -1;
}
len = (count - pos);
}
if (len <= 0) {
return 0;
}
System.arraycopy(buf, pos, b, off, len);
return len;
}
@Override
public final long skip(long n) {
n = count - pos;
}
if (n < 0) {
return 0;
}
return n;
}
@Override
public boolean markSupported() {
return false;
}
@Override
public void mark(int readlimit) {
throw new UnsupportedOperationException();
}
@Override
public void close() throws IOException {
this.buf = null;
}
}
package hivemall.utils.io;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.io.UnsupportedEncodingException;
import java.util.LinkedList;
public final class FastMultiByteArrayOutputStream extends OutputStream {
private static final int DEFAULT_BLOCK_SIZE = 8192;
private final int blockSize;
private byte[] buffer;
private int size;
private LinkedList<byte[]> buffers;
private int index;
public FastMultiByteArrayOutputStream() {
this(DEFAULT_BLOCK_SIZE);
}
public FastMultiByteArrayOutputStream(int aSize) {
this.blockSize = aSize;
this.buffer = new byte[aSize];
}
public int size() {
}
public byte[][] toMultiByteArray() {
if (buffers == null) {
final byte[][] mb = new byte[1][];
mb[0] = buffer;
return mb;
}
final int listsize = buffers.size();
final byte[][] mb = new byte[size][];
buffers.toArray(mb);
mb[listsize] = buffer;
return mb;
}
public byte[] toByteArray() {
final byte[] data = new byte[size()];
int pos = 0;
if (buffers != null) {
for (byte[] bytes : buffers) {
System.arraycopy(bytes, 0, data, pos, blockSize);
}
}
System.arraycopy(buffer, 0, data, pos, index);
return data;
}
public byte[] toByteArray_clear() {
final int size = size();
final byte[] data = new byte[size];
int pos = 0;
if (buffers != null) {
while (!buffers.isEmpty()) {
byte[] bytes = buffers.removeFirst();
System.arraycopy(bytes, 0, data, pos, blockSize);
}
}
System.arraycopy(buffer, 0, data, pos, index);
return data;
}
@Override
public String toString() {
return new String(toByteArray());
}
public String toString(String enc) throws UnsupportedEncodingException {
return new String(toByteArray(), enc);
}
@Override
public void write(int datum) {
if (index >= blockSize) {
addBuffer();
}
}
@Override
public void write(byte[] data, int offset, int length) {
if (data == null) {
throw new NullPointerException();
throw new IndexOutOfBoundsException();
} else {
int copyLength;
do {
if (index == blockSize) {
addBuffer();
}
copyLength = blockSize - index;
if (length < copyLength) {
copyLength = length;
}
System.arraycopy(data, offset, buffer, index, copyLength);
length -= copyLength;
} while (length > 0);
System.arraycopy(data, offset, buffer, index, length);
}
}
}
public void writeInt(int v) throws IOException {
addBuffer();
}
}
public void writeLong(long v) throws IOException {
addBuffer();
}
}
public void writeInts(int[] v, int off, int len) throws IOException {
writeInt(v[i]);
}
}
public void writeLongs(long[] v, int off, int len) throws IOException {
writeLong(v[i]);
}
}
public void reset() {
buffer = new byte[blockSize];
buffers = null;
size = 0;
index = 0;
}
public void writeTo(OutputStream out) throws IOException {
if (buffers != null) {
for (byte[] bytes : buffers) {
out.write(bytes, 0, blockSize);
}
}
out.write(buffer, 0, index);
}
public InputStream getInputStream() {
return new FastByteArrayInputStream(toByteArray());
}
private void addBuffer() {
if (buffers == null) {
buffers = new LinkedList<byte[]>();
}
buffers.addLast(buffer);
buffer = new byte[blockSize];
index = 0;
}
@Override
public void close() throws IOException {
clear();
}
public void clear() {
this.buffer = null;
this.buffers = null;
}
}
if (!file.exists()) {
if (file.isDirectory()) {
if (files != null && files.length > 0) {
for (File f : files) {
}
if (count == null) {
if (count == null) {
if (counter == null) {
for (Map.Entry<E, Integer> e : counter.entrySet()) {
if (counter == null) {
for (Map.Entry<E, Integer> e : counter.entrySet()) {
for (Map.Entry<E, Integer> e : counts.entrySet()) {
if (v >= maxValue) {
for (Map.Entry<E, Integer> e : counts.entrySet()) {
if (v <= minValue) {
if (count == null) {
try {
public final class MutableDouble extends Number implements Copyable<MutableDouble>,
Comparable<MutableDouble>, Serializable {
public final class MutableFloat extends Number implements Copyable<MutableFloat>,
Comparable<MutableFloat>, Serializable {
public final class MutableInt extends Number implements Copyable<MutableInt>,
Comparable<MutableInt>, Serializable {
if (obj instanceof MutableInt) {
public final class MutableLong extends Number implements Copyable<MutableLong>,
Comparable<MutableLong>, Serializable {
if (obj instanceof MutableLong) {
while (true) {
if (state.get()) {
private static final int[] PRIMES = {3, 5, 7, 13, 19, 31, 43, 61, 73, 89, 103, 109, 139, 151,
Integer.MAX_VALUE};
if (idx < 0) {
if (idx >= PRIMES.length) {
if (p < 0 || p > 1) {
if (range <= 0) {
if (p == 0) {
if (p == 1) {
if (v < 0) {
if (pos == -1) {
if (basePort == 0) {
if (basePort < 0 || basePort > 65535) {
if (isPortAvailable(i)) {
if (s != null) {
final ObjectName name = makeMBeanName("hivemall",
final ObjectName name = makeMBeanName("hivemall",
private static ObjectName makeMBeanName(@Nonnull final String domain,
@Nonnull final String type, @Nonnull final String channelName) {
private static String makeMBeanNameString(@Nonnull final String domain,
@Nonnull final String type, @Nonnull final String channelName) {
public ThroughputCounter(@Nonnull ScheduledExecutorService executor, long checkInterval,
@Nonnull MixServerMetrics metrics) {
if (interval == 0) {
if (logger.isInfoEnabled()) {
if (lastReads > 0 || lastWrites > 0) {
opts.addOption("workers", "num_workers", true,
"The number of MIX workers [default: max(1, round(procs * 1.5))] ");
opts.addOption("scale", "scalemodel", true,
"Scale values of prediction models to avoid overflow [default: 1.0 (no-scale)]");
opts.addOption("sync", "sync_threshold", true,
"Synchronization threshold using clock difference [default: 30]");
opts.addOption("ttl", "session_ttl", true,
"The TTL in sec that an idle session lives [default: 120 sec]");
opts.addOption("sweep", "session_sweep_interval", true,
"The interval in sec that the session expiry thread runs [default: 60 sec]");
opts.addOption("jmx", "metrics", false,
"Toggle this option to enable monitoring metrics using JMX [default: false]");
if (ssl) {
MixServerInitializer initializer = new MixServerInitializer(msgHandler, throughputCounter,
sslCtx);
sweepIntervalInSec, TimeUnit.SECONDS);
if (jmx) {
private void acceptConnections(@Nonnull MixServerInitializer initializer, int port,
@Nonnegative int numWorkers) throws InterruptedException {
public MixServerHandler(@Nonnull SessionStore sessionStore, @Nonnegative int syncThreshold,
@Nonnegative float scale) {
if (groupId == null) {
if (groupID == null) {
if (partial == null) {
if (existing != null) {
private void mix(final ChannelHandlerContext ctx, final MixMessage requestMsg,
final PartialResult partial, final SessionObject session) {
if (deltaUpdates <= 0) {
if (cancelRequest) {
responseMsg = new MixMessage(event, feature, averagedWeight, meanCovar,
if (responseMsg != null) {
public MixServerInitializer(@Nonnull MixServerHandler msgHandler,
@Nullable ThroughputCounter throughputCounter, @Nullable SslContext sslCtx) {
if (sslCtx != null) {
if (throughputCounter != null) {
public abstract void add(float localWeight, float covar, @Nonnegative int deltaUpdates,
float scale);
public abstract void subtract(float localWeight, float covar, @Nonnegative int deltaUpdates,
float scale);
if (obj == null) {
if (sessionObj == null) {
ConcurrentMap<Object, PartialResult> map = new ConcurrentHashMap<Object, PartialResult>(
EXPECTED_MODEL_SIZE);
if (existing != null) {
if (removedSession != null) {
while (itor.hasNext()) {
if (elapsedTime > ttl) {
if (logger.isInfoEnabled()) {
@Description(
name = "tokenize_ja",
value = "_FUNC_(String line [, const string mode = \"normal\", const list<string> stopWords, const list<string> stopTags])"
" - returns tokenized strings in array<string>")
if (arglen < 1 || arglen > 4) {
if (analyzer == null) {
if (arg0 == null) {
if (stream != null) {
if (arg == null) {
if ("NORMAL".equalsIgnoreCase(arg)) {
} else if ("SEARCH".equalsIgnoreCase(arg)) {
} else if ("EXTENDED".equalsIgnoreCase(arg)) {
} else if ("DEFAULT".equalsIgnoreCase(arg)) {
throw new UDFArgumentException(
if (array == null) {
if (array.length == 0) {
if (array == null) {
if (length == 0) {
if (s != null) {
while (stream.incrementToken()) {
@Description(name = "lr_datagen",
value = "_FUNC_(options string) - Generates a logistic regression dataset")
protected CommandLine processOptions(ObjectInspector[] objectInspectors)
throws UDFArgumentException {
@Description(name = "add_bias",
value = "_FUNC_(features in array<string>) - Returns features with a bias as array<string>")
if (arguments.length != 1) {
switch (arguments[0].getCategory()) {
if (elmOI.getCategory().equals(Category.PRIMITIVE)) {
if (((PrimitiveObjectInspector) elmOI).getPrimitiveCategory() == PrimitiveCategory.STRING) {
return ObjectInspectorFactory.getStandardListObjectInspector(argumentOI.getListElementObjectInspector());
assert (arguments.length == 1);
name = "add_feature_index",
value = "_FUNC_(dense features in array<double>) - Returns a feature vector with feature indicies")
if (arguments.length != 1) {
switch (arguments[0].getCategory()) {
if (elmOI.getCategory().equals(Category.PRIMITIVE)) {
if (((PrimitiveObjectInspector) elmOI).getPrimitiveCategory() == PrimitiveCategory.DOUBLE) {
return ObjectInspectorFactory.getStandardListObjectInspector(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
assert (arguments.length == 1);
}
@Description(name = "extract_feature",
value = "_FUNC_(feature in string) - Returns a parsed feature as string")
if (arguments.length != 1) {
assert (arguments.length == 1);
@Description(name = "extract_weight",
value = "_FUNC_(feature in string) - Returns the weight of a feature as string")
if (arguments.length != 1) {
return PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(PrimitiveCategory.DOUBLE);
assert (arguments.length == 1);
@Description(name = "sort_by_feature",
value = "_FUNC_(map in map<int,float>) - Returns a sorted map")
if (arguments.length != 1) {
switch (arguments[0].getCategory()) {
if (keyOI.getCategory().equals(Category.PRIMITIVE)
argumentOI.getMapKeyObjectInspector(), argumentOI.getMapValueObjectInspector());
assert (arguments.length == 1);
final Map<IntWritable, FloatWritable> input = (Map<IntWritable, FloatWritable>) argumentOI.getMap(arguments[0].get());
if (arguments.length != 1) {
throw new UDFArgumentLengthException("normalize() has an only single argument.");
switch (arguments[0].getCategory()) {
if (elmOI.getCategory().equals(Category.PRIMITIVE)) {
if (((PrimitiveObjectInspector) elmOI).getPrimitiveCategory() == PrimitiveCategory.STRING) {
ObjectInspector outputElemOI = ObjectInspectorFactory.getReflectionObjectInspector(
Text.class, ObjectInspectorOptions.JAVA);
ObjectInspector outputOI = ObjectInspectorFactory.getStandardListObjectInspector(outputElemOI);
assert (arguments.length == 1);
name = "minhashes",
value = "_FUNC_(features in array<string>, noWeight in boolean) - Returns hashed features as array<int>")
if (arguments.length != 2) {
switch (arguments[0].getCategory()) {
if (elmOI.getCategory().equals(Category.PRIMITIVE)) {
if (((PrimitiveObjectInspector) elmOI).getPrimitiveCategory() == PrimitiveCategory.STRING) {
return ObjectInspectorFactory.getStandardListObjectInspector(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(PrimitiveCategory.INT));
assert (arguments.length == 2);
final Boolean noWeight = PrimitiveObjectInspectorUtils.getBoolean(arguments[1].get(),
noWeightOI);
name = "rowid",
value = "_FUNC_() - Returns a generated row id of a form {TASK_ID}-{UUID}-{SEQUENCE_NUMBER}")
if (arguments.length != 0) {
assert (arguments.length == 0);
HiveUtils.validateFeatureOI(featureRawOI);
this.parseFeature = HiveUtils.isStringOI(featureRawOI);
HiveUtils.validateFeatureOI(_xOI.getListElementObjectInspector());
HiveUtils.validateFeatureOI(featureRawOI);
this.parseFeature = HiveUtils.isStringOI(featureRawOI);
public static void validateFeatureOI(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
final String typeName = oi.getTypeName();
if (!STRING_TYPE_NAME.equals(typeName) && !INT_TYPE_NAME.equals(typeName)
&& !BIGINT_TYPE_NAME.equals(typeName)) {
throw new UDFArgumentException(
"argument type for a feature must be List of key type [Int|BitInt|Text]: "
typeName);
}
}
this._entrySize = entrySize(_factor, _useFTRL, _useAdaGrad);
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
public final class FeatureUDF extends GenericUDF {
private PrimitiveObjectInspector.PrimitiveCategory featureCategory;
private PrimitiveObjectInspector.PrimitiveCategory weightCategory;
private ObjectInspectorConverters.Converter converter;
private ObjectInspector featureOI;
private ObjectInspector weightOI;
@Override
public ObjectInspector initialize(ObjectInspector[] objectInspectors)
throws UDFArgumentException {
if (objectInspectors.length != 2) {
throw new UDFArgumentException("_FUNC_ takes exactly 2 arguments, features label and weight");
}
if (!(objectInspectors[0] instanceof PrimitiveObjectInspector)) {
throw new UDFArgumentException("Expected numeric type or string but got "
objectInspectors[0].getTypeName());
}
if (!(objectInspectors[1] instanceof PrimitiveObjectInspector)) {
throw new UDFArgumentException("Expected numeric type but got "
objectInspectors[1].getTypeName());
}
featureCategory = parsePrimitiveCategory(objectInspectors[0], false);
featureOI = objectInspectors[0];
weightCategory = parsePrimitiveCategory(objectInspectors[1], true);
weightOI = objectInspectors[1];
converter = ObjectInspectorConverters.getConverter(
PrimitiveObjectInspectorFactory.javaStringObjectInspector,
PrimitiveObjectInspectorFactory.writableStringObjectInspector);
return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
private PrimitiveObjectInspector.PrimitiveCategory parsePrimitiveCategory(ObjectInspector oi, boolean isWeight)
throws UDFArgumentException {
if (oi instanceof IntObjectInspector) {
return PrimitiveObjectInspector.PrimitiveCategory.INT;
} else if (oi instanceof LongObjectInspector) {
return PrimitiveObjectInspector.PrimitiveCategory.LONG;
} else if (oi instanceof FloatObjectInspector) {
return PrimitiveObjectInspector.PrimitiveCategory.FLOAT;
} else if (oi instanceof DoubleObjectInspector) {
return PrimitiveObjectInspector.PrimitiveCategory.DOUBLE;
} else if (oi instanceof StringObjectInspector && !isWeight) {
return PrimitiveObjectInspector.PrimitiveCategory.STRING;
} else {
throw new UDFArgumentException("Expected numeric or string type but got "
oi.getTypeName());
}
private String parseAsString(PrimitiveObjectInspector.PrimitiveCategory category, Object value) {
if (category == PrimitiveObjectInspector.PrimitiveCategory.INT) {
return ((Integer)value).toString();
} else if (category == PrimitiveObjectInspector.PrimitiveCategory.LONG) {
return ((Long)value).toString();
} else if (category == PrimitiveObjectInspector.PrimitiveCategory.FLOAT) {
return ((Float)value).toString();
} else if (category == PrimitiveObjectInspector.PrimitiveCategory.DOUBLE) {
return ((Double)value).toString();
} else if (category == PrimitiveObjectInspector.PrimitiveCategory.STRING) {
return ((String)value);
} else {
@Override
public Object evaluate(DeferredObject[] args) throws HiveException {
if (args.length != 2) {
if (args[0].get() == null || args[1].get() == null) {
return null;
}
if (featureOI == null || weightOI == null) {
return new HiveException("Invalid ObjectInspector");
}
Object feature = ((PrimitiveObjectInspector)featureOI)
.getPrimitiveJavaObject(args[0].get());
Object weight = ((PrimitiveObjectInspector)weightOI)
.getPrimitiveJavaObject(args[1].get());
String featureStr = parseAsString(featureCategory, feature);
String weightStr = parseAsString(weightCategory, weight);
@Override
public String getDisplayString(String[] strings) {
positiveObjs[positiveObjs.length - 1] = 1;
negativeObjs[negativeObjs.length - 1] = 0;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
public final class FeatureUDF extends GenericUDF {
private PrimitiveObjectInspector.PrimitiveCategory featureCategory;
private PrimitiveObjectInspector.PrimitiveCategory weightCategory;
private ObjectInspectorConverters.Converter converter;
private ObjectInspector featureOI;
private ObjectInspector weightOI;
@Override
public ObjectInspector initialize(ObjectInspector[] objectInspectors)
throws UDFArgumentException {
if (objectInspectors.length != 2) {
throw new UDFArgumentException("_FUNC_ takes exactly 2 arguments, features label and weight");
}
if (!(objectInspectors[0] instanceof PrimitiveObjectInspector)) {
throw new UDFArgumentException("Expected numeric type or string but got "
objectInspectors[0].getTypeName());
}
if (!(objectInspectors[1] instanceof PrimitiveObjectInspector)) {
throw new UDFArgumentException("Expected numeric type but got "
objectInspectors[1].getTypeName());
}
featureCategory = parsePrimitiveCategory(objectInspectors[0], false);
featureOI = objectInspectors[0];
weightCategory = parsePrimitiveCategory(objectInspectors[1], true);
weightOI = objectInspectors[1];
converter = ObjectInspectorConverters.getConverter(
PrimitiveObjectInspectorFactory.javaStringObjectInspector,
PrimitiveObjectInspectorFactory.writableStringObjectInspector);
return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
private PrimitiveObjectInspector.PrimitiveCategory parsePrimitiveCategory(ObjectInspector oi, boolean isWeight)
throws UDFArgumentException {
if (oi instanceof IntObjectInspector) {
return PrimitiveObjectInspector.PrimitiveCategory.INT;
} else if (oi instanceof LongObjectInspector) {
return PrimitiveObjectInspector.PrimitiveCategory.LONG;
} else if (oi instanceof FloatObjectInspector) {
return PrimitiveObjectInspector.PrimitiveCategory.FLOAT;
} else if (oi instanceof DoubleObjectInspector) {
return PrimitiveObjectInspector.PrimitiveCategory.DOUBLE;
} else if (oi instanceof StringObjectInspector && !isWeight) {
return PrimitiveObjectInspector.PrimitiveCategory.STRING;
} else {
throw new UDFArgumentException("Expected numeric or string type but got "
oi.getTypeName());
}
private String parseAsString(PrimitiveObjectInspector.PrimitiveCategory category, Object value) {
if (category == PrimitiveObjectInspector.PrimitiveCategory.INT) {
return ((Integer)value).toString();
} else if (category == PrimitiveObjectInspector.PrimitiveCategory.LONG) {
return ((Long)value).toString();
} else if (category == PrimitiveObjectInspector.PrimitiveCategory.FLOAT) {
return ((Float)value).toString();
} else if (category == PrimitiveObjectInspector.PrimitiveCategory.DOUBLE) {
return ((Double)value).toString();
} else if (category == PrimitiveObjectInspector.PrimitiveCategory.STRING) {
return ((String)value);
} else {
@Override
public Object evaluate(DeferredObject[] args) throws HiveException {
if (args.length != 2) {
if (args[0].get() == null || args[1].get() == null) {
return null;
}
if (featureOI == null || weightOI == null) {
return new HiveException("Invalid ObjectInspector");
}
Object feature = ((PrimitiveObjectInspector)featureOI)
.getPrimitiveJavaObject(args[0].get());
Object weight = ((PrimitiveObjectInspector)weightOI)
.getPrimitiveJavaObject(args[1].get());
String featureStr = parseAsString(featureCategory, feature);
String weightStr = parseAsString(weightCategory, weight);
@Override
public String getDisplayString(String[] strings) {
public static boolean isPrimitiveOI(@Nonnull final ObjectInspector oi) {
return oi.getCategory() == Category.PRIMITIVE;
}
return true;
default:
return false;
}
}
public static boolean isIntegerOI(@Nonnull final ObjectInspector argOI) {
if (argOI.getCategory() != Category.PRIMITIVE) {
return false;
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case INT:
case SHORT:
case LONG:
case BYTE:
@Nonnull
throw new UDFArgumentException("Expecting PrimitiveObjectInspector: "
public static void validateFeatureOI(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
final String typeName = oi.getTypeName();
if (!STRING_TYPE_NAME.equals(typeName) && !INT_TYPE_NAME.equals(typeName)
&& !BIGINT_TYPE_NAME.equals(typeName)) {
throw new UDFArgumentException(
"argument type for a feature must be List of key type [Int|BitInt|Text]: "
typeName);
}
}
import hivemall.utils.hadoop.HiveUtils;
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import org.apache.hadoop.io.Text;
@Description(
name = "feature",
value = "_FUNC_(<string|int|long|short|byte> feature, <number> value) - Returns a feature string")
@Nullable
private Text _result;
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if (argOIs.length != 2) {
throw new UDFArgumentException(
"_FUNC_ takes exactly 2 arguments, feature index and value");
validateFeatureOI(argOIs[0]);
validateValueOI(argOIs[1]);
private static void validateFeatureOI(@Nonnull ObjectInspector argOI)
if (!HiveUtils.isPrimitiveOI(argOI)) {
throw new UDFArgumentException(
"_FUNC_ expects integer type or string for `feature` but got "
argOI.getTypeName());
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case INT:
case SHORT:
case LONG:
case BYTE:
case STRING:
break;
default: {
throw new UDFArgumentException(
"_FUNC_ expects integer type or string for `feature` but got "
argOI.getTypeName());
}
private static void validateValueOI(@Nonnull ObjectInspector argOI) throws UDFArgumentException {
if (!HiveUtils.isNumberOI(argOI)) {
throw new UDFArgumentException("_FUNC_ expects a number type for `value` but got "
argOI.getTypeName());
@Nullable
public Text evaluate(@Nonnull DeferredObject[] args) throws HiveException {
assert (args.length == 2) : args.length;
final Object arg0 = args[0].get();
if (arg0 == null) {
return null;
}
final Object arg1 = args[1].get();
if (arg1 == null) {
String featureStr = arg0.toString();
String valueStr = arg1.toString();
Text result = this._result;
if (result == null) {
result = new Text(fv);
this._result = result;
} else {
result.set(fv);
return result;
public String getDisplayString(@Nonnull String[] children) {
import hivemall.utils.hadoop.HiveUtils;
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@Description(
name = "feature",
value = "_FUNC_(<string|int|long|short|byte> feature, <number> value) - Returns a feature string")
public final class FeatureUDF extends GenericUDF {
@Nullable
private Text _result;
@Override
public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
if (argOIs.length != 2) {
throw new UDFArgumentException(
"_FUNC_ takes exactly 2 arguments, feature index and value");
}
validateFeatureOI(argOIs[0]);
validateValueOI(argOIs[1]);
return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
private static void validateFeatureOI(@Nonnull ObjectInspector argOI)
throws UDFArgumentException {
if (!HiveUtils.isPrimitiveOI(argOI)) {
throw new UDFArgumentException(
"_FUNC_ expects integer type or string for `feature` but got "
argOI.getTypeName());
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case INT:
case SHORT:
case LONG:
case BYTE:
case STRING:
break;
default: {
throw new UDFArgumentException(
"_FUNC_ expects integer type or string for `feature` but got "
argOI.getTypeName());
}
}
private static void validateValueOI(@Nonnull ObjectInspector argOI) throws UDFArgumentException {
if (!HiveUtils.isNumberOI(argOI)) {
throw new UDFArgumentException("_FUNC_ expects a number type for `value` but got "
argOI.getTypeName());
}
@Override
@Nullable
public Text evaluate(@Nonnull DeferredObject[] args) throws HiveException {
assert (args.length == 2) : args.length;
final Object arg0 = args[0].get();
if (arg0 == null) {
final Object arg1 = args[1].get();
if (arg1 == null) {
String featureStr = arg0.toString();
String valueStr = arg1.toString();
Text result = this._result;
if (result == null) {
result = new Text(fv);
this._result = result;
} else {
result.set(fv);
}
return result;
@Override
public String getDisplayString(@Nonnull String[] children) {
public static boolean isPrimitiveOI(@Nonnull final ObjectInspector oi) {
return oi.getCategory() == Category.PRIMITIVE;
}
return true;
default:
return false;
}
}
public static boolean isIntegerOI(@Nonnull final ObjectInspector argOI) {
if (argOI.getCategory() != Category.PRIMITIVE) {
return false;
}
final PrimitiveObjectInspector oi = (PrimitiveObjectInspector) argOI;
switch (oi.getPrimitiveCategory()) {
case INT:
case SHORT:
case LONG:
case BYTE:
@Nonnull
throw new UDFArgumentException("Expecting PrimitiveObjectInspector: "
public static void validateFeatureOI(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
final String typeName = oi.getTypeName();
if (!STRING_TYPE_NAME.equals(typeName) && !INT_TYPE_NAME.equals(typeName)
&& !BIGINT_TYPE_NAME.equals(typeName)) {
throw new UDFArgumentException(
"argument type for a feature must be List of key type [Int|BitInt|Text]: "
typeName);
}
}
public static final String VERSION = "0.4.2-rc.2";
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@Nonnull
public DoubleWritable evaluate(@Nullable List<FloatWritable> Pu,
@Nullable List<FloatWritable> Qi) throws HiveException {
return evaluate(Pu, Qi, null);
@Nonnull
public DoubleWritable evaluate(@Nullable List<FloatWritable> Pu,
@Nullable List<FloatWritable> Qi, @Nullable DoubleWritable mu) throws HiveException {
final double muValue = (mu == null) ? 0.d : mu.get();
return new DoubleWritable(muValue);
return new DoubleWritable(muValue);
return new DoubleWritable(muValue);
double ret = muValue;
FloatWritable Pu_k = Pu.get(k);
if (Pu_k == null) {
continue;
}
FloatWritable Qi_k = Qi.get(k);
if (Qi_k == null) {
continue;
}
return new DoubleWritable(ret);
@Nonnull
public DoubleWritable evaluate(@Nullable List<FloatWritable> Pu,
@Nullable List<FloatWritable> Qi, @Nullable DoubleWritable Bu,
@Nullable DoubleWritable Bi) throws HiveException {
return evaluate(Pu, Qi, Bu, Bi, null);
@Nonnull
public DoubleWritable evaluate(@Nullable List<FloatWritable> Pu,
@Nullable List<FloatWritable> Qi, @Nullable DoubleWritable Bu,
@Nullable DoubleWritable Bi, @Nullable DoubleWritable mu) throws HiveException {
final double muValue = (mu == null) ? 0.d : mu.get();
return new DoubleWritable(muValue);
final double BiValue = (Bi == null) ? 0.d : Bi.get();
final double BuValue = (Bu == null) ? 0.d : Bu.get();
return new DoubleWritable(ret);
return new DoubleWritable(muValue);
return new DoubleWritable(muValue);
return new DoubleWritable(ret);
return new DoubleWritable(ret);
FloatWritable Pu_k = Pu.get(k);
if (Pu_k == null) {
continue;
}
FloatWritable Qi_k = Qi.get(k);
if (Qi_k == null) {
continue;
}
return new DoubleWritable(ret);
protected transient FactorizationMachineModel _model;
processOptions(argOIs);
this._model = null;
protected FactorizationMachineModel initModel(@Nonnull FMHyperParameters params)
throws UDFArgumentException {
final FactorizationMachineModel model;
model = new FMIntFeatureMapModel(params);
model = new FMArrayModel(params);
model = new FMStringFeatureMapModel(params);
this._model = model;
return model;
if (_model == null) {
this._model = initModel(_params);
}
private transient FFMStringFeatureMapModel _ffmModel;
private transient IntArrayList _fieldList;
private transient DoubleArray3D _sumVfX;
protected FFMStringFeatureMapModel initModel(@Nonnull FMHyperParameters params)
throws UDFArgumentException {
public static final int BIAS_CLAUSE_HASHVAL = 0;
import hivemall.HivemallConstants;
static String featureHashing(@Nonnull final String fv, final int numFeatures) {
if (fv.equals(HivemallConstants.BIAS_CLAUSE)) {
return fv;
}
String tail = fv.substring(headPos);
if (f.equals(HivemallConstants.BIAS_CLAUSE)) {
double d = Double.parseDouble(v);
if (d == 1.d) {
return fv;
}
}
static int mhash(@Nonnull final String word, final int numFeatures) {
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
@Nullable
public IntWritable evaluate(@Nullable final String word) throws UDFArgumentException {
@Nullable
public IntWritable evaluate(@Nullable final String word, final int numFeatures)
@Nullable
public IntWritable evaluate(@Nullable final List<String> words) throws UDFArgumentException {
@Nullable
public IntWritable evaluate(@Nullable final List<String> words, final int numFeatures)
public static int mhash(@Nonnull final String word) {
public static int mhash(@Nonnull final String word, final int numFeatures) {
this._V = new float[params.numFeatures][];
protected float[] getV(int i, boolean init) {
final int idx = i - 1;
float[] v = _V[idx];
if (v == null && init) {
v = initV();
_V[idx] = v;
}
return v;
float[] v = getV(i, true);
return v[f];
float[] v = getV(i, true);
v[f] = nextVif;
protected float[] getV(int i, boolean init) {
protected float[] getV(int i, boolean init) {
final float[] vi = model.getV(i, false);
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
private PrimitiveObjectInspector modelTypeOI;
this.modelTypeOI = HiveUtils.asIntegerOI(argOIs[1]);
int modelTypeId = PrimitiveObjectInspectorUtils.getInt(arg1, modelTypeOI);
public static final String VERSION = "0.4.2-rc.2";
public static final int BIAS_CLAUSE_HASHVAL = 0;
this._V = new float[params.numFeatures][];
protected float[] getV(int i, boolean init) {
final int idx = i - 1;
float[] v = _V[idx];
if (v == null && init) {
v = initV();
_V[idx] = v;
}
return v;
float[] v = getV(i, true);
return v[f];
float[] v = getV(i, true);
v[f] = nextVif;
protected float[] getV(int i, boolean init) {
protected float[] getV(int i, boolean init) {
protected transient FactorizationMachineModel _model;
processOptions(argOIs);
this._model = null;
protected FactorizationMachineModel initModel(@Nonnull FMHyperParameters params)
throws UDFArgumentException {
final FactorizationMachineModel model;
model = new FMIntFeatureMapModel(params);
model = new FMArrayModel(params);
model = new FMStringFeatureMapModel(params);
this._model = model;
return model;
if (_model == null) {
this._model = initModel(_params);
}
final float[] vi = model.getV(i, false);
private transient FFMStringFeatureMapModel _ffmModel;
private transient IntArrayList _fieldList;
private transient DoubleArray3D _sumVfX;
protected FFMStringFeatureMapModel initModel(@Nonnull FMHyperParameters params)
throws UDFArgumentException {
import hivemall.HivemallConstants;
static String featureHashing(@Nonnull final String fv, final int numFeatures) {
if (fv.equals(HivemallConstants.BIAS_CLAUSE)) {
return fv;
}
String tail = fv.substring(headPos);
if (f.equals(HivemallConstants.BIAS_CLAUSE)) {
double d = Double.parseDouble(v);
if (d == 1.d) {
return fv;
}
}
static int mhash(@Nonnull final String word, final int numFeatures) {
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
@Nullable
public IntWritable evaluate(@Nullable final String word) throws UDFArgumentException {
@Nullable
public IntWritable evaluate(@Nullable final String word, final int numFeatures)
@Nullable
public IntWritable evaluate(@Nullable final List<String> words) throws UDFArgumentException {
@Nullable
public IntWritable evaluate(@Nullable final List<String> words, final int numFeatures)
public static int mhash(@Nonnull final String word) {
public static int mhash(@Nonnull final String word, final int numFeatures) {
import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@Nonnull
public DoubleWritable evaluate(@Nullable List<FloatWritable> Pu,
@Nullable List<FloatWritable> Qi) throws HiveException {
return evaluate(Pu, Qi, null);
@Nonnull
public DoubleWritable evaluate(@Nullable List<FloatWritable> Pu,
@Nullable List<FloatWritable> Qi, @Nullable DoubleWritable mu) throws HiveException {
final double muValue = (mu == null) ? 0.d : mu.get();
return new DoubleWritable(muValue);
return new DoubleWritable(muValue);
return new DoubleWritable(muValue);
double ret = muValue;
FloatWritable Pu_k = Pu.get(k);
if (Pu_k == null) {
continue;
}
FloatWritable Qi_k = Qi.get(k);
if (Qi_k == null) {
continue;
}
return new DoubleWritable(ret);
@Nonnull
public DoubleWritable evaluate(@Nullable List<FloatWritable> Pu,
@Nullable List<FloatWritable> Qi, @Nullable DoubleWritable Bu,
@Nullable DoubleWritable Bi) throws HiveException {
return evaluate(Pu, Qi, Bu, Bi, null);
@Nonnull
public DoubleWritable evaluate(@Nullable List<FloatWritable> Pu,
@Nullable List<FloatWritable> Qi, @Nullable DoubleWritable Bu,
@Nullable DoubleWritable Bi, @Nullable DoubleWritable mu) throws HiveException {
final double muValue = (mu == null) ? 0.d : mu.get();
return new DoubleWritable(muValue);
final double BiValue = (Bi == null) ? 0.d : Bi.get();
final double BuValue = (Bu == null) ? 0.d : Bu.get();
return new DoubleWritable(ret);
return new DoubleWritable(muValue);
return new DoubleWritable(muValue);
return new DoubleWritable(ret);
return new DoubleWritable(ret);
FloatWritable Pu_k = Pu.get(k);
if (Pu_k == null) {
continue;
}
FloatWritable Qi_k = Qi.get(k);
if (Qi_k == null) {
continue;
}
return new DoubleWritable(ret);
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
private PrimitiveObjectInspector modelTypeOI;
this.modelTypeOI = HiveUtils.asIntegerOI(argOIs[1]);
int modelTypeId = PrimitiveObjectInspectorUtils.getInt(arg1, modelTypeOI);
public static final long DEFAULT_RAND_AMPLIFY_SEED = 42;
import hivemall.UDTFWithOptions;
import hivemall.utils.lang.Primitives;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.Options;
@Description(name = "rand_amplify", value = "_FUNC_(const int xtimes, const int num_buffers, [options], *)"
public final class RandomAmplifierUDTF extends UDTFWithOptions implements DropoutListener<Object[]> {
private boolean hasSeedOption = false;
protected Options getOptions() {
Options opts = new Options();
opts.addOption("seed", true, "Seed value [default value is 42]");
return opts;
}
@Override
protected CommandLine processOptions(ObjectInspector[] argOIs) throws UDFArgumentException {
CommandLine cl = null;
if (argOIs.length >= 3 && HiveUtils.isConstString(argOIs[2])) {
String rawArgs = HiveUtils.getConstString(argOIs[2]);
cl = parseOptions(rawArgs);
if (cl.hasOption("seed")) {
useSeed = true;
hasSeedOption = true;
seed = Primitives.parseLong(cl.getOptionValue("seed"), HivemallConstants.DEFAULT_RAND_AMPLIFY_SEED);
}
return cl;
processOptions(argOIs);
int argStartIndex = hasSeedOption ? 3 : 2;
int argStartIndex = hasSeedOption ? 3 : 2;
final Object[] row = new Object[args.length - argStartIndex];
row[i - argStartIndex] = ObjectInspectorUtils.copyToStandardObject(arg, argOI,
import hivemall.utils.lang.Primitives;
import java.util.List;
@Description(name = "rand_amplify", value = "_FUNC_(const int xtimes [, const string options], *)"
private boolean hasOption = false;
private long seed = -1L;
private int numBuffers = 1000;
opts.addOption("seed", true, "Random seed value [default: -1L (random)]");
opts.addOption("buf", "num_buffers", true,
"The number of rows to keep in a buffer [default: 1000]");
if (argOIs.length >= 3 && HiveUtils.isConstString(argOIs[1])) {
String rawArgs = HiveUtils.getConstString(argOIs[1]);
this.hasOption = true;
this.seed = Primitives.parseLong(cl.getOptionValue("seed"), this.seed);
this.numBuffers = Primitives.parseInt(cl.getOptionValue("num_buffers"), this.numBuffers);
if (numArgs < 2) {
"_FUNC_(const int xtimes, [, const string options], *) takes at least two arguments");
if (xtimes < 1) {
this.amplifier = (seed == -1L) ? new RandomizedAmplifier<Object[]>(numBuffers, xtimes)
: new RandomizedAmplifier<Object[]>(numBuffers, xtimes, seed);
final List<String> fieldNames = new ArrayList<String>();
final List<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
final int argStartIndex = hasOption ? 2 : 1;
final int argStartIndex = hasOption ? 2 : 1;
import hivemall.UDTFWithOptions;
import hivemall.utils.lang.Primitives;
import java.util.List;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.Options;
@Description(name = "rand_amplify", value = "_FUNC_(const int xtimes [, const string options], *)"
public final class RandomAmplifierUDTF extends UDTFWithOptions implements DropoutListener<Object[]> {
private boolean hasOption = false;
private long seed = -1L;
private int numBuffers = 1000;
protected Options getOptions() {
Options opts = new Options();
opts.addOption("seed", true, "Random seed value [default: -1L (random)]");
opts.addOption("buf", "num_buffers", true,
"The number of rows to keep in a buffer [default: 1000]");
return opts;
}
@Override
protected CommandLine processOptions(ObjectInspector[] argOIs) throws UDFArgumentException {
CommandLine cl = null;
if (argOIs.length >= 3 && HiveUtils.isConstString(argOIs[1])) {
String rawArgs = HiveUtils.getConstString(argOIs[1]);
cl = parseOptions(rawArgs);
this.hasOption = true;
this.seed = Primitives.parseLong(cl.getOptionValue("seed"), this.seed);
this.numBuffers = Primitives.parseInt(cl.getOptionValue("num_buffers"), this.numBuffers);
return cl;
if (numArgs < 2) {
"_FUNC_(const int xtimes, [, const string options], *) takes at least two arguments");
if (xtimes < 1) {
processOptions(argOIs);
this.amplifier = (seed == -1L) ? new RandomizedAmplifier<Object[]>(numBuffers, xtimes)
: new RandomizedAmplifier<Object[]>(numBuffers, xtimes, seed);
final List<String> fieldNames = new ArrayList<String>();
final List<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
final int argStartIndex = hasOption ? 2 : 1;
final int argStartIndex = hasOption ? 2 : 1;
final Object[] row = new Object[args.length - argStartIndex];
row[i - argStartIndex] = ObjectInspectorUtils.copyToStandardObject(arg, argOI,
public static double nDCG(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
double idcg = IDCG(Math.min(recommendSize, groundTruth.size()));
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector;
value = "_FUNC_(array rankItems, array correctItems [, const int recommendSize = rankItems.size])"
return new BinaryEvaluator();
public static class BinaryEvaluator extends GenericUDAFEvaluator {
private WritableIntObjectInspector recommendSizeOI;
public BinaryEvaluator() {}
assert (parameters.length == 2 || parameters.length == 3) : parameters.length;
if (parameters.length == 3) {
this.recommendSizeOI = (WritableIntObjectInspector) parameters[2];
}
int recommendSize = rankedList.size();
if (parameters.length == 3) {
recommendSize = recommendSizeOI.get(parameters[2]);
}
if (recommendSize < 0 || recommendSize > rankedList.size()) {
throw new UDFArgumentException(
}
myAggr.iterate(rankedList, correctList, recommendSize);
void iterate(@Nonnull List<?> rankedList, @Nonnull List<?> correctList, @Nonnull int recommendSize) {
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector;
if (!HiveUtils.isPrimitiveTypeInfo(arg1type.getListElementTypeInfo()) &&
!HiveUtils.isStructTypeInfo(arg1type.getListElementTypeInfo())) {
return new Evaluator();
public static class Evaluator extends GenericUDAFEvaluator {
private ListObjectInspector recommendListOI;
private ListObjectInspector truthListOI;
public Evaluator() {}
this.recommendListOI = (ListObjectInspector) parameters[0];
this.truthListOI = (ListObjectInspector) parameters[1];
List<?> recommendList = recommendListOI.getList(parameters[0]);
if (recommendList == null) {
recommendList = Collections.emptyList();
List<?> truthList = truthListOI.getList(parameters[1]);
if (truthList == null) {
int recommendSize = recommendList.size();
if (recommendSize < 0 || recommendSize > recommendList.size()) {
boolean isBinary = !HiveUtils.isStructOI(recommendListOI.getListElementObjectInspector());
double ndcg = 0.0d;
if (isBinary) {
ndcg = BinaryResponsesMeasures.nDCG(recommendList, truthList, recommendSize);
} else {
List<Double> recommendRelScoreList = new ArrayList<Double>();
StructObjectInspector sOI = (StructObjectInspector) recommendListOI.getListElementObjectInspector();
List<?> fieldRefList = sOI.getAllStructFieldRefs();
StructField relScoreField = (StructField) fieldRefList.get(0);
WritableDoubleObjectInspector relScoreFieldOI =
(WritableDoubleObjectInspector) relScoreField.getFieldObjectInspector();
Object structObj = recommendList.get(i);
List<Object> fieldList = sOI.getStructFieldsDataAsList(structObj);
double relScore = (double) relScoreFieldOI.get(fieldList.get(0));
recommendRelScoreList.add(relScore);
}
List<Double> truthRelScoreList = new ArrayList<Double>();
WritableDoubleObjectInspector truthRelScoreOI =
(WritableDoubleObjectInspector) truthListOI.getListElementObjectInspector();
Object relScoreObj = truthList.get(i);
double relScore = (double) truthRelScoreOI.get(relScoreObj);
truthRelScoreList.add(relScore);
}
ndcg = GradedResponsesMeasures.nDCG(recommendRelScoreList, truthRelScoreList, recommendSize);
}
myAggr.iterate(ndcg);
void iterate(@Nonnull double ndcg) {
public static boolean isStructOI(@Nonnull final ObjectInspector oi) {
return oi.getCategory() == Category.STRUCT;
}
public static boolean isStructTypeInfo(@Nonnull TypeInfo typeInfo) {
return typeInfo.getCategory() == ObjectInspector.Category.STRUCT;
}
public static double Precision(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
return (double) countTruePositive(rankedList, groundTruth, recommendSize) / recommendSize;
}
public static double Recall(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
return (double) countTruePositive(rankedList, groundTruth, recommendSize) / groundTruth.size();
}
public static int countTruePositive(final List<?> rankedList, final List<?> groundTruth, final int recommendSize) {
int nTruePositive = 0;
Object item_id = rankedList.get(i);
if (groundTruth.contains(item_id)) {
nTruePositive;
}
}
return nTruePositive;
}
public static double MRR(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
Object item_id = rankedList.get(i);
if (groundTruth.contains(item_id)) {
}
}
return 0.0;
}
public static double MAP(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
int nTruePositive = 0;
double sumPrecision = 0.0;
Object item_id = rankedList.get(i);
if (groundTruth.contains(item_id)) {
nTruePositive;
}
}
return sumPrecision / groundTruth.size();
}
public static double AUC(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
int nTruePositive = 0, nCorrectPairs = 0;
Object item_id = rankedList.get(i);
if (groundTruth.contains(item_id)) {
nTruePositive;
} else {
}
}
int nPairs = nTruePositive * (recommendSize - nTruePositive);
return (double) nCorrectPairs / nPairs;
}
@Nullable
@Nonnull final PrimitiveObjectInspector elemOI) throws UDFArgumentException {
return asDoubleArray(argObj, listOI, elemOI, true);
}
@Nullable
public static double[] asDoubleArray(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI,
@Nonnull final PrimitiveObjectInspector elemOI, final boolean avoidNull)
throws UDFArgumentException {
if (avoidNull) {
continue;
}
@Nonnull
public static void toDoubleArray(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI,
@Nonnull final PrimitiveObjectInspector elemOI, @Nonnull final double[] out,
final boolean avoidNull) throws UDFArgumentException {
if (argObj == null) {
return;
}
final int length = listOI.getListLength(argObj);
if (out.length != length) {
}
Object o = listOI.getListElement(argObj, i);
if (o == null) {
if (avoidNull) {
continue;
}
}
double d = PrimitiveObjectInspectorUtils.getDouble(o, elemOI);
out[i] = d;
}
return;
}
@Nonnull
public static void toDoubleArray(@Nullable final Object argObj,
@Nonnull final ListObjectInspector listOI,
@Nonnull final PrimitiveObjectInspector elemOI, @Nonnull final double[] out,
final double nullValue) throws UDFArgumentException {
if (argObj == null) {
return;
}
final int length = listOI.getListLength(argObj);
if (out.length != length) {
}
Object o = listOI.getListElement(argObj, i);
if (o == null) {
out[i] = nullValue;
continue;
}
double d = PrimitiveObjectInspectorUtils.getDouble(o, elemOI);
out[i] = d;
}
return;
}
import hivemall.utils.lang.Preconditions;
import javax.annotation.Nonnull;
import org.apache.commons.math3.linear.DecompositionSolver;
import org.apache.commons.math3.linear.LUDecomposition;
import org.apache.commons.math3.linear.RealMatrix;
import org.apache.commons.math3.linear.RealVector;
import org.apache.commons.math3.linear.SingularValueDecomposition;
public static double pdf(final double x, final double x_hat, final double sigma) {
if (sigma == 0.d) {
return 0.d;
}
double diff = x - x_hat;
double numerator = Math.exp(-0.5d * diff * diff / sigma);
double denominator = Math.sqrt(2.d * Math.PI) * Math.sqrt(sigma);
return numerator / denominator;
}
public static double pdf(@Nonnull final RealVector x, @Nonnull final RealVector x_hat,
@Nonnull final RealMatrix sigma) {
final int dim = x.getDimension();
Preconditions.checkArgument(sigma.isSquare(), "Sigma is not square matrix");
LUDecomposition LU = new LUDecomposition(sigma);
final double detSigma = LU.getDeterminant();
double denominator = Math.pow(2.d * Math.PI, 0.5d * dim) * Math.pow(detSigma, 0.5d);
return 0.d;
}
final RealMatrix invSigma;
DecompositionSolver solver = LU.getSolver();
if (solver.isNonSingular() == false) {
SingularValueDecomposition svd = new SingularValueDecomposition(sigma);
} else {
invSigma = solver.getInverse();
}
RealVector diff = x.subtract(x_hat);
RealVector premultiplied = invSigma.preMultiply(diff);
double sum = premultiplied.dotProduct(diff);
double numerator = Math.exp(-0.5d * sum);
return numerator / denominator;
}
public static double logLoss(final double actual, final double predicted, final double sigma) {
double p = pdf(actual, predicted, sigma);
if (p == 0.d) {
return 0.d;
}
return -Math.log(p);
}
public static double logLoss(@Nonnull final RealVector actual,
@Nonnull final RealVector predicted, @Nonnull final RealMatrix sigma) {
double p = pdf(actual, predicted, sigma);
if (p == 0.d) {
return 0.d;
}
return -Math.log(p);
}
public static double hellingerDistance(@Nonnull final double mu1, @Nonnull final double sigma1,
@Nonnull final double mu2, @Nonnull final double sigma2) {
if (sigmaSum == 0.d) {
return 0.d;
}
double numerator = Math.pow(sigma1, 0.25d) * Math.pow(sigma2, 0.25d)
* Math.exp(-0.25d * Math.pow(mu1 - mu2, 2d) / sigmaSum);
double denominator = Math.sqrt(sigmaSum / 2d);
if (denominator == 0.d) {
return 1.d;
}
return 1.d - numerator / denominator;
}
public static double hellingerDistance(@Nonnull final RealVector mu1,
@Nonnull final RealMatrix sigma1, @Nonnull final RealVector mu2,
@Nonnull final RealMatrix sigma2) {
RealVector muSub = mu1.subtract(mu2);
RealMatrix sigmaMean = sigma1.add(sigma2).scalarMultiply(0.5d);
LUDecomposition LUsigmaMean = new LUDecomposition(sigmaMean);
double denominator = Math.sqrt(LUsigmaMean.getDeterminant());
if (denominator == 0.d) {
}
double sigma1Det = MatrixUtils.det(sigma1);
double sigma2Det = MatrixUtils.det(sigma2);
double numerator = Math.pow(sigma1Det, 0.25d) * Math.pow(sigma2Det, 0.25d)
* Math.exp(-0.125d * sigmaMeanInv.preMultiply(muSub).dotProduct(muSub));
return 1.d - numerator / denominator;
}
public static double nDCG(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
double idcg = IDCG(Math.min(recommendSize, groundTruth.size()));
public static double Precision(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
return (double) countTruePositive(rankedList, groundTruth, recommendSize) / recommendSize;
}
public static double Recall(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
return (double) countTruePositive(rankedList, groundTruth, recommendSize) / groundTruth.size();
}
public static int countTruePositive(final List<?> rankedList, final List<?> groundTruth, final int recommendSize) {
int nTruePositive = 0;
Object item_id = rankedList.get(i);
if (groundTruth.contains(item_id)) {
nTruePositive;
}
}
return nTruePositive;
}
public static double MRR(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
Object item_id = rankedList.get(i);
if (groundTruth.contains(item_id)) {
}
}
return 0.0;
}
public static double MAP(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
int nTruePositive = 0;
double sumPrecision = 0.0;
Object item_id = rankedList.get(i);
if (groundTruth.contains(item_id)) {
nTruePositive;
}
}
return sumPrecision / groundTruth.size();
}
public static double AUC(@Nonnull final List<?> rankedList, @Nonnull final List<?> groundTruth,
@Nonnull final int recommendSize) {
int nTruePositive = 0, nCorrectPairs = 0;
Object item_id = rankedList.get(i);
if (groundTruth.contains(item_id)) {
nTruePositive;
} else {
}
}
int nPairs = nTruePositive * (recommendSize - nTruePositive);
return (double) nCorrectPairs / nPairs;
}
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector;
value = "_FUNC_(array rankItems, array correctItems [, const int recommendSize = rankItems.size])"
if (!HiveUtils.isPrimitiveTypeInfo(arg1type.getListElementTypeInfo()) &&
!HiveUtils.isStructTypeInfo(arg1type.getListElementTypeInfo())) {
private ListObjectInspector recommendListOI;
private ListObjectInspector truthListOI;
private WritableIntObjectInspector recommendSizeOI;
assert (parameters.length == 2 || parameters.length == 3) : parameters.length;
this.recommendListOI = (ListObjectInspector) parameters[0];
this.truthListOI = (ListObjectInspector) parameters[1];
if (parameters.length == 3) {
this.recommendSizeOI = (WritableIntObjectInspector) parameters[2];
}
List<?> recommendList = recommendListOI.getList(parameters[0]);
if (recommendList == null) {
recommendList = Collections.emptyList();
List<?> truthList = truthListOI.getList(parameters[1]);
if (truthList == null) {
int recommendSize = recommendList.size();
if (parameters.length == 3) {
recommendSize = recommendSizeOI.get(parameters[2]);
}
if (recommendSize < 0 || recommendSize > recommendList.size()) {
throw new UDFArgumentException(
}
boolean isBinary = !HiveUtils.isStructOI(recommendListOI.getListElementObjectInspector());
double ndcg = 0.0d;
if (isBinary) {
ndcg = BinaryResponsesMeasures.nDCG(recommendList, truthList, recommendSize);
} else {
List<Double> recommendRelScoreList = new ArrayList<Double>();
StructObjectInspector sOI = (StructObjectInspector) recommendListOI.getListElementObjectInspector();
List<?> fieldRefList = sOI.getAllStructFieldRefs();
StructField relScoreField = (StructField) fieldRefList.get(0);
WritableDoubleObjectInspector relScoreFieldOI =
(WritableDoubleObjectInspector) relScoreField.getFieldObjectInspector();
Object structObj = recommendList.get(i);
List<Object> fieldList = sOI.getStructFieldsDataAsList(structObj);
double relScore = (double) relScoreFieldOI.get(fieldList.get(0));
recommendRelScoreList.add(relScore);
}
List<Double> truthRelScoreList = new ArrayList<Double>();
WritableDoubleObjectInspector truthRelScoreOI =
(WritableDoubleObjectInspector) truthListOI.getListElementObjectInspector();
Object relScoreObj = truthList.get(i);
double relScore = (double) truthRelScoreOI.get(relScoreObj);
truthRelScoreList.add(relScore);
}
ndcg = GradedResponsesMeasures.nDCG(recommendRelScoreList, truthRelScoreList, recommendSize);
}
myAggr.iterate(ndcg);
void iterate(@Nonnull double ndcg) {
public static boolean isStructOI(@Nonnull final ObjectInspector oi) {
return oi.getCategory() == Category.STRUCT;
}
public static boolean isStructTypeInfo(@Nonnull TypeInfo typeInfo) {
return typeInfo.getCategory() == ObjectInspector.Category.STRUCT;
}
if (i > 0 && (labels[i] - labels[i - 1]) > 1) {
@Override
public void remove() {
throw new UnsupportedOperationException();
}
@Override
public void remove() {
throw new UnsupportedOperationException();
}
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
@Nonnull
public static StructObjectInspector asStructOI(@Nonnull final ObjectInspector oi) throws UDFArgumentException {
if(oi.getCategory() != Category.STRUCT) {
}
return (StructObjectInspector) oi;
}
import hivemall.utils.lang.Preconditions;
import javax.annotation.CheckForNull;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@Nonnull
public static Writable copyToWritable(@Nonnull final Object obj,
@CheckForNull final PrimitiveObjectInspector oi) {
Preconditions.checkNotNull(oi);
Object ret = ObjectInspectorUtils.copyToStandardObject(obj, oi,
ObjectInspectorCopyOption.WRITABLE);
return (Writable) ret;
}
import javax.annotation.Nonnull;
public final class Identifier<T> implements Serializable {
private final Map<T, Integer> counts;
this(0);
public Identifier(int initSeq) {
this.counts = new HashMap<>(512);
this.sequence = initSeq;
}
public int valueOf(@Nonnull T key) {
int id = sequence;
counts.put(key, Integer.valueOf(id));
return id;
public void put(@Nonnull T key) {
Integer count = counts.get(key);
if (count != null) {
return;
}
int id = sequence;
counts.put(key, Integer.valueOf(id));
sequence;
}
public Map<T, Integer> getMap() {
return counts;
}
public int size() {
return sequence;
}
if (jobConf == null) {
}
@SuppressWarnings("deprecation")
static class MaxAgg extends AbstractAggregationBuffer {
MaxAgg() {
super();
}
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer agg,
Object[] parameters) throws HiveException {
public List<Object> terminatePartial(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer agg, Object partial)
throws HiveException {
public List<Object> terminate(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
@SuppressWarnings("deprecation")
@SuppressWarnings("deprecation")
@SuppressWarnings("deprecation")
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import javax.annotation.Nonnull;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AbstractAggregationBuffer;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructField;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
@SuppressWarnings("deprecation")
if (!HiveUtils.isPrimitiveTypeInfo(arg1type.getListElementTypeInfo())
&& !HiveUtils.isStructTypeInfo(arg1type.getListElementTypeInfo())) {
"]");
public static class AUCAggregationBuffer extends AbstractAggregationBuffer {
public AUCAggregationBuffer() {
super();
}
void iterate(@Nonnull List<?> recommendList, @Nonnull List<?> truthList,
@Nonnull int recommendSize) {
public static double nDCG(@Nonnull final List<?> rankedList,
@Nonnull final List<?> groundTruth, @Nonnull final int recommendSize) {
public static double Precision(@Nonnull final List<?> rankedList,
@Nonnull final List<?> groundTruth, @Nonnull final int recommendSize) {
public static double Recall(@Nonnull final List<?> rankedList,
@Nonnull final List<?> groundTruth, @Nonnull final int recommendSize) {
return (double) countTruePositive(rankedList, groundTruth, recommendSize)
/ groundTruth.size();
public static int countTruePositive(final List<?> rankedList, final List<?> groundTruth,
final int recommendSize) {
@Nonnull final int recommendSize) {
@Nonnull final int recommendSize) {
@Nonnull final int recommendSize) {
@SuppressWarnings("deprecation")
import javax.annotation.Nonnull;
@Nonnull final List<Double> truthTopRelScoreList, @Nonnull final int recommendSize) {
@SuppressWarnings("deprecation")
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import javax.annotation.Nonnull;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AbstractAggregationBuffer;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructField;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
if (!HiveUtils.isPrimitiveTypeInfo(arg1type.getListElementTypeInfo())
&& !HiveUtils.isStructTypeInfo(arg1type.getListElementTypeInfo())) {
public MAPAggregationBuffer getNewAggregationBuffer() throws HiveException {
MAPAggregationBuffer myAggr = new MAPAggregationBuffer();
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer agg,
Object[] parameters) throws HiveException {
"]");
public Object terminatePartial(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer agg, Object partial)
throws HiveException {
public DoubleWritable terminate(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public static class MAPAggregationBuffer extends AbstractAggregationBuffer {
public MAPAggregationBuffer() {
super();
}
void iterate(@Nonnull List<?> recommendList, @Nonnull List<?> truthList,
@Nonnull int recommendSize) {
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import javax.annotation.Nonnull;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AbstractAggregationBuffer;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructField;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
if (!HiveUtils.isPrimitiveTypeInfo(arg1type.getListElementTypeInfo())
&& !HiveUtils.isStructTypeInfo(arg1type.getListElementTypeInfo())) {
public MRRAggregationBuffer getNewAggregationBuffer() throws HiveException {
MRRAggregationBuffer myAggr = new MRRAggregationBuffer();
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer agg,
Object[] parameters) throws HiveException {
"]");
public Object terminatePartial(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer agg, Object partial)
throws HiveException {
public DoubleWritable terminate(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public static class MRRAggregationBuffer extends AbstractAggregationBuffer {
public MRRAggregationBuffer() {
super();
}
void iterate(@Nonnull List<?> recommendList, @Nonnull List<?> truthList,
@Nonnull int recommendSize) {
@SuppressWarnings("deprecation")
@SuppressWarnings("deprecation")
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AbstractAggregationBuffer;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
if (!HiveUtils.isPrimitiveTypeInfo(arg1type.getListElementTypeInfo())
&& !HiveUtils.isStructTypeInfo(arg1type.getListElementTypeInfo())) {
public NDCGAggregationBuffer getNewAggregationBuffer() throws HiveException {
NDCGAggregationBuffer myAggr = new NDCGAggregationBuffer();
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer agg,
Object[] parameters) throws HiveException {
"]");
WritableDoubleObjectInspector relScoreFieldOI = (WritableDoubleObjectInspector) relScoreField.getFieldObjectInspector();
WritableDoubleObjectInspector truthRelScoreOI = (WritableDoubleObjectInspector) truthListOI.getListElementObjectInspector();
ndcg = GradedResponsesMeasures.nDCG(recommendRelScoreList, truthRelScoreList,
recommendSize);
public Object terminatePartial(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer agg, Object partial)
throws HiveException {
public DoubleWritable terminate(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public static class NDCGAggregationBuffer extends AbstractAggregationBuffer {
public NDCGAggregationBuffer() {
super();
}
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import javax.annotation.Nonnull;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AbstractAggregationBuffer;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructField;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
if (!HiveUtils.isPrimitiveTypeInfo(arg1type.getListElementTypeInfo())
&& !HiveUtils.isStructTypeInfo(arg1type.getListElementTypeInfo())) {
public PrecisionAggregationBuffer getNewAggregationBuffer() throws HiveException {
PrecisionAggregationBuffer myAggr = new PrecisionAggregationBuffer();
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer agg,
Object[] parameters) throws HiveException {
"]");
public Object terminatePartial(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer agg, Object partial)
throws HiveException {
public DoubleWritable terminate(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public static class PrecisionAggregationBuffer extends AbstractAggregationBuffer {
public PrecisionAggregationBuffer() {
super();
}
void iterate(@Nonnull List<?> recommendList, @Nonnull List<?> truthList,
@Nonnull int recommendSize) {
@SuppressWarnings("deprecation")
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import javax.annotation.Nonnull;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AbstractAggregationBuffer;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructField;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
if (!HiveUtils.isPrimitiveTypeInfo(arg1type.getListElementTypeInfo())
&& !HiveUtils.isStructTypeInfo(arg1type.getListElementTypeInfo())) {
public RecallAggregationBuffer getNewAggregationBuffer() throws HiveException {
RecallAggregationBuffer myAggr = new RecallAggregationBuffer();
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer agg,
Object[] parameters) throws HiveException {
"]");
public Object terminatePartial(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer agg, Object partial)
throws HiveException {
public DoubleWritable terminate(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public static class RecallAggregationBuffer extends AbstractAggregationBuffer {
public RecallAggregationBuffer() {
super();
}
void iterate(@Nonnull List<?> recommendList, @Nonnull List<?> truthList,
@Nonnull int recommendSize) {
@SuppressWarnings("deprecation")
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AbstractAggregationBuffer;
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer agg,
Object[] parameters) throws HiveException {
public Object terminatePartial(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer agg, Object partial)
throws HiveException {
public DoubleWritable terminate(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public static class FMPredictAggregationBuffer extends AbstractAggregationBuffer {
FMPredictAggregationBuffer() {
super();
}
import hivemall.utils.lang.Preconditions;
import java.util.ArrayList;
import java.util.List;
value = "_FUNC_(int|bigint|float|double weight, const int num_of_bins[, const boolean auto_shrink = false])"
" - Return quantiles representing bins: array<double>")
public final class BuildBinsUDAF extends AbstractGenericUDAFResolver {
static final class BuildBinsAggregationBuffer extends AbstractAggregationBuffer {
BuildBinsAggregationBuffer() {}
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer agg,
Object[] parameters) throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer agg, Object other)
throws HiveException {
List<?> histogram = ((LazyBinaryArray) structOI.getStructFieldData(other,
histogramField)).getList();
@SuppressWarnings("serial")
public Object terminatePartial(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
partialResult[2] = (myagg.quantiles != null) ? WritableUtils.toWritableList(myagg.quantiles)
public Object terminate(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
List<DoubleWritable> result = new ArrayList<DoubleWritable>();
Preconditions.checkNotNull(myagg.quantiles);
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.apache.hadoop.hive.ql.udf.UDFType;
value = "_FUNC_(array<features::string> features, const map<string, array<double>> quantiles_map)"
" / _FUNC(int|bigint|float|double weight, const array<double> quantiles)"
" - Returns binned features as an array<features::string> / bin ID as int")
@UDFType(deterministic = true, stateful = false)
public final class FeatureBinningUDF extends GenericUDF {
public Object evaluate(DeferredObject[] dObj) throws HiveException {
Map<?, ?> _quantilesMap = quantilesMapOI.getMap(dObj[1].get());
List<?> fs = featuresOI.getList(dObj[0].get());
import hivemall.utils.lang.SizeOf;
import java.util.List;
public final class NumericHistogram {
static final class Coord implements Comparable<Coord> {
Coord() {}
public int compareTo(Coord other) {
return Double.compare(x, other.x);
public void merge(List<?> other, DoubleObjectInspector doi) {
@SuppressWarnings("deprecation")
@SuppressWarnings("deprecation")
@SuppressWarnings("deprecation")
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AbstractAggregationBuffer;
public ArrayAvgAggregationBuffer getNewAggregationBuffer() throws HiveException {
ArrayAvgAggregationBuffer aggr = new ArrayAvgAggregationBuffer();
public void reset(@SuppressWarnings("deprecation") AggregationBuffer aggr)
throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer aggr,
Object[] parameters) throws HiveException {
public Object terminatePartial(@SuppressWarnings("deprecation") AggregationBuffer aggr)
throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer aggr, Object partial)
throws HiveException {
public List<FloatWritable> terminate(@SuppressWarnings("deprecation") AggregationBuffer aggr)
throws HiveException {
public static class ArrayAvgAggregationBuffer extends AbstractAggregationBuffer {
public ArrayAvgAggregationBuffer() {
super();
}
@SuppressWarnings("deprecation")
static class ArrayAggregationBuffer extends AbstractAggregationBuffer {
ArrayAggregationBuffer() {
super();
}
public ArrayAggregationBuffer getNewAggregationBuffer() throws HiveException {
public void reset(@SuppressWarnings("deprecation") AggregationBuffer aggr)
throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer aggr,
Object[] parameters) throws HiveException {
public List<LongWritable> terminatePartial(
@SuppressWarnings("deprecation") AggregationBuffer aggr) throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer aggr, Object other)
throws HiveException {
public List<LongWritable> terminate(@SuppressWarnings("deprecation") AggregationBuffer aggr)
throws HiveException {
static class MapAggregationBuffer extends AbstractAggregationBuffer {
MapAggregationBuffer() {
super();
}
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public MapAggregationBuffer getNewAggregationBuffer() throws HiveException {
public void iterate(@SuppressWarnings("deprecation") AggregationBuffer agg,
Object[] parameters) throws HiveException {
public Map<Object, Object> terminatePartial(
@SuppressWarnings("deprecation") AggregationBuffer agg) throws HiveException {
public void merge(@SuppressWarnings("deprecation") AggregationBuffer agg, Object partial)
throws HiveException {
public Map<Object, Object> terminate(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
public void reset(@SuppressWarnings("deprecation") AggregationBuffer agg)
throws HiveException {
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
@Description(name = "build_bins",
value = "_FUNC_(number weight, const int num_of_bins[, const boolean auto_shrink = false])"
if (OIs.length != 2 && OIs.length != 3) {
if (!HiveUtils.isNumberOI(OIs[0])) {
throw new UDFArgumentTypeException(0, "Only number type argument is acceptable but "
}
if (!HiveUtils.isIntegerOI(OIs[1])) {
throw new UDFArgumentTypeException(1, "Only int type argument is acceptable but "
if (!HiveUtils.isBooleanOI(OIs[2])) {
throw new UDFArgumentTypeException(2,
" was passed as `auto_shrink`");
private BooleanObjectInspector autoShrinkOI;
private DoubleObjectInspector histogramElOI;
private DoubleObjectInspector quantileOI;
weightOI = HiveUtils.asDoubleCompatibleOI(OIs[0]);
nBins = HiveUtils.getConstInt(OIs[1]);
if (OIs.length == 3) {
autoShrink = HiveUtils.getConstBoolean(OIs[2]);
}
if (nBins < 2) {
}
final ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
return ObjectInspectorFactory.getStandardStructObjectInspector(
Arrays.asList("autoShrink", "histogram", "quantiles"), fieldOIs);
final int nQuantiles = nBins - 1;
final double[] result = new double[nQuantiles];
}
final BuildBinsAggregationBuffer myAgg = new BuildBinsAggregationBuffer();
myAgg.histogram = new NumericHistogram();
reset(myAgg);
return myAgg;
final BuildBinsAggregationBuffer myAgg = (BuildBinsAggregationBuffer) agg;
myAgg.autoShrink = autoShrink;
myAgg.histogram.reset();
myAgg.histogram.allocate(nBGBins);
myAgg.quantiles = quantiles;
Preconditions.checkArgument(parameters.length == 2 || parameters.length == 3);
final BuildBinsAggregationBuffer myAgg = (BuildBinsAggregationBuffer) agg;
myAgg.histogram.add(PrimitiveObjectInspectorUtils.getDouble(parameters[0], weightOI));
if (other == null) {
}
final BuildBinsAggregationBuffer myAgg = (BuildBinsAggregationBuffer) agg;
myAgg.autoShrink = autoShrinkOI.get(structOI.getStructFieldData(other, autoShrinkField));
final List<?> histogram = ((LazyBinaryArray) structOI.getStructFieldData(other,
myAgg.histogram.merge(histogram, histogramElOI);
final double[] quantiles = HiveUtils.asDoubleArray(
if (quantiles != null && quantiles.length > 0) {
myAgg.quantiles = quantiles;
}
final BuildBinsAggregationBuffer myAgg = (BuildBinsAggregationBuffer) agg;
final Object[] partialResult = new Object[3];
partialResult[0] = new BooleanWritable(myAgg.autoShrink);
partialResult[1] = myAgg.histogram.serialize();
partialResult[2] = (myAgg.quantiles != null) ? WritableUtils.toWritableList(myAgg.quantiles)
: Collections.singletonList(new DoubleWritable(0));
final BuildBinsAggregationBuffer myAgg = (BuildBinsAggregationBuffer) agg;
Preconditions.checkNotNull(myAgg.quantiles);
final List<DoubleWritable> result = new ArrayList<DoubleWritable>();
final double val = myAgg.histogram.quantile(myAgg.quantiles[i]);
if (!myAgg.autoShrink) {
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
import java.util.*;
value = "_FUNC_(array<features::string> features, const map<string, array<number>> quantiles_map)"
" / _FUNC(number weight, const array<number> quantiles)"
private ListObjectInspector featuresOI;
private StringObjectInspector featureOI;
private MapObjectInspector quantilesMapOI;
private StringObjectInspector keyOI;
private ListObjectInspector quantilesOI;
private PrimitiveObjectInspector quantileOI;
if (OIs.length != 2) {
}
if (HiveUtils.isListOI(OIs[0]) && HiveUtils.isMapOI(OIs[1])) {
if (!HiveUtils.isStringOI(((ListObjectInspector) OIs[0]).getListElementObjectInspector())) {
throw new UDFArgumentTypeException(0,
" was passed as `features`");
}
featuresOI = HiveUtils.asListOI(OIs[0]);
featureOI = HiveUtils.asStringOI(featuresOI.getListElementObjectInspector());
quantilesMapOI = HiveUtils.asMapOI(OIs[1]);
if (!HiveUtils.isStringOI(quantilesMapOI.getMapKeyObjectInspector())
|| !HiveUtils.isListOI(quantilesMapOI.getMapValueObjectInspector())
|| !HiveUtils.isNumberOI(((ListObjectInspector) quantilesMapOI.getMapValueObjectInspector()).getListElementObjectInspector())) {
throw new UDFArgumentTypeException(1,
"Only map<string, array<number>> type argument is acceptable but "
}
keyOI = HiveUtils.asStringOI(quantilesMapOI.getMapKeyObjectInspector());
quantilesOI = HiveUtils.asListOI(quantilesMapOI.getMapValueObjectInspector());
quantileOI = HiveUtils.asDoubleCompatibleOI(quantilesOI.getListElementObjectInspector());
} else if (HiveUtils.isPrimitiveOI(OIs[0]) && HiveUtils.isListOI(OIs[1])) {
weightOI = HiveUtils.asDoubleCompatibleOI(OIs[0]);
quantilesOI = HiveUtils.asListOI(OIs[1]);
if (!HiveUtils.isNumberOI(quantilesOI.getListElementObjectInspector())) {
throw new UDFArgumentTypeException(1,
" was passed as `quantiles`");
}
quantileOI = HiveUtils.asDoubleCompatibleOI(quantilesOI.getListElementObjectInspector());
throw new UDFArgumentTypeException(0,
"Only <array<features::string>, map<string, array<number>>> "
"or <number, array<number>> type arguments are accepted but <"
final Map<?, ?> _quantilesMap = quantilesMapOI.getMap(dObj[1].get());
final Text key = new Text(keyOI.getPrimitiveJavaObject(_key));
final double[] val = HiveUtils.asDoubleArray(_quantilesMap.get(key),
quantilesOI, quantileOI);
final List<?> fs = featuresOI.getList(dObj[0].get());
final List<Text> result = new ArrayList<Text>();
final String entry = featureOI.getPrimitiveJavaObject(f);
final int pos = entry.indexOf(":");
final Text key = new Text(entry.substring(0, pos));
private int findBin(double[] _quantiles, double d) throws HiveException {
if (_quantiles.length < 3) {
int res = Arrays.binarySearch(_quantiles, d);
return (res < 0) ? ~res - 1 : (res == 0) ? 0 : res - 1;
final StringBuilder sb = new StringBuilder();
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
public static StructObjectInspector asStructOI(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
if (oi.getCategory() != Category.STRUCT) {
return (StructObjectInspector) oi;
public static boolean isMapOI(@Nonnull final ObjectInspector oi) {
return oi.getCategory() == Category.MAP;
}
@Nonnull
public static MapObjectInspector asMapOI(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
if (oi.getCategory() != Category.MAP) {
}
return (MapObjectInspector) oi;
}
import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
@Description(name = "build_bins",
value = "_FUNC_(number weight, const int num_of_bins[, const boolean auto_shrink = false])"
if (OIs.length != 2 && OIs.length != 3) {
if (!HiveUtils.isNumberOI(OIs[0])) {
throw new UDFArgumentTypeException(0, "Only number type argument is acceptable but "
}
if (!HiveUtils.isIntegerOI(OIs[1])) {
throw new UDFArgumentTypeException(1, "Only int type argument is acceptable but "
if (!HiveUtils.isBooleanOI(OIs[2])) {
throw new UDFArgumentTypeException(2,
" was passed as `auto_shrink`");
private BooleanObjectInspector autoShrinkOI;
private DoubleObjectInspector histogramElOI;
private DoubleObjectInspector quantileOI;
weightOI = HiveUtils.asDoubleCompatibleOI(OIs[0]);
nBins = HiveUtils.getConstInt(OIs[1]);
if (OIs.length == 3) {
autoShrink = HiveUtils.getConstBoolean(OIs[2]);
}
if (nBins < 2) {
}
final ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
return ObjectInspectorFactory.getStandardStructObjectInspector(
Arrays.asList("autoShrink", "histogram", "quantiles"), fieldOIs);
final int nQuantiles = nBins - 1;
final double[] result = new double[nQuantiles];
}
final BuildBinsAggregationBuffer myAgg = new BuildBinsAggregationBuffer();
myAgg.histogram = new NumericHistogram();
reset(myAgg);
return myAgg;
final BuildBinsAggregationBuffer myAgg = (BuildBinsAggregationBuffer) agg;
myAgg.autoShrink = autoShrink;
myAgg.histogram.reset();
myAgg.histogram.allocate(nBGBins);
myAgg.quantiles = quantiles;
Preconditions.checkArgument(parameters.length == 2 || parameters.length == 3);
final BuildBinsAggregationBuffer myAgg = (BuildBinsAggregationBuffer) agg;
myAgg.histogram.add(PrimitiveObjectInspectorUtils.getDouble(parameters[0], weightOI));
if (other == null) {
}
final BuildBinsAggregationBuffer myAgg = (BuildBinsAggregationBuffer) agg;
myAgg.autoShrink = autoShrinkOI.get(structOI.getStructFieldData(other, autoShrinkField));
final List<?> histogram = ((LazyBinaryArray) structOI.getStructFieldData(other,
myAgg.histogram.merge(histogram, histogramElOI);
final double[] quantiles = HiveUtils.asDoubleArray(
if (quantiles != null && quantiles.length > 0) {
myAgg.quantiles = quantiles;
}
final BuildBinsAggregationBuffer myAgg = (BuildBinsAggregationBuffer) agg;
final Object[] partialResult = new Object[3];
partialResult[0] = new BooleanWritable(myAgg.autoShrink);
partialResult[1] = myAgg.histogram.serialize();
partialResult[2] = (myAgg.quantiles != null) ? WritableUtils.toWritableList(myAgg.quantiles)
: Collections.singletonList(new DoubleWritable(0));
final BuildBinsAggregationBuffer myAgg = (BuildBinsAggregationBuffer) agg;
Preconditions.checkNotNull(myAgg.quantiles);
final List<DoubleWritable> result = new ArrayList<DoubleWritable>();
final double val = myAgg.histogram.quantile(myAgg.quantiles[i]);
if (!myAgg.autoShrink) {
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
import java.util.*;
value = "_FUNC_(array<features::string> features, const map<string, array<number>> quantiles_map)"
" / _FUNC(number weight, const array<number> quantiles)"
private ListObjectInspector featuresOI;
private StringObjectInspector featureOI;
private MapObjectInspector quantilesMapOI;
private StringObjectInspector keyOI;
private ListObjectInspector quantilesOI;
private PrimitiveObjectInspector quantileOI;
if (OIs.length != 2) {
}
if (HiveUtils.isListOI(OIs[0]) && HiveUtils.isMapOI(OIs[1])) {
if (!HiveUtils.isStringOI(((ListObjectInspector) OIs[0]).getListElementObjectInspector())) {
throw new UDFArgumentTypeException(0,
" was passed as `features`");
}
featuresOI = HiveUtils.asListOI(OIs[0]);
featureOI = HiveUtils.asStringOI(featuresOI.getListElementObjectInspector());
quantilesMapOI = HiveUtils.asMapOI(OIs[1]);
if (!HiveUtils.isStringOI(quantilesMapOI.getMapKeyObjectInspector())
|| !HiveUtils.isListOI(quantilesMapOI.getMapValueObjectInspector())
|| !HiveUtils.isNumberOI(((ListObjectInspector) quantilesMapOI.getMapValueObjectInspector()).getListElementObjectInspector())) {
throw new UDFArgumentTypeException(1,
"Only map<string, array<number>> type argument is acceptable but "
}
keyOI = HiveUtils.asStringOI(quantilesMapOI.getMapKeyObjectInspector());
quantilesOI = HiveUtils.asListOI(quantilesMapOI.getMapValueObjectInspector());
quantileOI = HiveUtils.asDoubleCompatibleOI(quantilesOI.getListElementObjectInspector());
} else if (HiveUtils.isPrimitiveOI(OIs[0]) && HiveUtils.isListOI(OIs[1])) {
weightOI = HiveUtils.asDoubleCompatibleOI(OIs[0]);
quantilesOI = HiveUtils.asListOI(OIs[1]);
if (!HiveUtils.isNumberOI(quantilesOI.getListElementObjectInspector())) {
throw new UDFArgumentTypeException(1,
" was passed as `quantiles`");
}
quantileOI = HiveUtils.asDoubleCompatibleOI(quantilesOI.getListElementObjectInspector());
throw new UDFArgumentTypeException(0,
"Only <array<features::string>, map<string, array<number>>> "
"or <number, array<number>> type arguments are accepted but <"
final Map<?, ?> _quantilesMap = quantilesMapOI.getMap(dObj[1].get());
final Text key = new Text(keyOI.getPrimitiveJavaObject(_key));
final double[] val = HiveUtils.asDoubleArray(_quantilesMap.get(key),
quantilesOI, quantileOI);
final List<?> fs = featuresOI.getList(dObj[0].get());
final List<Text> result = new ArrayList<Text>();
final String entry = featureOI.getPrimitiveJavaObject(f);
final int pos = entry.indexOf(":");
final Text key = new Text(entry.substring(0, pos));
private int findBin(double[] _quantiles, double d) throws HiveException {
if (_quantiles.length < 3) {
int res = Arrays.binarySearch(_quantiles, d);
return (res < 0) ? ~res - 1 : (res == 0) ? 0 : res - 1;
final StringBuilder sb = new StringBuilder();
import org.apache.hadoop.hive.serde2.io.DoubleWritable;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
public static StructObjectInspector asStructOI(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
if (oi.getCategory() != Category.STRUCT) {
return (StructObjectInspector) oi;
public static boolean isMapOI(@Nonnull final ObjectInspector oi) {
return oi.getCategory() == Category.MAP;
}
@Nonnull
public static MapObjectInspector asMapOI(@Nonnull final ObjectInspector oi)
throws UDFArgumentException {
if (oi.getCategory() != Category.MAP) {
}
return (MapObjectInspector) oi;
}
import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
public static boolean isNumberOI(@Nonnull final ObjectInspector argOI) {
public static boolean isNumberListOI(@Nonnull final ObjectInspector oi) {
return isListOI(oi)
&& isNumberOI(((ListObjectInspector) oi).getListElementObjectInspector());
}
public static boolean isNumberListListOI(@Nonnull final ObjectInspector oi) {
return isListOI(oi)
&& isNumberListOI(((ListObjectInspector) oi).getListElementObjectInspector());
}
public static DoubleObjectInspector asDoubleOI(@Nonnull final ObjectInspector argOI)
throws UDFArgumentException {
if (!DOUBLE_TYPE_NAME.equals(argOI.getTypeName())) {
}
return (DoubleObjectInspector) argOI;
}
import javax.annotation.Nullable;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
@Nonnull
public static List<DoubleWritable> toWritableList(@Nonnull final double[] src,
@Nullable List<DoubleWritable> list) throws UDFArgumentException {
if (list == null) {
return toWritableList(src);
}
Preconditions.checkArgument(src.length == list.size(), UDFArgumentException.class);
list.set(i, new DoubleWritable(src[i]));
}
return list;
}
import javax.annotation.Nonnull;
public static <T, E extends Throwable> T checkNotNull(@Nullable T reference,
@Nonnull Class<E> clazz) throws E {
if (reference == null) {
final E throwable;
try {
throwable = clazz.newInstance();
} catch (InstantiationException | IllegalAccessException e) {
throw new IllegalStateException(
}
throw throwable;
}
return reference;
}
public static <E extends Throwable> void checkArgument(boolean expression,
@Nonnull Class<E> clazz) throws E {
if (!expression) {
final E throwable;
try {
throwable = clazz.newInstance();
} catch (InstantiationException | IllegalAccessException e) {
throw new IllegalStateException(
}
throw throwable;
}
}
import org.apache.commons.math3.distribution.ChiSquaredDistribution;
import org.apache.commons.math3.exception.DimensionMismatchException;
import org.apache.commons.math3.exception.NotPositiveException;
import org.apache.commons.math3.util.FastMath;
import org.apache.commons.math3.util.MathArrays;
import java.util.AbstractMap;
import java.util.Map;
public static double chiSquare(@Nonnull final double[] observed,
@Nonnull final double[] expected) {
if (observed.length < 2) {
throw new DimensionMismatchException(observed.length, 2);
}
if (expected.length != observed.length) {
throw new DimensionMismatchException(observed.length, expected.length);
}
MathArrays.checkPositive(expected);
for (double d : observed) {
if (d < 0.d) {
throw new NotPositiveException(d);
}
}
double sumObserved = 0.d;
double sumExpected = 0.d;
}
double ratio = 1.d;
boolean rescale = false;
if (FastMath.abs(sumObserved - sumExpected) > 10e-6) {
ratio = sumObserved / sumExpected;
rescale = true;
}
double sumSq = 0.d;
if (rescale) {
final double dev = observed[i] - ratio * expected[i];
} else {
final double dev = observed[i] - expected[i];
}
}
return sumSq;
}
public static double chiSquareTest(@Nonnull final double[] observed,
@Nonnull final double[] expected) {
final ChiSquaredDistribution distribution = new ChiSquaredDistribution(
expected.length - 1.d);
return 1.d - distribution.cumulativeProbability(chiSquare(observed, expected));
}
public static Map.Entry<double[], double[]> chiSquare(@Nonnull final double[][] observeds,
@Nonnull final double[][] expecteds) {
Preconditions.checkArgument(observeds.length == expecteds.length);
final int len = expecteds.length;
final int lenOfEach = expecteds[0].length;
final ChiSquaredDistribution distribution = new ChiSquaredDistribution(lenOfEach - 1.d);
final double[] chi2s = new double[len];
final double[] ps = new double[len];
chi2s[i] = chiSquare(observeds[i], expecteds[i]);
ps[i] = 1.d - distribution.cumulativeProbability(chi2s[i]);
}
return new AbstractMap.SimpleEntry<double[], double[]>(chi2s, ps);
}
import org.apache.hadoop.hive.ql.udf.UDFType;
@UDFType(deterministic = false, stateful = true)
import java.lang.reflect.Constructor;
import java.lang.reflect.InvocationTargetException;
public static <T, E extends Throwable> T checkNotNull(@Nullable T reference,
@Nonnull String errorMessage, @Nonnull Class<E> clazz) throws E {
if (reference == null) {
final E throwable;
try {
Constructor<E> constructor = clazz.getConstructor(String.class);
throwable = constructor.newInstance(errorMessage);
} catch (NoSuchMethodException | SecurityException e1) {
throw new IllegalStateException("Failed to get a Constructor(String): "
clazz.getName(), e1);
} catch (InstantiationException | IllegalAccessException | IllegalArgumentException
| InvocationTargetException e2) {
throw new IllegalStateException(
}
throw throwable;
}
return reference;
}
public static <E extends Throwable> void checkArgument(boolean expression,
@Nonnull String errorMessage, @Nonnull Class<E> clazz) throws E {
if (!expression) {
final E throwable;
try {
Constructor<E> constructor = clazz.getConstructor(String.class);
throwable = constructor.newInstance(errorMessage);
} catch (NoSuchMethodException | SecurityException e1) {
throw new IllegalStateException("Failed to get a Constructor(String): "
clazz.getName(), e1);
} catch (InstantiationException | IllegalAccessException | IllegalArgumentException
| InvocationTargetException e2) {
throw new IllegalStateException(
}
throw throwable;
}
}
public static double sign(final double x) {
if (x < 0.d) {
return -1.d;
} else if (x > 0.d) {
return 1.d;
}
}
import java.util.Arrays;
import org.apache.commons.math3.linear.Array2DRowRealMatrix;
import org.apache.commons.math3.linear.ArrayRealVector;
import org.apache.commons.math3.linear.RealVector;
@Nonnull
public static double[][] eye(int n) {
final double[][] eye = new double[n][n];
eye[i][i] = 1;
}
return eye;
}
public static double power1(@Nonnull final RealMatrix A, @Nonnull final double[] x0,
final int nIter, @Nonnull final double[] u, @Nonnull final double[] v) {
Preconditions.checkArgument(A.getColumnDimension() == x0.length,
"Column size of A and length of x should be same");
Preconditions.checkArgument(A.getRowDimension() == u.length,
"Row size of A and length of u should be same");
Preconditions.checkArgument(x0.length == v.length, "Length of x and u should be same");
RealMatrix AtA = A.transpose().multiply(A);
RealVector x = new ArrayRealVector(x0);
x = AtA.operate(x);
}
double xNorm = x.getNorm();
v[i] = x.getEntry(i) / xNorm;
}
RealVector Av = new ArrayRealVector(A.operate(v));
double s = Av.getNorm();
u[i] = Av.getEntry(i) / s;
}
return s;
}
public static void lanczosTridiagonalization(@Nonnull final RealMatrix C,
@Nonnull final double[] a, @Nonnull final RealMatrix T) {
Preconditions.checkArgument(Arrays.deepEquals(C.getData(), C.transpose().getData()),
"Target matrix C must be a symmetric matrix");
Preconditions.checkArgument(C.getColumnDimension() == a.length,
"Column size of A and length of a should be same");
Preconditions.checkArgument(T.getRowDimension() == T.getColumnDimension(),
"T must be a square matrix");
int s = T.getRowDimension();
T.setSubMatrix(new double[s][s], 0, 0);
RealVector a0 = new ArrayRealVector(a.length);
RealVector r = new ArrayRealVector(a);
double beta0 = 1.d;
RealVector a1 = r.mapDivide(beta0);
RealVector Ca1 = C.operate(a1);
double alpha1 = a1.dotProduct(Ca1);
r = Ca1.add(a1.mapMultiply(-1.d * alpha1)).add(a0.mapMultiply(-1.d * beta0));
double beta1 = r.getNorm();
T.setEntry(i, i, alpha1);
if (i - 1 >= 0) {
T.setEntry(i, i - 1, beta0);
}
}
a0 = a1.copy();
beta0 = beta1;
}
}
public static void tridiagonalQR(@Nonnull final RealMatrix T, @Nonnull final RealMatrix R,
@Nonnull final RealMatrix Qt) {
int n = T.getRowDimension();
Preconditions.checkArgument(n == R.getRowDimension() && n == R.getColumnDimension(),
"T and R must be the same shape");
Preconditions.checkArgument(n == Qt.getRowDimension() && n == Qt.getColumnDimension(),
"T and Qt must be the same shape");
R.setSubMatrix(T.getData(), 0, 0);
Qt.setSubMatrix(eye(n), 0, 0);
x = unitL2norm(x);
R.setSubMatrix(subR.subtract(x.outerProduct(subR.preMultiply(x)).scalarMultiply(2))
.getData(), i, 0);
Qt.setSubMatrix(subQt.subtract(x.outerProduct(subQt.preMultiply(x)).scalarMultiply(2))
.getData(), i, 0);
}
}
@Nonnull
static RealVector unitL2norm(@Nonnull final RealVector x) {
double x0 = x.getEntry(0);
double sign = MathUtils.sign(x0);
return x.unitVector();
}
public static void tridiagonalEigen(@Nonnull final RealMatrix T, @Nonnull final int nIter,
@Nonnull final double[] eigvals, @Nonnull final RealMatrix eigvecs) {
Preconditions.checkArgument(Arrays.deepEquals(T.getData(), T.transpose().getData()),
"Target matrix T must be a symmetric (tridiagonal) matrix");
Preconditions.checkArgument(eigvecs.getRowDimension() == eigvecs.getColumnDimension(),
"eigvecs must be a square matrix");
Preconditions.checkArgument(T.getRowDimension() == eigvecs.getRowDimension(),
"T and eigvecs must be the same shape");
Preconditions.checkArgument(eigvals.length == eigvecs.getRowDimension(),
"Number of eigenvalues and eigenvectors must be same");
int nEig = eigvals.length;
eigvecs.setSubMatrix(eye(nEig), 0, 0);
RealMatrix T_ = T.copy();
RealMatrix R = new Array2DRowRealMatrix(nEig, nEig);
RealMatrix Qt = new Array2DRowRealMatrix(nEig, nEig);
tridiagonalQR(T_, R, Qt);
RealMatrix Q = Qt.transpose();
T_ = R.multiply(Q);
eigvecs.setSubMatrix(eigvecs.multiply(Q).getData(), 0, 0);
}
eigvals[i] = T_.getEntry(i, i);
}
}
